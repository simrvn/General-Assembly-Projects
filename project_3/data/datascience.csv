approved_at_utc,subreddit,selftext,author_fullname,saved,mod_reason_title,gilded,clicked,title,link_flair_richtext,subreddit_name_prefixed,hidden,pwls,link_flair_css_class,downs,thumbnail_height,top_awarded_type,hide_score,name,quarantine,link_flair_text_color,upvote_ratio,author_flair_background_color,subreddit_type,ups,total_awards_received,media_embed,thumbnail_width,author_flair_template_id,is_original_content,user_reports,secure_media,is_reddit_media_domain,is_meta,category,secure_media_embed,link_flair_text,can_mod_post,score,approved_by,is_created_from_ads_ui,author_premium,thumbnail,edited,author_flair_css_class,author_flair_richtext,gildings,content_categories,is_self,mod_note,created,link_flair_type,wls,removed_by_category,banned_by,author_flair_type,domain,allow_live_comments,selftext_html,likes,suggested_sort,banned_at_utc,view_count,archived,no_follow,is_crosspostable,pinned,over_18,all_awardings,awarders,media_only,can_gild,spoiler,locked,author_flair_text,treatment_tags,visited,removed_by,num_reports,distinguished,subreddit_id,mod_reason_by,removal_reason,link_flair_background_color,id,is_robot_indexable,report_reasons,author,discussion_type,num_comments,send_replies,whitelist_status,contest_mode,mod_reports,author_patreon_flair,author_flair_text_color,permalink,parent_whitelist_status,stickied,url,subreddit_subscribers,created_utc,num_crossposts,media,is_video,link_flair_template_id,post_hint,preview,author_cakeday,crosspost_parent_list,url_overridden_by_dest,crosspost_parent,poll_data
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 13 Jun 2021 - 20 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_nyussm,False,dark,0.87,,public,16,0,{},,,False,[],,False,False,,{},Discussion,False,16,,False,False,self,False,,[],{},,True,,1623614432.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,nyussm,True,,datascience-bot,,108,False,all_ads,False,[],False,dark,/r/datascience/comments/nyussm/weekly_entering_transitioning_thread_13_jun_2021/,all_ads,True,https://www.reddit.com/r/datascience/comments/nyussm/weekly_entering_transitioning_thread_13_jun_2021/,515405,1623585632.0,0,,False,,,,,,,,
,datascience,"Hi Everyone, I am an industry data scientist. One of the problems that I find is that while working at a large company,  there is some adoption lag with some new tools + libraries. Could anyone help point me in the right direction for software tools + libraries that are picking up steam this year? I remember hearing stuff about the Julia Programming language a couple of years ago but not sure if that has risen in popularity",t2_89mmv5s6,False,,0,False,What are some exciting new tools/libraries in 2021?,[],r/datascience,False,6,tooling,0,,,False,t3_o3l9o0,False,dark,0.97,,public,196,0,{},,,False,[],,False,False,,{},Tooling,False,196,,False,False,self,False,,[],{},,True,,1624152781.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Everyone, I am an industry data scientist. One of the problems that I find is that while working at a large company,  there is some adoption lag with some new tools + libraries. Could anyone help point me in the right direction for software tools + libraries that are picking up steam this year? I remember hearing stuff about the Julia Programming language a couple of years ago but not sure if that has risen in popularity&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3l9o0,True,,nyc_brand,,49,True,all_ads,False,[],False,,/r/datascience/comments/o3l9o0/what_are_some_exciting_new_toolslibraries_in_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3l9o0/what_are_some_exciting_new_toolslibraries_in_2021/,515405,1624123981.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_2mezzo8f,False,,0,False,"Python or R, which programming language is better for data science and which has better scope in the future?",[],r/datascience,False,6,discussion,0,,,False,t3_o3u1dn,False,dark,0.66,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,False,,[],{},,True,,1624179303.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3u1dn,True,,iam_syndrome,,22,True,all_ads,False,[],False,,/r/datascience/comments/o3u1dn/python_or_r_which_programming_language_is_better/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3u1dn/python_or_r_which_programming_language_is_better/,515405,1624150503.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am a beginner and have learned Python, Pandas, Excel and the Basics of Hypothesis Testing and wanted to know what should I learn and what certifications I should prepare for If I want to get a Career in Business Analytics or Data Analytics so please can anyone tell me what should I do and also what type of projects I should do?",t2_cqf2yo7g,False,,0,False,"What tools, language and technologies to learn as a beginner in the field of data analytics?",[],r/datascience,False,6,education,0,,,False,t3_o3zdd3,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624199789.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a beginner and have learned Python, Pandas, Excel and the Basics of Hypothesis Testing and wanted to know what should I learn and what certifications I should prepare for If I want to get a Career in Business Analytics or Data Analytics so please can anyone tell me what should I do and also what type of projects I should do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3zdd3,True,,Emperor-SuN-5,,6,True,all_ads,False,[],False,,/r/datascience/comments/o3zdd3/what_tools_language_and_technologies_to_learn_as/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3zdd3/what_tools_language_and_technologies_to_learn_as/,515405,1624170989.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I read a lot of posts about how to land a data science position and what the biggest differences are between doing research in academia and working as a Data Scientist. Since I did the same journey myself, a couple of years ago, I thought it might be helpful if I summarised my experience and my thoughts about how to successfully do the transfer. My first post on the subject is here: [https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e](https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e)

&amp;#x200B;

Let me know what you think! Thanks!",t2_1tvky02i,False,,0,False,Some tips on how to go from academia to a Data Science position,[],r/datascience,False,6,career,0,,,False,t3_o3oppz,False,dark,0.78,,public,10,0,{},,,False,[],,False,False,,{},Career,False,10,,False,False,self,False,,[],{},,True,,1624162144.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read a lot of posts about how to land a data science position and what the biggest differences are between doing research in academia and working as a Data Scientist. Since I did the same journey myself, a couple of years ago, I thought it might be helpful if I summarised my experience and my thoughts about how to successfully do the transfer. My first post on the subject is here: &lt;a href=""https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e""&gt;https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Let me know what you think! Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3oppz,True,,_StunningStar_,,7,True,all_ads,False,[],False,,/r/datascience/comments/o3oppz/some_tips_on_how_to_go_from_academia_to_a_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3oppz/some_tips_on_how_to_go_from_academia_to_a_data/,515405,1624133344.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?auto=webp&amp;s=3ebe93d24dd00b154937c50e6cf0a5fceddb0b19', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff942eacf52a64eac70a062af72c51bc80bb732a', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7185268e63073e097e152401b14eee82bded5b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b02300caae4026c79be942f7ece883a973823af', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a436a18d2eb5a55a1e2908a28c94b5eabf2dd95', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a42e8c3ea7a81551be5f4148e509b5ad7e27633f', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=63434c9348ecc0216b810f017f77c2bbf973a450', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'nztKV3duavl5jt7iucQG6Q7vBNiwBwK-q1RBQde4U3E'}], 'enabled': False}",,,,,
,datascience,"Github link (includes a link to a Kaggle notebook to run it directly) -  [shreyansh26/ML-Optimizers-JAX](https://github.com/shreyansh26/ML-Optimizers-JAX)

Implementations of some popular optimizers from scratch for a simple model like Linear Regression. The goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the weights and the bias without explicitly writing their derivatives as a separate function. 

This can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail.",t2_5xzd9om,False,,0,False,ML Optimizers from scratch using JAX,[],r/datascience,False,6,projects,0,,,True,t3_o40pui,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,1624177108.0,,[],{},,True,,1624205580.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Github link (includes a link to a Kaggle notebook to run it directly) -  &lt;a href=""https://github.com/shreyansh26/ML-Optimizers-JAX""&gt;shreyansh26/ML-Optimizers-JAX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Implementations of some popular optimizers from scratch for a simple model like Linear Regression. The goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the weights and the bias without explicitly writing their derivatives as a separate function. &lt;/p&gt;

&lt;p&gt;This can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o40pui,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/datascience/comments/o40pui/ml_optimizers_from_scratch_using_jax/,all_ads,False,https://www.reddit.com/r/datascience/comments/o40pui/ml_optimizers_from_scratch_using_jax/,515405,1624176780.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?auto=webp&amp;s=c2c00ee3482e5bc793d82475409a54c07a9fb967', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb2880799dffc0a9f5cbceca5c08c501f0c14c0c', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f2b430ab9d9ff7108e05ff1d2b6975a1082147', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbe0616a95721cfa473210b7a8eca2955cb23ef9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47795bc9e153b29fd6828f6e70fee6e2b64a4ebb', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f9285692be2a3c7ee8cb6e6098f1b87b5643a4', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28021321b3765655bc5a62440e664221eb0d50d2', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Kz19YO1AEVVgU7yWFoST8FAepTIsUeAUmioW0RN4T94'}], 'enabled': False}",,,,,
,datascience,"Hi,

I have posted this on other subreddits last week but I didn't get a response so I thought I'd give this one a try. 

I work as a data analyst at a theme park (kind of like disney) but we operate only during 4-6 months in an year. I was asked to analyze the F&amp;B sales at the different locations in the park and compare this year's sales with last year's sales.

Couple of constraints and issues:

1. The number of outlets in an area keep changing every year. Last year we might have had 10 outlets in Area A but this year we only have 3. - To overcome this, I divided the sales by the number of outlets so I will be comparing the revenue per outlet for different years.
2. Different outlets operate for different days. For example, this year, outlet A might have operated only for 30 days but outlet B would have operated for 90 days. So outlet B's revenue would be higher. - To overcome this, I divided the revenue per outlet by the number of days the park operated this year. So let's say the park operated for a total of 100 days then I will divide the revenue per outlet/100 to get the revenue per outlet per day. Someone from the management pointed out that it would be better to divide the revenue per outlet by the number of days each outlet is operating to get the exact figures. That approach makes sense but it seems a bit more time consuming. Would it be worth redoing the analysis to change this?
3. Since we operate on a season basis, we operate for different days every season. For example, last time we operated from Oct'19 - Mar'20 but this time we operated from Oct'20-May'21. I thought that displaying the revenue per outlet per day would overcome this but someone from management mentioned that they would rather have me forecast the expected revenue from Apr'20 - May'20  so that we are comparing Oct'19-May'20 vs Oct'20-May'21. And then I can use this new forecasted revenue to get the revenue per outlet per day.
4. Because of covid, we had lesser footfall and hence lesser revenue this time. Someone suggested to normalize this year's sales by taking the footfall into account - i.e. take the drop in footfall from last season to this season (let's say the drop is 30%) then take 70% of last year's sales to accommodate the drop in footfall. Is this the right approach to it? Wouldn't comparing the revenue per outlet per day take care of the drop in footfall?
5. All of this needs to be in a dashboard and they want a functionality where they can make a like to like comparison - We will be reopening in Oct'21 and at that moment they want to compare their performance with previous season (i.e Oct '20). There are 2 problems with this:
   1. In Oct'20, we started on a monday (weekday) whereas in Oct'21 we might start on a saturday(weekend). Our weekend performance is higher than weekday so comparing monday vs saturday wouldn't be correct.
   2. As mentioned earlier we operate for different # of days across different season. For eg, this time we operated for 191 days whereas last time we operated for 161 days. Even if I compare  the 1st sunday of last time with 1st sunday of this time and so on, I will not have anything to compare with after we finish 161 days this season. How do I accurately make this comparison?

Sorry for the long post but I have not come across such constraints before so I'm not sure how to deal with all of it. I would really appreciate any help! Thanks in advance!",t2_5wjm49wo,False,,0,False,What approach do I use to analyze this F&amp;B Sales data?,[],r/datascience,False,6,discussion,0,,,True,t3_o40jxj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624205010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have posted this on other subreddits last week but I didn&amp;#39;t get a response so I thought I&amp;#39;d give this one a try. &lt;/p&gt;

&lt;p&gt;I work as a data analyst at a theme park (kind of like disney) but we operate only during 4-6 months in an year. I was asked to analyze the F&amp;amp;B sales at the different locations in the park and compare this year&amp;#39;s sales with last year&amp;#39;s sales.&lt;/p&gt;

&lt;p&gt;Couple of constraints and issues:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The number of outlets in an area keep changing every year. Last year we might have had 10 outlets in Area A but this year we only have 3. - To overcome this, I divided the sales by the number of outlets so I will be comparing the revenue per outlet for different years.&lt;/li&gt;
&lt;li&gt;Different outlets operate for different days. For example, this year, outlet A might have operated only for 30 days but outlet B would have operated for 90 days. So outlet B&amp;#39;s revenue would be higher. - To overcome this, I divided the revenue per outlet by the number of days the park operated this year. So let&amp;#39;s say the park operated for a total of 100 days then I will divide the revenue per outlet/100 to get the revenue per outlet per day. Someone from the management pointed out that it would be better to divide the revenue per outlet by the number of days each outlet is operating to get the exact figures. That approach makes sense but it seems a bit more time consuming. Would it be worth redoing the analysis to change this?&lt;/li&gt;
&lt;li&gt;Since we operate on a season basis, we operate for different days every season. For example, last time we operated from Oct&amp;#39;19 - Mar&amp;#39;20 but this time we operated from Oct&amp;#39;20-May&amp;#39;21. I thought that displaying the revenue per outlet per day would overcome this but someone from management mentioned that they would rather have me forecast the expected revenue from Apr&amp;#39;20 - May&amp;#39;20  so that we are comparing Oct&amp;#39;19-May&amp;#39;20 vs Oct&amp;#39;20-May&amp;#39;21. And then I can use this new forecasted revenue to get the revenue per outlet per day.&lt;/li&gt;
&lt;li&gt;Because of covid, we had lesser footfall and hence lesser revenue this time. Someone suggested to normalize this year&amp;#39;s sales by taking the footfall into account - i.e. take the drop in footfall from last season to this season (let&amp;#39;s say the drop is 30%) then take 70% of last year&amp;#39;s sales to accommodate the drop in footfall. Is this the right approach to it? Wouldn&amp;#39;t comparing the revenue per outlet per day take care of the drop in footfall?&lt;/li&gt;
&lt;li&gt;All of this needs to be in a dashboard and they want a functionality where they can make a like to like comparison - We will be reopening in Oct&amp;#39;21 and at that moment they want to compare their performance with previous season (i.e Oct &amp;#39;20). There are 2 problems with this:

&lt;ol&gt;
&lt;li&gt;In Oct&amp;#39;20, we started on a monday (weekday) whereas in Oct&amp;#39;21 we might start on a saturday(weekend). Our weekend performance is higher than weekday so comparing monday vs saturday wouldn&amp;#39;t be correct.&lt;/li&gt;
&lt;li&gt;As mentioned earlier we operate for different # of days across different season. For eg, this time we operated for 191 days whereas last time we operated for 161 days. Even if I compare  the 1st sunday of last time with 1st sunday of this time and so on, I will not have anything to compare with after we finish 161 days this season. How do I accurately make this comparison?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sorry for the long post but I have not come across such constraints before so I&amp;#39;m not sure how to deal with all of it. I would really appreciate any help! Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o40jxj,True,,Professional_Crazy49,,0,True,all_ads,False,[],False,,/r/datascience/comments/o40jxj/what_approach_do_i_use_to_analyze_this_fb_sales/,all_ads,False,https://www.reddit.com/r/datascience/comments/o40jxj/what_approach_do_i_use_to_analyze_this_fb_sales/,515405,1624176210.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have a dataset containing the monthly number of shipments for the years 2014 to 2019. I used the Facebook Prophet algorithm to fit a basic model. However, I am not sure how to implement cross-validation on this data. I could not find any blogs/GitHub files that perform cross-validation on monthly data. Is there a workaround for this?

Thanks in advance",t2_55fnfe2p,False,,0,False,Crossvalidation using Facebook Prophet,[],r/datascience,False,6,projects,0,,,False,t3_o3sjcm,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,self,False,,[],{},,True,,1624174117.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have a dataset containing the monthly number of shipments for the years 2014 to 2019. I used the Facebook Prophet algorithm to fit a basic model. However, I am not sure how to implement cross-validation on this data. I could not find any blogs/GitHub files that perform cross-validation on monthly data. Is there a workaround for this?&lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3sjcm,True,,xnxrxdh,,4,True,all_ads,False,[],False,,/r/datascience/comments/o3sjcm/crossvalidation_using_facebook_prophet/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3sjcm/crossvalidation_using_facebook_prophet/,515405,1624145317.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hey guys. Hope you are doing good today.

I am a python developer and I have very little experience in ML and Big data. I am also a professional stock trader.

So I want some technical advice or an idea  from the redditors here.

I wanna a build a full stack data science project something related to the stock market.

When I say a full stack I mean right from the start from getting data or extracting data and going through all the big data pipelines and building a predictive model and deploying it.

I mean I have some ideas but I am not able to technically produce it in a way like you guys could do. I am asking you because with experience you know something compared to me

I want answers like 

""Scrap financial data from a website, live stream process it using spark and setup in airflow and then take the data do a reinforcement learning algorithm and deploy it showing something in the frontend""

Forgive me if I am demanding too much,but please do your best. It would be helpful for others too",t2_6hyleuj8,False,,0,False,Full Data engineering/ Data science Pipeline on stock market idea,[],r/datascience,False,6,projects,0,,,False,t3_o3z9j9,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1624199253.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Hope you are doing good today.&lt;/p&gt;

&lt;p&gt;I am a python developer and I have very little experience in ML and Big data. I am also a professional stock trader.&lt;/p&gt;

&lt;p&gt;So I want some technical advice or an idea  from the redditors here.&lt;/p&gt;

&lt;p&gt;I wanna a build a full stack data science project something related to the stock market.&lt;/p&gt;

&lt;p&gt;When I say a full stack I mean right from the start from getting data or extracting data and going through all the big data pipelines and building a predictive model and deploying it.&lt;/p&gt;

&lt;p&gt;I mean I have some ideas but I am not able to technically produce it in a way like you guys could do. I am asking you because with experience you know something compared to me&lt;/p&gt;

&lt;p&gt;I want answers like &lt;/p&gt;

&lt;p&gt;&amp;quot;Scrap financial data from a website, live stream process it using spark and setup in airflow and then take the data do a reinforcement learning algorithm and deploy it showing something in the frontend&amp;quot;&lt;/p&gt;

&lt;p&gt;Forgive me if I am demanding too much,but please do your best. It would be helpful for others too&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3z9j9,True,,Shyprogrammer1,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3z9j9/full_data_engineering_data_science_pipeline_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3z9j9/full_data_engineering_data_science_pipeline_on/,515405,1624170453.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m considering a switch to data engineering as I really enjoy the engineering side of things I’ve had to do as a Data Scientist. 

As a Data Scientist what were you doing before? How do you like it? Any regrets?",t2_4z85fo82,False,,0,False,"Data Scientists who switched to Data Engineer, how is it going?",[],r/datascience,False,6,career,0,,,False,t3_o33nut,False,dark,0.96,,public,196,3,{},,,False,[],,False,False,,{},Career,False,196,,False,False,self,False,,[],{'gid_1': 2},,True,,1624089487.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m considering a switch to data engineering as I really enjoy the engineering side of things I’ve had to do as a Data Scientist. &lt;/p&gt;

&lt;p&gt;As a Data Scientist what were you doing before? How do you like it? Any regrets?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o33nut,True,,Dismal-Variation-12,,50,True,all_ads,False,[],False,,/r/datascience/comments/o33nut/data_scientists_who_switched_to_data_engineer_how/,all_ads,False,https://www.reddit.com/r/datascience/comments/o33nut/data_scientists_who_switched_to_data_engineer_how/,515405,1624060687.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all,

I was just wondering, is there a good place to read some good quality actual data science projects end to end?

Ideally, I think it would be good to just have a place with a good set of Jupyter notebooks of varying length and complexity that I could read to get a better understanding of the whole end to end process of a 'typical' data science project.

I understand that that code can be found on Kaggle and GitHub, but it feels like it is kind of hard work to be able to find them all organised in one place. Its hard to discern the good from the amateur, and a lot of the time its just small little 'tricks' and tutorials, with not much descriptive text of what the problem is and what they are trying to carry out. I'm kind of looking for some 'gold standard' data science projects I can read to get a better understanding of what is required and what to strive for.

Maybe I am just doing it wrong, but does any one have any suggestions?",t2_3z5dxa7s,False,,0,False,Where to find good Quality DS project examples?,[],r/datascience,False,6,discussion,0,,,False,t3_o3brf2,False,dark,0.94,,public,25,0,{},,,False,[],,False,False,,{},Discussion,False,25,,False,False,self,False,,[],{},,True,,1624121755.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I was just wondering, is there a good place to read some good quality actual data science projects end to end?&lt;/p&gt;

&lt;p&gt;Ideally, I think it would be good to just have a place with a good set of Jupyter notebooks of varying length and complexity that I could read to get a better understanding of the whole end to end process of a &amp;#39;typical&amp;#39; data science project.&lt;/p&gt;

&lt;p&gt;I understand that that code can be found on Kaggle and GitHub, but it feels like it is kind of hard work to be able to find them all organised in one place. Its hard to discern the good from the amateur, and a lot of the time its just small little &amp;#39;tricks&amp;#39; and tutorials, with not much descriptive text of what the problem is and what they are trying to carry out. I&amp;#39;m kind of looking for some &amp;#39;gold standard&amp;#39; data science projects I can read to get a better understanding of what is required and what to strive for.&lt;/p&gt;

&lt;p&gt;Maybe I am just doing it wrong, but does any one have any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3brf2,True,,beneath_the_knees,,11,True,all_ads,False,[],False,,/r/datascience/comments/o3brf2/where_to_find_good_quality_ds_project_examples/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3brf2/where_to_find_good_quality_ds_project_examples/,515405,1624092955.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,True,,,,
,datascience,"I know that GDBT model cannot be parallelized during training because the trees are built sequentially. What about its scoring / making prediction phase for one single sample? 

As far as I understand, we can evaluate the outputs of all trees at once (hence parallelizable), and the final prediction is just a linear combination. Is that correct?",t2_9aqvgklw,False,,0,False,Can the prediction of gradient boosted decision trees be parallelized?,[],r/datascience,False,6,discussion,0,,,False,t3_o3rbc2,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1624170153.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that GDBT model cannot be parallelized during training because the trees are built sequentially. What about its scoring / making prediction phase for one single sample? &lt;/p&gt;

&lt;p&gt;As far as I understand, we can evaluate the outputs of all trees at once (hence parallelizable), and the final prediction is just a linear combination. Is that correct?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3rbc2,True,,populus27,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3rbc2/can_the_prediction_of_gradient_boosted_decision/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3rbc2/can_the_prediction_of_gradient_boosted_decision/,515405,1624141353.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"After much thought, I have decided that getting a masters in data science is the correct career choice for me. I begin my studies this fall, and my first class is python for data analysis. 

Does anyone have advice they’d like to share? I know the basics of Python and I took an online udemy course for Python with data analysis back in 2019. 

My goal is to maximize what I learn in this course because I’d really like to increase my use of Python (for data science and just in general)",t2_8wi0rk4h,False,,0,False,"Starting my masters in DS this fall, any advice on my first class?",[],r/datascience,False,6,education,0,,,False,t3_o3ybbf,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1624195222.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After much thought, I have decided that getting a masters in data science is the correct career choice for me. I begin my studies this fall, and my first class is python for data analysis. &lt;/p&gt;

&lt;p&gt;Does anyone have advice they’d like to share? I know the basics of Python and I took an online udemy course for Python with data analysis back in 2019. &lt;/p&gt;

&lt;p&gt;My goal is to maximize what I learn in this course because I’d really like to increase my use of Python (for data science and just in general)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ybbf,True,,pizzagarrett,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3ybbf/starting_my_masters_in_ds_this_fall_any_advice_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ybbf/starting_my_masters_in_ds_this_fall_any_advice_on/,515405,1624166422.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Within Australia, large companies have something called grad programs. These grads rotate through the business 3 or 4 times in 6 months stints. These grads have to apply to an area of the business to work.
I need to write a pitch to get these new grads to apply to my area of the business.

Question is, if you are new to analytics/data science, why did you want to get into it? What did you hear about it for you to think yeah that’s what I want to do?",t2_a8wit5hx,False,,0,False,Sales pitch for DS,[],r/datascience,False,6,discussion,0,,,False,t3_o3u9v8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624180117.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Within Australia, large companies have something called grad programs. These grads rotate through the business 3 or 4 times in 6 months stints. These grads have to apply to an area of the business to work.
I need to write a pitch to get these new grads to apply to my area of the business.&lt;/p&gt;

&lt;p&gt;Question is, if you are new to analytics/data science, why did you want to get into it? What did you hear about it for you to think yeah that’s what I want to do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3u9v8,True,,EmergencyContact2016,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3u9v8/sales_pitch_for_ds/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3u9v8/sales_pitch_for_ds/,515405,1624151317.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My list:

1. Documentation
2. The summary of code logic as flowchart/diagram format
3. Code convention
4. Grab some general significant number to check if each step output is making sense
5. Prepare some common preproc function for the task with similar procedures.
6. Put the date/time as prefix for each simulation testing",t2_5czjyjhi,False,,0,False,What are your top things that are usually being overlooked but super important and can save lots of time and money,[],r/datascience,False,6,discussion,0,,,False,t3_o2x2me,False,dark,1.0,,public,200,0,{},,,False,[],,False,False,,{},Discussion,False,200,,False,False,self,1624043112.0,,[],{},,True,,1624070457.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My list:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;The summary of code logic as flowchart/diagram format&lt;/li&gt;
&lt;li&gt;Code convention&lt;/li&gt;
&lt;li&gt;Grab some general significant number to check if each step output is making sense&lt;/li&gt;
&lt;li&gt;Prepare some common preproc function for the task with similar procedures.&lt;/li&gt;
&lt;li&gt;Put the date/time as prefix for each simulation testing&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2x2me,True,,vietlinh12hoa,,37,True,all_ads,False,[],False,,/r/datascience/comments/o2x2me/what_are_your_top_things_that_are_usually_being/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2x2me/what_are_your_top_things_that_are_usually_being/,515405,1624041657.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’m a 2 year decision scientist and I’m on my own. I went from knowing little excel, let alone python or sql, to writing code all the time. But there are still things I don’t know. I was wondering if there was someone who wouldn’t mind being friends on discord or something where I can ask questions when they pop in my head. For the most part, I think Google answers the base line of my questions, but it would be nice to be able to tailor my question based on my role. 

An example is something like: it’s me, my laptop, and the company’s SQL server. Should I get Hadoop? Should I run pyspark for these pesky, longer operations when I don’t have multiple machines to run operations on? How can I grab weather data  for all US zip codes and store it, as it’s over 25-50 GB of data? 

Being the sole guy for these kinds of developments is challenging. But anything to make my life easier would be a blessing.",t2_9l80yrty,False,,0,False,Mentorship or a discord to ask questions?,[],r/datascience,False,6,career,0,,,False,t3_o3o5lm,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1624160502.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a 2 year decision scientist and I’m on my own. I went from knowing little excel, let alone python or sql, to writing code all the time. But there are still things I don’t know. I was wondering if there was someone who wouldn’t mind being friends on discord or something where I can ask questions when they pop in my head. For the most part, I think Google answers the base line of my questions, but it would be nice to be able to tailor my question based on my role. &lt;/p&gt;

&lt;p&gt;An example is something like: it’s me, my laptop, and the company’s SQL server. Should I get Hadoop? Should I run pyspark for these pesky, longer operations when I don’t have multiple machines to run operations on? How can I grab weather data  for all US zip codes and store it, as it’s over 25-50 GB of data? &lt;/p&gt;

&lt;p&gt;Being the sole guy for these kinds of developments is challenging. But anything to make my life easier would be a blessing.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3o5lm,True,,cruelbankai,,3,True,all_ads,False,[],False,,/r/datascience/comments/o3o5lm/mentorship_or_a_discord_to_ask_questions/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3o5lm/mentorship_or_a_discord_to_ask_questions/,515405,1624131702.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"(Disclaimer: I'm not affiliated with the project)

[https://github.com/mc2-project](https://github.com/mc2-project/mc2)

MC2 is a project that aims at helping data scientists and engineers collaborate and process collective data without sharing the content of the data with one another, not even the cloud provider. I think the project is very interesting and I'd like about the use cases and what you will do with it.",t2_wpnyl,False,,0,False,MC2: A Platform for Secure Analytics and Machine Learning,[],r/datascience,False,6,projects,0,,,False,t3_o3ofbu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1624161318.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(Disclaimer: I&amp;#39;m not affiliated with the project)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/mc2-project/mc2""&gt;https://github.com/mc2-project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MC2 is a project that aims at helping data scientists and engineers collaborate and process collective data without sharing the content of the data with one another, not even the cloud provider. I think the project is very interesting and I&amp;#39;d like about the use cases and what you will do with it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ofbu,True,,jchasinga,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3ofbu/mc2_a_platform_for_secure_analytics_and_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ofbu/mc2_a_platform_for_secure_analytics_and_machine/,515405,1624132518.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?auto=webp&amp;s=293d3b0321b4403bf0a59d6719a0fb2088904dbf', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=daa811d09a0acefb7b703773f3a1be4381f0c32e', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=686e1183b1f5a22ae2b77efb7d8084768d08b5fb', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1fac4b7b236fbdb639a1cf852a17a38f9d99d68', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b005e3d8f7511b9ca6f7cea7ee5b5e0588acb8d', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=31a3c73d523da7224f4974de990cf6b518c2cd8b', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09c413698eb66f9f4e95c9abec0bfc08e0d6b610', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Hq9BWX3G702TK0LNyn1QLKGeYG4MP29m4Km9Hj6gIsc'}], 'enabled': False}",,,,,
,datascience,"My company hired a junior BI analyst a couple of months back to handle the backlog of dashboarding/analysis demands I was not able to juggle in a timely manner due to more pressing data science/engineering concerns (I was the sole data person in the team at the time).

She's done a great job, but now that most of the overdue demands she was given are done, she has expressed interest in gradually transitioning to a more broad data role within the company. Most of her previous job experience is in a strict BI role (dashboard building, reporting, etc) but she has done a few python courses on her own.

Under my supervision, she has begun working on a few basic tasks like data cleaning, pipeline building, etc. I can tell she's interested, but her progress has been remarkably slow and underwhelming.   
People who have more experience with mentoring juniors, what are some things I can do to be a better mentor and help her develop in the field? I try to give detailed feedback in each of her deliveries but as I said progress is slow so far.  


\-------  
EDIT: I didn't expect this many good responses, thank you so much! Starting next week I'll try to come up with a more active plan of mentoring based on the input you guys gave. Here's hoping this post can help more people who are or might eventually be in a similar situation.  
",t2_8mqzapd7,False,,0,False,Tips on how to properly mentor and train junior data people?,[],r/datascience,False,6,career,0,,,False,t3_o2q05f,False,dark,0.97,,public,159,1,{},,,False,[],,False,False,,{},Career,False,159,,False,False,self,1624061082.0,,[],{'gid_1': 1},,True,,1624054295.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My company hired a junior BI analyst a couple of months back to handle the backlog of dashboarding/analysis demands I was not able to juggle in a timely manner due to more pressing data science/engineering concerns (I was the sole data person in the team at the time).&lt;/p&gt;

&lt;p&gt;She&amp;#39;s done a great job, but now that most of the overdue demands she was given are done, she has expressed interest in gradually transitioning to a more broad data role within the company. Most of her previous job experience is in a strict BI role (dashboard building, reporting, etc) but she has done a few python courses on her own.&lt;/p&gt;

&lt;p&gt;Under my supervision, she has begun working on a few basic tasks like data cleaning, pipeline building, etc. I can tell she&amp;#39;s interested, but her progress has been remarkably slow and underwhelming.&lt;br/&gt;
People who have more experience with mentoring juniors, what are some things I can do to be a better mentor and help her develop in the field? I try to give detailed feedback in each of her deliveries but as I said progress is slow so far.  &lt;/p&gt;

&lt;p&gt;-------&lt;br/&gt;
EDIT: I didn&amp;#39;t expect this many good responses, thank you so much! Starting next week I&amp;#39;ll try to come up with a more active plan of mentoring based on the input you guys gave. Here&amp;#39;s hoping this post can help more people who are or might eventually be in a similar situation.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2q05f,True,,false-shrimp,,21,True,all_ads,False,[],False,,/r/datascience/comments/o2q05f/tips_on_how_to_properly_mentor_and_train_junior/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2q05f/tips_on_how_to_properly_mentor_and_train_junior/,515405,1624025495.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi, I am a CS major. I graduated in 2019 and have been working in a company since then.

When I joined, they had just started an AI team. In fact, my manager was the first data science hire and I was the second one. We have been working on creating an MVP, and don't have any customers for it yet.

My manager is more inclined towards the business side of data science (so he's good in storytelling in demos, working with Sales and Product Management for the requirements etc.). We don't have any expert for guidance on ML product pipelines, algorithm improvement etc. so anything I have implemented so far has been by taking help from the internet.

As a college fresher, I got this job through a Python interview. I even left an offer of 2.5x pay in software development because I wanted to pursue data science.

I haven't interviewed for any other company since then. I don't really know what is usually involved in data science jobs. Have I gained any useful data science experience in order to get the next job?

My work involves:

\- setting up internal products on demo servers and preparing a story through dummy data/insights for customer demos or security conferences (40%)

\- designing database tables and creating reports and dashboards for the product (initially it was in PowerBI, then Jasper Reports and now in an internal product framework) (25%)

\- developing and maintaining Python jobs and integrating model training/detection with the main Java product through Flask (20%)

\- exploring, drawing analogies and implementing unsupervised ML/DL algorithms that can solve the use cases provided by Product Management. (15%)

 Is this what is usually involved in a real data science job? 

Also, I read that you should add in your resume how your work helped in making business value. But since we don't have any customers, what should I add?",t2_47gxdu5o,False,,0,False,Is this a data science job for real?,[],r/datascience,False,6,career,0,,,False,t3_o2wsh8,False,dark,0.91,,public,38,0,{},,,False,[],,False,False,,{},Career,False,38,,False,False,self,False,,[],{},,True,,1624069732.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am a CS major. I graduated in 2019 and have been working in a company since then.&lt;/p&gt;

&lt;p&gt;When I joined, they had just started an AI team. In fact, my manager was the first data science hire and I was the second one. We have been working on creating an MVP, and don&amp;#39;t have any customers for it yet.&lt;/p&gt;

&lt;p&gt;My manager is more inclined towards the business side of data science (so he&amp;#39;s good in storytelling in demos, working with Sales and Product Management for the requirements etc.). We don&amp;#39;t have any expert for guidance on ML product pipelines, algorithm improvement etc. so anything I have implemented so far has been by taking help from the internet.&lt;/p&gt;

&lt;p&gt;As a college fresher, I got this job through a Python interview. I even left an offer of 2.5x pay in software development because I wanted to pursue data science.&lt;/p&gt;

&lt;p&gt;I haven&amp;#39;t interviewed for any other company since then. I don&amp;#39;t really know what is usually involved in data science jobs. Have I gained any useful data science experience in order to get the next job?&lt;/p&gt;

&lt;p&gt;My work involves:&lt;/p&gt;

&lt;p&gt;- setting up internal products on demo servers and preparing a story through dummy data/insights for customer demos or security conferences (40%)&lt;/p&gt;

&lt;p&gt;- designing database tables and creating reports and dashboards for the product (initially it was in PowerBI, then Jasper Reports and now in an internal product framework) (25%)&lt;/p&gt;

&lt;p&gt;- developing and maintaining Python jobs and integrating model training/detection with the main Java product through Flask (20%)&lt;/p&gt;

&lt;p&gt;- exploring, drawing analogies and implementing unsupervised ML/DL algorithms that can solve the use cases provided by Product Management. (15%)&lt;/p&gt;

&lt;p&gt;Is this what is usually involved in a real data science job? &lt;/p&gt;

&lt;p&gt;Also, I read that you should add in your resume how your work helped in making business value. But since we don&amp;#39;t have any customers, what should I add?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2wsh8,True,,divyagupta25,,27,True,all_ads,False,[],False,,/r/datascience/comments/o2wsh8/is_this_a_data_science_job_for_real/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2wsh8/is_this_a_data_science_job_for_real/,515405,1624040932.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I took a data science with Python course on Udemy that was created in 2017. I’m wondering, how much do pandas and other classic DS libraries change (matplotlib, scikitlearn, seaborn etc.)? Should I take a refresher course?",t2_8wi0rk4h,False,,0,False,Do pandas and other Python data science libraries change a lot?,[],r/datascience,False,6,discussion,0,,,False,t3_o360o3,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1624098022.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I took a data science with Python course on Udemy that was created in 2017. I’m wondering, how much do pandas and other classic DS libraries change (matplotlib, scikitlearn, seaborn etc.)? Should I take a refresher course?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o360o3,True,,pizzagarrett,,7,True,all_ads,False,[],False,,/r/datascience/comments/o360o3/do_pandas_and_other_python_data_science_libraries/,all_ads,False,https://www.reddit.com/r/datascience/comments/o360o3/do_pandas_and_other_python_data_science_libraries/,515405,1624069222.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Not necessarily data engineering (ETL/ELT) work, but architecture and development work inside of the database/DWH.",t2_3uoce3bn,False,,0,False,How often do you find yourself performing analytics engineering duties?,[],r/datascience,False,6,discussion,0,,,False,t3_o3g1q0,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624137912.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Not necessarily data engineering (ETL/ELT) work, but architecture and development work inside of the database/DWH.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3g1q0,True,,Tender_Figs,,2,True,all_ads,False,[],False,,/r/datascience/comments/o3g1q0/how_often_do_you_find_yourself_performing/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3g1q0/how_often_do_you_find_yourself_performing/,515405,1624109112.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have 0 SWE background and my coding skills include SQL, R and Python (pandas, numpy etc). My work is on the analytics side so I rarely think in terms of OOP, and I dont do a lot of functions.

Even looking at the most basic leetcode problems, I get stuck with even where to start.  Terms binary trees, hashes, sorted lists etc is very foreign to me.

I want to start doing leetcode, but is there any other resources I should start with first just to kind of get the basics of the basics down?  The idea is that if I see a leetcode problem, I want to have some sort of reference of where to start.",t2_i6go6an,False,,0,False,‘Primer’ for leetcode?,[],r/datascience,False,6,education,0,,,False,t3_o31fkc,False,dark,0.94,,public,15,0,{},,,False,[],,False,False,,{},Education,False,15,,False,False,self,False,,[],{},,True,,1624082673.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 0 SWE background and my coding skills include SQL, R and Python (pandas, numpy etc). My work is on the analytics side so I rarely think in terms of OOP, and I dont do a lot of functions.&lt;/p&gt;

&lt;p&gt;Even looking at the most basic leetcode problems, I get stuck with even where to start.  Terms binary trees, hashes, sorted lists etc is very foreign to me.&lt;/p&gt;

&lt;p&gt;I want to start doing leetcode, but is there any other resources I should start with first just to kind of get the basics of the basics down?  The idea is that if I see a leetcode problem, I want to have some sort of reference of where to start.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o31fkc,True,,mrdlau,,18,True,all_ads,False,[],False,,/r/datascience/comments/o31fkc/primer_for_leetcode/,all_ads,False,https://www.reddit.com/r/datascience/comments/o31fkc/primer_for_leetcode/,515405,1624053873.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I work on a ""Data Science"" team that does work that is hybrid of typical DS/DE/DA work.

Some days it's simple SQL and Tableau, other times its building pipelines through real time ingestion with kafka or batch ETLs using Airflow, sometimes it's hard statistical analysis and occasionally we do actual Machine Learning... 

We are all Data Scientists by title but play the hybrid of just being good with data, regardless of specifics coming to DA versus DE versus DS..

Recently I've felt like I've been missing the DS side of things. I get the occasional one off request for building data models but don't ever really deploy anything with them... Usually just local models and then sharing the results.

I am wanting to start putting some models ""in production"" as a way to show my usual stakeholders the value that ML can have for the business and I'm curious in all the ways you guys do this on your team? I specifically am curious about two scenarios...

**Scenario 1: Model only needs to be running when the ETL runs.** 

Most of our work is batch processing data at some interval. I typically utilize airflow with kubernetes pod operators to spin up python containers that have the code I need for the ETL. I figured a super simple way to do this was to just store a trained model in the container itself and have the scripts run the models the same way I would locally whenever the airflow job is triggered. This seems simple, but I'm sure my idea is flawed. I know a major limitation here is that to retrain the model, I need to pull my container back to my local machine, retrain the model locally, rebuild the container and push it back to artifactory so the ETL can now use the updated version. There's also the limitation that other people can't use the model since its only present ""in production"" when airflow triggers the pod to spin up.

**Scenario 2: Model needs to be available and running at all times**

I often utilize kafka triggers deployed in kubernetes pods to listen to topics of interest. While they usually just write data to a db or send notifications to stakeholders about the event that occurred, I see a lot of potential in being able to point data from an event into a classification or regression model in real time and then store the results of the model for later use. To do this, I think my approach would be to build a flask API that is deployed in the same kubernetes cluster that I could point my kafka trigger towards... I have played with flask before, but am not as great with the whole devops side in terms of configuring IPs and making sure my flask API is available for other ""apps"" to use... 

I would love your feedback on how your team deploys models and code, especially if you run into the above two scenarios :)",t2_33en6nux,False,,0,False,How does your team deploy models?,[],r/datascience,False,6,discussion,0,,,False,t3_o2u9dy,False,dark,0.94,,public,34,0,{},,,False,[],,False,False,,{},Discussion,False,34,,False,False,self,False,,[],{},,True,,1624064600.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work on a &amp;quot;Data Science&amp;quot; team that does work that is hybrid of typical DS/DE/DA work.&lt;/p&gt;

&lt;p&gt;Some days it&amp;#39;s simple SQL and Tableau, other times its building pipelines through real time ingestion with kafka or batch ETLs using Airflow, sometimes it&amp;#39;s hard statistical analysis and occasionally we do actual Machine Learning... &lt;/p&gt;

&lt;p&gt;We are all Data Scientists by title but play the hybrid of just being good with data, regardless of specifics coming to DA versus DE versus DS..&lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;ve felt like I&amp;#39;ve been missing the DS side of things. I get the occasional one off request for building data models but don&amp;#39;t ever really deploy anything with them... Usually just local models and then sharing the results.&lt;/p&gt;

&lt;p&gt;I am wanting to start putting some models &amp;quot;in production&amp;quot; as a way to show my usual stakeholders the value that ML can have for the business and I&amp;#39;m curious in all the ways you guys do this on your team? I specifically am curious about two scenarios...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1: Model only needs to be running when the ETL runs.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Most of our work is batch processing data at some interval. I typically utilize airflow with kubernetes pod operators to spin up python containers that have the code I need for the ETL. I figured a super simple way to do this was to just store a trained model in the container itself and have the scripts run the models the same way I would locally whenever the airflow job is triggered. This seems simple, but I&amp;#39;m sure my idea is flawed. I know a major limitation here is that to retrain the model, I need to pull my container back to my local machine, retrain the model locally, rebuild the container and push it back to artifactory so the ETL can now use the updated version. There&amp;#39;s also the limitation that other people can&amp;#39;t use the model since its only present &amp;quot;in production&amp;quot; when airflow triggers the pod to spin up.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2: Model needs to be available and running at all times&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I often utilize kafka triggers deployed in kubernetes pods to listen to topics of interest. While they usually just write data to a db or send notifications to stakeholders about the event that occurred, I see a lot of potential in being able to point data from an event into a classification or regression model in real time and then store the results of the model for later use. To do this, I think my approach would be to build a flask API that is deployed in the same kubernetes cluster that I could point my kafka trigger towards... I have played with flask before, but am not as great with the whole devops side in terms of configuring IPs and making sure my flask API is available for other &amp;quot;apps&amp;quot; to use... &lt;/p&gt;

&lt;p&gt;I would love your feedback on how your team deploys models and code, especially if you run into the above two scenarios :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2u9dy,True,,Beertarian,,15,True,all_ads,False,[],False,,/r/datascience/comments/o2u9dy/how_does_your_team_deploy_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2u9dy/how_does_your_team_deploy_models/,515405,1624035800.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello.

I am an undergraduate student looking for more potential career opportunities.

Are there data scientists here from actuarial background who can share more experience on it?

Thanks in advance.",t2_1rkw6x8n,False,,0,False,Any Actuary transition to data scientists?,[],r/datascience,False,6,career,0,,,False,t3_o395o5,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1624109860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello.&lt;/p&gt;

&lt;p&gt;I am an undergraduate student looking for more potential career opportunities.&lt;/p&gt;

&lt;p&gt;Are there data scientists here from actuarial background who can share more experience on it?&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o395o5,True,,markpreston54,,4,True,all_ads,False,[],False,,/r/datascience/comments/o395o5/any_actuary_transition_to_data_scientists/,all_ads,False,https://www.reddit.com/r/datascience/comments/o395o5/any_actuary_transition_to_data_scientists/,515405,1624081060.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," Hello all,

I have been admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan to start in Fall 2021:

[https://www.si.umich.edu/programs/master-applied-data-science-online](https://www.si.umich.edu/programs/master-applied-data-science-online)

Does anyone have any insights on the program? If I understand correctly, the program is very new and the first cohort of students must be about to graduate now. I would love to read the experience of some people that have taken the program and whether they thought it was worth it as well as any objective opinions in general.

I have found very few first-hand opinions on the MADS at Michigan out there, and the opinions that I have read (mostly here on reddit) were on the negative side. Given that other schools such as Georgia Tech have tons of overwhelmingly positive opinions -both on the quality of the classes as well as the price-, I am scared this is not the right move for me. It is a big investment in terms of time and money after all.

My background:

&amp;#x200B;

* Bachelor in Economics (10 years ago)
* 5+ years of Data Analytics work in large tech company in Bay AreA
* Obtained 2 professional certificates at UC Berkeley Extension: 1 in Programming, 1 in Data Science
   * Medium Proficiency in Python, SQL + Statistics/Probability
* I intend to take degree in 3 years (while working full-time)
* I can afford the tuition (\~$45k before employer contributions) if program is worth it. It won't put me into hardship or debt. 
* What I want to get from the degree: applied knowledge that I can use at the workplace to move onto more technical roles. 
   * Also, a degree that will 'officially' open the doors to the mentioned roles, as many positions state that a Masters Degree is the minimum required qualification.
* If I were to decline the offer, my current options are not many:
   * Master of Science in Data Science at Colorado Boulder (everyone is 'admitted' through their Introduction courses that take place every 3 months): https://www.colorado.edu/program/data-science/coursera-overview
   * Apply to other schools for the Spring 2022 semester (Georgia Tech would be the first target; any other recommendations?)",t2_51engwuk,False,,0,False,Admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan - Thoughts?,[],r/datascience,False,6,education,0,,,False,t3_o3ajx5,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1624116206.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I have been admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan to start in Fall 2021:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.si.umich.edu/programs/master-applied-data-science-online""&gt;https://www.si.umich.edu/programs/master-applied-data-science-online&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Does anyone have any insights on the program? If I understand correctly, the program is very new and the first cohort of students must be about to graduate now. I would love to read the experience of some people that have taken the program and whether they thought it was worth it as well as any objective opinions in general.&lt;/p&gt;

&lt;p&gt;I have found very few first-hand opinions on the MADS at Michigan out there, and the opinions that I have read (mostly here on reddit) were on the negative side. Given that other schools such as Georgia Tech have tons of overwhelmingly positive opinions -both on the quality of the classes as well as the price-, I am scared this is not the right move for me. It is a big investment in terms of time and money after all.&lt;/p&gt;

&lt;p&gt;My background:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bachelor in Economics (10 years ago)&lt;/li&gt;
&lt;li&gt;5+ years of Data Analytics work in large tech company in Bay AreA&lt;/li&gt;
&lt;li&gt;Obtained 2 professional certificates at UC Berkeley Extension: 1 in Programming, 1 in Data Science

&lt;ul&gt;
&lt;li&gt;Medium Proficiency in Python, SQL + Statistics/Probability&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;I intend to take degree in 3 years (while working full-time)&lt;/li&gt;
&lt;li&gt;I can afford the tuition (~$45k before employer contributions) if program is worth it. It won&amp;#39;t put me into hardship or debt. &lt;/li&gt;
&lt;li&gt;What I want to get from the degree: applied knowledge that I can use at the workplace to move onto more technical roles. 

&lt;ul&gt;
&lt;li&gt;Also, a degree that will &amp;#39;officially&amp;#39; open the doors to the mentioned roles, as many positions state that a Masters Degree is the minimum required qualification.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I were to decline the offer, my current options are not many:

&lt;ul&gt;
&lt;li&gt;Master of Science in Data Science at Colorado Boulder (everyone is &amp;#39;admitted&amp;#39; through their Introduction courses that take place every 3 months): &lt;a href=""https://www.colorado.edu/program/data-science/coursera-overview""&gt;https://www.colorado.edu/program/data-science/coursera-overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Apply to other schools for the Spring 2022 semester (Georgia Tech would be the first target; any other recommendations?)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ajx5,True,,rakhed1,,18,True,all_ads,False,[],False,,/r/datascience/comments/o3ajx5/admitted_to_the_master_of_applied_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ajx5/admitted_to_the_master_of_applied_data_science/,515405,1624087406.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?auto=webp&amp;s=21967ac2e4e8d2260dec86533b2daf1d668e4569', 'width': 500, 'height': 281}, 'resolutions': [{'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc5389a13a598c72a09381953f4c6a38829ba222', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c55a38a70fbe75184ab51833b9e04e51170c0759', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74994dc942a0fd65b3c3a8383d2454e13d9f762a', 'width': 320, 'height': 179}], 'variants': {}, 'id': 'OG8afLvNUP-fqr1Uh-23YpAEqhmL_QnNnPnkzAZPWKM'}], 'enabled': False}",,,,,
,datascience,"I have a great opportunity in a company, it's a B2B company. The CEO of this company really want to hire me but he wants to test me one day to be sure. This company use a call center and the CEO ask me to find a way to increase the pourcentage of people accept to take the call. In this moment the call center get 30% of people taking the phone call. The compagny wants an increase of 5%. So i need to build a model. What kind of variables should i take according to you to build it (sexe, age, professional activity,...) ?  I have no work experiences but just theorical knowledge. 

I need some advices. 
Thanks 

Ps: Sorry for my Frenchglish",t2_7mv7a33f,False,,0,False,Create a model to improve a business issue,[],r/datascience,False,6,career,0,,,False,t3_o3d3jm,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1624127423.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a great opportunity in a company, it&amp;#39;s a B2B company. The CEO of this company really want to hire me but he wants to test me one day to be sure. This company use a call center and the CEO ask me to find a way to increase the pourcentage of people accept to take the call. In this moment the call center get 30% of people taking the phone call. The compagny wants an increase of 5%. So i need to build a model. What kind of variables should i take according to you to build it (sexe, age, professional activity,...) ?  I have no work experiences but just theorical knowledge. &lt;/p&gt;

&lt;p&gt;I need some advices. 
Thanks &lt;/p&gt;

&lt;p&gt;Ps: Sorry for my Frenchglish&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3d3jm,True,,battlefieldanalytica,,7,True,all_ads,False,[],False,,/r/datascience/comments/o3d3jm/create_a_model_to_improve_a_business_issue/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3d3jm/create_a_model_to_improve_a_business_issue/,515405,1624098623.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"So, for the past 3 years, I have worked in multiple dead-end jobs due to having no goal or career in mind, leading to not going to college or university, which I really do hate myself for. Worked in a bar, two warehouse jobs, and now work in an apprenticeship in a financial services company as a Quality Assurance Analyst. 

As part of my apprenticeship, I take part in courses where they are relevant to our jobs and I was introduced to Data Science which caught my attention. We get told what it is, it’s purpose, the jobs that come under it, and I don’t think I ever felt in love with such a thing before in my life. 

Fast forward three months, I have signed up for online courses, been reading articles, programming languages, and it’s never gone away, and that’s rare for someone like me with Autism and ADHD. But one thing I keep seeing is articles and people saying “How to Become a Data Analyst with No Degree or Experience”. 

This has me interested but also very wary due to the current work climate and strict job requirements. So, I want to ask everyone: Is it really possible to land a job as a Data Analyst with a portfolio background, with no experience, and no degree? 

I’m happy for the job I am grateful to have kept since pre-pandemic but I know I wouldn’t like to stay in a job like that, or in the same company which is pretty terrible to staff. 

If any advice or knowledge can be given as well, it would be much appreciated in my career path. Thank you.",t2_881086cq,False,,0,False,Is it really possible?,[],r/datascience,False,6,career,0,,,False,t3_o32fsq,False,dark,0.72,,public,6,0,{},,,False,[],,False,False,,{},Career,False,6,,False,False,self,False,,[],{},,True,,1624085673.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, for the past 3 years, I have worked in multiple dead-end jobs due to having no goal or career in mind, leading to not going to college or university, which I really do hate myself for. Worked in a bar, two warehouse jobs, and now work in an apprenticeship in a financial services company as a Quality Assurance Analyst. &lt;/p&gt;

&lt;p&gt;As part of my apprenticeship, I take part in courses where they are relevant to our jobs and I was introduced to Data Science which caught my attention. We get told what it is, it’s purpose, the jobs that come under it, and I don’t think I ever felt in love with such a thing before in my life. &lt;/p&gt;

&lt;p&gt;Fast forward three months, I have signed up for online courses, been reading articles, programming languages, and it’s never gone away, and that’s rare for someone like me with Autism and ADHD. But one thing I keep seeing is articles and people saying “How to Become a Data Analyst with No Degree or Experience”. &lt;/p&gt;

&lt;p&gt;This has me interested but also very wary due to the current work climate and strict job requirements. So, I want to ask everyone: Is it really possible to land a job as a Data Analyst with a portfolio background, with no experience, and no degree? &lt;/p&gt;

&lt;p&gt;I’m happy for the job I am grateful to have kept since pre-pandemic but I know I wouldn’t like to stay in a job like that, or in the same company which is pretty terrible to staff. &lt;/p&gt;

&lt;p&gt;If any advice or knowledge can be given as well, it would be much appreciated in my career path. Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o32fsq,True,,mintcoffeebug,,6,True,all_ads,False,[],False,,/r/datascience/comments/o32fsq/is_it_really_possible/,all_ads,False,https://www.reddit.com/r/datascience/comments/o32fsq/is_it_really_possible/,515405,1624056873.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hey guys so I’m undecided yet on what I want to major in but data science has caught my eye. I was just wondering what specific entry level jobs could I get out of college? Is it so in demand that you can work in pretty much any sector? Is there such thing as an associates  in data science?

Im not very good at math but I plan on taking remedial math in college, assuming they have that; will I be fine? 

What kind of internships can benefit me to set myself up for success?",t2_3bthqqi7,False,,0,False,What jobs and sectors can I get with a data science degree?,[],r/datascience,False,6,education,0,,,False,t3_o36yvp,False,dark,0.59,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1624101623.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys so I’m undecided yet on what I want to major in but data science has caught my eye. I was just wondering what specific entry level jobs could I get out of college? Is it so in demand that you can work in pretty much any sector? Is there such thing as an associates  in data science?&lt;/p&gt;

&lt;p&gt;Im not very good at math but I plan on taking remedial math in college, assuming they have that; will I be fine? &lt;/p&gt;

&lt;p&gt;What kind of internships can benefit me to set myself up for success?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o36yvp,True,,jaylenbrowny,,3,True,all_ads,False,[],False,,/r/datascience/comments/o36yvp/what_jobs_and_sectors_can_i_get_with_a_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/o36yvp/what_jobs_and_sectors_can_i_get_with_a_data/,515405,1624072823.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Untill college, I was kind of extrovert who eventually with age started becoming introvert. I started my career in sales but one thing after another made me switch to Data science. After 5 years of job, I'm mostly introvert and data science actually helps me this as many other career options do indeed need extrovert characteristics.
To break down on how it helps 2 major examples are:
•Communication:
Its mostly through email, this helps me avoid unnecessary talking. Moreover any verbal meeting mostly translate into me listening to the person and identify the problem statement and then let my work speak.

•Technology
Working on laptop, pushing code on git for collaboration etc all further helps me with least human interaction.

Things were it hurts me most.
•Switching job
Terrible time at interview since now you have to speak and make eye contact

•Office friends
Many a times it had happened that I'm the last person to get to know of office gossip. In my past jobs it was kind off necessity since it helped me know if company will shutdown or not. My last 4 company where I worked all shutdown and in 2, I was among last to know that it was going down.",t2_9zy06zws,False,,0,False,How many people is data science are introverts?,[],r/datascience,False,6,fun,0,,,False,t3_o34mzt,False,dark,0.67,,public,4,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,4,,False,False,self,False,,[],{},,True,,1624092917.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Untill college, I was kind of extrovert who eventually with age started becoming introvert. I started my career in sales but one thing after another made me switch to Data science. After 5 years of job, I&amp;#39;m mostly introvert and data science actually helps me this as many other career options do indeed need extrovert characteristics.
To break down on how it helps 2 major examples are:
•Communication:
Its mostly through email, this helps me avoid unnecessary talking. Moreover any verbal meeting mostly translate into me listening to the person and identify the problem statement and then let my work speak.&lt;/p&gt;

&lt;p&gt;•Technology
Working on laptop, pushing code on git for collaboration etc all further helps me with least human interaction.&lt;/p&gt;

&lt;p&gt;Things were it hurts me most.
•Switching job
Terrible time at interview since now you have to speak and make eye contact&lt;/p&gt;

&lt;p&gt;•Office friends
Many a times it had happened that I&amp;#39;m the last person to get to know of office gossip. In my past jobs it was kind off necessity since it helped me know if company will shutdown or not. My last 4 company where I worked all shutdown and in 2, I was among last to know that it was going down.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o34mzt,True,,data_hop,,6,True,all_ads,False,[],False,,/r/datascience/comments/o34mzt/how_many_people_is_data_science_are_introverts/,all_ads,False,https://www.reddit.com/r/datascience/comments/o34mzt/how_many_people_is_data_science_are_introverts/,515405,1624064117.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_3mpdgvl7,False,,0,False,What course/book/talk etc. added the most value to your career as a data scientist?,[],r/datascience,False,6,discussion,0,,,False,t3_o2hiye,False,dark,0.99,,public,144,1,{},,,False,[],,False,False,,{},Discussion,False,144,,False,False,self,False,,[],{},,True,,1624022739.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2hiye,True,,benthecoderX,,32,True,all_ads,False,[],False,,/r/datascience/comments/o2hiye/what_coursebooktalk_etc_added_the_most_value_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2hiye/what_coursebooktalk_etc_added_the_most_value_to/,515405,1623993939.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[https://martin-thoma.com/images/2016/01/ml-classifiers-2.png](https://martin-thoma.com/images/2016/01/ml-classifiers-2.png)

These pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: ""red"" and ""blue"".

Shouldn't all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean ""an overlapping decision boundary""?

Thanks",t2_3tosvccj,False,,0,False,"can someone please explain what the ""white color shades"" mean in this picture?",[],r/datascience,False,6,discussion,0,,,False,t3_o2qr6i,False,dark,0.92,,public,19,0,{},,,False,[],,False,False,,{},Discussion,False,19,,False,False,self,False,,[],{},,True,,1624056352.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://martin-thoma.com/images/2016/01/ml-classifiers-2.png""&gt;https://martin-thoma.com/images/2016/01/ml-classifiers-2.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: &amp;quot;red&amp;quot; and &amp;quot;blue&amp;quot;.&lt;/p&gt;

&lt;p&gt;Shouldn&amp;#39;t all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean &amp;quot;an overlapping decision boundary&amp;quot;?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2qr6i,True,,jj4646,,5,True,all_ads,False,[],False,,/r/datascience/comments/o2qr6i/can_someone_please_explain_what_the_white_color/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2qr6i/can_someone_please_explain_what_the_white_color/,515405,1624027552.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?auto=webp&amp;s=6c851a26b932f89c3893032e66e34b3d707d10bf', 'width': 988, 'height': 718}, 'resolutions': [{'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8b8bb013f48747bd1299d307627a8ead0b79895', 'width': 108, 'height': 78}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81487fe3975442ef3106ad9c62c22516022d13a3', 'width': 216, 'height': 156}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e565a4aa170327c35d05952e2e5ea1ae298fb7ae', 'width': 320, 'height': 232}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a70a531fb9b09544521400d244898c2ea26812c0', 'width': 640, 'height': 465}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=39eabd46817c4651e0bbbbb9f64e2752c2e83c8d', 'width': 960, 'height': 697}], 'variants': {}, 'id': 'UTdvJd2FNGWl3ZnR5XDXOxEiR_Z5BOBaqJWXE-QQN8w'}], 'enabled': False}",,,,,
,datascience,"I have a couple projects I’d like to work on. But I’m terrible at holding myself accountable to making progress on projects. I’d like to get together with a handful of people to work on our own projects, but we’d meet every couple weeks to give updates and feedback.

If anyone else is in the Chicago area, I’d love to meet in person. (I’ve spent enough time cooped up over the past year.)

If you’re interested, PM me.

EDIT: Wow! Thanks everyone for the interest! We started a discord server for the group. I don't want to post it directly on the sub, but if you're interested, send me a PM and I'll respond with the discord link. I'm logging off for the night, so I may not get back to you until tomorrow.",t2_jqnnc,False,,0,False,Anyone interested on getting together to focus on personal projects?,[],r/datascience,False,6,projects,0,,,False,t3_o2cr6e,False,dark,0.95,,public,238,3,{},,,False,[],,False,False,,{},Projects,False,238,,False,False,self,1623992291.0,,[],{},,True,,1624006814.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a couple projects I’d like to work on. But I’m terrible at holding myself accountable to making progress on projects. I’d like to get together with a handful of people to work on our own projects, but we’d meet every couple weeks to give updates and feedback.&lt;/p&gt;

&lt;p&gt;If anyone else is in the Chicago area, I’d love to meet in person. (I’ve spent enough time cooped up over the past year.)&lt;/p&gt;

&lt;p&gt;If you’re interested, PM me.&lt;/p&gt;

&lt;p&gt;EDIT: Wow! Thanks everyone for the interest! We started a discord server for the group. I don&amp;#39;t want to post it directly on the sub, but if you&amp;#39;re interested, send me a PM and I&amp;#39;ll respond with the discord link. I&amp;#39;m logging off for the night, so I may not get back to you until tomorrow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2cr6e,True,,MindlessTime,,96,True,all_ads,False,[],False,,/r/datascience/comments/o2cr6e/anyone_interested_on_getting_together_to_focus_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2cr6e/anyone_interested_on_getting_together_to_focus_on/,515405,1623978014.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m a beginner and I’m planning on creating a career in cloud services. I currently have little to no knowledge on cloud platforms and I am wanting to learn more. As a first step I’m planning to prepare for the basic certification offered by AWS I.e. Cloud Practitioner.
It would be great if anyone can provide me resources to begin with. I’m more of a video person than book person so if you have any online MOOC course that will be perfect for my goal. Thanks!",t2_9ga7xr3c,False,,0,False,How to start preparing for AWS cloud practitioner certification,[],r/datascience,False,6,discussion,0,,,False,t3_o33taz,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1624089965.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a beginner and I’m planning on creating a career in cloud services. I currently have little to no knowledge on cloud platforms and I am wanting to learn more. As a first step I’m planning to prepare for the basic certification offered by AWS I.e. Cloud Practitioner.
It would be great if anyone can provide me resources to begin with. I’m more of a video person than book person so if you have any online MOOC course that will be perfect for my goal. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o33taz,True,,ElNinoo9,,2,True,all_ads,False,[],False,,/r/datascience/comments/o33taz/how_to_start_preparing_for_aws_cloud_practitioner/,all_ads,False,https://www.reddit.com/r/datascience/comments/o33taz/how_to_start_preparing_for_aws_cloud_practitioner/,515405,1624061165.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have CS Bachelors and am interested in going thru a curriculum or learning the core fundamentals of data engineering and gaining practical experience doing projects. I'm at stage 0 right now. Have reasonable familiarity with python, pandas, basic experience with sql (relational).

Does anyone have any resources (articles, online classes/courses or tracks, books, etc..) they can share so I may learn the core fundamentals for data engineering starting as a beginner? A lot of what I've seen online is like cloudera, google, aws, etc.. offering some classes tailored to their proprietary systems but idk which softwares/programs are most relevant in Data Engineering. So its hard to tell what is a good resource for learning core fundamentals versus Company A wanting to grow users in their proprietary ecosystem (which may not be used much by many companies).

**In short, I'd like to know what the most widely used skills/softwares/programs are for data engineering and what the best resources are to learn &amp; apply said learning pragmatically.** Ideally cheap/free but is not necessarily a requirement.

I appreciate any help in advance. :)",t2_yxicn,False,,0,False,Resources for Learning Data Engineering?,[],r/datascience,False,6,education,0,,,False,t3_o31p08,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,1624054919.0,,[],{},,True,,1624083481.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have CS Bachelors and am interested in going thru a curriculum or learning the core fundamentals of data engineering and gaining practical experience doing projects. I&amp;#39;m at stage 0 right now. Have reasonable familiarity with python, pandas, basic experience with sql (relational).&lt;/p&gt;

&lt;p&gt;Does anyone have any resources (articles, online classes/courses or tracks, books, etc..) they can share so I may learn the core fundamentals for data engineering starting as a beginner? A lot of what I&amp;#39;ve seen online is like cloudera, google, aws, etc.. offering some classes tailored to their proprietary systems but idk which softwares/programs are most relevant in Data Engineering. So its hard to tell what is a good resource for learning core fundamentals versus Company A wanting to grow users in their proprietary ecosystem (which may not be used much by many companies).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In short, I&amp;#39;d like to know what the most widely used skills/softwares/programs are for data engineering and what the best resources are to learn &amp;amp; apply said learning pragmatically.&lt;/strong&gt; Ideally cheap/free but is not necessarily a requirement.&lt;/p&gt;

&lt;p&gt;I appreciate any help in advance. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o31p08,True,,zeroskater45,,7,True,all_ads,False,[],False,,/r/datascience/comments/o31p08/resources_for_learning_data_engineering/,all_ads,False,https://www.reddit.com/r/datascience/comments/o31p08/resources_for_learning_data_engineering/,515405,1624054681.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"What software did the author use to create the wonderful visualizations in this Transformer writeup? I would love to recreate his work. 

[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)",t2_2ich5u4y,False,,0,False,ML Visualization Software,[],r/datascience,False,6,education,0,,,False,t3_o32wxx,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624087138.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What software did the author use to create the wonderful visualizations in this Transformer writeup? I would love to recreate his work. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://jalammar.github.io/illustrated-transformer/""&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o32wxx,True,,Stutoucan12,,4,True,all_ads,False,[],False,,/r/datascience/comments/o32wxx/ml_visualization_software/,all_ads,False,https://www.reddit.com/r/datascience/comments/o32wxx/ml_visualization_software/,515405,1624058338.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?auto=webp&amp;s=6f463f6bff513c6d017fa5538a00f4548a2dcfe1', 'width': 1436, 'height': 804}, 'resolutions': [{'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8a7e804fa309d37c49f00a733d9224f0be49337', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b16204a4e79f791d02291fe76812185b9020ae5f', 'width': 216, 'height': 120}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4574a3af653fcbecd95c46b4b76706aef9b447a1', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=236b712e35df9668aa6ac8750ddd6b46b6ce24cf', 'width': 640, 'height': 358}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78c4a9af8900e812f85c7fa529f53043e236922a', 'width': 960, 'height': 537}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c049be5f511c9d47b872329f22723838d608ee3a', 'width': 1080, 'height': 604}], 'variants': {}, 'id': 'c1mFuTYSarvnl5bzqbH7vE9qZMX_5O-mA_uV6IM3pBM'}], 'enabled': False}",,,,,
,datascience,"With \\things starting to reopen my company is restarting training and conference travelling.

I was taking a look at the Disney Conference on Data and Analytics. I know that most people find the Disney conferences very useful and I was wondering if anyone in DS had attended this conference and if so, what they thought of it.

[https://disneydataconference.com/](https://disneydataconference.com/)

FYI - not affiliated in with Disney in any manner, just a Canadian looking for a winter trip to somewhere warm ;)

Or are there any good ones looking to be in person in the fall? I know a lot of this may be still be subject to cancellations depending on what happens again in the fall.",t2_6nl3f37f,False,,0,False,Conferences for 2021 - Disney Conference,[],r/datascience,False,6,education,0,,,False,t3_o316ni,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624081910.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With \things starting to reopen my company is restarting training and conference travelling.&lt;/p&gt;

&lt;p&gt;I was taking a look at the Disney Conference on Data and Analytics. I know that most people find the Disney conferences very useful and I was wondering if anyone in DS had attended this conference and if so, what they thought of it.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://disneydataconference.com/""&gt;https://disneydataconference.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;FYI - not affiliated in with Disney in any manner, just a Canadian looking for a winter trip to somewhere warm ;)&lt;/p&gt;

&lt;p&gt;Or are there any good ones looking to be in person in the fall? I know a lot of this may be still be subject to cancellations depending on what happens again in the fall.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o316ni,True,,data_chameleon,,1,True,all_ads,False,[],False,,/r/datascience/comments/o316ni/conferences_for_2021_disney_conference/,all_ads,False,https://www.reddit.com/r/datascience/comments/o316ni/conferences_for_2021_disney_conference/,515405,1624053110.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?auto=webp&amp;s=0cf4aa92a5ea30a8c9cd895126b35ed845bc94d4', 'width': 752, 'height': 395}, 'resolutions': [{'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61f23680e1b9a9b7e4d226f31a6dcedbf1f99866', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00f8a00c6ad70f30e19afcf16bbd310c645fed16', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e6b6b1d8c4344113fa620266387bc03acdd9b5b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32f50b936d570fd5cd0f35ae6d6eb5487f2a84d5', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'K1siLmi-ySXB--VuJx92KjBEFmUnwSLBbgpH-DiI5kk'}], 'enabled': False}",,,,,
,datascience,"Hey all!

I setup a short survey **print(**[https://forms.gle/2XGL3q8RGpZjmLi78](https://forms.gle/2XGL3q8RGpZjmLi78)**)** to learn more about problems that other developers (data engineers, data scientists) face when it comes to documentation. It would help me out if you'd be willing to give some feedback about the challenges that you face. This is a problem that I face when working on new projects and on new teams and something that I want to build and solve for.

Challenges could include like: time that it takes to create, knowledge transfer of what you work on, having to explain to non-technical people vs. new developers.",t2_127oi7,False,,0,False,survey for data scientists: issues faced around documentation / on-boarding / KT,[],r/datascience,False,6,discussion,0,,,False,t3_o36h26,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624099770.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all!&lt;/p&gt;

&lt;p&gt;I setup a short survey &lt;strong&gt;print(&lt;/strong&gt;&lt;a href=""https://forms.gle/2XGL3q8RGpZjmLi78""&gt;https://forms.gle/2XGL3q8RGpZjmLi78&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt; to learn more about problems that other developers (data engineers, data scientists) face when it comes to documentation. It would help me out if you&amp;#39;d be willing to give some feedback about the challenges that you face. This is a problem that I face when working on new projects and on new teams and something that I want to build and solve for.&lt;/p&gt;

&lt;p&gt;Challenges could include like: time that it takes to create, knowledge transfer of what you work on, having to explain to non-technical people vs. new developers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o36h26,True,,guru223,,1,True,all_ads,False,[],False,,/r/datascience/comments/o36h26/survey_for_data_scientists_issues_faced_around/,all_ads,False,https://www.reddit.com/r/datascience/comments/o36h26/survey_for_data_scientists_issues_faced_around/,515405,1624070970.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?auto=webp&amp;s=6295ba82373dab020feb7eae12a61705ebe044bc', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ec2b17d66ce9ca67a1c87f9179de8e907e27aee', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17598c88788d482122b5cfe2a79163908774960d', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a6fd85b5bb7a6ba3992167181da32d340c39db1', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3db12eaf4eaace3c7ef8fe5e4ef3e7eaa1af42e', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42ae751cb04df084b056a18daaf2d7d07b8c4fe9', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6073ac1b229d6db9325306f4b33bb59b544af50b', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '_mfYyVSEkOcMSL-fSbM3-1BCr-m3sawG63jbtsML9-s'}], 'enabled': False}",,,,,
,datascience,"Here's the hypothetical from the interviewer:

FB launched a Zoom-like feature. It was generally well-accepted and its usage is growing.

You work at Instagram. How would you evaluate if IG should add that Zoom-like feature?

(in other words, a synchronous communication app within a heterogeneous network (FB) is being evaluted for launch in an otherwise asynchronous homogenous network (IG).

My response:

Clarifying questions:

\- Can some people use the FB Zoom feature with a higher / different  access level than others?

\- What requirements, minimums, or thresholds must be achieved in order to obtain higher access (such as:

o a Facebook business page,

o a Facebook business page with  &gt;1,000 followers

o a Facebook user with &gt;500 edges (relationships since [Facebook can process one trillion edge graphs)](https://research.fb.com/publications/one-trillion-edges-graph-processing-at-facebook-scale/) which the people (nodes) desiring higher access must have people

\- Higher access might include: the ability to invite more than 20 people (20 being the number the hypothetical provided); the ability to place other companies’ advertisements in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; the ability to place advertisements of the host’s company in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; a longer Zoom meeting time (&gt;60 minutes for example)

(clarifying questions partially answered but  mostly deferred)

I would review the data from the A/B experiment on this feature's usage and adoption at FB. I imagine this A/B test would have a dependent variable / control group / training set in machine learning (ML) (user behavior before Zoom feature) and an independent variable / test group / test set in ML (user behavior after Zoom feature).  user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).

I'd also analyze datasets which tracked:

\- how many invitees became hosts and set up their own FB Zoom meeting within 1,2,3,4 and +4 weeks;

\- How many invitees purchased one of the host’s products that were advertised in the Zoom meeting?

\- How many invitees purchased one of the non-host products that were advertised in the Zoom meeting?

The A/B experiment I'd design at IG:

Dependent variable / control group / training set: IG user behavior before Zoom feature

Independent variable / test group / test set: IG user behavior after Zoom feature

user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).

Last, I'd run an SQL inner join of the networks of people who used FB Zoom  and the networks of those same people on IG. I'd use clustering ML (entropy weight k-means) to then look for trends or patterns which might explain why the Zoom feature was used more (or less) on FB than IG; which gender uses the Zoom feature more; which network (IG or FB) leads to more Zoom invites being sent out; do IG or FB Zoom meetings lead to more purchases? What type of purchases (category)? What price range? How does price, category and frequency of purchase vary based on GIS or location data of Zoom host and location of invitees? Do the data suggest this might be growing into something like the Influencers feature?",t2_80vvyzaz,False,,0,False,FAANG interview prep: A/B testing - please be merciless in your reply,[],r/datascience,False,6,,0,,,False,t3_o35vgb,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,1624109659.0,,[],{},,True,,1624097478.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here&amp;#39;s the hypothetical from the interviewer:&lt;/p&gt;

&lt;p&gt;FB launched a Zoom-like feature. It was generally well-accepted and its usage is growing.&lt;/p&gt;

&lt;p&gt;You work at Instagram. How would you evaluate if IG should add that Zoom-like feature?&lt;/p&gt;

&lt;p&gt;(in other words, a synchronous communication app within a heterogeneous network (FB) is being evaluted for launch in an otherwise asynchronous homogenous network (IG).&lt;/p&gt;

&lt;p&gt;My response:&lt;/p&gt;

&lt;p&gt;Clarifying questions:&lt;/p&gt;

&lt;p&gt;- Can some people use the FB Zoom feature with a higher / different  access level than others?&lt;/p&gt;

&lt;p&gt;- What requirements, minimums, or thresholds must be achieved in order to obtain higher access (such as:&lt;/p&gt;

&lt;p&gt;o a Facebook business page,&lt;/p&gt;

&lt;p&gt;o a Facebook business page with  &amp;gt;1,000 followers&lt;/p&gt;

&lt;p&gt;o a Facebook user with &amp;gt;500 edges (relationships since &lt;a href=""https://research.fb.com/publications/one-trillion-edges-graph-processing-at-facebook-scale/""&gt;Facebook can process one trillion edge graphs)&lt;/a&gt; which the people (nodes) desiring higher access must have people&lt;/p&gt;

&lt;p&gt;- Higher access might include: the ability to invite more than 20 people (20 being the number the hypothetical provided); the ability to place other companies’ advertisements in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; the ability to place advertisements of the host’s company in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; a longer Zoom meeting time (&amp;gt;60 minutes for example)&lt;/p&gt;

&lt;p&gt;(clarifying questions partially answered but  mostly deferred)&lt;/p&gt;

&lt;p&gt;I would review the data from the A/B experiment on this feature&amp;#39;s usage and adoption at FB. I imagine this A/B test would have a dependent variable / control group / training set in machine learning (ML) (user behavior before Zoom feature) and an independent variable / test group / test set in ML (user behavior after Zoom feature).  user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).&lt;/p&gt;

&lt;p&gt;I&amp;#39;d also analyze datasets which tracked:&lt;/p&gt;

&lt;p&gt;- how many invitees became hosts and set up their own FB Zoom meeting within 1,2,3,4 and +4 weeks;&lt;/p&gt;

&lt;p&gt;- How many invitees purchased one of the host’s products that were advertised in the Zoom meeting?&lt;/p&gt;

&lt;p&gt;- How many invitees purchased one of the non-host products that were advertised in the Zoom meeting?&lt;/p&gt;

&lt;p&gt;The A/B experiment I&amp;#39;d design at IG:&lt;/p&gt;

&lt;p&gt;Dependent variable / control group / training set: IG user behavior before Zoom feature&lt;/p&gt;

&lt;p&gt;Independent variable / test group / test set: IG user behavior after Zoom feature&lt;/p&gt;

&lt;p&gt;user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).&lt;/p&gt;

&lt;p&gt;Last, I&amp;#39;d run an SQL inner join of the networks of people who used FB Zoom  and the networks of those same people on IG. I&amp;#39;d use clustering ML (entropy weight k-means) to then look for trends or patterns which might explain why the Zoom feature was used more (or less) on FB than IG; which gender uses the Zoom feature more; which network (IG or FB) leads to more Zoom invites being sent out; do IG or FB Zoom meetings lead to more purchases? What type of purchases (category)? What price range? How does price, category and frequency of purchase vary based on GIS or location data of Zoom host and location of invitees? Do the data suggest this might be growing into something like the Influencers feature?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,o35vgb,True,,Objective-Patient-37,,3,True,all_ads,False,[],False,,/r/datascience/comments/o35vgb/faang_interview_prep_ab_testing_please_be/,all_ads,False,https://www.reddit.com/r/datascience/comments/o35vgb/faang_interview_prep_ab_testing_please_be/,515405,1624068678.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi All!

Could I get your opinions on the best value data science bootcamps out there? It's something I've looked at for a long time, but I've been hesitant to pick one because there seems to be so many of them.

The first one that comes to mind is General Assembly, and I also see Springboard. Let me know your experience with these and any others out there.

Thanks in advance!",t2_218h5k2r,False,,0,False,What Are The Best Data Science Bootcamps?,[],r/datascience,False,6,education,0,,,False,t3_o347ji,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1624091365.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All!&lt;/p&gt;

&lt;p&gt;Could I get your opinions on the best value data science bootcamps out there? It&amp;#39;s something I&amp;#39;ve looked at for a long time, but I&amp;#39;ve been hesitant to pick one because there seems to be so many of them.&lt;/p&gt;

&lt;p&gt;The first one that comes to mind is General Assembly, and I also see Springboard. Let me know your experience with these and any others out there.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o347ji,True,,RegularAd1,,7,True,all_ads,False,[],False,,/r/datascience/comments/o347ji/what_are_the_best_data_science_bootcamps/,all_ads,False,https://www.reddit.com/r/datascience/comments/o347ji/what_are_the_best_data_science_bootcamps/,515405,1624062565.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I'm a newbie who just started to learn data science via Kaggle courses and the above-said parameter was used when defining the model.   
e.g - `data_model = data_model.DecisionTreeRegressor(random_state=1)`

They mention that this was to get the same results every time. But even after I changed values, or just completely removed the parameter, the same results were generated, or at least that's what it seems like, looking at the `predictions.head()` 

So what's the use of that parameter here? Why do we even use it when it brings no change?

.

Also, please give me any advice you'd liek to as I've only started learning like a week ago.",t2_6phv9ewx,False,,0,False,What is the random_state parameter for DecisionTreeRegressor?,[],r/datascience,False,6,education,0,,,False,t3_o319vj,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1624082192.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a newbie who just started to learn data science via Kaggle courses and the above-said parameter was used when defining the model.&lt;br/&gt;
e.g - &lt;code&gt;data_model = data_model.DecisionTreeRegressor(random_state=1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;They mention that this was to get the same results every time. But even after I changed values, or just completely removed the parameter, the same results were generated, or at least that&amp;#39;s what it seems like, looking at the &lt;code&gt;predictions.head()&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;So what&amp;#39;s the use of that parameter here? Why do we even use it when it brings no change?&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Also, please give me any advice you&amp;#39;d liek to as I&amp;#39;ve only started learning like a week ago.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o319vj,True,,AtomicAnnihilator,,4,True,all_ads,False,[],False,,/r/datascience/comments/o319vj/what_is_the_random_state_parameter_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/o319vj/what_is_the_random_state_parameter_for/,515405,1624053392.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Non-native English speaker here. 

What is the difference between a Data Analyst and a Database Analyst? 

Is the occupation of a Data Analyst part of occupation unit group [2172--Database Analysts and Data Administrators](https://noc.esdc.gc.ca/Structure/NocProfile?objectid=xYpcSE6sKP672q64fVjEXwuRuW7o0pqx1PT2WTqtRT1u0YlGq3so6qLpM%2FO7M0Ic4ZDQ9YoZIGfvucsB9Lr7ohU%2BkbKG85oz8uuocyqCXV0%3D)? If not, which occupational unit does it fall into in the Canadian National Occupational Classification System?",t2_24o20tpg,False,,0,False,Is the occupation of a Data Analyst and Database Analyst same or are they different? Under what occupational group does a Data Analyst fall into?,[],r/datascience,False,6,career,0,,,False,t3_o2p1cp,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1624051508.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Non-native English speaker here. &lt;/p&gt;

&lt;p&gt;What is the difference between a Data Analyst and a Database Analyst? &lt;/p&gt;

&lt;p&gt;Is the occupation of a Data Analyst part of occupation unit group &lt;a href=""https://noc.esdc.gc.ca/Structure/NocProfile?objectid=xYpcSE6sKP672q64fVjEXwuRuW7o0pqx1PT2WTqtRT1u0YlGq3so6qLpM%2FO7M0Ic4ZDQ9YoZIGfvucsB9Lr7ohU%2BkbKG85oz8uuocyqCXV0%3D""&gt;2172--Database Analysts and Data Administrators&lt;/a&gt;? If not, which occupational unit does it fall into in the Canadian National Occupational Classification System?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2p1cp,True,,Experimentalphone,,4,True,all_ads,False,[],False,,/r/datascience/comments/o2p1cp/is_the_occupation_of_a_data_analyst_and_database/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2p1cp/is_the_occupation_of_a_data_analyst_and_database/,515405,1624022708.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello!

I am just finishing my first year at my first job after grad school. In the past year I have realized my manager most of the times overestimate the deadline of the task he gives me. I'm not talking about building ML models but Data cleaning, prep or mostly building dashboards. Just to give you a context on overestimation, sometimes he'll ask me to build a dashboard in a week but I end up developing in a day or two. It's actually nice because the rest of the time I use it for my upskilling.

Similar thing just happened today as well so I was just wondering if you guys also come across similar situations and what do you do in cases like these?

Thanks!",t2_bv171ji2,False,,0,False,How common is it for managers in Analytics field to overestimate the project/task deadline?,[],r/datascience,False,6,discussion,0,,,False,t3_o2x0t5,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624070326.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am just finishing my first year at my first job after grad school. In the past year I have realized my manager most of the times overestimate the deadline of the task he gives me. I&amp;#39;m not talking about building ML models but Data cleaning, prep or mostly building dashboards. Just to give you a context on overestimation, sometimes he&amp;#39;ll ask me to build a dashboard in a week but I end up developing in a day or two. It&amp;#39;s actually nice because the rest of the time I use it for my upskilling.&lt;/p&gt;

&lt;p&gt;Similar thing just happened today as well so I was just wondering if you guys also come across similar situations and what do you do in cases like these?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2x0t5,True,,quite--average,,7,True,all_ads,False,[],False,,/r/datascience/comments/o2x0t5/how_common_is_it_for_managers_in_analytics_field/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2x0t5/how_common_is_it_for_managers_in_analytics_field/,515405,1624041526.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. He's been having trouble with installing tensorflow on his M1 mac. I was sure there would be an online utility to do something like that without the need to train locally, but I can't seem to find a decent resource for him to use.

Any suggestions?",t2_4i1hm,False,,0,False,There has to be an online tool for text generation right?,[],r/datascience,False,6,discussion,0,,,False,t3_o2w4qb,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624068041.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. He&amp;#39;s been having trouble with installing tensorflow on his M1 mac. I was sure there would be an online utility to do something like that without the need to train locally, but I can&amp;#39;t seem to find a decent resource for him to use.&lt;/p&gt;

&lt;p&gt;Any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2w4qb,True,,AI52487963,,0,True,all_ads,False,[],False,,/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/,515405,1624039241.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What are others’ thoughts on paying your own way to attend conferences directly related to your job? Is it common practice?  Are there salary ranges or other criteria you consider when making this kind of choice?

Back story: I used to work as a government contractor and we were only allowed to attend one conference per year. On that salary, I really couldn’t afford to pay my own way so I only went and presented at one. I recently changed jobs to a small startup, with a 25% pay increase but no real benefits to speak of yet (VERY small and new startup). There’s a conference that is squarely in line with my job description and potentially really important because of both the subject matter and the focus.  The boss is leaning towards only sending one of us and having that person take screenshots. Honestly, I don’t think this is functional for two reasons: 1) if I’m at a conference I want to focus on it and not be taking screenshots like crazy for 4 days, and 2) if I’m not at the conference, screenshots are not going to be useful to me really.  So I’m considering just paying for it myself ($500).  It will mean I can’t get (right away) the new computer I also need for my job, but this conference is really important for both me and the person who signaled it to the boss (and would get to go...that’s only fair and I’m totally ok with it).  However, paying my own way also sends a signal that I’m happy to do this going forward - which I am not. Conferences are expensive, and as a W2 employee they are not tax-deductible. What would you guys do?",t2_47ysknnw,False,,0,False,Conferences - paying your own way,[],r/datascience,False,6,career,0,,,False,t3_o2vm51,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1624066738.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are others’ thoughts on paying your own way to attend conferences directly related to your job? Is it common practice?  Are there salary ranges or other criteria you consider when making this kind of choice?&lt;/p&gt;

&lt;p&gt;Back story: I used to work as a government contractor and we were only allowed to attend one conference per year. On that salary, I really couldn’t afford to pay my own way so I only went and presented at one. I recently changed jobs to a small startup, with a 25% pay increase but no real benefits to speak of yet (VERY small and new startup). There’s a conference that is squarely in line with my job description and potentially really important because of both the subject matter and the focus.  The boss is leaning towards only sending one of us and having that person take screenshots. Honestly, I don’t think this is functional for two reasons: 1) if I’m at a conference I want to focus on it and not be taking screenshots like crazy for 4 days, and 2) if I’m not at the conference, screenshots are not going to be useful to me really.  So I’m considering just paying for it myself ($500).  It will mean I can’t get (right away) the new computer I also need for my job, but this conference is really important for both me and the person who signaled it to the boss (and would get to go...that’s only fair and I’m totally ok with it).  However, paying my own way also sends a signal that I’m happy to do this going forward - which I am not. Conferences are expensive, and as a W2 employee they are not tax-deductible. What would you guys do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2vm51,True,,sovrappensiero1,,2,True,all_ads,False,[],False,,/r/datascience/comments/o2vm51/conferences_paying_your_own_way/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2vm51/conferences_paying_your_own_way/,515405,1624037938.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I often see in textbooks and blogs, when dealing with Bayesian Models, popular choices of priors seem to be well-known distributions (e.g. the gaussian prior). But how would you go about selecting priors for a specific model?

For example, suppose you are interested in making some bayesian model (e.g. for causal inference) on some medical data. One of your variables - 'smoking status', you have knowledge that patients who smoke vs patients who don't smoke have different life expectancies. 

Using historical data, how would you choose a ""prior"" for this model? 

Thanks",t2_o4xj9,False,,0,False,Has anyone had any experience selecting priors for Bayesian Models in real life?,[],r/datascience,False,6,discussion,0,,,False,t3_o2gg88,False,dark,0.91,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1624018825.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I often see in textbooks and blogs, when dealing with Bayesian Models, popular choices of priors seem to be well-known distributions (e.g. the gaussian prior). But how would you go about selecting priors for a specific model?&lt;/p&gt;

&lt;p&gt;For example, suppose you are interested in making some bayesian model (e.g. for causal inference) on some medical data. One of your variables - &amp;#39;smoking status&amp;#39;, you have knowledge that patients who smoke vs patients who don&amp;#39;t smoke have different life expectancies. &lt;/p&gt;

&lt;p&gt;Using historical data, how would you choose a &amp;quot;prior&amp;quot; for this model? &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2gg88,True,,blueest,,10,True,all_ads,False,[],False,,/r/datascience/comments/o2gg88/has_anyone_had_any_experience_selecting_priors/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2gg88/has_anyone_had_any_experience_selecting_priors/,515405,1623990025.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What exactly does a data scientist at a company like Amazon or Google does? What are the must have skills for survival in the industry? And where should a complete noob startm,t2_4tfitpj6,False,,0,False,Getting started with data science,[],r/datascience,False,6,education,0,,,False,t3_o320dd,False,dark,0.3,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1624084479.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What exactly does a data scientist at a company like Amazon or Google does? What are the must have skills for survival in the industry? And where should a complete noob startm&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o320dd,True,,notrealkhushi,,0,True,all_ads,False,[],False,,/r/datascience/comments/o320dd/getting_started_with_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o320dd/getting_started_with_data_science/,515405,1624055679.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_63ay634e,False,,0,False,"Straight from Science Fiction! 🤯😍 ""robot can mimic varieties of human expressions across many human subjects""",[],r/datascience,False,6,projects,0,42.0,,False,t3_o30gtg,False,dark,0.17,,public,0,0,{},140.0,,False,[],,False,False,,{},Projects,False,0,,False,False,https://b.thumbs.redditmedia.com/8-hHr0Ij-JOzgti1uKlHhkv14DKpNauH6bs931CVvwY.jpg,False,,[],{},,False,,1624079782.0,text,6,,,text,self.LatestInML,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o30gtg,True,,cv2020br,,0,True,all_ads,False,[],False,,/r/datascience/comments/o30gtg/straight_from_science_fiction_robot_can_mimic/,all_ads,False,/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/,515405,1624050982.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""[link to paper](https://www.catalyzex.com/paper/arxiv:2105.12724)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1t39a8iw83671.png?width=1974&amp;format=png&amp;auto=webp&amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29\n\n👇 Free extension to get code for ML papers (❤️'d by Andrew Ng)\n\nChrome: https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil\n\nFirefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex"", 'author_fullname': 't2_63ay634e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Straight from Science Fiction! 🤯😍 ""robot can mimic varieties of human expressions across many human subjects""', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 42, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'1t39a8iw83671': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 32, 'x': 108, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed0cd1c5a3fcb3c54fe025a94affaa9c861d9a54'}, {'y': 64, 'x': 216, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734e006c59841135dbd8d8691e01f24e5c45c0e7'}, {'y': 96, 'x': 320, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c25af8dde7bb0e1ded71b622a55ae50572c9164c'}, {'y': 192, 'x': 640, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54a891e6ea076a294c25b718d3dd57d8b56f5454'}, {'y': 288, 'x': 960, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd6a64f8802c5b582896db4e0fd9e91ccc469ad4'}, {'y': 324, 'x': 1080, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7231d2c28aa14ba7d2d150ae3895a1416ec18cef'}], 's': {'y': 594, 'x': 1974, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;format=png&amp;auto=webp&amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29'}, 'id': '1t39a8iw83671'}}, 'name': 't3_o304e9', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/8-hHr0Ij-JOzgti1uKlHhkv14DKpNauH6bs931CVvwY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1624078788.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.catalyzex.com/paper/arxiv:2105.12724""&gt;link to paper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29""&gt;https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39;d by Andrew Ng)&lt;/p&gt;\n\n&lt;p&gt;Chrome: &lt;a href=""https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'o304e9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cv2020br', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/', 'subreddit_subscribers': 7080, 'created_utc': 1624049988.0, 'num_crossposts': 6, 'media': None, 'is_video': False}]",/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/,t3_o304e9,
,datascience,"Most of the stock price prediction tutorials use a single stock as example with LSTM models. Although stocks have been proved to be random walks and containing to use LSTM for real time predictions is ""not recommended"".

Say that you stuck to LSTM, how does it happen in real world for 10 different stocks (it could be 20,50, etc.)?

Do you train 10 different LSTM models? Or use a different approach? 

The Idea is to tell which amongst these 10 stocks would rise and which ones will dip.

Thanks",t2_2mmql89p,False,,0,False,Multiple stock predictions,[],r/datascience,False,6,discussion,0,,,False,t3_o2eq7y,False,dark,0.78,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1624013137.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most of the stock price prediction tutorials use a single stock as example with LSTM models. Although stocks have been proved to be random walks and containing to use LSTM for real time predictions is &amp;quot;not recommended&amp;quot;.&lt;/p&gt;

&lt;p&gt;Say that you stuck to LSTM, how does it happen in real world for 10 different stocks (it could be 20,50, etc.)?&lt;/p&gt;

&lt;p&gt;Do you train 10 different LSTM models? Or use a different approach? &lt;/p&gt;

&lt;p&gt;The Idea is to tell which amongst these 10 stocks would rise and which ones will dip.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2eq7y,True,,grid_world,,12,True,all_ads,False,[],False,,/r/datascience/comments/o2eq7y/multiple_stock_predictions/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2eq7y/multiple_stock_predictions/,515405,1623984337.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Does anyone know what are the most modern statistical models being used for time series analysis? I have heard of transformer and attention mechanisms models that are used for modelling sequential data - but these seem to be more relevant for modelling data from the NLP domain. When it comes to classical time series modelling (e.g. a vector of temperature measurements) : does anyone know what are some of the more modern models being used for this? I did some searching online : it seems like ARIMA style models were some of the first ones, followed by state space models/hidden markov, and the more recent ones being RNN and LSTM. 

Are LSTM and RNN the most modern models that are being used for classical time series problems?

Thanks",t2_xtuyc,False,,0,False,Modern Time Series Models,[],r/datascience,False,6,discussion,0,,,False,t3_o2i7b9,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1624025208.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know what are the most modern statistical models being used for time series analysis? I have heard of transformer and attention mechanisms models that are used for modelling sequential data - but these seem to be more relevant for modelling data from the NLP domain. When it comes to classical time series modelling (e.g. a vector of temperature measurements) : does anyone know what are some of the more modern models being used for this? I did some searching online : it seems like ARIMA style models were some of the first ones, followed by state space models/hidden markov, and the more recent ones being RNN and LSTM. &lt;/p&gt;

&lt;p&gt;Are LSTM and RNN the most modern models that are being used for classical time series problems?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2i7b9,True,,ottawalanguages,,2,True,all_ads,False,[],False,,/r/datascience/comments/o2i7b9/modern_time_series_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2i7b9/modern_time_series_models/,515405,1623996408.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am currently a sr. analyst doing research on business questions and support our DS team as a SME. It's a fairly relaxed but unsatisfying job. The pay is ok but enough. We will have hybrid model when going back and the commute is only 10 minutes.

I went to my (grad) school networking event, pitched myself to a company, and got offered a managerial role. My responsibility is building out data solutions such as dashboards, analytics, and ML models, in addition to overseeing a small data process/ETL team.

Asides from the increased workload, this role requires 5 days in-office and the commute is 40+ min one way. There's no plan to move to hybrid in near future. I also have no experience in managing a team, let alone doing that while building out data functions from scratch.

Now the plus side is, it's a 20% increase in total comp. I have a lot of say in how the data analytics will run. Lastly, the chance of managerial experience seems to be valuable and rare.

I'm not too worried about delivering the technical aspect of work. I have experience building out dashboard, db, and pipeline. I have also delivered 1 ML model into production and currently delivering another one, granted they are off-the-shelf models. I'll essentially be repeating what I had been doing, given that new company is in the same business.

I think the company is sincere in matching my ask and trusting me with a managerial role. That said, would you consider this a good move? Do you think there's a high chance of failure considering that I had no prior experience in managing and have to build a new function at the same time? Do you think it's worth the quality-of-life drop for advancement in career?",t2_qinw9,False,,0,False,Offered managerial role for likely a big hit on QOL,[],r/datascience,False,6,career,0,,,False,t3_o2ew30,False,dark,0.75,,public,6,0,{},,,False,[],,False,False,,{},Career,False,6,,False,False,self,False,,[],{},,True,,1624013670.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am currently a sr. analyst doing research on business questions and support our DS team as a SME. It&amp;#39;s a fairly relaxed but unsatisfying job. The pay is ok but enough. We will have hybrid model when going back and the commute is only 10 minutes.&lt;/p&gt;

&lt;p&gt;I went to my (grad) school networking event, pitched myself to a company, and got offered a managerial role. My responsibility is building out data solutions such as dashboards, analytics, and ML models, in addition to overseeing a small data process/ETL team.&lt;/p&gt;

&lt;p&gt;Asides from the increased workload, this role requires 5 days in-office and the commute is 40+ min one way. There&amp;#39;s no plan to move to hybrid in near future. I also have no experience in managing a team, let alone doing that while building out data functions from scratch.&lt;/p&gt;

&lt;p&gt;Now the plus side is, it&amp;#39;s a 20% increase in total comp. I have a lot of say in how the data analytics will run. Lastly, the chance of managerial experience seems to be valuable and rare.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not too worried about delivering the technical aspect of work. I have experience building out dashboard, db, and pipeline. I have also delivered 1 ML model into production and currently delivering another one, granted they are off-the-shelf models. I&amp;#39;ll essentially be repeating what I had been doing, given that new company is in the same business.&lt;/p&gt;

&lt;p&gt;I think the company is sincere in matching my ask and trusting me with a managerial role. That said, would you consider this a good move? Do you think there&amp;#39;s a high chance of failure considering that I had no prior experience in managing and have to build a new function at the same time? Do you think it&amp;#39;s worth the quality-of-life drop for advancement in career?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2ew30,True,,monkeyunited,,16,True,all_ads,False,[],False,,/r/datascience/comments/o2ew30/offered_managerial_role_for_likely_a_big_hit_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2ew30/offered_managerial_role_for_likely_a_big_hit_on/,515405,1623984870.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"PLEASE TELL ME IF THIS VIOLATES RULE 9 AND ILL REMOVE IT THANKS

So I really don't get what data science is, and how its different from statistics. I've read a bunch of websites but still can't come up with like a simple explanation of what it is. From my understanding, you get the meaning from data, while in statistics you make the data? Also, I don't get why coding is needed for data? What does an average day look like for a data scientist? Do you like to make the data and then say what it means or something? Sorry if I sound really dumb.",t2_84h8gjfg,False,,0,False,Can you guys help me understand what data science is?,[],r/datascience,False,6,discussion,0,,,False,t3_o2q9o8,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624055010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;PLEASE TELL ME IF THIS VIOLATES RULE 9 AND ILL REMOVE IT THANKS&lt;/p&gt;

&lt;p&gt;So I really don&amp;#39;t get what data science is, and how its different from statistics. I&amp;#39;ve read a bunch of websites but still can&amp;#39;t come up with like a simple explanation of what it is. From my understanding, you get the meaning from data, while in statistics you make the data? Also, I don&amp;#39;t get why coding is needed for data? What does an average day look like for a data scientist? Do you like to make the data and then say what it means or something? Sorry if I sound really dumb.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2q9o8,True,,thumbsgloved,,6,True,all_ads,False,[],False,,/r/datascience/comments/o2q9o8/can_you_guys_help_me_understand_what_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2q9o8/can_you_guys_help_me_understand_what_data_science/,515405,1624026210.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone,

Did anyone work as a Data scientist in ecommerce industry? I am currently working with Bloomreach, which can provide full customer experience without really knowing advanced programming.

I am looking for something cheaper alternative tool for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. What tools/combination of tools are best for you?

I tried almost all CDP demos, but i want to know some experiences from e-commerce segment. For info, we are small agency focused on automation and data.",t2_31exp9bt,False,,0,False,CDP / XCDP for beginner data scientist,[],r/datascience,False,6,tooling,0,,,False,t3_o2jtgd,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1624031872.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;Did anyone work as a Data scientist in ecommerce industry? I am currently working with Bloomreach, which can provide full customer experience without really knowing advanced programming.&lt;/p&gt;

&lt;p&gt;I am looking for something cheaper alternative tool for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. What tools/combination of tools are best for you?&lt;/p&gt;

&lt;p&gt;I tried almost all CDP demos, but i want to know some experiences from e-commerce segment. For info, we are small agency focused on automation and data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2jtgd,True,,Sonny-Orkidea,,0,True,all_ads,False,[],False,,/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/,515405,1624003072.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_9xi78ijs,False,,0,False,How to Scrape Q&amp;A Sites like Quora,[],r/datascience,False,6,fun,0,47.0,,False,t3_o2jj7g,False,dark,0.4,,public,0,0,{},140.0,,False,[],,False,False,,{},Fun/Trivia,False,0,,False,False,https://b.thumbs.redditmedia.com/ZPruvwEjwMO1_p3Xg81GIuydJf5kHTp4sPMyISuq7Po.jpg,False,,[],{},,False,,1624030708.0,text,6,,,text,reddit.com,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2jj7g,True,,Octoparseideas,,1,True,all_ads,False,[],False,,/r/datascience/comments/o2jj7g/how_to_scrape_qa_sites_like_quora/,all_ads,False,https://www.reddit.com/user/Octoparseideas/comments/o2jfp6/how_to_scrape_qa_sites_like_quora/?utm_source=share&amp;utm_medium=web2x&amp;context=3,515405,1624001908.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,https://www.reddit.com/user/Octoparseideas/comments/o2jfp6/how_to_scrape_qa_sites_like_quora/?utm_source=share&amp;utm_medium=web2x&amp;context=3,,
,datascience,"Mid Level Director in early 40s working in large Financial Data company making a decent living responsible for Change an Risk for a product within Operations Group.  Also have a small team of 3 Data Scientist/Engineers and double the headcount next year with focus on New Data Integration, Data Source/Scraping, RPA, AI/NLP Modeling and Advanced Analytics/Visualization with Tableau/PBI.  

While this is 1/3 of my responsibilities, its where my interest and curiosity lies.  My background is more Finance and Operations, intermediate SQL.  My involvement in all this is more around coming up with ideas, push on execution, project manage it all while tasking my guys on execution etc.  But because I know nothing about python, the different packages etc.  I always wanted to learn more.   Since I'm on an operations team and not Technology, are use cases are fairly basic etc but it's still fun to find opportunities to apply such tech to automate and reduce risk etc.   Our company offer tuition reimbursement of 20k per calendar year.  So I can pick any MSC degree over 3 calendar years but in 2 full years and it would be free.  I wouldnt do it for the ""starting Salary"" since I make above any MSC or MBA starting salary.  It's more for trying to further my career (sharpening the toolbox) in prep for any potential career/company changes if at all and staying relevant. 

Question for you guys who have done such degrees and are working in such field.  Would this make sense for me?  Should it be Data Science?  Or would Data Analytics make more sense based on what I do and what level I'm at?  Is DS too nitty gritty?  Too close to the actual work?  Would it further it further my career?  Considering I have a team of Data Scientists or Data Engineers etc, does it make sense to learn it all from the group up?  Thoughts?",t2_1lhphj92,False,,0,False,"40s Mid Level Manager in FinTech with Interest in DS, should I do a part time MS Degree?",[],r/datascience,False,6,education,0,,,False,t3_o22b58,False,dark,0.81,,public,10,0,{},,,False,[],,False,False,,{},Education,False,10,,False,False,self,False,,[],{},,True,,1623978757.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Mid Level Director in early 40s working in large Financial Data company making a decent living responsible for Change an Risk for a product within Operations Group.  Also have a small team of 3 Data Scientist/Engineers and double the headcount next year with focus on New Data Integration, Data Source/Scraping, RPA, AI/NLP Modeling and Advanced Analytics/Visualization with Tableau/PBI.  &lt;/p&gt;

&lt;p&gt;While this is 1/3 of my responsibilities, its where my interest and curiosity lies.  My background is more Finance and Operations, intermediate SQL.  My involvement in all this is more around coming up with ideas, push on execution, project manage it all while tasking my guys on execution etc.  But because I know nothing about python, the different packages etc.  I always wanted to learn more.   Since I&amp;#39;m on an operations team and not Technology, are use cases are fairly basic etc but it&amp;#39;s still fun to find opportunities to apply such tech to automate and reduce risk etc.   Our company offer tuition reimbursement of 20k per calendar year.  So I can pick any MSC degree over 3 calendar years but in 2 full years and it would be free.  I wouldnt do it for the &amp;quot;starting Salary&amp;quot; since I make above any MSC or MBA starting salary.  It&amp;#39;s more for trying to further my career (sharpening the toolbox) in prep for any potential career/company changes if at all and staying relevant. &lt;/p&gt;

&lt;p&gt;Question for you guys who have done such degrees and are working in such field.  Would this make sense for me?  Should it be Data Science?  Or would Data Analytics make more sense based on what I do and what level I&amp;#39;m at?  Is DS too nitty gritty?  Too close to the actual work?  Would it further it further my career?  Considering I have a team of Data Scientists or Data Engineers etc, does it make sense to learn it all from the group up?  Thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o22b58,True,,ddavid1101,,32,True,all_ads,False,[],False,,/r/datascience/comments/o22b58/40s_mid_level_manager_in_fintech_with_interest_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/o22b58/40s_mid_level_manager_in_fintech_with_interest_in/,515405,1623949957.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So I come from different industry (\~5 years experience working in energy) and then move to the new company in eCommerce since February (so technically I'm like a grad in eCommerce). I'm the first data scientist in the company, so there is no team and I likely spend a lot of time working with business guys. Before that, the company hired external consultants for analytics. And, the best I can have from them it's only Excel file, code, without any official business or technical documentation. So the only way to find out the logic calculation is to look at the code.

The business lead I usually reported to also take a maternity leave since April.

The company are in rush to produce Annual Customer Report on the my first day, which even after 4 months I hardly knows anything. Today this morning the top manager reviewed my performance, and they are not quite pleased with what I'm delivering. Then they decide not to extend my contract. They expect a data scientist to lead them to the place they want, coming up with ideas, questions and making decision. This is in my opinion a little bit more for business analysts who are already in the company for years, and also a little bit unrealisitc if the company do not have a proper technical team. (The good things is that the company uses SAS, and if they terminate then I can come back with Python.)

I wonder what's your thought in this situation? Usually, do you think 4 - 6 months are good time enough to onboard data scientists (if he didn't have any relevant domain knowledge before)?",t2_5czjyjhi,False,,0,False,"So, being fired after 4 month",[],r/datascience,False,6,career,0,,,False,t3_o1ulr0,False,dark,0.84,,public,21,0,{},,,False,[],,False,False,,{},Career,False,21,,False,False,self,1623929726.0,,[],{},,True,,1623956731.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I come from different industry (~5 years experience working in energy) and then move to the new company in eCommerce since February (so technically I&amp;#39;m like a grad in eCommerce). I&amp;#39;m the first data scientist in the company, so there is no team and I likely spend a lot of time working with business guys. Before that, the company hired external consultants for analytics. And, the best I can have from them it&amp;#39;s only Excel file, code, without any official business or technical documentation. So the only way to find out the logic calculation is to look at the code.&lt;/p&gt;

&lt;p&gt;The business lead I usually reported to also take a maternity leave since April.&lt;/p&gt;

&lt;p&gt;The company are in rush to produce Annual Customer Report on the my first day, which even after 4 months I hardly knows anything. Today this morning the top manager reviewed my performance, and they are not quite pleased with what I&amp;#39;m delivering. Then they decide not to extend my contract. They expect a data scientist to lead them to the place they want, coming up with ideas, questions and making decision. This is in my opinion a little bit more for business analysts who are already in the company for years, and also a little bit unrealisitc if the company do not have a proper technical team. (The good things is that the company uses SAS, and if they terminate then I can come back with Python.)&lt;/p&gt;

&lt;p&gt;I wonder what&amp;#39;s your thought in this situation? Usually, do you think 4 - 6 months are good time enough to onboard data scientists (if he didn&amp;#39;t have any relevant domain knowledge before)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o1ulr0,True,,vietlinh12hoa,,18,True,all_ads,False,[],False,,/r/datascience/comments/o1ulr0/so_being_fired_after_4_month/,all_ads,False,https://www.reddit.com/r/datascience/comments/o1ulr0/so_being_fired_after_4_month/,515405,1623927931.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," I graduated with a degree in ML/dataScience 2 years ago and since then been working for a small company as a ML Engineer in their R&amp;D team. Unfortunately, I’m not getting the growth I expected I would in this role. It could be because I’m the only ML engineer, no other data scientist or anyone working on the things I’m working on. 
 Additionally, there’s also no data engineers and no data pipelines, no cloud, the data is all scattered in files and folders. As mentioned, the company is not in Cloud yet, so no computing power either. Working as an ML engineer right out of bachelors and not having a team or even a mentor has been kind of rough to say the least. 

 So here I am asking for advise- I really am not sure what to do next. Whether to get an MS in a similar field (data analytics/ML) and then switch my job or to switch first and get some more experience and then think about doing MS. 

I can tell you where I want to be though- currently I’m doing core algorithm building and prediction modelling on moving parts but I think I want to venture towards the customer facing stuff – more on the lines of Market analysis or Product analysis. 

I guess I’m here because I need some advice- any advice helps. Tell me about your journey into ML/ data analytics- what has worked for you what hasn’t. Any certifications or online programs you recommend to thrive and get noticed for more opportunities and grow in the field. Any MS programs that are good and worth exploring. Any advice that allows me to figure out what to do next helps. 

Thank you for taking your time reading this and leaving a comment!",t2_11x3qj,False,,0,False,Recent graduate in Data Science,[],r/datascience,False,6,career,0,,,False,t3_o233zs,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1623980794.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I graduated with a degree in ML/dataScience 2 years ago and since then been working for a small company as a ML Engineer in their R&amp;amp;D team. Unfortunately, I’m not getting the growth I expected I would in this role. It could be because I’m the only ML engineer, no other data scientist or anyone working on the things I’m working on. 
 Additionally, there’s also no data engineers and no data pipelines, no cloud, the data is all scattered in files and folders. As mentioned, the company is not in Cloud yet, so no computing power either. Working as an ML engineer right out of bachelors and not having a team or even a mentor has been kind of rough to say the least. &lt;/p&gt;

&lt;p&gt;So here I am asking for advise- I really am not sure what to do next. Whether to get an MS in a similar field (data analytics/ML) and then switch my job or to switch first and get some more experience and then think about doing MS. &lt;/p&gt;

&lt;p&gt;I can tell you where I want to be though- currently I’m doing core algorithm building and prediction modelling on moving parts but I think I want to venture towards the customer facing stuff – more on the lines of Market analysis or Product analysis. &lt;/p&gt;

&lt;p&gt;I guess I’m here because I need some advice- any advice helps. Tell me about your journey into ML/ data analytics- what has worked for you what hasn’t. Any certifications or online programs you recommend to thrive and get noticed for more opportunities and grow in the field. Any MS programs that are good and worth exploring. Any advice that allows me to figure out what to do next helps. &lt;/p&gt;

&lt;p&gt;Thank you for taking your time reading this and leaving a comment!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o233zs,True,,goketchumall,,10,True,all_ads,False,[],False,,/r/datascience/comments/o233zs/recent_graduate_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o233zs/recent_graduate_in_data_science/,515405,1623951994.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all,

The project that I’m working right now is about image recognition. For the data pipeline we are using the following pipeline:
- Take photos with our cellphones
- Divide them into batches of 100 (called tasks), Upload them into CVAT (hosted in a aws machine) and label them
- Download the images/labels to our local machines
- Add some additional metadata to the labels and upload them to a aws s3 bucket. 

This has some manual labor as there is label happening all the time and it would be desirable that the s3 bucket would have the updated information without much of an hassle. 

What would you suggest in this case? What tools that you would think to automate this process?

I think that the bottleneck here is CVAT which only works with the creation of “tasks” and each one of them can’t have a lot of images since it will make the application slow. 

Much thanks.",t2_e7fj1,False,,0,False,What would be the best workflow for this image dataset use case,[],r/datascience,False,6,discussion,0,,,False,t3_o271l3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623990653.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;The project that I’m working right now is about image recognition. For the data pipeline we are using the following pipeline:
- Take photos with our cellphones
- Divide them into batches of 100 (called tasks), Upload them into CVAT (hosted in a aws machine) and label them
- Download the images/labels to our local machines
- Add some additional metadata to the labels and upload them to a aws s3 bucket. &lt;/p&gt;

&lt;p&gt;This has some manual labor as there is label happening all the time and it would be desirable that the s3 bucket would have the updated information without much of an hassle. &lt;/p&gt;

&lt;p&gt;What would you suggest in this case? What tools that you would think to automate this process?&lt;/p&gt;

&lt;p&gt;I think that the bottleneck here is CVAT which only works with the creation of “tasks” and each one of them can’t have a lot of images since it will make the application slow. &lt;/p&gt;

&lt;p&gt;Much thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o271l3,True,,NewKlear09,,0,True,all_ads,False,[],False,,/r/datascience/comments/o271l3/what_would_be_the_best_workflow_for_this_image/,all_ads,False,https://www.reddit.com/r/datascience/comments/o271l3/what_would_be_the_best_workflow_for_this_image/,515405,1623961853.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all. I’m working on a project that requires a story or vision. I don’t want it to look crude but don’t need to spend a ton of time on it. What’s a good compromise to start with?

I figure a good starting point is a step up from PowerPoint. No disrespect to PPT but…

Thanks!",t2_2o0q5m4h,False,,0,False,How do you create storyboards?,[],r/datascience,False,6,tooling,0,,,False,t3_o29mq6,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1623997503.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I’m working on a project that requires a story or vision. I don’t want it to look crude but don’t need to spend a ton of time on it. What’s a good compromise to start with?&lt;/p&gt;

&lt;p&gt;I figure a good starting point is a step up from PowerPoint. No disrespect to PPT but…&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o29mq6,True,,rotterdamn8,,5,True,all_ads,False,[],False,,/r/datascience/comments/o29mq6/how_do_you_create_storyboards/,all_ads,False,https://www.reddit.com/r/datascience/comments/o29mq6/how_do_you_create_storyboards/,515405,1623968703.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"So I'm actually applying to Data Engineering jobs, so may not even be relevant.

However I'm never sure when it's okay to actually add something to your resume.

In the case of R, I only spent maybe a couple months on it. I learned the Tidyverse package, including Dplyr.

I also created a single project which included various graphs, along with decision trees, linear regression, and clustering algorithms using the caret package.

But I wouldn't say I'm proficient I don't think (I'm just good at Googling stuff).

I don't know if it's still okay to just stick it in with my list of skills.",t2_cmy227mg,False,,0,False,How good at R or Python do you have to be before you add it to your resume?,[],r/datascience,False,6,,0,,,False,t3_o12a74,False,dark,0.95,,public,297,0,{},,,False,[],,False,False,,{},Job Search,False,297,,False,False,self,False,,[],{},,True,,1623870317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m actually applying to Data Engineering jobs, so may not even be relevant.&lt;/p&gt;

&lt;p&gt;However I&amp;#39;m never sure when it&amp;#39;s okay to actually add something to your resume.&lt;/p&gt;

&lt;p&gt;In the case of R, I only spent maybe a couple months on it. I learned the Tidyverse package, including Dplyr.&lt;/p&gt;

&lt;p&gt;I also created a single project which included various graphs, along with decision trees, linear regression, and clustering algorithms using the caret package.&lt;/p&gt;

&lt;p&gt;But I wouldn&amp;#39;t say I&amp;#39;m proficient I don&amp;#39;t think (I&amp;#39;m just good at Googling stuff).&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s still okay to just stick it in with my list of skills.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,o12a74,True,,SWE-Aaron,,107,True,all_ads,False,[],False,,/r/datascience/comments/o12a74/how_good_at_r_or_python_do_you_have_to_be_before/,all_ads,False,https://www.reddit.com/r/datascience/comments/o12a74/how_good_at_r_or_python_do_you_have_to_be_before/,515405,1623841517.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,When using XGBregressor to predict propensity scores on a binary classification (eg propensity to convert/churn etc). Would the best performance metric be a regression metric like mae or a classification metric like log loss?,t2_3wvr93n,False,,0,False,Best performance metric for XGBregressor?,[],r/datascience,False,6,discussion,0,,,False,t3_o26s33,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623989971.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When using XGBregressor to predict propensity scores on a binary classification (eg propensity to convert/churn etc). Would the best performance metric be a regression metric like mae or a classification metric like log loss?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o26s33,True,,andrew__jason,,5,True,all_ads,False,[],False,,/r/datascience/comments/o26s33/best_performance_metric_for_xgbregressor/,all_ads,False,https://www.reddit.com/r/datascience/comments/o26s33/best_performance_metric_for_xgbregressor/,515405,1623961171.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’ve been working in this field at a consulting firm for 2 years now, and a question that always rests heavy on my mind is whether I should be applying my skills to a sector that has more direct social impact. The need for data analytics/science seems to follow the money trail, as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it. 

There are definitely options to do data science in a more impactful way (e.g. non-profits, environmental data, life sciences data), but I have the overall impression that these fields pay less. More importantly, it can be frustrating to do DS for due to the lack of work done previously, leading to messy and lots of unclean/scattered data. In other words, because the majority of people take the higher paying jobs over the ones with higher social impact, the high social impact data roles and ecosystems remain underdeveloped.

My question is: how do you grapple with this reality?

Do you grind at a corporate job and donate part of your income? Do you teach on the side? Answer a lot of Stack Overflow questions? Invest it in your children’s future? Some pro bono DS work for non-profits in your free time? 

Or just ignore all of it, because life is short, and you worked incredibly hard to get to where you’re at?",t2_138g5d,False,,0,False,Grappling with the social impact of data-related careers,[],r/datascience,False,6,discussion,0,,,False,t3_o1abco,False,dark,0.79,,public,16,1,{},,,False,[],,False,False,,{},Discussion,False,16,,False,False,self,False,,[],{},,True,,1623893319.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’ve been working in this field at a consulting firm for 2 years now, and a question that always rests heavy on my mind is whether I should be applying my skills to a sector that has more direct social impact. The need for data analytics/science seems to follow the money trail, as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it. &lt;/p&gt;

&lt;p&gt;There are definitely options to do data science in a more impactful way (e.g. non-profits, environmental data, life sciences data), but I have the overall impression that these fields pay less. More importantly, it can be frustrating to do DS for due to the lack of work done previously, leading to messy and lots of unclean/scattered data. In other words, because the majority of people take the higher paying jobs over the ones with higher social impact, the high social impact data roles and ecosystems remain underdeveloped.&lt;/p&gt;

&lt;p&gt;My question is: how do you grapple with this reality?&lt;/p&gt;

&lt;p&gt;Do you grind at a corporate job and donate part of your income? Do you teach on the side? Answer a lot of Stack Overflow questions? Invest it in your children’s future? Some pro bono DS work for non-profits in your free time? &lt;/p&gt;

&lt;p&gt;Or just ignore all of it, because life is short, and you worked incredibly hard to get to where you’re at?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'award_1f0462ee-18f5-4f33-89cf-f1f79336a452', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=16&amp;height=16&amp;auto=webp&amp;s=3ca7dc1f4e12ca386a561446e72f772d38ba49d8', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=32&amp;height=32&amp;auto=webp&amp;s=c19d1e661e4aa6a9326a9f0b74b3ebf5d9f7a75e', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=48&amp;height=48&amp;auto=webp&amp;s=ed063580825e72b0ae63fe30c807b453b1362694', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=64&amp;height=64&amp;auto=webp&amp;s=7176b4b72b850e3e052138fe8b3967c4c5b52dae', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=128&amp;height=128&amp;auto=webp&amp;s=f7b307840995777f9ae04699d019740658ba0e77', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing. Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome (Pro)', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=16&amp;height=16&amp;auto=webp&amp;s=3ca7dc1f4e12ca386a561446e72f772d38ba49d8', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=32&amp;height=32&amp;auto=webp&amp;s=c19d1e661e4aa6a9326a9f0b74b3ebf5d9f7a75e', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=48&amp;height=48&amp;auto=webp&amp;s=ed063580825e72b0ae63fe30c807b453b1362694', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=64&amp;height=64&amp;auto=webp&amp;s=7176b4b72b850e3e052138fe8b3967c4c5b52dae', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=128&amp;height=128&amp;auto=webp&amp;s=f7b307840995777f9ae04699d019740658ba0e77', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o1abco,True,,SubtleCoconut,,22,True,all_ads,False,[],False,,/r/datascience/comments/o1abco/grappling_with_the_social_impact_of_datarelated/,all_ads,False,https://www.reddit.com/r/datascience/comments/o1abco/grappling_with_the_social_impact_of_datarelated/,515405,1623864519.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have worked with R as my primary language dealing with everything from econometrics/ML applications, GPS analysis using APIs, NLP contract analysis and dealing with unstructured data. I would say if I have a problem and can use R then I can likely devise some way to solve it.

&amp;#x200B;

However, I want to move into a new role and I am finding despite my 5+ years with R as my primary language that Python is more preferred when I talk with recruiters, so I have picked it up and I was hoping to get some guidance with good examples of code. In R it is very easy and code 'flows' between statements using the pipe operator, but python I am finding it less intuitive from trial and error to do multistep aggregations, data summarization, etc. 

&amp;#x200B;

Anyone have a good guide of how to use python for complex data summaries? I want to be able to something like ifelse(col1 %in% \[""phrase1"", ""phrase2""\], Col2, NA) and get a count of how many unique values pass that logic. And I might have a half dozen conditional things like this as I have to evaluate many similar statistics for different time periods quite often. Kaggle has some data cleaning, but that data is perfect compared to some of my sources as a consultant.

&amp;#x200B;

&amp;#x200B;

Obviously this below chunk of code should be broken up across multiple lines, correct?

`df_BorState = df[df.BorrowerCity==""San Francisco""].groupby( [""BorrowerState"", ""BorrowerCity"", ""BorrowerZip""] ).agg( Tot_Amount = ('CurrentApprovalAmount', 'sum') ).sort_values(""Tot_Amount"", ascending = False)`

&amp;#x200B;

Is something like this standardized?

`df_BorState = df[df.BorrowerCity==""San Francisco""].groupby(` 

`[""BorrowerState"", ""BorrowerCity"", ""BorrowerZip""] ).agg(` 

`Tot_Amount = ('CurrentApprovalAmount', 'sum') ).sort_values(`

`""Tot_Amount"", ascending = False)`",t2_6phog,False,,0,False,Data Wrangling - Multiline Python Statements? Trying to learn best syntax for Python coming from R,[],r/datascience,False,6,projects,0,,,False,t3_o18wu3,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},Projects,False,9,,False,False,self,False,,[],{},,True,,1623889817.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have worked with R as my primary language dealing with everything from econometrics/ML applications, GPS analysis using APIs, NLP contract analysis and dealing with unstructured data. I would say if I have a problem and can use R then I can likely devise some way to solve it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, I want to move into a new role and I am finding despite my 5+ years with R as my primary language that Python is more preferred when I talk with recruiters, so I have picked it up and I was hoping to get some guidance with good examples of code. In R it is very easy and code &amp;#39;flows&amp;#39; between statements using the pipe operator, but python I am finding it less intuitive from trial and error to do multistep aggregations, data summarization, etc. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyone have a good guide of how to use python for complex data summaries? I want to be able to something like ifelse(col1 %in% [&amp;quot;phrase1&amp;quot;, &amp;quot;phrase2&amp;quot;], Col2, NA) and get a count of how many unique values pass that logic. And I might have a half dozen conditional things like this as I have to evaluate many similar statistics for different time periods quite often. Kaggle has some data cleaning, but that data is perfect compared to some of my sources as a consultant.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Obviously this below chunk of code should be broken up across multiple lines, correct?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df_BorState = df[df.BorrowerCity==&amp;quot;San Francisco&amp;quot;].groupby( [&amp;quot;BorrowerState&amp;quot;, &amp;quot;BorrowerCity&amp;quot;, &amp;quot;BorrowerZip&amp;quot;] ).agg( Tot_Amount = (&amp;#39;CurrentApprovalAmount&amp;#39;, &amp;#39;sum&amp;#39;) ).sort_values(&amp;quot;Tot_Amount&amp;quot;, ascending = False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is something like this standardized?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df_BorState = df[df.BorrowerCity==&amp;quot;San Francisco&amp;quot;].groupby(&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;[&amp;quot;BorrowerState&amp;quot;, &amp;quot;BorrowerCity&amp;quot;, &amp;quot;BorrowerZip&amp;quot;] ).agg(&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tot_Amount = (&amp;#39;CurrentApprovalAmount&amp;#39;, &amp;#39;sum&amp;#39;) ).sort_values(&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;Tot_Amount&amp;quot;, ascending = False)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o18wu3,True,,Unhelpful_Scientist,,14,True,all_ads,False,[],False,,/r/datascience/comments/o18wu3/data_wrangling_multiline_python_statements_trying/,all_ads,False,https://www.reddit.com/r/datascience/comments/o18wu3/data_wrangling_multiline_python_statements_trying/,515405,1623861017.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"EDIT:  /u/pedrosorio Makes good points.  I've changed the prior rankings to reflect the team ELOs from the start of qualifying, and have also included UEFA nations games.  What I really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  The model is good enough for me for now.  I appreciate all your comments though, so please share.

Last week, I posted some predictions for the 2020 Euro.  Now that the first round is over, we can examine some of my performance.

My predictions and results for the first round are shown in [this](https://i.imgur.com/qkY6SQc.png) table (sorry it isn't prettier).  I achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  My multiclass ROC for predicting the outcome is 0.77.  In short, in the first 12 games I perform slightly better than random guessing (which is honestly fine for me).  However, most people who have watched international football wouldn't assign all match events as equally likely (is Italy drawing Turkey really as probable as Italy losing to Turkey?  No).  Its hard for me to measure against a ""reasonable guesser"".  My work pool records all our guesses, and so at the end of the group stage I can use that as a sort of ensemble method to compare against.  We'll see.

[Here](https://i.imgur.com/mZaivJY.png) are match predictions for the remaining group stage games conditioned on the results of the first games.  The model is not perfect, and still makes some weird predictions.  For example, Portugal is given higher probability to beat France than they are to beat Germany even though France beat Germany in the first round.  If you subscribe to some sort of sports law of transitivity, this may sound weird.

My predictions for the second round and the results of the first can be found [here](https://github.com/Dpananos/Euro2021Predictions/tree/main/predictions).",t2_131vu3d,False,,0,False,Euro 2020 Predictions Update,[],r/datascience,False,6,discussion,0,,,False,t3_o13xow,False,dark,0.85,,public,14,0,{},,,False,[],,False,False,,{},Discussion,False,14,,False,False,self,1623862907.0,modflair,[],{},,True,,1623876063.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;EDIT:  &lt;a href=""/u/pedrosorio""&gt;/u/pedrosorio&lt;/a&gt; Makes good points.  I&amp;#39;ve changed the prior rankings to reflect the team ELOs from the start of qualifying, and have also included UEFA nations games.  What I really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  The model is good enough for me for now.  I appreciate all your comments though, so please share.&lt;/p&gt;

&lt;p&gt;Last week, I posted some predictions for the 2020 Euro.  Now that the first round is over, we can examine some of my performance.&lt;/p&gt;

&lt;p&gt;My predictions and results for the first round are shown in &lt;a href=""https://i.imgur.com/qkY6SQc.png""&gt;this&lt;/a&gt; table (sorry it isn&amp;#39;t prettier).  I achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  My multiclass ROC for predicting the outcome is 0.77.  In short, in the first 12 games I perform slightly better than random guessing (which is honestly fine for me).  However, most people who have watched international football wouldn&amp;#39;t assign all match events as equally likely (is Italy drawing Turkey really as probable as Italy losing to Turkey?  No).  Its hard for me to measure against a &amp;quot;reasonable guesser&amp;quot;.  My work pool records all our guesses, and so at the end of the group stage I can use that as a sort of ensemble method to compare against.  We&amp;#39;ll see.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.imgur.com/mZaivJY.png""&gt;Here&lt;/a&gt; are match predictions for the remaining group stage games conditioned on the results of the first games.  The model is not perfect, and still makes some weird predictions.  For example, Portugal is given higher probability to beat France than they are to beat Germany even though France beat Germany in the first round.  If you subscribe to some sort of sports law of transitivity, this may sound weird.&lt;/p&gt;

&lt;p&gt;My predictions for the second round and the results of the first can be found &lt;a href=""https://github.com/Dpananos/Euro2021Predictions/tree/main/predictions""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,Data Scientist,[],False,,,,t5_2sptq,,,,o13xow,True,,__compactsupport__,,9,True,all_ads,False,[],False,dark,/r/datascience/comments/o13xow/euro_2020_predictions_update/,all_ads,False,https://www.reddit.com/r/datascience/comments/o13xow/euro_2020_predictions_update/,515405,1623847263.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?auto=webp&amp;s=913ae94abd4249ac0a8a5ef9514b9ecd6ceca699', 'width': 1974, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4826127422dcc33c904fdcfb14d072c69817bb3d', 'width': 108, 'height': 22}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=923efe3de1aa23f8264eb6160d91d6e8a2dc57d9', 'width': 216, 'height': 45}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61db3dd1f07d2ce17d4e8772e879653c0b0daa88', 'width': 320, 'height': 68}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4aa2160d74c2461538cc50bf34a3cdff25daae9e', 'width': 640, 'height': 136}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=400bd8683be55b7d294d2e6f8febc13b553231ce', 'width': 960, 'height': 204}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37b16a58d5f1a7c84fb8ad12cb25743e5f6237dd', 'width': 1080, 'height': 229}], 'variants': {}, 'id': 'Q3ECQTNHwHDcXOJH3MMWHh6eWyeUz-FxweRRKGRtIWU'}], 'enabled': False}",,,,,
,datascience,"Hello!

Sorry for another R, Python post. 

Recently I got a task at work in which I had to read through multiple sheets of excel, clean, transform, reshape it and make it into a single dataframe. I hadn't done this type of task in either R or Python. Since, it was not really a time constraint task, I decided to do it in both and learn how to do it in both languages. I am better with R than Python. I barely know Python actually. So I started doing it in R and comfortably (with google) did it without taking much time. After that I tried it in Python but I'm still struggling to finish it. I will be able to do it but it's taking me significantly more time than R.

Is that just the learning curve of Python since I barely know the language or some things are just easier in R and I should just do it in the language I'm comfortable with? I'm afraid that that I'll never be able to learn Python like this and won't getting any interviews since I don't know how to do stuff in Python.

Thanks! Sorry for the long post.",t2_bv171ji2,False,,0,False,Does knowing R instead of Python makes you unhireable?,[],r/datascience,False,6,discussion,0,,,False,t3_o0neg0,False,dark,0.86,,public,255,2,{},,,False,[],,False,False,,{},Discussion,False,255,,False,False,self,False,,[],{},,True,,1623816582.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Sorry for another R, Python post. &lt;/p&gt;

&lt;p&gt;Recently I got a task at work in which I had to read through multiple sheets of excel, clean, transform, reshape it and make it into a single dataframe. I hadn&amp;#39;t done this type of task in either R or Python. Since, it was not really a time constraint task, I decided to do it in both and learn how to do it in both languages. I am better with R than Python. I barely know Python actually. So I started doing it in R and comfortably (with google) did it without taking much time. After that I tried it in Python but I&amp;#39;m still struggling to finish it. I will be able to do it but it&amp;#39;s taking me significantly more time than R.&lt;/p&gt;

&lt;p&gt;Is that just the learning curve of Python since I barely know the language or some things are just easier in R and I should just do it in the language I&amp;#39;m comfortable with? I&amp;#39;m afraid that that I&amp;#39;ll never be able to learn Python like this and won&amp;#39;t getting any interviews since I don&amp;#39;t know how to do stuff in Python.&lt;/p&gt;

&lt;p&gt;Thanks! Sorry for the long post.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0neg0,True,,quite--average,,182,True,all_ads,False,[],False,,/r/datascience/comments/o0neg0/does_knowing_r_instead_of_python_makes_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0neg0/does_knowing_r_instead_of_python_makes_you/,515405,1623787782.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Suppose you have a predictive model that has been used for a few years - at what point do people decide to make changes to the model (e.g. add new variables, retrain the model with new data)? Is there a standard procedure for doing this? Are old and new models often run in parallel? 

I would be curious to hear how this problem is being handled across the industry.

Thanks",t2_o4xj9,False,,0,False,When to update models?,[],r/datascience,False,6,discussion,0,,,False,t3_o17b6n,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623885520.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you have a predictive model that has been used for a few years - at what point do people decide to make changes to the model (e.g. add new variables, retrain the model with new data)? Is there a standard procedure for doing this? Are old and new models often run in parallel? &lt;/p&gt;

&lt;p&gt;I would be curious to hear how this problem is being handled across the industry.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o17b6n,True,,blueest,,9,True,all_ads,False,[],False,,/r/datascience/comments/o17b6n/when_to_update_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o17b6n/when_to_update_models/,515405,1623856720.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Do you think that remote working for data science or analytics roles will be a thing post-covid too? I know that it cannot be like the past year that every job was remote, but will many jobs keep the remote status in the following years regardless of covid?",t2_2h9gdmeu,False,,0,False,remote work,[],r/datascience,False,6,discussion,0,,,False,t3_o11rn9,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623868311.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you think that remote working for data science or analytics roles will be a thing post-covid too? I know that it cannot be like the past year that every job was remote, but will many jobs keep the remote status in the following years regardless of covid?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o11rn9,True,,Hipocampus777,,9,True,all_ads,False,[],False,,/r/datascience/comments/o11rn9/remote_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/o11rn9/remote_work/,515405,1623839511.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a list of x,y coordinates  that represent the location of cells in a slice of tissue.  I can generate a scatterplot of that data and make a map.  Now I want to remove a bunch of those points because they are not trustworthy.   I know how to do that 1 point at a time, but I have thousands of points that need to be removed on each of hundreds of scatterplots.  

Do you know if there is any way to highlight a lot of points at once on a scatterplot and then just delete them from the data set?

It doesn't matter what program/platform is used to do this.

It seems like a straightforward question but I can't find any way to do it, nor anyone who does.",t2_3u89r,False,,0,False,How to remove data from a scatterplot,[],r/datascience,False,6,discussion,0,,,False,t3_o0ztez,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623859863.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a list of x,y coordinates  that represent the location of cells in a slice of tissue.  I can generate a scatterplot of that data and make a map.  Now I want to remove a bunch of those points because they are not trustworthy.   I know how to do that 1 point at a time, but I have thousands of points that need to be removed on each of hundreds of scatterplots.  &lt;/p&gt;

&lt;p&gt;Do you know if there is any way to highlight a lot of points at once on a scatterplot and then just delete them from the data set?&lt;/p&gt;

&lt;p&gt;It doesn&amp;#39;t matter what program/platform is used to do this.&lt;/p&gt;

&lt;p&gt;It seems like a straightforward question but I can&amp;#39;t find any way to do it, nor anyone who does.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0ztez,True,,synzeta,,19,True,all_ads,False,[],False,,/r/datascience/comments/o0ztez/how_to_remove_data_from_a_scatterplot/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0ztez/how_to_remove_data_from_a_scatterplot/,515405,1623831063.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

As the title suggests, off late, I have been struggling to communicate with non-technical stakeholders efficiently. For example, there was this one time where one of the stakeholders asked me to explain what a model did and I ended up combining a few technical terms together in a sentence which ended up confusing the stakeholder even more.

It is not that I suck at communicating in general. If I were to have a conversation regarding a simple analysis/explaining some viz, I do it pretty well. But, when it comes to explaining something more technical to someone with a non-technical background (like the inner workings of the model), I mess up big time.

My initial thought was that I might be messing up the explanations because I don't understand the models well enough, and I have been reading up all the basics from scratch in hopes that it would help me become better at explaining concepts. 

While I do continue my re-reading of the basics, are there any other ways that I could improve my technical explanations? Thanks!",t2_68s0c143,False,,0,False,How does one become a better communicator?,[],r/datascience,False,6,career,0,,,False,t3_o0koo8,False,dark,0.93,,public,48,0,{},,,False,[],,False,False,,{},Career,False,48,,False,False,self,False,,[],{},,True,,1623809367.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;As the title suggests, off late, I have been struggling to communicate with non-technical stakeholders efficiently. For example, there was this one time where one of the stakeholders asked me to explain what a model did and I ended up combining a few technical terms together in a sentence which ended up confusing the stakeholder even more.&lt;/p&gt;

&lt;p&gt;It is not that I suck at communicating in general. If I were to have a conversation regarding a simple analysis/explaining some viz, I do it pretty well. But, when it comes to explaining something more technical to someone with a non-technical background (like the inner workings of the model), I mess up big time.&lt;/p&gt;

&lt;p&gt;My initial thought was that I might be messing up the explanations because I don&amp;#39;t understand the models well enough, and I have been reading up all the basics from scratch in hopes that it would help me become better at explaining concepts. &lt;/p&gt;

&lt;p&gt;While I do continue my re-reading of the basics, are there any other ways that I could improve my technical explanations? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0koo8,True,,poplunoir,,23,True,all_ads,False,[],False,,/r/datascience/comments/o0koo8/how_does_one_become_a_better_communicator/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0koo8/how_does_one_become_a_better_communicator/,515405,1623780567.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"at a first glance LP seems promising...but honestly the techniques seem really dated....there must be modern tools vs using a simplex method to optimize.

appreciate any insight, and experiences shared

thanks",t2_4jdxx,False,,0,False,Is linear programming part of DS?,[],r/datascience,False,6,discussion,0,,,False,t3_o06z5u,False,dark,0.88,,public,127,0,{},,,False,[],,False,False,,{},Discussion,False,127,,False,False,self,False,,[],{},,True,,1623764863.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;at a first glance LP seems promising...but honestly the techniques seem really dated....there must be modern tools vs using a simplex method to optimize.&lt;/p&gt;

&lt;p&gt;appreciate any insight, and experiences shared&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o06z5u,True,,DeeJayCruiser,,60,True,all_ads,False,[],False,,/r/datascience/comments/o06z5u/is_linear_programming_part_of_ds/,all_ads,False,https://www.reddit.com/r/datascience/comments/o06z5u/is_linear_programming_part_of_ds/,515405,1623736063.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've started a new job as a Sr. Data Analyst. I am ramping up and am about a month into the position. Ramping up in the pandemic is a different experience than when I ramped up in a previous role in the office (obviously). The most notable part is knowing when to reach out to others and seek out that sort of 'whiteboard' session.

I feel that I am missing that now and don't have a great feel for my team. I have a project now but ultimately I feel disconnected. I am looking to do more catch ups and understand that with time rapport builds. Slack is part of the culture, but the group chat isn't too active.

When I have ramped up other individuals in the past I try to meet often and be available for their questions. I try to also have specific times to check in to help facilitate communication and let the person know there isn't a 'quota' to their questions.

What are ways others have been able to ramp up in the pandemic or fully remote? Any tips?

Edit: grammar",t2_d1rr0,False,,0,False,Ramping Up Help from Coworkers,[],r/datascience,False,6,discussion,0,,,False,t3_o0oz27,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,1623953680.0,,[],{},,True,,1623821035.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve started a new job as a Sr. Data Analyst. I am ramping up and am about a month into the position. Ramping up in the pandemic is a different experience than when I ramped up in a previous role in the office (obviously). The most notable part is knowing when to reach out to others and seek out that sort of &amp;#39;whiteboard&amp;#39; session.&lt;/p&gt;

&lt;p&gt;I feel that I am missing that now and don&amp;#39;t have a great feel for my team. I have a project now but ultimately I feel disconnected. I am looking to do more catch ups and understand that with time rapport builds. Slack is part of the culture, but the group chat isn&amp;#39;t too active.&lt;/p&gt;

&lt;p&gt;When I have ramped up other individuals in the past I try to meet often and be available for their questions. I try to also have specific times to check in to help facilitate communication and let the person know there isn&amp;#39;t a &amp;#39;quota&amp;#39; to their questions.&lt;/p&gt;

&lt;p&gt;What are ways others have been able to ramp up in the pandemic or fully remote? Any tips?&lt;/p&gt;

&lt;p&gt;Edit: grammar&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0oz27,True,,NameNumber7,,1,True,all_ads,False,[],False,,/r/datascience/comments/o0oz27/ramping_up_help_from_coworkers/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0oz27/ramping_up_help_from_coworkers/,515405,1623792235.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_34qgdyb6,False,,0,False,"I have a large dataset (100 mil rows) in Russian, I want to translate it into English. I was using Googletranslate API, which was showing error coz of a high number of requests. Is there anything else I can do??",[],r/datascience,False,6,projects,0,,,False,t3_o08ue4,False,dark,0.77,,public,16,0,{},,,False,[],,False,False,,{},Projects,False,16,,False,False,self,False,,[],{},,True,,1623772043.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o08ue4,True,,yaakarsh1011,,18,True,all_ads,False,[],False,,/r/datascience/comments/o08ue4/i_have_a_large_dataset_100_mil_rows_in_russian_i/,all_ads,False,https://www.reddit.com/r/datascience/comments/o08ue4/i_have_a_large_dataset_100_mil_rows_in_russian_i/,515405,1623743243.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m a rising senior currently interning at a decent company as swe, but I recently got offered a fall internship at a lower tier company in data science. I’m really interested in data science and haven’t really been exposed to it, but I’m worried this will look like a step down resume wise. The pay for the ds role is about half as much as my current swe role too for context. If I took the ds role, I would have less time on school as well since it’s in the fall. So would it just be more worthwhile to focus on school and maybe research there than this internship? I would still apply to data science jobs in the future regardless. Any thoughts would be appreciated!",t2_5bpl12gd,False,,0,False,Is it worth taking a lower tier DS internship after mid tier SWE internship if I’m interested in DS?,[],r/datascience,False,6,career,0,,,False,t3_o0m6w9,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623813326.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a rising senior currently interning at a decent company as swe, but I recently got offered a fall internship at a lower tier company in data science. I’m really interested in data science and haven’t really been exposed to it, but I’m worried this will look like a step down resume wise. The pay for the ds role is about half as much as my current swe role too for context. If I took the ds role, I would have less time on school as well since it’s in the fall. So would it just be more worthwhile to focus on school and maybe research there than this internship? I would still apply to data science jobs in the future regardless. Any thoughts would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0m6w9,True,,piledriver42069,,6,True,all_ads,False,[],False,,/r/datascience/comments/o0m6w9/is_it_worth_taking_a_lower_tier_ds_internship/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0m6w9/is_it_worth_taking_a_lower_tier_ds_internship/,515405,1623784526.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"If given some say in setting a job title, for someone who does both (and will be expected to do both in the role), would you go for a merged title like ""Data Scientist &amp; Engineer"", or just pick one? Context: data scientist roles are already on the resume, so goal is to highlight the additional skill/responsibility, if appropriate.",t2_64vd4,False,,0,False,Choosing a combined title? E.g. Data Scientist &amp; Engineer,[],r/datascience,False,6,career,0,,,False,t3_o0p64q,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623821599.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If given some say in setting a job title, for someone who does both (and will be expected to do both in the role), would you go for a merged title like &amp;quot;Data Scientist &amp;amp; Engineer&amp;quot;, or just pick one? Context: data scientist roles are already on the resume, so goal is to highlight the additional skill/responsibility, if appropriate.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0p64q,True,,le_sacre,,5,True,all_ads,False,[],False,,/r/datascience/comments/o0p64q/choosing_a_combined_title_eg_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0p64q/choosing_a_combined_title_eg_data_scientist/,515405,1623792799.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm pretty happy with my full time job as a senior model developer (but I do a lot of analytics on model results as well), including the steady paycheck and job security it comes with. But sometimes I don't get to do the most interesting work so I think about getting into part-time freelancing. I had one person in my network pitch me an app idea for his small business. I had a conversation with their ceo, he seemed super interested, pitched me some requirements, answered some questions. I then roughed out the specific work required, how much time it would take, and used $50/hour as my baseline pay needed even though that's below what my regular job pays. Normally I'd need more for working beyond my full-time job because it would require sacrificing my social life, but the project was extremely interesting and could have led to a good relationship and more work down the road. But when he saw my estimate that it'd take 200 hours to complete aka $10k he said my estimates all made sense but thanks but no thanks. The fact that he didn't even attempt to counter makes me think he was expecting to pay like $1k for it, even though the guy who pitched the idea to me said that for $10k they'd recoup costs in a few months because they paid contractors to manually do the work my app would have automated, plus they lose money on mistakes the contractors regularly make.

&amp;#x200B;

So my question is what's the market like for work like this? Are people actually able to build custom data-driven apps where they're paid 5 figures for completed projects? Or is my experience more typical where there's just a sticker shock? What kind of clients are paying this? How do you get your foot in the door? Would a good github or personal page with personal project examples be valuable?",t2_6xi2clnm,False,,0,False,Finding Freelancing Opportunities,[],r/datascience,False,6,network,0,,,False,t3_o0oxpm,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Networking,False,1,,False,False,self,False,,[],{},,True,,1623820932.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m pretty happy with my full time job as a senior model developer (but I do a lot of analytics on model results as well), including the steady paycheck and job security it comes with. But sometimes I don&amp;#39;t get to do the most interesting work so I think about getting into part-time freelancing. I had one person in my network pitch me an app idea for his small business. I had a conversation with their ceo, he seemed super interested, pitched me some requirements, answered some questions. I then roughed out the specific work required, how much time it would take, and used $50/hour as my baseline pay needed even though that&amp;#39;s below what my regular job pays. Normally I&amp;#39;d need more for working beyond my full-time job because it would require sacrificing my social life, but the project was extremely interesting and could have led to a good relationship and more work down the road. But when he saw my estimate that it&amp;#39;d take 200 hours to complete aka $10k he said my estimates all made sense but thanks but no thanks. The fact that he didn&amp;#39;t even attempt to counter makes me think he was expecting to pay like $1k for it, even though the guy who pitched the idea to me said that for $10k they&amp;#39;d recoup costs in a few months because they paid contractors to manually do the work my app would have automated, plus they lose money on mistakes the contractors regularly make.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So my question is what&amp;#39;s the market like for work like this? Are people actually able to build custom data-driven apps where they&amp;#39;re paid 5 figures for completed projects? Or is my experience more typical where there&amp;#39;s just a sticker shock? What kind of clients are paying this? How do you get your foot in the door? Would a good github or personal page with personal project examples be valuable?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0oxpm,True,,yoi12321,,6,True,all_ads,False,[],False,,/r/datascience/comments/o0oxpm/finding_freelancing_opportunities/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0oxpm/finding_freelancing_opportunities/,515405,1623792132.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I joined a new organization in January, and I am now the only member of the data team remaining. 

The bad - My boss, the data engineer, and two analysts (the whole team besides me) have all already resigned or are going to be leaving by the end of the month.  Some senior leaders in other departments have also resigned in the last few weeks. Retention has apparently been an issue here for many years, although I have had a pretty pleasant experience working with my colleagues and stakeholders so far. I have only met lots of bright, motivated people, so maybe they are all just getting poached by other companies. My pay is below market-rate. 

The good - We have an embedded consulting group that handles a lot of our project management, data engineering, and data analysis, and they are staying, if not expanding in the short-term. We also have a new team lead and department director starting soon. I interviewed the team lead, and have seen the resume of the director, and am confident I can learn a lot from both of them. The work we do here is incredibly interesting and meaningful and I am motivated to build great data products. 

Obviously this situation is a cause for concern. Has anyone weathered a similar situation that turned out for the best? Or is this likely going to be just miserable plodding along for the next year or two if I stay?",t2_pr6lcu2,False,,0,False,100% Turnover on Data Team,[],r/datascience,False,6,discussion,0,,,False,t3_nzsoi4,False,dark,0.97,,public,105,0,{},,,False,[],,False,False,,{},Discussion,False,105,,False,False,self,False,,[],{},,True,,1623722135.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I joined a new organization in January, and I am now the only member of the data team remaining. &lt;/p&gt;

&lt;p&gt;The bad - My boss, the data engineer, and two analysts (the whole team besides me) have all already resigned or are going to be leaving by the end of the month.  Some senior leaders in other departments have also resigned in the last few weeks. Retention has apparently been an issue here for many years, although I have had a pretty pleasant experience working with my colleagues and stakeholders so far. I have only met lots of bright, motivated people, so maybe they are all just getting poached by other companies. My pay is below market-rate. &lt;/p&gt;

&lt;p&gt;The good - We have an embedded consulting group that handles a lot of our project management, data engineering, and data analysis, and they are staying, if not expanding in the short-term. We also have a new team lead and department director starting soon. I interviewed the team lead, and have seen the resume of the director, and am confident I can learn a lot from both of them. The work we do here is incredibly interesting and meaningful and I am motivated to build great data products. &lt;/p&gt;

&lt;p&gt;Obviously this situation is a cause for concern. Has anyone weathered a similar situation that turned out for the best? Or is this likely going to be just miserable plodding along for the next year or two if I stay?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzsoi4,True,,most_humblest_ever,,34,True,all_ads,False,[],False,,/r/datascience/comments/nzsoi4/100_turnover_on_data_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzsoi4/100_turnover_on_data_team/,515405,1623693335.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What do you focus on now, and how do you approach new ideas? 

Do you have a framework to protect yourself and your team from risky work?",t2_5e34w9d2,False,,0,False,"Data scientists in leadership positions, what are your strengths?",[],r/datascience,False,6,discussion,0,,,False,t3_nzhzxq,False,dark,0.99,,public,352,3,{},,,False,[],,False,False,,{},Discussion,False,352,,False,False,self,False,,[],{'gid_1': 1},,True,,1623687740.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you focus on now, and how do you approach new ideas? &lt;/p&gt;

&lt;p&gt;Do you have a framework to protect yourself and your team from risky work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzhzxq,True,,expatwithajetpack,,62,True,all_ads,False,[],False,,/r/datascience/comments/nzhzxq/data_scientists_in_leadership_positions_what_are/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzhzxq/data_scientists_in_leadership_positions_what_are/,515405,1623658940.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am looking for Python coding example for - recommender system for medicines. The closest I have come to is [this](https://www.kaggle.com/chocozzz/recommendation-medicines-by-using-a-review).

Can you suggest something better?

Thanks!",t2_2mmql89p,False,,0,False,Recommender System for Medicines - Code Example,[],r/datascience,False,6,discussion,0,,,False,t3_o0fxws,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623796860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for Python coding example for - recommender system for medicines. The closest I have come to is &lt;a href=""https://www.kaggle.com/chocozzz/recommendation-medicines-by-using-a-review""&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Can you suggest something better?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0fxws,True,,grid_world,,1,True,all_ads,False,[],False,,/r/datascience/comments/o0fxws/recommender_system_for_medicines_code_example/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0fxws/recommender_system_for_medicines_code_example/,515405,1623768060.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G7-gjDBRxBZWaTaVTvB0T4nZxHmSe__JeUiMaix8mbA.jpg?auto=webp&amp;s=c94aa6553da546b50c91ac16027480b89d933ee4', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'AV9ukwzWnwzwB0dFAXmSFzM_xMdA1yCD6CZAK1Cjyro'}], 'enabled': False}",,,,,
,datascience,"Suppose you are working on a supervised binary classification task. You have patient medical information (e.g. age, weight, gender, height, blood pressure, etc) and whether they have a certain disease or not (this is the response variable, ""yes"" or ""no""). Let's imagine that determining if patients have this disease is time consuming and costly - so a machine learning approach is being considered.

Let's assume that this disease is very rare. In your data set, only 1% of patients have this disease. Thus, the dataset is imbalanced. 

Intuitively, we know that any machine learning algorithm trained on this data will likely perform poorly. That is, the performance will likely be deceptive: you might get an accuracy of 99%, but misclassify all of the patients who have the disease. 

Mathematically speaking: is there any mathematical explanation for this very logical concept?

 E.g. if only study 1 hour for a chemistry exam, I might only learn how to solve 2-3 types of problems - thus, on a true/false style chemistry exam, there will be many questions that I don't know how to answer because I never saw them before, and I will be likely to perform badly on material that I have not prepared for. Do machine learning models work the same way?

For popular algorithms like neural networks, xgboost and random forest - can it be shown that for classification problems, you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance? 

On a more abstract side, I have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples. E.g. a 5 year old child can ""learn"" what is an ""elephant"" after seeing a few pictures of an elephant (e.g. it's perfectly reasonable to expect that a young child would see a picture of the cartoon character Dumbo and identify Dumbo as an elephant after coming back from a zoo), but a machine learning algorithm would likely need thousands and thousands of pictures of elephants (and likely require to see the same pictures upside down, inverted, with added noise, different color scheme, etc) prior to be able to generalize and learn the concept of an elephant. Perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease, since there are so few of them?

Does the above concept have anything to do with the ""bias-variance tradeoff""? Or is it just logic - if there is not enough variability and information within the data, the machine learning model just learns the ""noise"" within the dataset? I am really curious to see if such a threshold for measuring ""minimum level of variability within the data"" has ever been studied?

PS: in a 1 dimensional sense, on a number line, if you have a ""point"" at 3 and another ""point"" at 5 - you could consider all inferences outside of 3 and 5 as ""extrapolation"" and all inferences between 3 and 5 as  ""interpolation"". When dealing with higher dimensional data, could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as ""interpolation"" and observstions that are farther away as ""extrapolation""? In reality, can you just consider all prediction as extrapolation - small scale extrapolation for closer points, large scale extrapolation for further points?

Thanks",t2_3f0i9m72,False,,0,False,Dealing with imbalanced datasets,[],r/datascience,False,6,discussion,0,,,False,t3_o0exje,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623794111.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you are working on a supervised binary classification task. You have patient medical information (e.g. age, weight, gender, height, blood pressure, etc) and whether they have a certain disease or not (this is the response variable, &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot;). Let&amp;#39;s imagine that determining if patients have this disease is time consuming and costly - so a machine learning approach is being considered.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s assume that this disease is very rare. In your data set, only 1% of patients have this disease. Thus, the dataset is imbalanced. &lt;/p&gt;

&lt;p&gt;Intuitively, we know that any machine learning algorithm trained on this data will likely perform poorly. That is, the performance will likely be deceptive: you might get an accuracy of 99%, but misclassify all of the patients who have the disease. &lt;/p&gt;

&lt;p&gt;Mathematically speaking: is there any mathematical explanation for this very logical concept?&lt;/p&gt;

&lt;p&gt;E.g. if only study 1 hour for a chemistry exam, I might only learn how to solve 2-3 types of problems - thus, on a true/false style chemistry exam, there will be many questions that I don&amp;#39;t know how to answer because I never saw them before, and I will be likely to perform badly on material that I have not prepared for. Do machine learning models work the same way?&lt;/p&gt;

&lt;p&gt;For popular algorithms like neural networks, xgboost and random forest - can it be shown that for classification problems, you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance? &lt;/p&gt;

&lt;p&gt;On a more abstract side, I have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples. E.g. a 5 year old child can &amp;quot;learn&amp;quot; what is an &amp;quot;elephant&amp;quot; after seeing a few pictures of an elephant (e.g. it&amp;#39;s perfectly reasonable to expect that a young child would see a picture of the cartoon character Dumbo and identify Dumbo as an elephant after coming back from a zoo), but a machine learning algorithm would likely need thousands and thousands of pictures of elephants (and likely require to see the same pictures upside down, inverted, with added noise, different color scheme, etc) prior to be able to generalize and learn the concept of an elephant. Perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease, since there are so few of them?&lt;/p&gt;

&lt;p&gt;Does the above concept have anything to do with the &amp;quot;bias-variance tradeoff&amp;quot;? Or is it just logic - if there is not enough variability and information within the data, the machine learning model just learns the &amp;quot;noise&amp;quot; within the dataset? I am really curious to see if such a threshold for measuring &amp;quot;minimum level of variability within the data&amp;quot; has ever been studied?&lt;/p&gt;

&lt;p&gt;PS: in a 1 dimensional sense, on a number line, if you have a &amp;quot;point&amp;quot; at 3 and another &amp;quot;point&amp;quot; at 5 - you could consider all inferences outside of 3 and 5 as &amp;quot;extrapolation&amp;quot; and all inferences between 3 and 5 as  &amp;quot;interpolation&amp;quot;. When dealing with higher dimensional data, could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as &amp;quot;interpolation&amp;quot; and observstions that are farther away as &amp;quot;extrapolation&amp;quot;? In reality, can you just consider all prediction as extrapolation - small scale extrapolation for closer points, large scale extrapolation for further points?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0exje,True,,SQL_beginner,,7,True,all_ads,False,[],False,,/r/datascience/comments/o0exje/dealing_with_imbalanced_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0exje/dealing_with_imbalanced_datasets/,515405,1623765311.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in the situation where SME refuse to believe the model I created despite the good score (&gt;70% f1 score). Now Im clueless on how to convince them. 

As a background, I did my degree in the related field.  So, I have good understanding about the input data. Plus, via EDA, I can clearly see the seperation between classes in output. So, I'm not using Deep Learning to develop thr model, just simple logistic regression, hence the result is pretty easy to intepret and present.

Have you encountered this situation? How you go about convincing SME?",t2_8w8sc9b,False,,0,False,Convincing SME depite good result model,[],r/datascience,False,6,discussion,0,,,False,t3_o07hrm,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623766773.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the situation where SME refuse to believe the model I created despite the good score (&amp;gt;70% f1 score). Now Im clueless on how to convince them. &lt;/p&gt;

&lt;p&gt;As a background, I did my degree in the related field.  So, I have good understanding about the input data. Plus, via EDA, I can clearly see the seperation between classes in output. So, I&amp;#39;m not using Deep Learning to develop thr model, just simple logistic regression, hence the result is pretty easy to intepret and present.&lt;/p&gt;

&lt;p&gt;Have you encountered this situation? How you go about convincing SME?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o07hrm,True,,ampang_boy,,8,True,all_ads,False,[],False,,/r/datascience/comments/o07hrm/convincing_sme_depite_good_result_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/o07hrm/convincing_sme_depite_good_result_model/,515405,1623737973.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," 

There is a well-known algorithm in statistics called SMOTE (Synthetic Minority Over Sampling Technique) which is often used to ""balance"" and ""imbalanced"" data set:

[https://en.wikipedia.org/wiki/Oversampling\_and\_undersampling\_in\_data\_analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)

[https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/#:~:text=SMOTE%20(synthetic%20minority%20oversampling%20technique)%20is%20one%20of%20the%20most,instances%20between%20existing%20minority%20instances)

[https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)

If I have understood correctly, the premise of the SMOTE algorithm is as follows: Suppose you have a dataset containing information for medical patients that are ""healthy"" and ""not healthy"". But, let's assume that the majority of the patients within your dataset are ""healthy"" (the composition of healthy: not healthy being 95:5) . If you want to make a statistical model for this data, the data does not contain enough information for ""not healthy"" patients, and it will be very challenging to build a reliable statistical model that can make accurate predictions for ""not healthy"" patients. Thus, the SMOTE algorithm can fix this problem by:

1. ""rebalancing"" the data set (e.g. after SMOTE, your data set can have a composition of 70:30)
2. creating ""new"" data points from the ""existing"" data : as I understand, this is done by multiplying a given vector corresponding to a randomly selected individual observation, by some random number between 0 and 1.

This leads me to my question: Suppose you have already have a balanced dataset (e.g. the healthy: not healthy has a composition of 60:40), but let's assume that you have a relatively small dat set to begin with (e.g. 500 rows). Can you use the SMOTE algorithm to create new data points so that your dataset is bigger? I understand that no algorithm can magically compensate for data quality issues, but at the same time I don't see any major flaws with using SMOTE on already balanced data?

For reference, I illustrated this process below using R:

I would be interested in hearing a second opinion - Thanks!

    #load and install libraries 
    remotes::install_version(""DMwR"", version=""0.4.1"") 
    library(DMwR) 
    
     #create some fake data and put them into a data frame called ""f""  
    
    var_1&lt;- rnorm(100,1,4) 
    var_2 &lt;-rnorm(100,10,5)
     var_3&lt;- c(""0"",""2"", ""4"")
     var_3 &lt;- sample(var_3, 100, replace=TRUE, prob=c(0.3, 0.6, 0.1)) 
    
     response&lt;- c(""1"",""0"") 
    response &lt;- sample(response, 100, replace=TRUE, prob=c(0.3, 0.7)) 
    
     #put them into a data frame called ""f"" 
    
    f &lt;- data.frame(var_1, var_2, var_3, response) 
    
     #declare var_3 and response_variable as factors 
    f$var_3 = as.factor(f$var_3)
     f$response = as.factor(f$response)  
    
    #SMOTE algorithm  
    #simulate new points from the first class
    
     smoted_data_over &lt;- SMOTE(response~., f, perc.over=100) 
    
     #simulate new points from the second class 
    smoted_data_under &lt;- SMOTE(response~., f, perc.under=100)  
    
    #combine everything together into a final new data file 
    final &lt;-rbind(f, smoted_data_over, smoted_data_under)",t2_3f0i9m72,False,,0,False,Generating New Data Points with SMOTE,[],r/datascience,False,6,discussion,0,,,False,t3_o0e56q,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623791864.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There is a well-known algorithm in statistics called SMOTE (Synthetic Minority Over Sampling Technique) which is often used to &amp;quot;balance&amp;quot; and &amp;quot;imbalanced&amp;quot; data set:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis""&gt;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/#:%7E:text=SMOTE%20(synthetic%20minority%20oversampling%20technique""&gt;https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python&lt;/a&gt;%20is%20one%20of%20the%20most,instances%20between%20existing%20minority%20instances)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/""&gt;https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If I have understood correctly, the premise of the SMOTE algorithm is as follows: Suppose you have a dataset containing information for medical patients that are &amp;quot;healthy&amp;quot; and &amp;quot;not healthy&amp;quot;. But, let&amp;#39;s assume that the majority of the patients within your dataset are &amp;quot;healthy&amp;quot; (the composition of healthy: not healthy being 95:5) . If you want to make a statistical model for this data, the data does not contain enough information for &amp;quot;not healthy&amp;quot; patients, and it will be very challenging to build a reliable statistical model that can make accurate predictions for &amp;quot;not healthy&amp;quot; patients. Thus, the SMOTE algorithm can fix this problem by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&amp;quot;rebalancing&amp;quot; the data set (e.g. after SMOTE, your data set can have a composition of 70:30)&lt;/li&gt;
&lt;li&gt;creating &amp;quot;new&amp;quot; data points from the &amp;quot;existing&amp;quot; data : as I understand, this is done by multiplying a given vector corresponding to a randomly selected individual observation, by some random number between 0 and 1.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This leads me to my question: Suppose you have already have a balanced dataset (e.g. the healthy: not healthy has a composition of 60:40), but let&amp;#39;s assume that you have a relatively small dat set to begin with (e.g. 500 rows). Can you use the SMOTE algorithm to create new data points so that your dataset is bigger? I understand that no algorithm can magically compensate for data quality issues, but at the same time I don&amp;#39;t see any major flaws with using SMOTE on already balanced data?&lt;/p&gt;

&lt;p&gt;For reference, I illustrated this process below using R:&lt;/p&gt;

&lt;p&gt;I would be interested in hearing a second opinion - Thanks!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#load and install libraries 
remotes::install_version(&amp;quot;DMwR&amp;quot;, version=&amp;quot;0.4.1&amp;quot;) 
library(DMwR) 

 #create some fake data and put them into a data frame called &amp;quot;f&amp;quot;  

var_1&amp;lt;- rnorm(100,1,4) 
var_2 &amp;lt;-rnorm(100,10,5)
 var_3&amp;lt;- c(&amp;quot;0&amp;quot;,&amp;quot;2&amp;quot;, &amp;quot;4&amp;quot;)
 var_3 &amp;lt;- sample(var_3, 100, replace=TRUE, prob=c(0.3, 0.6, 0.1)) 

 response&amp;lt;- c(&amp;quot;1&amp;quot;,&amp;quot;0&amp;quot;) 
response &amp;lt;- sample(response, 100, replace=TRUE, prob=c(0.3, 0.7)) 

 #put them into a data frame called &amp;quot;f&amp;quot; 

f &amp;lt;- data.frame(var_1, var_2, var_3, response) 

 #declare var_3 and response_variable as factors 
f$var_3 = as.factor(f$var_3)
 f$response = as.factor(f$response)  

#SMOTE algorithm  
#simulate new points from the first class

 smoted_data_over &amp;lt;- SMOTE(response~., f, perc.over=100) 

 #simulate new points from the second class 
smoted_data_under &amp;lt;- SMOTE(response~., f, perc.under=100)  

#combine everything together into a final new data file 
final &amp;lt;-rbind(f, smoted_data_over, smoted_data_under)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0e56q,True,,SQL_beginner,,3,True,all_ads,False,[],False,,/r/datascience/comments/o0e56q/generating_new_data_points_with_smote/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0e56q/generating_new_data_points_with_smote/,515405,1623763064.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/apkmo5zs_-YmFFDjmG62TyIS5jvdW18a790XKORnEu8.jpg?auto=webp&amp;s=d430e4efd42baf58b4bc71c9549944b7bdf40715', 'width': 50, 'height': 39}, 'resolutions': [], 'variants': {}, 'id': 'K8Lxcv5aHkjcbujO-2N7cVXtdXkwYe8BHCOouTCCjb8'}], 'enabled': False}",,,,,
,datascience,"I\`ve been struggling to maintain version control of Jupyter Notebooks through pure Git because of all the issues of git dffing detecting cell output changes and stuff.

&amp;#x200B;

Do you use any specific tools to keep up a good gitlfow-like version control scheme of your data science jupyter notebooks?",t2_nkoag,False,,0,False,What do you use to version control Jupyter Notebooks?,[],r/datascience,False,6,discussion,0,,,False,t3_nzo5lk,False,dark,0.86,,public,26,0,{},,,False,[],,False,False,,{},Discussion,False,26,,False,False,self,False,,[],{},,True,,1623710009.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I`ve been struggling to maintain version control of Jupyter Notebooks through pure Git because of all the issues of git dffing detecting cell output changes and stuff.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Do you use any specific tools to keep up a good gitlfow-like version control scheme of your data science jupyter notebooks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzo5lk,True,,RoyalScores,,21,True,all_ads,False,[],False,,/r/datascience/comments/nzo5lk/what_do_you_use_to_version_control_jupyter/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzo5lk/what_do_you_use_to_version_control_jupyter/,515405,1623681209.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’m very much a beginner and learning R and Python but goodness do I prefer R. In future roles, will I be able to say that I prefer to use R over Python or is there not that kind of flexibility?",t2_5bg0t3mm,False,,0,False,Can I set a language preference?,[],r/datascience,False,6,career,0,,,False,t3_o01iho,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623746715.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m very much a beginner and learning R and Python but goodness do I prefer R. In future roles, will I be able to say that I prefer to use R over Python or is there not that kind of flexibility?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o01iho,True,,ThrowRA-11789,,25,True,all_ads,False,[],False,,/r/datascience/comments/o01iho/can_i_set_a_language_preference/,all_ads,False,https://www.reddit.com/r/datascience/comments/o01iho/can_i_set_a_language_preference/,515405,1623717915.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Do you have any favourite bloggers that are professionals and have high quality content that you love reading? Would love to add them to my reading list. Cheers.,t2_3mpdgvl7,False,,0,False,Any good personal blogs that has quality data science content?,[],r/datascience,False,6,discussion,0,,,False,t3_nzqoeo,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623716815.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you have any favourite bloggers that are professionals and have high quality content that you love reading? Would love to add them to my reading list. Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzqoeo,True,,benthecoderX,,15,True,all_ads,False,[],False,,/r/datascience/comments/nzqoeo/any_good_personal_blogs_that_has_quality_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzqoeo/any_good_personal_blogs_that_has_quality_data/,515405,1623688015.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I was looking into tableau online which seems fantastic, but is a little expensive for my needs.  Ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.

Anyone have another suggestions?

&amp;#x200B;

Thanks",t2_9s3bg,False,,0,False,Suggestions for BI visualization tool that supports live feed connection from BQ?,[],r/datascience,False,6,tooling,0,,,False,t3_nzyady,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623736965.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was looking into tableau online which seems fantastic, but is a little expensive for my needs.  Ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.&lt;/p&gt;

&lt;p&gt;Anyone have another suggestions?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzyady,True,,tcbjj,,8,True,all_ads,False,[],False,,/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/,515405,1623708165.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi 

Recently I'm doing my EDA and I found that my analyst is my weaknesses.

My friend said textbooks are the best and I can read more articles published on Towards Datascience, etc secondly. 

Just wondering how you guys improve analysis

I'd like to model some great analysts or collect those best analytical writings.

Would you like to share any best analytical articles or your favourite authors?


Thanks",t2_2xpkxipc,False,,0,False,How do you improve analytical thinking/writing?,[],r/datascience,False,6,discussion,0,,,False,t3_nzxqxm,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1623709743.0,,[],{},,True,,1623735494.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;m doing my EDA and I found that my analyst is my weaknesses.&lt;/p&gt;

&lt;p&gt;My friend said textbooks are the best and I can read more articles published on Towards Datascience, etc secondly. &lt;/p&gt;

&lt;p&gt;Just wondering how you guys improve analysis&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to model some great analysts or collect those best analytical writings.&lt;/p&gt;

&lt;p&gt;Would you like to share any best analytical articles or your favourite authors?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzxqxm,True,,homchange,,13,True,all_ads,False,[],False,,/r/datascience/comments/nzxqxm/how_do_you_improve_analytical_thinkingwriting/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzxqxm/how_do_you_improve_analytical_thinkingwriting/,515405,1623706694.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let’s say we’re a company like Spotify and want to recommend songs to you based on songs you’ve listened to.

Say we have a constraint that we don’t want to funnel all of our impressions to a small % of songs (which is likely to occur with user based collaborative filtering), but rather would prefer broad coverage of viewership across our whole library.

We want this to be personalized to each user and we want to iteratively learn as you continue to listen to new songs. 

How would you go about building this architecture? What does it look like? What models do you opt to use?

Everything I’ve thought of seems to have some sort of hold up so I feel I’m missing something. I assume we can’t train an RNN for every user due to likely limited sample data (they’ve only listened to so many songs themselves) and computational cost of maintaining hundreds of thousands or millions of NNs. Traditional content based recommendations using like simple cosine similarity may not be able to capture some of the more complex nonlinear relationships without exceptional upfront feature engineering (e.g. I like electronic songs, but only if they’re between X and Y BPM, with female vocalists). 

What am I missing? Do we have a good solution for this type of problem?",t2_87cp2,False,,0,False,How would you go about building a content based recommendation system with reinforcement?,[],r/datascience,False,6,discussion,0,,,False,t3_nz22z5,False,dark,0.98,,public,118,0,{},,,False,[],,False,False,,{},Discussion,False,118,,False,False,self,1623607767.0,,[],{},,True,,1623636113.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let’s say we’re a company like Spotify and want to recommend songs to you based on songs you’ve listened to.&lt;/p&gt;

&lt;p&gt;Say we have a constraint that we don’t want to funnel all of our impressions to a small % of songs (which is likely to occur with user based collaborative filtering), but rather would prefer broad coverage of viewership across our whole library.&lt;/p&gt;

&lt;p&gt;We want this to be personalized to each user and we want to iteratively learn as you continue to listen to new songs. &lt;/p&gt;

&lt;p&gt;How would you go about building this architecture? What does it look like? What models do you opt to use?&lt;/p&gt;

&lt;p&gt;Everything I’ve thought of seems to have some sort of hold up so I feel I’m missing something. I assume we can’t train an RNN for every user due to likely limited sample data (they’ve only listened to so many songs themselves) and computational cost of maintaining hundreds of thousands or millions of NNs. Traditional content based recommendations using like simple cosine similarity may not be able to capture some of the more complex nonlinear relationships without exceptional upfront feature engineering (e.g. I like electronic songs, but only if they’re between X and Y BPM, with female vocalists). &lt;/p&gt;

&lt;p&gt;What am I missing? Do we have a good solution for this type of problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nz22z5,True,,reddstaw,,12,True,all_ads,False,[],False,,/r/datascience/comments/nz22z5/how_would_you_go_about_building_a_content_based/,all_ads,False,https://www.reddit.com/r/datascience/comments/nz22z5/how_would_you_go_about_building_a_content_based/,515405,1623607313.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What do you think about a cluster analysis to segment customers? I feel like a manual segmentation is often times better, especially when it comes to more personalized marketing. 

I think that clustering makes sense when there are distinct groups in the population. However, I think that these groups can easily be identified by EDA (finding thresholds of certain variables) in most cases. There is a possibility for identifying relevant groups only through cluster analysis, but I think that a) those cases are rare and b) the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication.

Does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value (e.g. because of more personalized communication)? I am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds/criteria for clusters.",t2_41ms80a8,False,,0,False,Cluster Analysis for Customer Segmentation,[],r/datascience,False,6,discussion,0,,,False,t3_nzkpwk,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623699104.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you think about a cluster analysis to segment customers? I feel like a manual segmentation is often times better, especially when it comes to more personalized marketing. &lt;/p&gt;

&lt;p&gt;I think that clustering makes sense when there are distinct groups in the population. However, I think that these groups can easily be identified by EDA (finding thresholds of certain variables) in most cases. There is a possibility for identifying relevant groups only through cluster analysis, but I think that a) those cases are rare and b) the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication.&lt;/p&gt;

&lt;p&gt;Does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value (e.g. because of more personalized communication)? I am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds/criteria for clusters.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzkpwk,True,,tstr2609,,10,True,all_ads,False,[],False,,/r/datascience/comments/nzkpwk/cluster_analysis_for_customer_segmentation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzkpwk/cluster_analysis_for_customer_segmentation/,515405,1623670304.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Topic. Given the lack of future covariates, can one use (S)ARIMAX to predict *k* (instead of just one) time steps ahead?

Another related question: Can we use SARIMAX to predict non-negative integer series, such as count of item sale? If yes, how? If not, which alternatives do you recommend?",t2_9aqvgklw,False,,0,False,Is it possible to use (S)ARIMAX to predict multiple time steps ahead?,[],r/datascience,False,6,discussion,0,,,False,t3_nznbfn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,1623681163.0,,[],{},,True,,1623707674.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Topic. Given the lack of future covariates, can one use (S)ARIMAX to predict &lt;em&gt;k&lt;/em&gt; (instead of just one) time steps ahead?&lt;/p&gt;

&lt;p&gt;Another related question: Can we use SARIMAX to predict non-negative integer series, such as count of item sale? If yes, how? If not, which alternatives do you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nznbfn,True,,populus27,,5,True,all_ads,False,[],False,,/r/datascience/comments/nznbfn/is_it_possible_to_use_sarimax_to_predict_multiple/,all_ads,False,https://www.reddit.com/r/datascience/comments/nznbfn/is_it_possible_to_use_sarimax_to_predict_multiple/,515405,1623678874.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What made you realize that you wanted to pursue data science? Did you go to school for it or did you wind up the field by accident? Was there a certain class you took the sparked your interest?,t2_2gujwkgv,False,,0,False,When did you know the data science was for you?,[],r/datascience,False,6,discussion,0,,,False,t3_nzf1fb,False,dark,0.63,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623675425.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What made you realize that you wanted to pursue data science? Did you go to school for it or did you wind up the field by accident? Was there a certain class you took the sparked your interest?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzf1fb,True,,J24C,,16,True,all_ads,False,[],False,,/r/datascience/comments/nzf1fb/when_did_you_know_the_data_science_was_for_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzf1fb/when_did_you_know_the_data_science_was_for_you/,515405,1623646625.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey everyone. I'm currently working actively on 2 projects, with 3 more coming up in the next weeks. I've been working with SQL trying to define patient cohorts for later statistical analysis in R. The SQL work has been slow to my liking but I only started working here in January and on a new database, of which I had to move out from after the first 2 months, to another database (same data source but different structure) so I had to redo a good chunk of the initial progress. I would say I've been only making real progress during the past 4 months. Add to that that I have at least 2 meetings a week.

My current contract is supposed to expire by the end of this month, and they'll extend it for another 3 months for me to deliver the first 2 projects, but they still expect me to work to some degree on the other 3 that are coming.  

My question is if you feel like they're setting me up to fail on this position or working on 5 projects is actually doable? Obviously I'm going to focus on the ones that they're going to evaluate me on, but considering I'm the first data scientist here I think they're not used to having the vast majority of the time defining the data extraction parameters and cleaning the data, and only a small portion on the actual statistical analysis or ML modelling.

At the time of starting this job I had 9 months of experience, but this is my first job after getting my MSc. I already had some SQL experience but mainly from internet tutorials and some simple joins from my previous position.

What do you think?",t2_5u939xo,False,,0,False,How many projects are you able to work efficiently on?,[],r/datascience,False,6,career,0,,,False,t3_nzlzih,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623703599.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone. I&amp;#39;m currently working actively on 2 projects, with 3 more coming up in the next weeks. I&amp;#39;ve been working with SQL trying to define patient cohorts for later statistical analysis in R. The SQL work has been slow to my liking but I only started working here in January and on a new database, of which I had to move out from after the first 2 months, to another database (same data source but different structure) so I had to redo a good chunk of the initial progress. I would say I&amp;#39;ve been only making real progress during the past 4 months. Add to that that I have at least 2 meetings a week.&lt;/p&gt;

&lt;p&gt;My current contract is supposed to expire by the end of this month, and they&amp;#39;ll extend it for another 3 months for me to deliver the first 2 projects, but they still expect me to work to some degree on the other 3 that are coming.  &lt;/p&gt;

&lt;p&gt;My question is if you feel like they&amp;#39;re setting me up to fail on this position or working on 5 projects is actually doable? Obviously I&amp;#39;m going to focus on the ones that they&amp;#39;re going to evaluate me on, but considering I&amp;#39;m the first data scientist here I think they&amp;#39;re not used to having the vast majority of the time defining the data extraction parameters and cleaning the data, and only a small portion on the actual statistical analysis or ML modelling.&lt;/p&gt;

&lt;p&gt;At the time of starting this job I had 9 months of experience, but this is my first job after getting my MSc. I already had some SQL experience but mainly from internet tutorials and some simple joins from my previous position.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzlzih,True,,Trucomallica,,13,True,all_ads,False,[],False,,/r/datascience/comments/nzlzih/how_many_projects_are_you_able_to_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzlzih/how_many_projects_are_you_able_to_work/,515405,1623674799.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hey Guys,

I am currently working on my thesis. In this thesis I work with time series data and use a model based on fused regression estimated on a rolling window. Unfortunately I do not habe enough obervations to utilize cross-validation or even any kind of validation. If someone here could point me on something I could use instead to choose my hyperparameters I would be incredibly thankful.

Thanks in advance and have a nice day.

Edit:
To clarify a little

The task is basically to evaluate penalized regression models against a standard factor model (essentially linear regression) in hedge fund replication. 

At every point t I estimate a model based on the data in the rolling window abd use the coefficients as portfolio weights in t+1.",t2_5avdhsjx,False,,0,False,Alternatives to Cross-Validation,[],r/datascience,False,6,discussion,0,,,False,t3_nzie21,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1623669100.0,,[],{},,True,,1623689547.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;

&lt;p&gt;I am currently working on my thesis. In this thesis I work with time series data and use a model based on fused regression estimated on a rolling window. Unfortunately I do not habe enough obervations to utilize cross-validation or even any kind of validation. If someone here could point me on something I could use instead to choose my hyperparameters I would be incredibly thankful.&lt;/p&gt;

&lt;p&gt;Thanks in advance and have a nice day.&lt;/p&gt;

&lt;p&gt;Edit:
To clarify a little&lt;/p&gt;

&lt;p&gt;The task is basically to evaluate penalized regression models against a standard factor model (essentially linear regression) in hedge fund replication. &lt;/p&gt;

&lt;p&gt;At every point t I estimate a model based on the data in the rolling window abd use the coefficients as portfolio weights in t+1.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzie21,True,,0din23,,8,True,all_ads,False,[],False,,/r/datascience/comments/nzie21/alternatives_to_crossvalidation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzie21/alternatives_to_crossvalidation/,515405,1623660747.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone,

for a university project we were given the following simple algorithm and I was wondering if it has a name or is similar to an existing method. I'd be glad if you could leave some comments.

So we have data points with 2 features in 3 evenly distributed categories. One can easily see 3 clusters but there is quite some overlap.

The suggested method wants us to determine 3 centres, one for each category, and categorize data points by determining the closest centre. These centres should be optimized so that the number of wrong categorizations is minimized. (Any optimization methods recommended? There seem to be a lot of local minima so we used scipys dual-annealing and most likely found the optimum but maybe there is a faster way?).

So is there any name for this categorization Method? Or a very similar one? All the methodes we looked at (sklearn) are much more sohphisticated.

We wanted to also use knn as an alternative method and compare the two using cross-validation.

If you have any other ideas I'd be very happy to hear them.

Thanks for reading and have a nice day!

P.S.: Cool to see this subreddit exists. subscribed!",t2_fm7fx,False,,0,False,Is there a name for this simple Machine Learning Method? Or a similar one?,[],r/datascience,False,6,education,0,,,False,t3_nzi1lx,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623687949.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;for a university project we were given the following simple algorithm and I was wondering if it has a name or is similar to an existing method. I&amp;#39;d be glad if you could leave some comments.&lt;/p&gt;

&lt;p&gt;So we have data points with 2 features in 3 evenly distributed categories. One can easily see 3 clusters but there is quite some overlap.&lt;/p&gt;

&lt;p&gt;The suggested method wants us to determine 3 centres, one for each category, and categorize data points by determining the closest centre. These centres should be optimized so that the number of wrong categorizations is minimized. (Any optimization methods recommended? There seem to be a lot of local minima so we used scipys dual-annealing and most likely found the optimum but maybe there is a faster way?).&lt;/p&gt;

&lt;p&gt;So is there any name for this categorization Method? Or a very similar one? All the methodes we looked at (sklearn) are much more sohphisticated.&lt;/p&gt;

&lt;p&gt;We wanted to also use knn as an alternative method and compare the two using cross-validation.&lt;/p&gt;

&lt;p&gt;If you have any other ideas I&amp;#39;d be very happy to hear them.&lt;/p&gt;

&lt;p&gt;Thanks for reading and have a nice day!&lt;/p&gt;

&lt;p&gt;P.S.: Cool to see this subreddit exists. subscribed!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzi1lx,True,,pitano,,11,True,all_ads,False,[],False,,/r/datascience/comments/nzi1lx/is_there_a_name_for_this_simple_machine_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzi1lx/is_there_a_name_for_this_simple_machine_learning/,515405,1623659149.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Supposed we have a linear transformation 
A = [[2,1], [1,2]] 
Why does when i use linalg in python and input this matrix it produces eigenvector and eigenvalues when this linear transformation has both changed the direction(amplitude) of the default coordinate system?",t2_67s1slvl,False,,0,False,Why does this linear transformation produces any eigenvector,[],r/datascience,False,6,education,0,,,False,t3_nzmrwc,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1623706048.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Supposed we have a linear transformation 
A = [[2,1], [1,2]] 
Why does when i use linalg in python and input this matrix it produces eigenvector and eigenvalues when this linear transformation has both changed the direction(amplitude) of the default coordinate system?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzmrwc,True,,izner82,,2,True,all_ads,False,[],False,,/r/datascience/comments/nzmrwc/why_does_this_linear_transformation_produces_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzmrwc/why_does_this_linear_transformation_produces_any/,515405,1623677248.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_43ojyjs,False,,0,False,Is going from a data science job to people analytics (corporate human resources) in the same company a demotion?,[],r/datascience,False,6,career,0,,,False,t3_nzigaa,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1623689830.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzigaa,True,,the_siloviki,,9,True,all_ads,False,[],False,,/r/datascience/comments/nzigaa/is_going_from_a_data_science_job_to_people/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzigaa/is_going_from_a_data_science_job_to_people/,515405,1623661030.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am looking to study recommendation system techniques (such as collaborative filtering), and how to build + evaluate machine learning models for ranking (instead of classification) problems. Google search results have been messy so far.

Does anyone have a good overview for these two topics?",t2_9aqvgklw,False,,0,False,Is there a comprehensive overview of ML ranking + recommendation system techniques?,[],r/datascience,False,6,discussion,0,,,False,t3_nyses8,False,dark,0.98,,public,56,0,{},,,False,[],,False,False,,{},Discussion,False,56,,False,False,self,False,,[],{},,True,,1623603981.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to study recommendation system techniques (such as collaborative filtering), and how to build + evaluate machine learning models for ranking (instead of classification) problems. Google search results have been messy so far.&lt;/p&gt;

&lt;p&gt;Does anyone have a good overview for these two topics?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyses8,True,,populus27,,7,True,all_ads,False,[],False,,/r/datascience/comments/nyses8/is_there_a_comprehensive_overview_of_ml_ranking/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyses8/is_there_a_comprehensive_overview_of_ml_ranking/,515405,1623575181.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Noob here.  I have very basic skills in Python using PyCharm.  

I just picked up Python for Data Science for Dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.

In this book, the author uses Jupyter Notebook.  Before I go and install another program and head down the path of learning it, I'm wondering if this is the right tool to be using. 

My goals: Well, I guess I'd just like to expand my knowledge of Python.  I don't use it for work or anything, yet...  I'd like to move into an FP&amp;A role and I know understanding Python is sometimes advantageous.  I do realize that doing data science with Python is probably more than would be needed in an FP&amp;A role, and that's OK.  I think I may just like to learn how to use Python more because I'm just a very analytical person by nature and maybe someday I'll use it to put together analyses of Coronavirus data.  But since I am new with learning coding languages, if Jupyter is good as a starting point, that's OK too.  Have to admit that the CLI screenshots in the book intimidated me,  but I'm OK learning it since I know CLI is kind of a part of being a techy and it's probably about time I got more comfortable with it.",t2_4800s9po,False,,0,False,Using Jupyter Notebook vs something else?,[],r/datascience,False,6,education,0,,,False,t3_nyizb5,False,dark,0.95,,public,137,0,{},,,False,[],,False,False,,{},Education,False,137,,False,False,self,False,,[],{},,True,,1623567522.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Noob here.  I have very basic skills in Python using PyCharm.  &lt;/p&gt;

&lt;p&gt;I just picked up Python for Data Science for Dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.&lt;/p&gt;

&lt;p&gt;In this book, the author uses Jupyter Notebook.  Before I go and install another program and head down the path of learning it, I&amp;#39;m wondering if this is the right tool to be using. &lt;/p&gt;

&lt;p&gt;My goals: Well, I guess I&amp;#39;d just like to expand my knowledge of Python.  I don&amp;#39;t use it for work or anything, yet...  I&amp;#39;d like to move into an FP&amp;amp;A role and I know understanding Python is sometimes advantageous.  I do realize that doing data science with Python is probably more than would be needed in an FP&amp;amp;A role, and that&amp;#39;s OK.  I think I may just like to learn how to use Python more because I&amp;#39;m just a very analytical person by nature and maybe someday I&amp;#39;ll use it to put together analyses of Coronavirus data.  But since I am new with learning coding languages, if Jupyter is good as a starting point, that&amp;#39;s OK too.  Have to admit that the CLI screenshots in the book intimidated me,  but I&amp;#39;m OK learning it since I know CLI is kind of a part of being a techy and it&amp;#39;s probably about time I got more comfortable with it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyizb5,True,,lljc00,,109,True,all_ads,False,[],False,,/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/,515405,1623538722.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Are there any other types of data that we interpret with programming?

I assume theirs multiple different types of tabular data.. sequential is anything with a time series such as audio, &amp; image is a 2D picture is that all their is?


Thankyou",t2_2v31gwfg,False,,0,False,Tabular sequential and image data. Is there any other type of data?,[],r/datascience,False,6,discussion,0,,,False,t3_nyxfbk,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623623134.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any other types of data that we interpret with programming?&lt;/p&gt;

&lt;p&gt;I assume theirs multiple different types of tabular data.. sequential is anything with a time series such as audio, &amp;amp; image is a 2D picture is that all their is?&lt;/p&gt;

&lt;p&gt;Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyxfbk,True,,quantumwoooo,,9,True,all_ads,False,[],False,,/r/datascience/comments/nyxfbk/tabular_sequential_and_image_data_is_there_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyxfbk/tabular_sequential_and_image_data_is_there_any/,515405,1623594334.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

Not sure if this post violates the on-topic rule, it is DS-related in the applied sense in terms of being a practicioner, sorry if it is considered OT.

I usually work locally on my machine, which has always been a laptop with an i7 or i9 with a lower-end nvidia GPU for small to medium-sized modeling tasks.  I'm going to start a new job soon and will have my choice of work laptop. Big compute tasks can be performed on the cloud, however for prototype/POC work with limited datasets that don't require very intense hyperparameter searches, I typically work locally.

I've been reading some interesting things about the performance of ML libraries on M1 machines and it looks like deep learning packages as well as low level vector libraries and libraries built on top of them such as numpy are very quick these days with the m1.

Is anybody using an M1 machine these days for DS? I won't have time to mess around with complex builds and such, I'm generally somebody who just relies on anaconda to install what I need and make sure all of the packages work nicely together. Is the M1 ""There yet"" in terms of being ready to hit the road for DS work with minimal fuss?

My other question/concern is memory allocation for the gpu cores when using DL libraries. Since the memory is ""unified"", if I have 16 gigs, how is that split between general system use and GPU use?

Thanks!",t2_1dikvtfr,False,,0,False,Anybody using a M1 Apple product for local modeling work?,[],r/datascience,False,6,tooling,0,,,False,t3_ny7wy5,False,dark,0.9,,public,95,0,{},,,False,[],,False,False,,{},Tooling,False,95,,False,False,self,False,,[],{},,True,,1623536706.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Not sure if this post violates the on-topic rule, it is DS-related in the applied sense in terms of being a practicioner, sorry if it is considered OT.&lt;/p&gt;

&lt;p&gt;I usually work locally on my machine, which has always been a laptop with an i7 or i9 with a lower-end nvidia GPU for small to medium-sized modeling tasks.  I&amp;#39;m going to start a new job soon and will have my choice of work laptop. Big compute tasks can be performed on the cloud, however for prototype/POC work with limited datasets that don&amp;#39;t require very intense hyperparameter searches, I typically work locally.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been reading some interesting things about the performance of ML libraries on M1 machines and it looks like deep learning packages as well as low level vector libraries and libraries built on top of them such as numpy are very quick these days with the m1.&lt;/p&gt;

&lt;p&gt;Is anybody using an M1 machine these days for DS? I won&amp;#39;t have time to mess around with complex builds and such, I&amp;#39;m generally somebody who just relies on anaconda to install what I need and make sure all of the packages work nicely together. Is the M1 &amp;quot;There yet&amp;quot; in terms of being ready to hit the road for DS work with minimal fuss?&lt;/p&gt;

&lt;p&gt;My other question/concern is memory allocation for the gpu cores when using DL libraries. Since the memory is &amp;quot;unified&amp;quot;, if I have 16 gigs, how is that split between general system use and GPU use?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ny7wy5,True,,ShmDoubleO,,32,True,all_ads,False,[],False,,/r/datascience/comments/ny7wy5/anybody_using_a_m1_apple_product_for_local/,all_ads,False,https://www.reddit.com/r/datascience/comments/ny7wy5/anybody_using_a_m1_apple_product_for_local/,515405,1623507906.0,1,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all, 

there is certainly a lot of hype still about Data Science and Machine learning, as somebody who tried ""sour side"" as well (few failed projects because quality of data, not really significant results) 

I was wondering if somebody had any success stories, with quick ML projects that had real business impact and were set into process to bring value. Also if it was worth to go down Python way (I have experience doing my master thesis where I used ML) or much quicker way with ""pre-defined"" tools, like Power BI, or those online engines.  
   
Now I got promoted to Business Intelligence manager position, only successful part of ML I did so far was with engine inside of Power BI. Down the line there are certainly some ideas I have, with speech recognition, better assigning employees to customers, finding right time to contact right customer.   


But uncertainity of ML and its timing to get it done scares me (for some thing I know I can bring 50-60% of value by doing visual analysis, solving the big chunks of decisions in 10% of the time).  


So generally I am advocate of ""ML is hype that is not worth it, and it will stay like this until we have done majority of analysis visually first"". I can see how they are failing in other business unit, where they hired niche data analysts and data scientist, before even having data in db, so it only enforces my thinking.  


Also if somebody has in mind employee churn project, thats a no-go with quality of data in HR (not updating positions, wrong assigned managers, not standardized position names...)

tl;dr: is e2e ML deliverable by single person in company as side project, with bringing real value (relatively quickly)?",t2_827ll6b9,False,,0,False,Any successful 1 man end to end stories?,[],r/datascience,False,6,projects,0,,,False,t3_nyhh3s,False,dark,0.88,,public,22,0,{},,,False,[],,False,False,,{},Projects,False,22,,False,False,self,False,,[],{},,True,,1623563166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, &lt;/p&gt;

&lt;p&gt;there is certainly a lot of hype still about Data Science and Machine learning, as somebody who tried &amp;quot;sour side&amp;quot; as well (few failed projects because quality of data, not really significant results) &lt;/p&gt;

&lt;p&gt;I was wondering if somebody had any success stories, with quick ML projects that had real business impact and were set into process to bring value. Also if it was worth to go down Python way (I have experience doing my master thesis where I used ML) or much quicker way with &amp;quot;pre-defined&amp;quot; tools, like Power BI, or those online engines.  &lt;/p&gt;

&lt;p&gt;Now I got promoted to Business Intelligence manager position, only successful part of ML I did so far was with engine inside of Power BI. Down the line there are certainly some ideas I have, with speech recognition, better assigning employees to customers, finding right time to contact right customer.   &lt;/p&gt;

&lt;p&gt;But uncertainity of ML and its timing to get it done scares me (for some thing I know I can bring 50-60% of value by doing visual analysis, solving the big chunks of decisions in 10% of the time).  &lt;/p&gt;

&lt;p&gt;So generally I am advocate of &amp;quot;ML is hype that is not worth it, and it will stay like this until we have done majority of analysis visually first&amp;quot;. I can see how they are failing in other business unit, where they hired niche data analysts and data scientist, before even having data in db, so it only enforces my thinking.  &lt;/p&gt;

&lt;p&gt;Also if somebody has in mind employee churn project, thats a no-go with quality of data in HR (not updating positions, wrong assigned managers, not standardized position names...)&lt;/p&gt;

&lt;p&gt;tl;dr: is e2e ML deliverable by single person in company as side project, with bringing real value (relatively quickly)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyhh3s,True,,UnderstandingFit9152,,13,True,all_ads,False,[],False,,/r/datascience/comments/nyhh3s/any_successful_1_man_end_to_end_stories/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyhh3s/any_successful_1_man_end_to_end_stories/,515405,1623534366.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm a new grad MS working as an entry-level data scientist. Most of the PhDs at my company are either at the senior level or managerial, but most also have a few years of experience too.

In general how much years of experience do you need to go from my current level to a senior data scientist and then to the principal/managerial level? I know a few new grad colleagues in SWE who got to senior engineer shortly after one year, but it seems like that trajectory is not exactly the same for DS.

I know this will also vary by company (with some tech companies requiring a PhD for data scientists at the lowest level).",t2_3ahwym06,False,,0,False,YoE to go from entry-level/MS data scientist to mid-level/senior data scientist? (USA),[],r/datascience,False,6,career,0,,,False,t3_nyegax,False,dark,0.55,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623555139.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a new grad MS working as an entry-level data scientist. Most of the PhDs at my company are either at the senior level or managerial, but most also have a few years of experience too.&lt;/p&gt;

&lt;p&gt;In general how much years of experience do you need to go from my current level to a senior data scientist and then to the principal/managerial level? I know a few new grad colleagues in SWE who got to senior engineer shortly after one year, but it seems like that trajectory is not exactly the same for DS.&lt;/p&gt;

&lt;p&gt;I know this will also vary by company (with some tech companies requiring a PhD for data scientists at the lowest level).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyegax,True,,bigchungusmode96,,12,True,all_ads,False,[],False,,/r/datascience/comments/nyegax/yoe_to_go_from_entrylevelms_data_scientist_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyegax/yoe_to_go_from_entrylevelms_data_scientist_to/,515405,1623526339.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm a senior at university and I got a pretty nice internship somehow. I keep getting assigned work with nlp stuff that I don't know how to do. I read the theory behind it some time ago and I watch youtube videos, but I haven't had the opportunity to practice yet. Is this feeling of not knowing what you're doing normal? I've mainly worked with basic machine learning in the past, not much deep learning.

&amp;#x200B;

EDIT: Thanks for all the responses guys. I had some anxiety coming in this week and its settling down. Lots of great advice here as well.",t2_3ygdf0hp,False,,0,False,Is it common to feel like you have no idea what you're doing in an internship?,[],r/datascience,False,6,discussion,0,,,False,t3_nxi5db,False,dark,0.97,,public,353,2,{},,,False,[],,False,False,,{},Discussion,False,353,,False,False,self,1623432685.0,,[],{},,True,,1623452785.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a senior at university and I got a pretty nice internship somehow. I keep getting assigned work with nlp stuff that I don&amp;#39;t know how to do. I read the theory behind it some time ago and I watch youtube videos, but I haven&amp;#39;t had the opportunity to practice yet. Is this feeling of not knowing what you&amp;#39;re doing normal? I&amp;#39;ve mainly worked with basic machine learning in the past, not much deep learning.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: Thanks for all the responses guys. I had some anxiety coming in this week and its settling down. Lots of great advice here as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxi5db,True,,trippygrass,,66,True,all_ads,False,[],False,,/r/datascience/comments/nxi5db/is_it_common_to_feel_like_you_have_no_idea_what/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxi5db/is_it_common_to_feel_like_you_have_no_idea_what/,515405,1623423985.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am offered a data scientist job at a budding health startup. As per the CEO job is ""not just about prediction"" but helping them to make a better product. At this new start up: i would be involved in deciding which product would fit vertically/horizontally with the current product, finding and discussing with right stakeholders out side the company, finding/buying data, making models. I sense the CEO is keen to license other models to integrate within the product. 

I am fine with product development part but I would like to build model myself. I am afraid if the CEO just lincense other model then i would be end up setting infrastructure to use the models within current products. 

Another key point is I am currently working in non mathematics/physics academic lab (where i do not do any modeling), and would like to transition to data science, learned data science through bootcamp/personal projects.

 My concern is would I get stuck within product infrastructure without making my hands dirty!

Edited: for clarity",t2_de03u,False,,0,False,Should I take this Data scientist job offer,[],r/datascience,False,6,career,0,,,False,t3_nye9ya,False,dark,0.36,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623539463.0,,[],{},,True,,1623554645.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am offered a data scientist job at a budding health startup. As per the CEO job is &amp;quot;not just about prediction&amp;quot; but helping them to make a better product. At this new start up: i would be involved in deciding which product would fit vertically/horizontally with the current product, finding and discussing with right stakeholders out side the company, finding/buying data, making models. I sense the CEO is keen to license other models to integrate within the product. &lt;/p&gt;

&lt;p&gt;I am fine with product development part but I would like to build model myself. I am afraid if the CEO just lincense other model then i would be end up setting infrastructure to use the models within current products. &lt;/p&gt;

&lt;p&gt;Another key point is I am currently working in non mathematics/physics academic lab (where i do not do any modeling), and would like to transition to data science, learned data science through bootcamp/personal projects.&lt;/p&gt;

&lt;p&gt;My concern is would I get stuck within product infrastructure without making my hands dirty!&lt;/p&gt;

&lt;p&gt;Edited: for clarity&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nye9ya,True,,karanphosphatase,,14,True,all_ads,False,[],False,,/r/datascience/comments/nye9ya/should_i_take_this_data_scientist_job_offer/,all_ads,False,https://www.reddit.com/r/datascience/comments/nye9ya/should_i_take_this_data_scientist_job_offer/,515405,1623525845.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Change my mind.,t2_pyilr,False,,0,False,Julia has rendered Pandas obsolete,[],r/datascience,False,6,discussion,0,,,False,t3_nyj11a,False,dark,0.24,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623567657.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Change my mind.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyj11a,True,,LornartheBreton,,10,True,all_ads,False,[],False,,/r/datascience/comments/nyj11a/julia_has_rendered_pandas_obsolete/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyj11a/julia_has_rendered_pandas_obsolete/,515405,1623538857.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What steps would you take to make sense of a large new dataset that has been sent to you? 



Had the above question pop up in an interview. So from a data science perspective, what would you do?",t2_4kym3qu,False,,0,False,Steps when working with a new database,[],r/datascience,False,6,discussion,0,,,False,t3_nxru9w,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,1623450706.0,,[],{},,True,,1623478820.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What steps would you take to make sense of a large new dataset that has been sent to you? &lt;/p&gt;

&lt;p&gt;Had the above question pop up in an interview. So from a data science perspective, what would you do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxru9w,True,,Plebn,,5,True,all_ads,False,[],False,,/r/datascience/comments/nxru9w/steps_when_working_with_a_new_database/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxru9w/steps_when_working_with_a_new_database/,515405,1623450020.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,True,,,,
,datascience,"There was some good discussion in a previous post about not ""working"" the full day.  Im curious to hear for days when there is work to do that isnt entirely motivating, what do you do to keep energy up?  Since I've been with jobs that involve sitting all day, I usually hit a slump around 2pm and am trying to find more techniques for picking up my energy and focus.

Sometimes I'll take my dog for a walk or give myself a 10 minute Reddit break, but what works for you?",t2_1xf97mwo,False,,0,False,What do you do to combat the afternoon slump?,[],r/datascience,False,6,discussion,0,,,False,t3_nwww2s,False,dark,0.96,,public,278,1,{},,,False,[],,False,False,,{},Discussion,False,278,,False,False,self,False,,[],{},,True,,1623383881.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There was some good discussion in a previous post about not &amp;quot;working&amp;quot; the full day.  Im curious to hear for days when there is work to do that isnt entirely motivating, what do you do to keep energy up?  Since I&amp;#39;ve been with jobs that involve sitting all day, I usually hit a slump around 2pm and am trying to find more techniques for picking up my energy and focus.&lt;/p&gt;

&lt;p&gt;Sometimes I&amp;#39;ll take my dog for a walk or give myself a 10 minute Reddit break, but what works for you?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwww2s,True,,Beekle1014,,115,True,all_ads,False,[],False,,/r/datascience/comments/nwww2s/what_do_you_do_to_combat_the_afternoon_slump/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwww2s/what_do_you_do_to_combat_the_afternoon_slump/,515405,1623355081.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win Kaggle competitions (i.e. perform well for medium sized tabular datasets) compared to deep neural networks and linear regression models?

Heuristically, here are my general conclusions:

1) GLM (general linear models) perform best on smaller sized datasets, provided certain statistical assumptions are met.

2) boosting and bagging algorithms (e.g. random forest and xgboost) perform best on larger tabular datasets, and do not require many statistical assumptions.

3) deep neural networks perform best on very large datasets, preferably on non tabular datasets (e.g. tensors, pictures, audio, computer vision, text/nlp).

But can there be any mathematical reasons that try to explain these general conclusions (provided these conclusions are correct)? 

For instance, suppose there is one response variable and one predictor variable, and when graphed together they look like a sine wave - it seems unlikely that a linear regression model could perform well. Perhaps this is because a linear model can only capture a linear trend? Perhaps it is too hard to understand the exact assumptions required for GLM models to work on real world data, or they are too prone to overfit on complex data?

The same way, is there any math that explains why alphaGO, self driving cars and Google's BERT NLP model are all based on neural networks - and not using random forest and xgboost? Is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets? Perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data, which is just not computationally possible ... or would surely result in overfitting?

And the same way, is there any math that explains why deep neural networks aren't as successful as random forest and xgboost on medium sized tabluar datasets? Do deep neural networks simply require too much effort to select the right combinations of hyperparameters, and its just not worth it for medium sized datasets when random forests work well given significantly less effort? Are deep neural networks to prone to overfit medium datasets? 

Of course, all of this comes to down to trial and error: if a certain model fits the training and test data well - then use that model. But just using mathematical logic and intuition, can we develop some general guidelines that tell us which conditions and types/size of data are favorable for specific algorithms? This could potentially save us a lot of time by directly trying better suited models for the task at hand (e.g. not even trying to use logistic regression for alphaGO). 

So in the end: Beyond empirical results, could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks? And beyond empirical results, could there be any reasons why random forest and xgboost are not chosen for the ImageNet competition?

Thanks",t2_3f0i9m72,False,,0,False,"Can we begin to understand possible mathematical reasons as to why algorithms like ""xgboost"" and ""random forest"" win Kaggle Competitions, instead of neural networks?",[],r/datascience,False,6,discussion,0,,,False,t3_nxn6we,False,dark,0.53,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623465962.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win Kaggle competitions (i.e. perform well for medium sized tabular datasets) compared to deep neural networks and linear regression models?&lt;/p&gt;

&lt;p&gt;Heuristically, here are my general conclusions:&lt;/p&gt;

&lt;p&gt;1) GLM (general linear models) perform best on smaller sized datasets, provided certain statistical assumptions are met.&lt;/p&gt;

&lt;p&gt;2) boosting and bagging algorithms (e.g. random forest and xgboost) perform best on larger tabular datasets, and do not require many statistical assumptions.&lt;/p&gt;

&lt;p&gt;3) deep neural networks perform best on very large datasets, preferably on non tabular datasets (e.g. tensors, pictures, audio, computer vision, text/nlp).&lt;/p&gt;

&lt;p&gt;But can there be any mathematical reasons that try to explain these general conclusions (provided these conclusions are correct)? &lt;/p&gt;

&lt;p&gt;For instance, suppose there is one response variable and one predictor variable, and when graphed together they look like a sine wave - it seems unlikely that a linear regression model could perform well. Perhaps this is because a linear model can only capture a linear trend? Perhaps it is too hard to understand the exact assumptions required for GLM models to work on real world data, or they are too prone to overfit on complex data?&lt;/p&gt;

&lt;p&gt;The same way, is there any math that explains why alphaGO, self driving cars and Google&amp;#39;s BERT NLP model are all based on neural networks - and not using random forest and xgboost? Is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets? Perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data, which is just not computationally possible ... or would surely result in overfitting?&lt;/p&gt;

&lt;p&gt;And the same way, is there any math that explains why deep neural networks aren&amp;#39;t as successful as random forest and xgboost on medium sized tabluar datasets? Do deep neural networks simply require too much effort to select the right combinations of hyperparameters, and its just not worth it for medium sized datasets when random forests work well given significantly less effort? Are deep neural networks to prone to overfit medium datasets? &lt;/p&gt;

&lt;p&gt;Of course, all of this comes to down to trial and error: if a certain model fits the training and test data well - then use that model. But just using mathematical logic and intuition, can we develop some general guidelines that tell us which conditions and types/size of data are favorable for specific algorithms? This could potentially save us a lot of time by directly trying better suited models for the task at hand (e.g. not even trying to use logistic regression for alphaGO). &lt;/p&gt;

&lt;p&gt;So in the end: Beyond empirical results, could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks? And beyond empirical results, could there be any reasons why random forest and xgboost are not chosen for the ImageNet competition?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxn6we,True,,SQL_beginner,,7,True,all_ads,False,[],False,,/r/datascience/comments/nxn6we/can_we_begin_to_understand_possible_mathematical/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxn6we/can_we_begin_to_understand_possible_mathematical/,515405,1623437162.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_c8kps30y,False,,0,False,How many of you bioinformaticians or related (working or studying) know or have used Docker and BioPortainer?,[],r/datascience,False,6,discussion,0,,,False,t3_nxr7dl,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,default,False,,[],{},,False,,1623477009.0,text,6,,,text,self.bioinformatics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxr7dl,True,,DavidAciole,,1,True,all_ads,False,[],False,,/r/datascience/comments/nxr7dl/how_many_of_you_bioinformaticians_or_related/,all_ads,False,/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/,515405,1623448209.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?auto=webp&amp;s=225830c14f9c675a2d6936716884c914be25234b', 'width': 1587, 'height': 1123}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5bc7475a8a74e6f6ac488cfb9e89c09a7d9f55d', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc1899399640458afe2aeb59b65945468fa4bd68', 'width': 216, 'height': 152}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f0110fb3f94a17146d11ed01f341a7f0596c6a8', 'width': 320, 'height': 226}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4676d9baeb5de5d8400122da2a7ee74174f7678e', 'width': 640, 'height': 452}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0285b27f19659b2cd877138d29f54817b18f24c7', 'width': 960, 'height': 679}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1c1a8be22952bfc6606c9b9a9b8d06daf0708b7', 'width': 1080, 'height': 764}], 'variants': {}, 'id': '4BTJwXlrWtbTBSDy62N6aTx6FtLxgiNw-XQdSpoTzu0'}], 'enabled': False}",,"[{'approved_at_utc': None, 'subreddit': 'bioinformatics', 'selftext': '[BioPortainer](https://bioportainer.github.io/BioPortainer/) and [Docker](https://www.docker.com) can be used by bioinformaticians to elegantly manage virtual environments. How many of you know/work with [Docker](https://www.docker.com)? And how many use bioinformatics-directed platforms to manage it, like [BioPortainer](https://bioportainer.github.io/BioPortainer/)? For those who used it, what you think about it?\n\n[View Poll](https://www.reddit.com/poll/nxr27j)', 'author_fullname': 't2_c8kps30y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How many of you bioinformaticians or related (working or studying) know or have used Docker and BioPortainer?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/bioinformatics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nxr27j', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.66, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1623448511.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1623476604.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.bioinformatics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://bioportainer.github.io/BioPortainer/""&gt;BioPortainer&lt;/a&gt; and &lt;a href=""https://www.docker.com""&gt;Docker&lt;/a&gt; can be used by bioinformaticians to elegantly manage virtual environments. How many of you know/work with &lt;a href=""https://www.docker.com""&gt;Docker&lt;/a&gt;? And how many use bioinformatics-directed platforms to manage it, like &lt;a href=""https://bioportainer.github.io/BioPortainer/""&gt;BioPortainer&lt;/a&gt;? For those who used it, what you think about it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nxr27j""&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?auto=webp&amp;s=225830c14f9c675a2d6936716884c914be25234b', 'width': 1587, 'height': 1123}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5bc7475a8a74e6f6ac488cfb9e89c09a7d9f55d', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc1899399640458afe2aeb59b65945468fa4bd68', 'width': 216, 'height': 152}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f0110fb3f94a17146d11ed01f341a7f0596c6a8', 'width': 320, 'height': 226}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4676d9baeb5de5d8400122da2a7ee74174f7678e', 'width': 640, 'height': 452}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0285b27f19659b2cd877138d29f54817b18f24c7', 'width': 960, 'height': 679}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1c1a8be22952bfc6606c9b9a9b8d06daf0708b7', 'width': 1080, 'height': 764}], 'variants': {}, 'id': '4BTJwXlrWtbTBSDy62N6aTx6FtLxgiNw-XQdSpoTzu0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '20e12ad6-7f51-11e4-a315-22000b3617ab', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh0x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nxr27j', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'DavidAciole', 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': {'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1624052604338, 'options': [{'text': 'Wtf is Docker?', 'vote_count': 57, 'id': '8508428'}, {'text': 'Wtf is BioPortainer?', 'vote_count': 145, 'id': '8508429'}, {'text': 'I know Docker, but never used it', 'vote_count': 163, 'id': '8508430'}, {'text': 'I use Docker with BioPortainer', 'vote_count': 7, 'id': '8508431'}, {'text': 'I use Docker without BioPortainer', 'vote_count': 200, 'id': '8508432'}, {'text': ""I have no idea what I'm doing here"", 'vote_count': 186, 'id': '8508433'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 758, 'total_stake_amount': None}, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/', 'subreddit_subscribers': 65941, 'created_utc': 1623447804.0, 'num_crossposts': 5, 'media': None, 'is_video': False}]",/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/,t3_nxr27j,
,datascience,"Currently I’m working for a customer, which only allows visual basic and java. The project I got is to make visualisation, create a report in powerpoint automatically. This should be done easily in python if using matplotlib and pptx-python. Has anyone done data analytics in java like this before?",t2_a1bnnkle,False,,0,False,Data Analytics with java,[],r/datascience,False,6,discussion,0,,,False,t3_nxlv5l,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623462424.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I’m working for a customer, which only allows visual basic and java. The project I got is to make visualisation, create a report in powerpoint automatically. This should be done easily in python if using matplotlib and pptx-python. Has anyone done data analytics in java like this before?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxlv5l,True,,Individual-Sweet-734,,3,True,all_ads,False,[],False,,/r/datascience/comments/nxlv5l/data_analytics_with_java/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxlv5l/data_analytics_with_java/,515405,1623433624.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," Is  there any good idea of a mobile app which would require on-device  inference? I can't think of any. In my opinion, MobileNet times have  passed. Now, creating mobile-efficient models are only useful to surpass  some benchmarks. A weird flex and that's all in my view.

It  seems all useful ML-based mobile apps have already been implemented:  face recognition, image filters, sound classification, voice  recognition...

Is there any need for deploying a ML model on a mobile device and not using an external server + API?",t2_cl8ig5la,False,,0,False,Are mobile neural nets still relevant?,[],r/datascience,False,6,discussion,0,,,False,t3_nxjdcb,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623455985.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is  there any good idea of a mobile app which would require on-device  inference? I can&amp;#39;t think of any. In my opinion, MobileNet times have  passed. Now, creating mobile-efficient models are only useful to surpass  some benchmarks. A weird flex and that&amp;#39;s all in my view.&lt;/p&gt;

&lt;p&gt;It  seems all useful ML-based mobile apps have already been implemented:  face recognition, image filters, sound classification, voice  recognition...&lt;/p&gt;

&lt;p&gt;Is there any need for deploying a ML model on a mobile device and not using an external server + API?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxjdcb,True,,adenml,,5,True,all_ads,False,[],False,,/r/datascience/comments/nxjdcb/are_mobile_neural_nets_still_relevant/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxjdcb/are_mobile_neural_nets_still_relevant/,515405,1623427185.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I remember Andrew Gelman had a nice little Stan model for the World Cup.  Since the Euro starts tomorrow, I thought I would try my hand at forecasting some of the games.  I'm not a sports data scientist (I'm more of a biostats guy), but I do like Bayes and I do like international football, so I thought what the hell.

To keep myself honest, I'd like to post a few predictions here.  To keep in the spirit of the sub, I am willing to discuss data, models, critiques of either, with you all.

[Here](https://github.com/Dpananos/Euro2021Predictions/blob/main/predictions/predictions.csv) are my predictions for the group stage.  Since its on github, you can see if I cheat and push predictions after the fact.

I plan to make predictions for each major stage and then update the model after that (so the group stage will pass, I'll calculate my Brier score and cross-entropy loss, I'll refit the model, and go from there).

But the real point of the post is to let you all know you're free to piggy-back off my efforts.  Clone the repo and try your hand.  I'd love to learn a thing or two about these sorts of modelling problems from the community.",t2_131vu3d,False,,0,False,Euro 2020 Predictions,[],r/datascience,False,6,discussion,0,,,False,t3_nx1guv,False,dark,0.91,,public,40,0,{},,,False,[],,False,False,,{},Discussion,False,40,,False,False,self,False,modflair,[],{},,True,,1623395906.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I remember Andrew Gelman had a nice little Stan model for the World Cup.  Since the Euro starts tomorrow, I thought I would try my hand at forecasting some of the games.  I&amp;#39;m not a sports data scientist (I&amp;#39;m more of a biostats guy), but I do like Bayes and I do like international football, so I thought what the hell.&lt;/p&gt;

&lt;p&gt;To keep myself honest, I&amp;#39;d like to post a few predictions here.  To keep in the spirit of the sub, I am willing to discuss data, models, critiques of either, with you all.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/Dpananos/Euro2021Predictions/blob/main/predictions/predictions.csv""&gt;Here&lt;/a&gt; are my predictions for the group stage.  Since its on github, you can see if I cheat and push predictions after the fact.&lt;/p&gt;

&lt;p&gt;I plan to make predictions for each major stage and then update the model after that (so the group stage will pass, I&amp;#39;ll calculate my Brier score and cross-entropy loss, I&amp;#39;ll refit the model, and go from there).&lt;/p&gt;

&lt;p&gt;But the real point of the post is to let you all know you&amp;#39;re free to piggy-back off my efforts.  Clone the repo and try your hand.  I&amp;#39;d love to learn a thing or two about these sorts of modelling problems from the community.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,Data Scientist,[],False,,,,t5_2sptq,,,,nx1guv,True,,__compactsupport__,,15,True,all_ads,False,[],False,dark,/r/datascience/comments/nx1guv/euro_2020_predictions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx1guv/euro_2020_predictions/,515405,1623367106.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?auto=webp&amp;s=0417942dffa68513b288d4522285b20ab33421d0', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b490b45b3c3c81707e0c2e51074f9d79ca125f67', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ec142b1e0b92d6bbacf20aaee0ef6c1765af46b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4d81256dd502a9fda572cc6a2db719d5e383ae1', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd7f9bd78fdf9576c2e15278e6d38efd8d0cb776', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f1f817575ddc8b9f914d3f80c84c6b52d74403c', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79a9c64aff6057d14c9ade6f77bf4cf7630a8fca', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'qLoV4x39urq-R60R0FGRs1usiDBtr5KQCPn6tdleV6Y'}], 'enabled': False}",,,,,
,datascience,Hi all. I have two offers. One is from a big4bank for a data analyst internship (pays less) but I believe they use state of art technology and there is a lot to learn; The work is all for internal stuff.  The second offer is to work as a project analyst in which I will be working with clients. The company is big but not in the top10 or anything. The advantage of this company is that I’ll be working with clients which means more exposure and connections. They also use near top tech. I'm not sure which one will provide most growth. Any advice appreciated,t2_7ciqprlz,False,,0,False,Comparing a data analyst intern (Bank) offer with IT project analyst (Consultancy) offer,[],r/datascience,False,6,,0,,,False,t3_nxc78c,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Job Search,False,5,,False,False,self,1623411455.0,,[],{},,True,,1623433997.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I have two offers. One is from a big4bank for a data analyst internship (pays less) but I believe they use state of art technology and there is a lot to learn; The work is all for internal stuff.  The second offer is to work as a project analyst in which I will be working with clients. The company is big but not in the top10 or anything. The advantage of this company is that I’ll be working with clients which means more exposure and connections. They also use near top tech. I&amp;#39;m not sure which one will provide most growth. Any advice appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nxc78c,True,,MalangChic,,19,True,all_ads,False,[],False,,/r/datascience/comments/nxc78c/comparing_a_data_analyst_intern_bank_offer_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxc78c/comparing_a_data_analyst_intern_bank_offer_with/,515405,1623405197.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hello guys, I'd like to discuss about data product concept or situation in a growing company. 

So here is the situation. I am is a one of the data lead in a fast growing company in southeast asia. I bet in this year, we've doubled our MAU and getting the attention of market.

But, right now, in our data team, there are a lot of different spectrum that need to be achieved, ranging from data warehousing, data collection (ingestion), government, into creating data product like recommendation system, fraud detection, etc.

The problem here is, the data team, doesn’t have the skill like project management or PM skill related like scrum or agile methodologies to perform those wide ranging tasks.

Does anyone in this Data Science forum has a problem to integrate your expertise into your product ? Or even doesn't have any idea to breakdown your analysis or model creation into several part of tasks?

Cause, I feel that we are a bunch of experts that having no strategical or tactical solution for managing project.

Does anyone here has a same experience when serving data for product team or your company?",t2_42bjrsld,False,,0,False,Question about Data Product,[],r/datascience,False,6,discussion,0,,,False,t3_nxkfwj,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623458727.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, I&amp;#39;d like to discuss about data product concept or situation in a growing company. &lt;/p&gt;

&lt;p&gt;So here is the situation. I am is a one of the data lead in a fast growing company in southeast asia. I bet in this year, we&amp;#39;ve doubled our MAU and getting the attention of market.&lt;/p&gt;

&lt;p&gt;But, right now, in our data team, there are a lot of different spectrum that need to be achieved, ranging from data warehousing, data collection (ingestion), government, into creating data product like recommendation system, fraud detection, etc.&lt;/p&gt;

&lt;p&gt;The problem here is, the data team, doesn’t have the skill like project management or PM skill related like scrum or agile methodologies to perform those wide ranging tasks.&lt;/p&gt;

&lt;p&gt;Does anyone in this Data Science forum has a problem to integrate your expertise into your product ? Or even doesn&amp;#39;t have any idea to breakdown your analysis or model creation into several part of tasks?&lt;/p&gt;

&lt;p&gt;Cause, I feel that we are a bunch of experts that having no strategical or tactical solution for managing project.&lt;/p&gt;

&lt;p&gt;Does anyone here has a same experience when serving data for product team or your company?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxkfwj,True,,fazz21,,0,True,all_ads,False,[],False,,/r/datascience/comments/nxkfwj/question_about_data_product/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxkfwj/question_about_data_product/,515405,1623429927.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"https://www.nature.com/articles/s41467-020-17280-8/figures/2

These graphs come from the following article on covid-19 and statistical models: https://www.nature.com/articles/s41467-020-17280-8

These graphs show the ""30 Day Risk Probabilities"" of ""critically ill patients"" vs ""other patients"". In these graphs, is a value of ""1"" supposed to indicate ""very high risk that a patient develops covid-19 symptoms""? So basically, these graphs are showing the day-by-day risk that (different groups of) patients develop covid-19 symptoms after their last hospital visit?

Thanks",t2_o4xj9,False,,0,False,Can someone please help me understand these graphs?,[],r/datascience,False,6,discussion,0,,,False,t3_nxk4vu,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623457937.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.nature.com/articles/s41467-020-17280-8/figures/2""&gt;https://www.nature.com/articles/s41467-020-17280-8/figures/2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These graphs come from the following article on covid-19 and statistical models: &lt;a href=""https://www.nature.com/articles/s41467-020-17280-8""&gt;https://www.nature.com/articles/s41467-020-17280-8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These graphs show the &amp;quot;30 Day Risk Probabilities&amp;quot; of &amp;quot;critically ill patients&amp;quot; vs &amp;quot;other patients&amp;quot;. In these graphs, is a value of &amp;quot;1&amp;quot; supposed to indicate &amp;quot;very high risk that a patient develops covid-19 symptoms&amp;quot;? So basically, these graphs are showing the day-by-day risk that (different groups of) patients develop covid-19 symptoms after their last hospital visit?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxk4vu,True,,blueest,,1,True,all_ads,False,[],False,,/r/datascience/comments/nxk4vu/can_someone_please_help_me_understand_these_graphs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxk4vu/can_someone_please_help_me_understand_these_graphs/,515405,1623429137.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello there I'm a data science student and I was looking for the Neoj4 book for graphs databases and machine learning, being a student I can't get the free copy.
Can someone pass it to me or suggest another book.

Thanks for the attention.",t2_4elw77qh,False,,0,False,Neoj4 books,[],r/datascience,False,6,education,0,,,False,t3_nxbx5g,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623432887.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there I&amp;#39;m a data science student and I was looking for the Neoj4 book for graphs databases and machine learning, being a student I can&amp;#39;t get the free copy.
Can someone pass it to me or suggest another book.&lt;/p&gt;

&lt;p&gt;Thanks for the attention.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxbx5g,True,,NitPo,,2,True,all_ads,False,[],False,,/r/datascience/comments/nxbx5g/neoj4_books/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxbx5g/neoj4_books/,515405,1623404087.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi all,

I'm building my project portfolios, aiming for banks, advertisement firms..

I have a serious question: what's our competitive advantage as data scientists comparing to those software engineers/web developer?

This question has been wandering back of my head.

For SE, they can find jobs quite quickly even freelancing jobs. More demands for it in the job markets

For DS, you have to be really competitive across mathematics/statistics, software engineering, the domain knowledge. Fairly saying researchers could be data scientists. 

My daily frustration is there are more demands for senior DS rather than juniors...

Is there any freelancing jobs for DS?

Cheers!",t2_2xpkxipc,False,,0,False,What kind of freelancing jobs can data scientist do?,[],r/datascience,False,6,projects,0,,,False,t3_nwuzir,False,dark,0.92,,public,27,0,{},,,False,[],,False,False,,{},Projects,False,27,,False,False,self,False,,[],{},,True,,1623379103.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m building my project portfolios, aiming for banks, advertisement firms..&lt;/p&gt;

&lt;p&gt;I have a serious question: what&amp;#39;s our competitive advantage as data scientists comparing to those software engineers/web developer?&lt;/p&gt;

&lt;p&gt;This question has been wandering back of my head.&lt;/p&gt;

&lt;p&gt;For SE, they can find jobs quite quickly even freelancing jobs. More demands for it in the job markets&lt;/p&gt;

&lt;p&gt;For DS, you have to be really competitive across mathematics/statistics, software engineering, the domain knowledge. Fairly saying researchers could be data scientists. &lt;/p&gt;

&lt;p&gt;My daily frustration is there are more demands for senior DS rather than juniors...&lt;/p&gt;

&lt;p&gt;Is there any freelancing jobs for DS?&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwuzir,True,,homchange,,25,True,all_ads,False,[],False,,/r/datascience/comments/nwuzir/what_kind_of_freelancing_jobs_can_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwuzir/what_kind_of_freelancing_jobs_can_data_scientist/,515405,1623350303.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi!

Work from home has been wonderful ever since it has been implemented but I've found myself not working much on days like today. I just wasn't feeling like it. I'm not sure if it's a good thing or a bad thing about work from home. 

Do you guys have days like this too?

Not sure if it helps but I'm not missing out on any targets, deadlines. Manager is quite happy with what I'm delivering and I might even get promoted next year. 
But today I didn't have much to do and I just felt like relaxing and listening to a podcast instead of upskilling or working on left over small tasks at work.
Also, I'm a junior. Just finished my first year after grad school.

Thanks!",t2_bv171ji2,False,,0,False,Is it normal to feel guilty when you don't work much on a work day?,[],r/datascience,False,6,discussion,0,,,False,t3_nw9zkt,False,dark,0.95,,public,498,4,{},,,False,[],,False,False,,{},Discussion,False,498,,False,False,self,False,,[],{},,True,,1623312012.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Work from home has been wonderful ever since it has been implemented but I&amp;#39;ve found myself not working much on days like today. I just wasn&amp;#39;t feeling like it. I&amp;#39;m not sure if it&amp;#39;s a good thing or a bad thing about work from home. &lt;/p&gt;

&lt;p&gt;Do you guys have days like this too?&lt;/p&gt;

&lt;p&gt;Not sure if it helps but I&amp;#39;m not missing out on any targets, deadlines. Manager is quite happy with what I&amp;#39;m delivering and I might even get promoted next year. 
But today I didn&amp;#39;t have much to do and I just felt like relaxing and listening to a podcast instead of upskilling or working on left over small tasks at work.
Also, I&amp;#39;m a junior. Just finished my first year after grad school.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw9zkt,True,,quite--average,,147,True,all_ads,False,[],False,,/r/datascience/comments/nw9zkt/is_it_normal_to_feel_guilty_when_you_dont_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw9zkt/is_it_normal_to_feel_guilty_when_you_dont_work/,515405,1623283212.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi guys! So I'm enrolled in a university course called Legal Analytics for which I had to do a group research project on a dataset of case law metadata. So our group basically made dictionaries to map existing metadata values to a new label (i.e. apply label x if y exists in metadata).

In order to improve our dictionaries we repeated a certain exploratory exercise multiple times to discover additional values and add those to the dictionaries and after each update of the dictionaries we repeated the labelling proces as well. Ultimately, we visualized this process and the difference in labelled cases between each repetition of updating dictionaries and labelling cases in some line charts. In our draft research paper we referred to each repetition as an 'iteration' and to the process as a whole as 'classification'.

Now, this is where my question and confusion comes in. We've got some PhDs tutoring this course and grading our research projects. A part of their feedback on our draft research paper was about our use of terminology; they basically said that terms like iteration and classification are reserved to machine learning and we shouldn't use those terms in our paper.

This left me not entirely convinced, because while machine learning is all the fuzz these days these terms are surely not limited to the field of machine learning? So, are my tutors right? Are more simple solutions suddenly excluded from using this kind of terminology? I'll be looking forward to your responses!",t2_cugn2,False,,0,False,Analytical uni project: can't use certain terminology in research paper?,[],r/datascience,False,6,discussion,0,,,False,t3_nwv0ve,False,dark,0.72,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1623379195.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! So I&amp;#39;m enrolled in a university course called Legal Analytics for which I had to do a group research project on a dataset of case law metadata. So our group basically made dictionaries to map existing metadata values to a new label (i.e. apply label x if y exists in metadata).&lt;/p&gt;

&lt;p&gt;In order to improve our dictionaries we repeated a certain exploratory exercise multiple times to discover additional values and add those to the dictionaries and after each update of the dictionaries we repeated the labelling proces as well. Ultimately, we visualized this process and the difference in labelled cases between each repetition of updating dictionaries and labelling cases in some line charts. In our draft research paper we referred to each repetition as an &amp;#39;iteration&amp;#39; and to the process as a whole as &amp;#39;classification&amp;#39;.&lt;/p&gt;

&lt;p&gt;Now, this is where my question and confusion comes in. We&amp;#39;ve got some PhDs tutoring this course and grading our research projects. A part of their feedback on our draft research paper was about our use of terminology; they basically said that terms like iteration and classification are reserved to machine learning and we shouldn&amp;#39;t use those terms in our paper.&lt;/p&gt;

&lt;p&gt;This left me not entirely convinced, because while machine learning is all the fuzz these days these terms are surely not limited to the field of machine learning? So, are my tutors right? Are more simple solutions suddenly excluded from using this kind of terminology? I&amp;#39;ll be looking forward to your responses!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwv0ve,True,,Jansenzoon,,20,True,all_ads,False,[],False,,/r/datascience/comments/nwv0ve/analytical_uni_project_cant_use_certain/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwv0ve/analytical_uni_project_cant_use_certain/,515405,1623350395.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am still in academics and most of my activities and projects have had limited scope and it's nothing like what I may work like in industry. I would really appreciate if someone can share their experience of working as a data scientist in the industry.

How do you go about finding a problem/challenge/idea of what to work on? What steps does a project go through? Do you follow SDLC kind of methodologies for a DS project? Do you participate in model deployment? What tools do you use? 

I know this is not a structured list, but even though I am working through school it is really difficult for me to absorb anything without actually understanding industry practices",t2_a4yvnttd,False,,0,False,Project lifecycle,[],r/datascience,False,6,education,0,,,False,t3_nx3q3s,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623402778.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am still in academics and most of my activities and projects have had limited scope and it&amp;#39;s nothing like what I may work like in industry. I would really appreciate if someone can share their experience of working as a data scientist in the industry.&lt;/p&gt;

&lt;p&gt;How do you go about finding a problem/challenge/idea of what to work on? What steps does a project go through? Do you follow SDLC kind of methodologies for a DS project? Do you participate in model deployment? What tools do you use? &lt;/p&gt;

&lt;p&gt;I know this is not a structured list, but even though I am working through school it is really difficult for me to absorb anything without actually understanding industry practices&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx3q3s,True,,DietMediocre8993,,3,True,all_ads,False,[],False,,/r/datascience/comments/nx3q3s/project_lifecycle/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx3q3s/project_lifecycle/,515405,1623373978.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I head a BI department for a startup; well not much of a startup; We are almost a small enterprise now.

Currently, I run a suite of BI applications from my machine. My main applications that are run regularly are:

* MS Office
* Alteryx
* Database (PostGres)
* Python
* Power BI
* Tableau

Its quite capable for what it does however, we are looking to make the access for these tools organisational and hence, we are going to invest in a server where all these applications will reside.

Can the community please help me with the best server configurations for running the above applications seamlessly? 

Of course, we are looking to have Windows Server as an OS.",t2_kxy1kb7,False,,0,False,What server configuration should I get for Organisational data analytics applications?,[],r/datascience,False,6,discussion,0,,,False,t3_nx0v79,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623394184.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I head a BI department for a startup; well not much of a startup; We are almost a small enterprise now.&lt;/p&gt;

&lt;p&gt;Currently, I run a suite of BI applications from my machine. My main applications that are run regularly are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MS Office&lt;/li&gt;
&lt;li&gt;Alteryx&lt;/li&gt;
&lt;li&gt;Database (PostGres)&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Power BI&lt;/li&gt;
&lt;li&gt;Tableau&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Its quite capable for what it does however, we are looking to make the access for these tools organisational and hence, we are going to invest in a server where all these applications will reside.&lt;/p&gt;

&lt;p&gt;Can the community please help me with the best server configurations for running the above applications seamlessly? &lt;/p&gt;

&lt;p&gt;Of course, we are looking to have Windows Server as an OS.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx0v79,True,,card_chase,,2,True,all_ads,False,[],False,,/r/datascience/comments/nx0v79/what_server_configuration_should_i_get_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx0v79/what_server_configuration_should_i_get_for/,515405,1623365384.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have north of 100 products with a request to build more accurate forecast for them. For a 1-2 month turnaround, what are folks thought process on the approach: 1) go for each distinct product and determine best model and parameters, or 2) find a higher level hierarchy to reduce cardinality from 100 plus to let’s say 10-12 models which is more manageable. 

First time working on a ts project and would appreciate your thoughts. Ty!",t2_3hsrhmve,False,,0,False,Time Series for 125 distinct products: Suggestions,[],r/datascience,False,6,discussion,0,,,False,t3_nwrtvs,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1623371236.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have north of 100 products with a request to build more accurate forecast for them. For a 1-2 month turnaround, what are folks thought process on the approach: 1) go for each distinct product and determine best model and parameters, or 2) find a higher level hierarchy to reduce cardinality from 100 plus to let’s say 10-12 models which is more manageable. &lt;/p&gt;

&lt;p&gt;First time working on a ts project and would appreciate your thoughts. Ty!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwrtvs,True,,Novel_Frosting_1977,,8,False,all_ads,False,[],False,,/r/datascience/comments/nwrtvs/time_series_for_125_distinct_products_suggestions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwrtvs/time_series_for_125_distinct_products_suggestions/,515405,1623342436.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"How do y’all go about explaining results to the business that aren’t what they expected to be? For example, I have a model that is using variable B as the most important for predictions. The business is very upset because they believe that variable A should be the most important for predictions. We’ve showed them numerous examples within the dataset that was used for training as to why variable A is not the most important but they’re still dead set on hard coding that into the code regardless if it doesn’t improve the results. Any pointers here??",t2_dxitb2k,False,,0,False,Explaining results to the business side,[],r/datascience,False,6,discussion,0,,,False,t3_nwrrb2,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623371052.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do y’all go about explaining results to the business that aren’t what they expected to be? For example, I have a model that is using variable B as the most important for predictions. The business is very upset because they believe that variable A should be the most important for predictions. We’ve showed them numerous examples within the dataset that was used for training as to why variable A is not the most important but they’re still dead set on hard coding that into the code regardless if it doesn’t improve the results. Any pointers here??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwrrb2,True,,WittyWillow3,,13,True,all_ads,False,[],False,,/r/datascience/comments/nwrrb2/explaining_results_to_the_business_side/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwrrb2/explaining_results_to_the_business_side/,515405,1623342252.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Been studying data science/machine learning for about a year now and struggling to remember all the terms and information like what each model does, validation methods, statistics etc. How do you remember all these terms, what they do and when to use them?",t2_1bbzzghq,False,,0,False,How do you remember all the data science/machine learning terms?,[],r/datascience,False,6,discussion,0,,,False,t3_nwj24j,False,dark,0.95,,public,18,0,{},,,False,[],,False,False,,{},Discussion,False,18,,False,False,self,False,,[],{},,True,,1623344757.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Been studying data science/machine learning for about a year now and struggling to remember all the terms and information like what each model does, validation methods, statistics etc. How do you remember all these terms, what they do and when to use them?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwj24j,True,,Barlton_Canks,,21,True,all_ads,False,[],False,,/r/datascience/comments/nwj24j/how_do_you_remember_all_the_data_sciencemachine/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwj24j/how_do_you_remember_all_the_data_sciencemachine/,515405,1623315957.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_34qgdyb6,False,,0,False,"I am looking for a personalization project, can you suggest what new things we can do with some open source data?",[],r/datascience,False,6,projects,0,,,False,t3_nx72ro,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1623413807.0,text,6,,,text,self.datascience,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx72ro,True,,yaakarsh1011,,12,True,all_ads,False,[],False,,/r/datascience/comments/nx72ro/i_am_looking_for_a_personalization_project_can/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx72ro/i_am_looking_for_a_personalization_project_can/,515405,1623385007.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"&amp;#x200B;

Hi, I used a feed forward neural network on Keras to approximate a concave one dimensional function , I would like to find the argmax and the max of my neural network, what would be the easiest way to solve this ? Should I implement something myself or does Keras already have some buil in function for that ?

Thanks!",t2_64rkugj2,False,,0,False,neural network optimization,[],r/datascience,False,6,discussion,0,,,False,t3_nx0ygv,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623394452.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hi, I used a feed forward neural network on Keras to approximate a concave one dimensional function , I would like to find the argmax and the max of my neural network, what would be the easiest way to solve this ? Should I implement something myself or does Keras already have some buil in function for that ?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx0ygv,True,,draleo183013,,33,True,all_ads,False,[],False,,/r/datascience/comments/nx0ygv/neural_network_optimization/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx0ygv/neural_network_optimization/,515405,1623365652.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a problem statement of designing a recommender system for a pharmaceutical company which also produces healthcare products (vitamins, body care products, etc.). But they don't have an e-commerce website similar to Amazon, etc. Therefore, data collection is incomplete and dodgy to say the least.

Their current approach involves placing adverts on social media platforms like Facebook, Instagram, etc. and then getting/gathering user data from such platform by either using platform specific APIs and/or web scraping.

To design a recommender system for this company, can you suggest:

1. what data/features I might need?
2. which system to use- collaborative/content based filtering

I am new to recommender system domain and this will be my pilot project.

Thanks!",t2_2mmql89p,False,,0,False,Recommender System Advice,[],r/datascience,False,6,discussion,0,,,False,t3_nwnlc4,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623360306.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a problem statement of designing a recommender system for a pharmaceutical company which also produces healthcare products (vitamins, body care products, etc.). But they don&amp;#39;t have an e-commerce website similar to Amazon, etc. Therefore, data collection is incomplete and dodgy to say the least.&lt;/p&gt;

&lt;p&gt;Their current approach involves placing adverts on social media platforms like Facebook, Instagram, etc. and then getting/gathering user data from such platform by either using platform specific APIs and/or web scraping.&lt;/p&gt;

&lt;p&gt;To design a recommender system for this company, can you suggest:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;what data/features I might need?&lt;/li&gt;
&lt;li&gt;which system to use- collaborative/content based filtering&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am new to recommender system domain and this will be my pilot project.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwnlc4,True,,grid_world,,2,True,all_ads,False,[],False,,/r/datascience/comments/nwnlc4/recommender_system_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwnlc4/recommender_system_advice/,515405,1623331506.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Outside of a job working for a company or freelancing online as an employee or contractor. 

What other options are possible for a software developer to make money using their programming/computer science skills/data science background? 

Any of you using your technology/data skills to make money outside of your regular job?",t2_a4a10m8w,False,,0,False,Ways to make money using tech/data skills outside of a regular job?,[],r/datascience,False,6,career,0,,,False,t3_nwfvo3,False,dark,0.81,,public,15,0,{},,,False,[],,False,False,,{},Career,False,15,,False,False,self,False,,[],{},,True,,1623331527.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Outside of a job working for a company or freelancing online as an employee or contractor. &lt;/p&gt;

&lt;p&gt;What other options are possible for a software developer to make money using their programming/computer science skills/data science background? &lt;/p&gt;

&lt;p&gt;Any of you using your technology/data skills to make money outside of your regular job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwfvo3,True,,jodster327,,29,True,all_ads,False,[],False,,/r/datascience/comments/nwfvo3/ways_to_make_money_using_techdata_skills_outside/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwfvo3/ways_to_make_money_using_techdata_skills_outside/,515405,1623302727.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I’m a newbie in data science and I haven’t worked on any real life projects till now. 
The projects I’ve completed have been part of some online course where the instructors either give away the solution/approach to the problem or give you a big enough hint so that you don’t think about the approach but just code whatever they say.

I’ve been trying to solve a couple of projects of my own and it’s been hard to build that ds intuition. 
Normally, how do you guys approach a new problem? What kind of preprocessing do you do to your data, which model to fit, which metrics to choose? How do you guys decide it? I’d really like if you could help me develop some intuition on this",t2_9ga7xr3c,False,,0,False,How do you guys approach a new data science project?,[],r/datascience,False,6,discussion,0,,,False,t3_nwuckk,False,dark,0.44,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623377532.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a newbie in data science and I haven’t worked on any real life projects till now. 
The projects I’ve completed have been part of some online course where the instructors either give away the solution/approach to the problem or give you a big enough hint so that you don’t think about the approach but just code whatever they say.&lt;/p&gt;

&lt;p&gt;I’ve been trying to solve a couple of projects of my own and it’s been hard to build that ds intuition. 
Normally, how do you guys approach a new problem? What kind of preprocessing do you do to your data, which model to fit, which metrics to choose? How do you guys decide it? I’d really like if you could help me develop some intuition on this&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwuckk,True,,ElNinoo9,,1,True,all_ads,False,[],False,,/r/datascience/comments/nwuckk/how_do_you_guys_approach_a_new_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwuckk/how_do_you_guys_approach_a_new_data_science/,515405,1623348732.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If someone at work approaches you to consult on a type of analytical project that you’ve never worked on before, what’s the best way to respond?

A. Sorry, I haven’t done this before and can’t really help you.
B. While I’m not familiar with this kind of analysis, I’d be glad to take a look and provide my input. 
C. Send it my way and I’ll make magic happen.
D. ?",t2_qnn4s,False,,0,False,Data project you’ve never worked on before,[],r/datascience,False,6,discussion,0,,,False,t3_nvylxh,False,dark,0.93,,public,92,0,{},,,False,[],,False,False,,{},Discussion,False,92,,False,False,self,False,,[],{},,True,,1623281360.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If someone at work approaches you to consult on a type of analytical project that you’ve never worked on before, what’s the best way to respond?&lt;/p&gt;

&lt;p&gt;A. Sorry, I haven’t done this before and can’t really help you.
B. While I’m not familiar with this kind of analysis, I’d be glad to take a look and provide my input. 
C. Send it my way and I’ll make magic happen.
D. ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvylxh,True,,Aoiumi1234,,32,True,all_ads,False,[],False,,/r/datascience/comments/nvylxh/data_project_youve_never_worked_on_before/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvylxh/data_project_youve_never_worked_on_before/,515405,1623252560.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi guys, hope you can help me out with this one! 

I'm looking to do some research I've never done before; to track / visualize how many times a word has been mentioned on a website and on what dates they were mentioned. 

So for example, lets say I want to get data on how many times the word ""Covid-19"" has been mentioned on a specific news website on each date over the past 2 years. I don't have a clue on how to attach a date to each mention, how could I do this? 

Thanks in advance!",t2_yjkb6,False,,0,False,Need some advice on tracking specific words on a website over time,[],r/datascience,False,6,tooling,0,,,False,t3_nwlc2r,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1623353565.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, hope you can help me out with this one! &lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking to do some research I&amp;#39;ve never done before; to track / visualize how many times a word has been mentioned on a website and on what dates they were mentioned. &lt;/p&gt;

&lt;p&gt;So for example, lets say I want to get data on how many times the word &amp;quot;Covid-19&amp;quot; has been mentioned on a specific news website on each date over the past 2 years. I don&amp;#39;t have a clue on how to attach a date to each mention, how could I do this? &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwlc2r,True,,twitchy-y,,4,True,all_ads,False,[],False,,/r/datascience/comments/nwlc2r/need_some_advice_on_tracking_specific_words_on_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwlc2r/need_some_advice_on_tracking_specific_words_on_a/,515405,1623324765.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I hope this is allowed here because the interview I am about to have is Data Analyst instead of Data Science.

&amp;#x200B;

So I have been doing python self-learn. I even took IBM certificate on Data Science. Sadly enough I did not have much experience in doing it. My only IT experience is an Web Developer internship that I just recently ended. 

&amp;#x200B;

I am determined to give Data Science/Analyst a try. Now that I will be interviewed for one, my imposter syndrome just kicked in and feels like I might not qualified for the job. What are the good advice for the interview? I have python and SQL skill myself. The company I applied for listed required around 1 - 2 years of analyst experience, also needed work on Azure. This could be my very first data related job, and I wish to overcome my imposter syndrome...",t2_4e9t63m8,False,,0,False,Imposter Syndrome kicks in for next week interview (Data Analyst),[],r/datascience,False,6,career,0,,,False,t3_nwjmc1,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623347129.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I hope this is allowed here because the interview I am about to have is Data Analyst instead of Data Science.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I have been doing python self-learn. I even took IBM certificate on Data Science. Sadly enough I did not have much experience in doing it. My only IT experience is an Web Developer internship that I just recently ended. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am determined to give Data Science/Analyst a try. Now that I will be interviewed for one, my imposter syndrome just kicked in and feels like I might not qualified for the job. What are the good advice for the interview? I have python and SQL skill myself. The company I applied for listed required around 1 - 2 years of analyst experience, also needed work on Azure. This could be my very first data related job, and I wish to overcome my imposter syndrome...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwjmc1,True,,xopherwwl,,6,True,all_ads,False,[],False,,/r/datascience/comments/nwjmc1/imposter_syndrome_kicks_in_for_next_week/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwjmc1/imposter_syndrome_kicks_in_for_next_week/,515405,1623318329.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi,

Now I’m sure I’m not the only one but - although causal models are the holy grail, due to time and cost and laziness constraints all my models are just association models, not necessarily causal but observational in nature.

Easy, so I then communicate the associative nature of model and no DO operator or intervention has been modelled at all.
E.g. recently and elasticity model i modelled observed price % change against qty sold %  change.
But then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e.g. promoting a product in a given week.

So my main question - how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model?

Obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs. But then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long.

Do we just accept the mental leap from association to assuming some causal relation at times (in that intervening now will somehow get similar results as merely observing did) and keep collecting our pay checks with a smile?
I’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives.",t2_64z1j9uv,False,,0,False,Association model used as causal models,[],r/datascience,False,6,education,0,,,False,t3_nw9slt,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1623311412.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Now I’m sure I’m not the only one but - although causal models are the holy grail, due to time and cost and laziness constraints all my models are just association models, not necessarily causal but observational in nature.&lt;/p&gt;

&lt;p&gt;Easy, so I then communicate the associative nature of model and no DO operator or intervention has been modelled at all.
E.g. recently and elasticity model i modelled observed price % change against qty sold %  change.
But then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e.g. promoting a product in a given week.&lt;/p&gt;

&lt;p&gt;So my main question - how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model?&lt;/p&gt;

&lt;p&gt;Obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs. But then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long.&lt;/p&gt;

&lt;p&gt;Do we just accept the mental leap from association to assuming some causal relation at times (in that intervening now will somehow get similar results as merely observing did) and keep collecting our pay checks with a smile?
I’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw9slt,True,,darter_analyst,,5,True,all_ads,False,[],False,,/r/datascience/comments/nw9slt/association_model_used_as_causal_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw9slt/association_model_used_as_causal_models/,515405,1623282612.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hello, I’m an undergrad student whose been working with my college team doing baseball analytics work. Current role with a few other students had been to look at their pitch sequencing data and to find insights. I had an idea today that I was presenting to them, and I made sure not to overwhelm them with technicals, provided insight into methodology, (didn’t state a value proposition, which is where I probably messed up), but as I’m going, midway through my 30-45 sec breakdown the professor cuts me off and starts talking about another students idea. Doesn’t even let me finish. I was pretty disappointed because I was pretty confident in my idea, but he didn’t want to continue listening.

My question is, does this happen a lot in industry when presenting to management? Do they sometimes dismiss your ideas like this? 

Also maybe give me some advice on how to pitch something too, I am planning on scheduling a meeting with those two professors again to give them a full run through of my idea and why it’s useful.",t2_5w4i5kd1,False,,0,False,"Professor cuts me off mid presentation and dismissed my idea, how common is this in industry with management?",[],r/datascience,False,6,discussion,0,,,False,t3_nwbzk3,False,dark,0.58,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623318224.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I’m an undergrad student whose been working with my college team doing baseball analytics work. Current role with a few other students had been to look at their pitch sequencing data and to find insights. I had an idea today that I was presenting to them, and I made sure not to overwhelm them with technicals, provided insight into methodology, (didn’t state a value proposition, which is where I probably messed up), but as I’m going, midway through my 30-45 sec breakdown the professor cuts me off and starts talking about another students idea. Doesn’t even let me finish. I was pretty disappointed because I was pretty confident in my idea, but he didn’t want to continue listening.&lt;/p&gt;

&lt;p&gt;My question is, does this happen a lot in industry when presenting to management? Do they sometimes dismiss your ideas like this? &lt;/p&gt;

&lt;p&gt;Also maybe give me some advice on how to pitch something too, I am planning on scheduling a meeting with those two professors again to give them a full run through of my idea and why it’s useful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwbzk3,True,,veeeerain,,15,True,all_ads,False,[],False,,/r/datascience/comments/nwbzk3/professor_cuts_me_off_mid_presentation_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwbzk3/professor_cuts_me_off_mid_presentation_and/,515405,1623289424.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi there, I have a super basic/amateur question and apologies for it being elementary. But I figured that this group would be the right set of professionals to ask. (PS - did I use the right flair?) 

I have some basic grouping (sums) and filtering needs for a large CSV data set. (500K plus records). Excel on the Mac is abysmal for such tasks, even though a basic pivot table would accomplish what I need. 

Is there any tool out there (could be web-based or local) that fits the bill, and sits between Excel and a full-fledged database? (I’d rather not import to SQL and query it back out.) 

Any thoughts/suggestions would be appreciated!",t2_bui35ym,False,,0,False,Super Basic Question: Filter &amp; Grouping Tool for Mac?,[],r/datascience,False,6,tooling,0,,,False,t3_nwhghp,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623337894.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, I have a super basic/amateur question and apologies for it being elementary. But I figured that this group would be the right set of professionals to ask. (PS - did I use the right flair?) &lt;/p&gt;

&lt;p&gt;I have some basic grouping (sums) and filtering needs for a large CSV data set. (500K plus records). Excel on the Mac is abysmal for such tasks, even though a basic pivot table would accomplish what I need. &lt;/p&gt;

&lt;p&gt;Is there any tool out there (could be web-based or local) that fits the bill, and sits between Excel and a full-fledged database? (I’d rather not import to SQL and query it back out.) &lt;/p&gt;

&lt;p&gt;Any thoughts/suggestions would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwhghp,True,,wkeber,,3,True,all_ads,False,[],False,,/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/,515405,1623309094.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Real-time technologies are powerful but add significant complexity to your data architecture.

Find out how to reap the benefits of real-time processing with the least architectural changes and maintenance effort: https://dashbird.io/blog/real-time-processing-analytical/",t2_6eow2vnb,False,,0,False,Is real-time processing worth it for your analytical use cases?,[],r/datascience,False,6,discussion,0,,,False,t3_nwkfcf,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623350404.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Real-time technologies are powerful but add significant complexity to your data architecture.&lt;/p&gt;

&lt;p&gt;Find out how to reap the benefits of real-time processing with the least architectural changes and maintenance effort: &lt;a href=""https://dashbird.io/blog/real-time-processing-analytical/""&gt;https://dashbird.io/blog/real-time-processing-analytical/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwkfcf,True,,Dashbird,,0,True,all_ads,False,[],False,,/r/datascience/comments/nwkfcf/is_realtime_processing_worth_it_for_your/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwkfcf/is_realtime_processing_worth_it_for_your/,515405,1623321604.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?auto=webp&amp;s=ac9f24fab2a9805f51a0a01e7f17ac448ce608c0', 'width': 1000, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9161938e7257e4c0569c80bcd5259d163f8b9bb1', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0228bffc89df0acf7a21624ecd32649e077b1604', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2c26774ec7861e88a167f2a00f6ba5ddd6a71b', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=051d8de17ee457c6a5746644cde39f5f07f6f821', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9c6a5face24fd4028a2add16411b02491aafb3c', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'sVlk7Y9ZxBsh0S3J6tiSTcn04usQl3zDO62iNvc_Lj4'}], 'enabled': False}",,,,,
,datascience,"I am thinking about joining Kaggle competitions to try and win money prizes, however I am unsure about the time and money (cloud compute?) it would take to get a real shot at it.

Did you try to join a competition with the specific goal to win money? Any advice?",t2_cn40g2j,False,,0,False,Shooting for Kaggle Competition Prizes - is it worth it?,[],r/datascience,False,6,discussion,0,,,False,t3_nwf2wq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623328454.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am thinking about joining Kaggle competitions to try and win money prizes, however I am unsure about the time and money (cloud compute?) it would take to get a real shot at it.&lt;/p&gt;

&lt;p&gt;Did you try to join a competition with the specific goal to win money? Any advice?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwf2wq,True,,EnricoT0,,5,True,all_ads,False,[],False,,/r/datascience/comments/nwf2wq/shooting_for_kaggle_competition_prizes_is_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwf2wq/shooting_for_kaggle_competition_prizes_is_it/,515405,1623299654.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"**Use-case:** *A particular product has X remaining items in stocks. Predict when the product is sold out, given both time-invariant and time-varying covariates.*

Is a multivariate Cox regression model appropriate for this problem, where the ""event"" is the product being sold out? What about if the items can be restocked (i.e. by users issuing returns), effectively increasing the stock in the process?

Is there a more sensible approach? I tried to look up something like ARIMAX for non-negative + discrete forecasting to predict remaining stock, but it seems unnecessarily complicated.",t2_9aqvgklw,False,,0,False,Predicting when a product will be sold out - is survival analysis appropriate?,[],r/datascience,False,6,discussion,0,,,False,t3_nw5ptn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623299888.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;Use-case:&lt;/strong&gt; &lt;em&gt;A particular product has X remaining items in stocks. Predict when the product is sold out, given both time-invariant and time-varying covariates.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Is a multivariate Cox regression model appropriate for this problem, where the &amp;quot;event&amp;quot; is the product being sold out? What about if the items can be restocked (i.e. by users issuing returns), effectively increasing the stock in the process?&lt;/p&gt;

&lt;p&gt;Is there a more sensible approach? I tried to look up something like ARIMAX for non-negative + discrete forecasting to predict remaining stock, but it seems unnecessarily complicated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw5ptn,True,,populus27,,7,True,all_ads,False,[],False,,/r/datascience/comments/nw5ptn/predicting_when_a_product_will_be_sold_out_is/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw5ptn/predicting_when_a_product_will_be_sold_out_is/,515405,1623271088.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a survey where users rate a number of recommendations. The user does not know that half the recommendations are made by system A, and the other half are made by system B.

I initially thought this was called A/B testing, however, that does not seem to apply. What is the correct name for this ?",t2_r6jti,False,,0,False,What is the correct terminology for this type of survey?,[],r/datascience,False,6,discussion,0,,,False,t3_nvup3x,False,dark,0.88,,public,12,0,{},,,False,[],,False,False,,{},Discussion,False,12,,False,False,self,False,,[],{},,True,,1623270400.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a survey where users rate a number of recommendations. The user does not know that half the recommendations are made by system A, and the other half are made by system B.&lt;/p&gt;

&lt;p&gt;I initially thought this was called A/B testing, however, that does not seem to apply. What is the correct name for this ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvup3x,True,,thunderbirdsetup,,8,True,all_ads,False,[],False,,/r/datascience/comments/nvup3x/what_is_the_correct_terminology_for_this_type_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvup3x/what_is_the_correct_terminology_for_this_type_of/,515405,1623241600.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am retooling and revisiting STATA and came across my old notes on when to use each regression type.  

&amp;#x200B;

For when to use bivariate regression, my notes read: 

&amp;#x200B;

*""Don't. It's trash. Bivariate (one independent and one dependent) models don’t tell you much other than confirming if there is a strong correlation.""* 

&amp;#x200B;

I don't recall ever actually using it, and I started googling - but now I am puzzled.  

&amp;#x200B;

Is ""bivariate regression"" just a measure of correlation between independent variable X and dependent variable Y?? 

If so, when would it ever be useful?",t2_8azmn3,False,,0,False,"Question: when (if ever) is ""bivariate regression"" useful??",[],r/datascience,False,6,discussion,0,,,False,t3_nw4wxh,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623297721.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am retooling and revisiting STATA and came across my old notes on when to use each regression type.  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For when to use bivariate regression, my notes read: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;quot;Don&amp;#39;t. It&amp;#39;s trash. Bivariate (one independent and one dependent) models don’t tell you much other than confirming if there is a strong correlation.&amp;quot;&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t recall ever actually using it, and I started googling - but now I am puzzled.  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is &amp;quot;bivariate regression&amp;quot; just a measure of correlation between independent variable X and dependent variable Y?? &lt;/p&gt;

&lt;p&gt;If so, when would it ever be useful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw4wxh,True,,TimboCA,,14,True,all_ads,False,[],False,,/r/datascience/comments/nw4wxh/question_when_if_ever_is_bivariate_regression/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw4wxh/question_when_if_ever_is_bivariate_regression/,515405,1623268921.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, I'm trying to construct a model that takes in the first two/three input values from the user and based on those values, decides the best 4th independent variable to ask the user to optimize the time needed to bucket the user into one of the 5-6 different buckets (supervised clusters). The initial approaches that I was considering were bidirectional LSTM/RNN's.. but after reading up on these two, I'm now thinking there may be more suitable/clever approaches to tackles this. Thank you, the deep learning gods of Reddit xx",t2_4okx2lwr,False,,0,False,Dynamic model that predicts the best next input variable to ask based on the first two/three inputs,[],r/datascience,False,6,projects,0,,,False,t3_nvz9f7,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1623283105.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m trying to construct a model that takes in the first two/three input values from the user and based on those values, decides the best 4th independent variable to ask the user to optimize the time needed to bucket the user into one of the 5-6 different buckets (supervised clusters). The initial approaches that I was considering were bidirectional LSTM/RNN&amp;#39;s.. but after reading up on these two, I&amp;#39;m now thinking there may be more suitable/clever approaches to tackles this. Thank you, the deep learning gods of Reddit xx&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvz9f7,True,,lalopark,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvz9f7/dynamic_model_that_predicts_the_best_next_input/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvz9f7/dynamic_model_that_predicts_the_best_next_input/,515405,1623254305.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"How would you predict who someone may want to send a Snapchat or Gmail to?

I cant really think how to approach this interview question, can anyone help me get started with it?",t2_zqp1wbh,False,,0,False,Predicting who someone may want to send a message to,[],r/datascience,False,6,education,0,,,False,t3_nvrxp6,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},Education,False,7,,False,False,self,False,,[],{},,True,,1623260996.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How would you predict who someone may want to send a Snapchat or Gmail to?&lt;/p&gt;

&lt;p&gt;I cant really think how to approach this interview question, can anyone help me get started with it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvrxp6,True,,SilurianWenlock,,7,True,all_ads,False,[],False,,/r/datascience/comments/nvrxp6/predicting_who_someone_may_want_to_send_a_message/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvrxp6/predicting_who_someone_may_want_to_send_a_message/,515405,1623232196.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"As the title suggests, there are a lot of good reviews on Datacamp, however, i've taken courses on edx before and they are amazing. There are a few from MIT and IBM etc. 

for a beginner, what would you recommend and why?",t2_13t60b,False,,0,False,"Datacamp vs edx, which would you recommend and why?",[],r/datascience,False,6,education,0,,,False,t3_nv8fwf,False,dark,0.96,,public,132,1,{},,,False,[],,False,False,,{},Education,False,132,,False,False,self,False,,[],{},,True,,1623199282.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title suggests, there are a lot of good reviews on Datacamp, however, i&amp;#39;ve taken courses on edx before and they are amazing. There are a few from MIT and IBM etc. &lt;/p&gt;

&lt;p&gt;for a beginner, what would you recommend and why?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv8fwf,True,,drugsarebadmky,,69,True,all_ads,False,[],False,,/r/datascience/comments/nv8fwf/datacamp_vs_edx_which_would_you_recommend_and_why/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv8fwf/datacamp_vs_edx_which_would_you_recommend_and_why/,515405,1623170482.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_6rd1h,False,,0,False,"Sell me on your physical input set up - keyboards, mice, accessories",[],r/datascience,False,6,discussion,0,,,False,t3_nvvid4,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623272811.0,text,6,,,text,self.datascience,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvvid4,True,,greenearrow,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvvid4/sell_me_on_your_physical_input_set_up_keyboards/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvvid4/sell_me_on_your_physical_input_set_up_keyboards/,515405,1623244011.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I am curious to know how everyone version controls their datasets in projects and research projects?,t2_i1a46n1,False,,0,False,[Q] How do you version control datasets?,[],r/datascience,False,6,discussion,0,,,False,t3_nvhd0j,False,dark,0.78,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1623222717.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am curious to know how everyone version controls their datasets in projects and research projects?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhd0j,True,,enzsio,,20,True,all_ads,False,[],False,,/r/datascience/comments/nvhd0j/q_how_do_you_version_control_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhd0j/q_how_do_you_version_control_datasets/,515405,1623193917.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, right now I'm struggling to pull data in order to make reports or any analytics. Everything is scattered around either in excel files or in a bunch of platforms. I just started so I don't have much working experience and I know, you guys, might be able to give some tips that could help me on this. Really appreciate it!",t2_117xez,False,,0,False,"[Q] currently working as DA, company doesn't have any cloud platform or servers",[],r/datascience,False,6,career,0,,,False,t3_nvm9qj,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1623238526.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, right now I&amp;#39;m struggling to pull data in order to make reports or any analytics. Everything is scattered around either in excel files or in a bunch of platforms. I just started so I don&amp;#39;t have much working experience and I know, you guys, might be able to give some tips that could help me on this. Really appreciate it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvm9qj,True,,diraceusse,,4,True,all_ads,False,[],False,,/r/datascience/comments/nvm9qj/q_currently_working_as_da_company_doesnt_have_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvm9qj/q_currently_working_as_da_company_doesnt_have_any/,515405,1623209726.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Good afternoon. I am facing a challenging situation on my career now and I would like some advice. I will provide a very brief summary of my experience.

First of all, I am based in Portugal, Europe. I have a a Mechanical Engineering Degree focused on Energy Systems  (With a bunch of optimisation, ML and numerical simulations mixed in, including the master thesis, as well as years in extra curricular activities revolving around Python and ML projects.). I have 2 years and a couple of months experience in Data Science in the following roles:

1- Data Science Consultant for 1y and 2m. Role included developing a genetic algorithm, time series analysis with LSTM, Prophet and Arima, as well as a Big Data project on Databricks and SQL. I started feeling like I wanted to be part of internal Data Science team, so that I could really own my projects and that is why I left for company 2:

2- Data Scientist for 7m in an Energy Related Company. Me and another person were brought in to start a Data Science team.  This company was not a technological company and, there was not much to do really. We were supposed to get Data to start working on some use cases, but after 7 months we did not have any data nor any indication that we would have in the future. During my time in there I explored outlier detection methods like Isolation Forests and LOF, did a local web page using Flask, Javascript and Bootstrap, and also worked on some data cleaning and process automation pipelines. I had an offer via a recruiter that contacted me on Linkedin that I felt would make my job much more meaningful, and at the time it was very hard for me to justify not taking it, so I took it.

3- 7m Fintech Startup. This was pretty much what I wanted in a Data Science Role. Build a model using H20 AI for client grading and well as lot of other analysis for managing client and portfolio risk. I felt like my job truly mattered and I was extremely satisfied with it and super excited for all the possibilities that I would have, but due to a very unexpected issue with financing we ended up closing after 7m on the role.

More and more I have been craving time to truly dedicate myself to learn and develop some personal projects, mostly related to crypto and financial markets. My question is, can I afford to have a gap on my CV, assuming I fill it with personal projects or do I need to start looking for jobs immediately? Also, since I left university  I wanted to start an international career and move to Germany, Sweden, Switzerland, Austria, UK... ( I have been learning German for 2 years). Would this be a good time to truly try to move? Do I even have a chance if I just send CV's from Portugal (I am wiling to reallocate) and get a job on any of this countries before I move or would I really have to move there before I start trying to get a job?

Thanks all!",t2_wik6z,False,,0,False,"Startup went bankrupt, need career advice",[],r/datascience,False,6,career,0,,,False,t3_nv6d5y,False,dark,0.87,,public,26,1,{},,,False,[],,False,False,,{},Career,False,26,,False,False,self,1623165828.0,,[],{},,True,,1623194407.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Good afternoon. I am facing a challenging situation on my career now and I would like some advice. I will provide a very brief summary of my experience.&lt;/p&gt;

&lt;p&gt;First of all, I am based in Portugal, Europe. I have a a Mechanical Engineering Degree focused on Energy Systems  (With a bunch of optimisation, ML and numerical simulations mixed in, including the master thesis, as well as years in extra curricular activities revolving around Python and ML projects.). I have 2 years and a couple of months experience in Data Science in the following roles:&lt;/p&gt;

&lt;p&gt;1- Data Science Consultant for 1y and 2m. Role included developing a genetic algorithm, time series analysis with LSTM, Prophet and Arima, as well as a Big Data project on Databricks and SQL. I started feeling like I wanted to be part of internal Data Science team, so that I could really own my projects and that is why I left for company 2:&lt;/p&gt;

&lt;p&gt;2- Data Scientist for 7m in an Energy Related Company. Me and another person were brought in to start a Data Science team.  This company was not a technological company and, there was not much to do really. We were supposed to get Data to start working on some use cases, but after 7 months we did not have any data nor any indication that we would have in the future. During my time in there I explored outlier detection methods like Isolation Forests and LOF, did a local web page using Flask, Javascript and Bootstrap, and also worked on some data cleaning and process automation pipelines. I had an offer via a recruiter that contacted me on Linkedin that I felt would make my job much more meaningful, and at the time it was very hard for me to justify not taking it, so I took it.&lt;/p&gt;

&lt;p&gt;3- 7m Fintech Startup. This was pretty much what I wanted in a Data Science Role. Build a model using H20 AI for client grading and well as lot of other analysis for managing client and portfolio risk. I felt like my job truly mattered and I was extremely satisfied with it and super excited for all the possibilities that I would have, but due to a very unexpected issue with financing we ended up closing after 7m on the role.&lt;/p&gt;

&lt;p&gt;More and more I have been craving time to truly dedicate myself to learn and develop some personal projects, mostly related to crypto and financial markets. My question is, can I afford to have a gap on my CV, assuming I fill it with personal projects or do I need to start looking for jobs immediately? Also, since I left university  I wanted to start an international career and move to Germany, Sweden, Switzerland, Austria, UK... ( I have been learning German for 2 years). Would this be a good time to truly try to move? Do I even have a chance if I just send CV&amp;#39;s from Portugal (I am wiling to reallocate) and get a job on any of this countries before I move or would I really have to move there before I start trying to get a job?&lt;/p&gt;

&lt;p&gt;Thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv6d5y,True,,Elbarro,,15,True,all_ads,False,[],False,,/r/datascience/comments/nv6d5y/startup_went_bankrupt_need_career_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv6d5y/startup_went_bankrupt_need_career_advice/,515405,1623165607.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Sometimes I fell 90% of my time is spent in data wrangling/munging is on data exploration - and most of that is spent trying to make sense of fields created by some long lost business logic or trying to discover how that dataset came to be.

As a consultant, I cry tears of join when I see a team that keeps a data dictionary or a well-organized catalog. Even if on Excel.

What is the best documentation practice you've seen?",t2_1sv1g0c4,False,,0,False,How do you document your datasets?,[],r/datascience,False,6,discussion,0,,,False,t3_nv9xlh,False,dark,0.87,,public,15,1,{},,,False,[],,False,False,,{},Discussion,False,15,,False,False,self,False,,[],{'gid_1': 1},,True,,1623203216.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sometimes I fell 90% of my time is spent in data wrangling/munging is on data exploration - and most of that is spent trying to make sense of fields created by some long lost business logic or trying to discover how that dataset came to be.&lt;/p&gt;

&lt;p&gt;As a consultant, I cry tears of join when I see a team that keeps a data dictionary or a well-organized catalog. Even if on Excel.&lt;/p&gt;

&lt;p&gt;What is the best documentation practice you&amp;#39;ve seen?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv9xlh,True,,FernandoCordeiro,,14,True,all_ads,False,[],False,,/r/datascience/comments/nv9xlh/how_do_you_document_your_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv9xlh/how_do_you_document_your_datasets/,515405,1623174416.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Dear Excel Geeks, we do have it ExcelTips group, but the mode is not active anymore. So I have created [another one](https://www.reddit.com/r/ExcelTips_ActiveGroup/).

If you are a pro-Excel user, [**you can share Excel Tips and Trick in this group**](https://www.reddit.com/r/ExcelTips_ActiveGroup/). If you starting with Excel you can join as well to follow the pro-users.

Reson I created because if it wasn't for the people who share their knowledge absolutely free, I wouldn't have survived and able to make a career as Data Analyst.

I still remember vividly, when I first got a job as Social Media Analyst in Shanghai, hardly had any knowledge about Excel, [Mike Girvin channel on YouTube name ExcelIsFun](https://www.youtube.com/user/ExcelIsFun/channels), absolutely saved my ass. If you are an absolute beginner in Microsoft Excel, I would highly recommend checking him.",t2_7v2c8olg,False,,0,False,Calling out Excel Pro Users,[],r/datascience,False,6,education,0,,,False,t3_nvs6nt,False,dark,0.47,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1623261958.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear Excel Geeks, we do have it ExcelTips group, but the mode is not active anymore. So I have created &lt;a href=""https://www.reddit.com/r/ExcelTips_ActiveGroup/""&gt;another one&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are a pro-Excel user, &lt;a href=""https://www.reddit.com/r/ExcelTips_ActiveGroup/""&gt;&lt;strong&gt;you can share Excel Tips and Trick in this group&lt;/strong&gt;&lt;/a&gt;. If you starting with Excel you can join as well to follow the pro-users.&lt;/p&gt;

&lt;p&gt;Reson I created because if it wasn&amp;#39;t for the people who share their knowledge absolutely free, I wouldn&amp;#39;t have survived and able to make a career as Data Analyst.&lt;/p&gt;

&lt;p&gt;I still remember vividly, when I first got a job as Social Media Analyst in Shanghai, hardly had any knowledge about Excel, &lt;a href=""https://www.youtube.com/user/ExcelIsFun/channels""&gt;Mike Girvin channel on YouTube name ExcelIsFun&lt;/a&gt;, absolutely saved my ass. If you are an absolute beginner in Microsoft Excel, I would highly recommend checking him.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvs6nt,True,,Deepak__Deepu,,1,True,all_ads,False,[],False,,/r/datascience/comments/nvs6nt/calling_out_excel_pro_users/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvs6nt/calling_out_excel_pro_users/,515405,1623233158.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?auto=webp&amp;s=a3718084c207d04c3ac1364c4f1ad3748d1726fc', 'width': 900, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f726bbbe009c2a29a95363492fe1770916a35bd', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf036e7053befcf16715f695ab3cae00e0bc8d1c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95d6096ac728f22e903d93078befdbb69d77682e', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4461d2d8a92b6c5204a76dafa2f96a1169b33f9', 'width': 640, 'height': 640}], 'variants': {}, 'id': 'rzPZzWEZghiyAIMmIgCSkEnf9kvLr1nYNU3QaswpW18'}], 'enabled': False}",,,,,
,datascience,"I'm currently looking around for a different data science job and I'm not exactly sure what I'm looking for but I could roughly classify the *types* of jobs I'm hearing about into a few broad categories. I'll list them and give my current thought but I was hoping you could all weigh in with your experience and help me make the right decision.

&amp;#x200B;

1) The BIG tech company. These are your FAANG+M's. I assume that they are the ones doing really cutting-edge stuff and I imagine the work is pretty fun. I also assume they pay really well. I'm not sure if I can get past one of those leetcode interviews though. This one is probably out of reach for me but I imagine I'd take it if I had the chance.

2) The small tech company. ""we just got XXXX Million in funding and just landed a contract with &lt;big company that you've heard of&gt;"". Yeah IDK this is the one that I'm most wary about. I think ""fast-paced"" and ""great for self-starters"" are just code language for ""shitty work-life balance"" and ""poorly organized"". But of course I could be wrong

3) The big retailer/non-technical company that has a whizzbang data science team. This sounds kinda fun but then other times I wonder if it's a little mundane. Anyone actually doing advanced statistical models and ML or are we just A/B testing all day long?

4) the huge non-technical company that doesn't yet have a data science team. ""Yeah we think we could find a lot of business value if we brought on a data scientist"". This is the one I'm most conflicted about. Some of the most fun I've ever had with data science is when I'm working with people who have absolutely no tech background and couldn't even imagine what was possible. I think in the right situation you could really be hot shit around there. The thing I'm most worried about is that you'd soon run out of things to do. With some of these places the work they describe sounds more like a 3-month project than a career.

&amp;#x200B;

What are some of your thoughts?",t2_1lmws42,False,,0,False,Advice on types of jobs,[],r/datascience,False,6,,0,,,False,t3_nviakd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,False,,[],{},,True,,1623225465.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently looking around for a different data science job and I&amp;#39;m not exactly sure what I&amp;#39;m looking for but I could roughly classify the &lt;em&gt;types&lt;/em&gt; of jobs I&amp;#39;m hearing about into a few broad categories. I&amp;#39;ll list them and give my current thought but I was hoping you could all weigh in with your experience and help me make the right decision.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;1) The BIG tech company. These are your FAANG+M&amp;#39;s. I assume that they are the ones doing really cutting-edge stuff and I imagine the work is pretty fun. I also assume they pay really well. I&amp;#39;m not sure if I can get past one of those leetcode interviews though. This one is probably out of reach for me but I imagine I&amp;#39;d take it if I had the chance.&lt;/p&gt;

&lt;p&gt;2) The small tech company. &amp;quot;we just got XXXX Million in funding and just landed a contract with &amp;lt;big company that you&amp;#39;ve heard of&amp;gt;&amp;quot;. Yeah IDK this is the one that I&amp;#39;m most wary about. I think &amp;quot;fast-paced&amp;quot; and &amp;quot;great for self-starters&amp;quot; are just code language for &amp;quot;shitty work-life balance&amp;quot; and &amp;quot;poorly organized&amp;quot;. But of course I could be wrong&lt;/p&gt;

&lt;p&gt;3) The big retailer/non-technical company that has a whizzbang data science team. This sounds kinda fun but then other times I wonder if it&amp;#39;s a little mundane. Anyone actually doing advanced statistical models and ML or are we just A/B testing all day long?&lt;/p&gt;

&lt;p&gt;4) the huge non-technical company that doesn&amp;#39;t yet have a data science team. &amp;quot;Yeah we think we could find a lot of business value if we brought on a data scientist&amp;quot;. This is the one I&amp;#39;m most conflicted about. Some of the most fun I&amp;#39;ve ever had with data science is when I&amp;#39;m working with people who have absolutely no tech background and couldn&amp;#39;t even imagine what was possible. I think in the right situation you could really be hot shit around there. The thing I&amp;#39;m most worried about is that you&amp;#39;d soon run out of things to do. With some of these places the work they describe sounds more like a 3-month project than a career.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What are some of your thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nviakd,True,,old_mcfartigan,,11,True,all_ads,False,[],False,,/r/datascience/comments/nviakd/advice_on_types_of_jobs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nviakd/advice_on_types_of_jobs/,515405,1623196665.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I honestly, don't think people wanting to break into Data Science really know what all it entails. It just sounds good, and sounds like it will make them lots of money.

No one tells people what comes with the job. There are a lot of headaches that come with it, and you have to be a very patient person.

When any person starts out in IT, they learn some psychology. How to manage users and their expectations. You learn what to say and what not to say. You learn how to appear confident and reassuring even if you're getting up to speed in the moment. The good ones do anyway.

Data Science, BI, DA - you have to have those skills multiplied by ten. You have to be better than the rest at managing expectations. You have to learn how to avoid support drains, and be thinking ahead all of the time.

The data science people are the only people I respect as much as the people in Systems. Because other fields, you learn one thing and only one side of it, call yourself an engineer despite knowing one side. Sys Engineers have to know a little about everything and base knowledge in all kinds of things/ They are constantly growing. Data Science folks are similar because they have to know a wide assortment of things, and they have to know all of the tips and tricks at their disposal to get their desired result. Which means they will know Python, multiple types of SQL, Pandas, Jupyter, and so on. They'll pivot in Excel in a pinch if they need to.

But the main reason I respect them is just because of how patient they have to be to want to work in their field for 30+ years.

Our DA left in 2018 and one of my roles was a senior DBA, so they just put her job on top of mine. I learned a lot and I got very good at SQL and streamlining and reducing task turn around for reports and data tasks. But I obviously didn't have the time to dive ultra deep into the rabbit hole, and I didn't want to. Because I knew it wasn't for me.

We were acquired, and I transitioned all of that stuff onto the BI team of the new company. I have so much respect for those people. I am still answering questions and taking one off requests. This morning I was just hit in the face with how much I dislike actually doing he DS/DA side. A Sales Senior Manager needed something with some data. I asked a follow up question. I needed a key piece of info to ensure I did the right thing and didn't have to do re work later. They said they would get it to me later.

They emailed it to me at 7:11am this morning, then messaged me before my shift - ""Hey, I don't see the data task with the blah blah being done. We needed it 6/3."" And I am thinking - then why wait until 6/7 to give me the info. We got the request 6/4, and I asked you on 6/4, then you waited the weekend to get it to me.

And those individuals who just keep coming back telling you the data wasn't what they expected or wanted when it is what they asked for.. I'm so happy to be just a senior sys engineer again working on large scale infra.

It's not for everyone, and I think they need to talk about and teach managing expectations so you don't shoot yourself in the foot. Luckily the BI team of the new company are phenomenal, and now I am out of the game. 

But I am learning more Python at home in my spare time and things like Jupyter so I don't regress skill wise. Python is useful in what I do anyway. I've rewritten several PS automation scripts in it.",t2_mfhlt,False,,0,False,"Data Science and Data Analytics is becoming ultra glorified / romanticized, and I don't think people are really told what they are getting into.",[],r/datascience,False,6,discussion,0,,,False,t3_nue01q,False,dark,0.95,,public,979,8,{},,,False,[],,False,False,,{},Discussion,False,979,,False,False,self,1623076963.0,,[],{'gid_1': 2},,True,,1623105228.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I honestly, don&amp;#39;t think people wanting to break into Data Science really know what all it entails. It just sounds good, and sounds like it will make them lots of money.&lt;/p&gt;

&lt;p&gt;No one tells people what comes with the job. There are a lot of headaches that come with it, and you have to be a very patient person.&lt;/p&gt;

&lt;p&gt;When any person starts out in IT, they learn some psychology. How to manage users and their expectations. You learn what to say and what not to say. You learn how to appear confident and reassuring even if you&amp;#39;re getting up to speed in the moment. The good ones do anyway.&lt;/p&gt;

&lt;p&gt;Data Science, BI, DA - you have to have those skills multiplied by ten. You have to be better than the rest at managing expectations. You have to learn how to avoid support drains, and be thinking ahead all of the time.&lt;/p&gt;

&lt;p&gt;The data science people are the only people I respect as much as the people in Systems. Because other fields, you learn one thing and only one side of it, call yourself an engineer despite knowing one side. Sys Engineers have to know a little about everything and base knowledge in all kinds of things/ They are constantly growing. Data Science folks are similar because they have to know a wide assortment of things, and they have to know all of the tips and tricks at their disposal to get their desired result. Which means they will know Python, multiple types of SQL, Pandas, Jupyter, and so on. They&amp;#39;ll pivot in Excel in a pinch if they need to.&lt;/p&gt;

&lt;p&gt;But the main reason I respect them is just because of how patient they have to be to want to work in their field for 30+ years.&lt;/p&gt;

&lt;p&gt;Our DA left in 2018 and one of my roles was a senior DBA, so they just put her job on top of mine. I learned a lot and I got very good at SQL and streamlining and reducing task turn around for reports and data tasks. But I obviously didn&amp;#39;t have the time to dive ultra deep into the rabbit hole, and I didn&amp;#39;t want to. Because I knew it wasn&amp;#39;t for me.&lt;/p&gt;

&lt;p&gt;We were acquired, and I transitioned all of that stuff onto the BI team of the new company. I have so much respect for those people. I am still answering questions and taking one off requests. This morning I was just hit in the face with how much I dislike actually doing he DS/DA side. A Sales Senior Manager needed something with some data. I asked a follow up question. I needed a key piece of info to ensure I did the right thing and didn&amp;#39;t have to do re work later. They said they would get it to me later.&lt;/p&gt;

&lt;p&gt;They emailed it to me at 7:11am this morning, then messaged me before my shift - &amp;quot;Hey, I don&amp;#39;t see the data task with the blah blah being done. We needed it 6/3.&amp;quot; And I am thinking - then why wait until 6/7 to give me the info. We got the request 6/4, and I asked you on 6/4, then you waited the weekend to get it to me.&lt;/p&gt;

&lt;p&gt;And those individuals who just keep coming back telling you the data wasn&amp;#39;t what they expected or wanted when it is what they asked for.. I&amp;#39;m so happy to be just a senior sys engineer again working on large scale infra.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s not for everyone, and I think they need to talk about and teach managing expectations so you don&amp;#39;t shoot yourself in the foot. Luckily the BI team of the new company are phenomenal, and now I am out of the game. &lt;/p&gt;

&lt;p&gt;But I am learning more Python at home in my spare time and things like Jupyter so I don&amp;#39;t regress skill wise. Python is useful in what I do anyway. I&amp;#39;ve rewritten several PS automation scripts in it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nue01q,True,,PartTimeTulsa,,199,True,all_ads,False,[],False,,/r/datascience/comments/nue01q/data_science_and_data_analytics_is_becoming_ultra/,all_ads,False,https://www.reddit.com/r/datascience/comments/nue01q/data_science_and_data_analytics_is_becoming_ultra/,515405,1623076428.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases?

Keywords would be: Hadoop and MapReduce, Spark, parallel and distributed databases, Data warehousing

This is an advanced course on databases at our university so I wanted to know how important such knowledge is if some one is lets say working as an ML engineer/data scientist.",t2_8i9sqtxm,False,,0,False,How important is it to have a good grasp on scalable databases as a data scientist/ML engineer?,[],r/datascience,False,6,education,0,,,False,t3_nvhi9a,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623223166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases?&lt;/p&gt;

&lt;p&gt;Keywords would be: Hadoop and MapReduce, Spark, parallel and distributed databases, Data warehousing&lt;/p&gt;

&lt;p&gt;This is an advanced course on databases at our university so I wanted to know how important such knowledge is if some one is lets say working as an ML engineer/data scientist.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhi9a,True,,Peter2448,,8,True,all_ads,False,[],False,,/r/datascience/comments/nvhi9a/how_important_is_it_to_have_a_good_grasp_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhi9a/how_important_is_it_to_have_a_good_grasp_on/,515405,1623194366.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So, I started my engineering journey with Electrical Engineering but somehow got interested in data science, I learned most of the things on my own, I studied about things required to be in the CS field but felt like I knew nothing and hence asked a few of my CS students to do a project with me so I can learn, they weren't that motivated so I did somethings myself. I was learning and going good doing my own project.

Though I didn't get any jobs in the field because I wasn't having a CS degree to be sitting in interviews plus due to pandemic, the company wants better at a lesser price, understandable to me. Somehow I got some research projects in the field of NLP and work independently suggesting my own ideas depending on the problem statement and Data peovided.

Now, I am in an MNC as a data science and analytics intern, with two other young team members, who are very excited, giving ideas and just TALKING, they don't know how to DO it, ultimately increasing my work because they don't need the job as they are just students and I need it, I don't want any bad impression from my side. On top of that, the project they gave us is of making a recommendation engine, they are not giving any idea, everything is on us.

I can make a recommendation engine like that, item-based, user-based anyone, and can try to build a hybrid one, depending on the data we have. They asked us to look for the domain yourself, it should have a unique feature too because, at the presentation, you have to tell that ""why the product is different"", I mean 8 weeks of time, no data nothing, need an MVP in 2 weeks, data has to searched and it should have a unique feature. Plus this one intern irritates that heck out of me........

Now I am thinking I was better working alone, but I have seen some great teammates, Please advise me anything on this. I don't know the feeling is right or not, or is it the lockdown eating my head. I am willing to correct my thought process but I need some advice from someone in the same field without being judged.

Update: Just got the problem system even broader, they need a personalization system not specifically a recommendation sytem.",t2_34qgdyb6,False,,0,False,"This is going to be a rant, so sit back",[],r/datascience,False,6,career,0,,,False,t3_nvn42p,False,dark,0.45,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623214529.0,,[],{},,True,,1623241447.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I started my engineering journey with Electrical Engineering but somehow got interested in data science, I learned most of the things on my own, I studied about things required to be in the CS field but felt like I knew nothing and hence asked a few of my CS students to do a project with me so I can learn, they weren&amp;#39;t that motivated so I did somethings myself. I was learning and going good doing my own project.&lt;/p&gt;

&lt;p&gt;Though I didn&amp;#39;t get any jobs in the field because I wasn&amp;#39;t having a CS degree to be sitting in interviews plus due to pandemic, the company wants better at a lesser price, understandable to me. Somehow I got some research projects in the field of NLP and work independently suggesting my own ideas depending on the problem statement and Data peovided.&lt;/p&gt;

&lt;p&gt;Now, I am in an MNC as a data science and analytics intern, with two other young team members, who are very excited, giving ideas and just TALKING, they don&amp;#39;t know how to DO it, ultimately increasing my work because they don&amp;#39;t need the job as they are just students and I need it, I don&amp;#39;t want any bad impression from my side. On top of that, the project they gave us is of making a recommendation engine, they are not giving any idea, everything is on us.&lt;/p&gt;

&lt;p&gt;I can make a recommendation engine like that, item-based, user-based anyone, and can try to build a hybrid one, depending on the data we have. They asked us to look for the domain yourself, it should have a unique feature too because, at the presentation, you have to tell that &amp;quot;why the product is different&amp;quot;, I mean 8 weeks of time, no data nothing, need an MVP in 2 weeks, data has to searched and it should have a unique feature. Plus this one intern irritates that heck out of me........&lt;/p&gt;

&lt;p&gt;Now I am thinking I was better working alone, but I have seen some great teammates, Please advise me anything on this. I don&amp;#39;t know the feeling is right or not, or is it the lockdown eating my head. I am willing to correct my thought process but I need some advice from someone in the same field without being judged.&lt;/p&gt;

&lt;p&gt;Update: Just got the problem system even broader, they need a personalization system not specifically a recommendation sytem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvn42p,True,,yaakarsh1011,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvn42p/this_is_going_to_be_a_rant_so_sit_back/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvn42p/this_is_going_to_be_a_rant_so_sit_back/,515405,1623212647.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Ever thought to yourself that even though you are perfectly capable at your job, or maybe even the most knowledgable person, you are often underestimated in your role. Or worse, you find someone else (even though less capable technically) making the decisions and telling you how the project should be handled. You want, ask and expect a new opportunity or an exciting project from your leadership, but you simply watch them go to someone else in quiet despair.  


You feel under-appreciated, ignored, and mostly misunderstood. You probably have several great ideas for your company, but if only someone were to listen to you.  


I work with several people in Tech, IT, and Data Science, and this is such a common problem that I encounter.  


The reason for this is ‘Perception”. See, there’s a big difference between being an expert vs being perceived as one. Being an expert will help you excel in the opportunities you land in, but how people perceive you is what will land you in opportunities in the first place.  


Almost everyone is focused on honing their technical skills, which by the way is a great pursuit. But most ignore the art of basic persuasion and charm which keeps them from getting truly satisfying roles and opportunities.  


So how does one gets perceived the right way…?  


It has to do with the way you communicate. Experts have a way of talking that automatically demands compliance, respect, and conviction from others. Think of your last visit to a doctor. Did you argue with the doctor, or just wondered in your head that they probably know nothing, or simply dismissed what they asked you to do? Instead, you complied with whatever you said.  


Effective persuasion is a learnable skill. Once you make a few key changes to the way you communicate, the environment around you and the way people treat you changes dramatically. You will command respect and unquestionable trust that will get you elite opportunities in your industry. So you can later prove it through your capabilities.  


When you present something, people will just tune in to your reality as you speak. In fact, they literally will come to help you with your work even when you don’t ask them. All of this happens subconsciously when you know basic charisma and persuasion.",t2_4jjahzgd,False,,0,False,This one thing could be holding back your career in Data Science...,[],r/datascience,False,6,career,0,,,False,t3_nvhtly,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1623224075.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ever thought to yourself that even though you are perfectly capable at your job, or maybe even the most knowledgable person, you are often underestimated in your role. Or worse, you find someone else (even though less capable technically) making the decisions and telling you how the project should be handled. You want, ask and expect a new opportunity or an exciting project from your leadership, but you simply watch them go to someone else in quiet despair.  &lt;/p&gt;

&lt;p&gt;You feel under-appreciated, ignored, and mostly misunderstood. You probably have several great ideas for your company, but if only someone were to listen to you.  &lt;/p&gt;

&lt;p&gt;I work with several people in Tech, IT, and Data Science, and this is such a common problem that I encounter.  &lt;/p&gt;

&lt;p&gt;The reason for this is ‘Perception”. See, there’s a big difference between being an expert vs being perceived as one. Being an expert will help you excel in the opportunities you land in, but how people perceive you is what will land you in opportunities in the first place.  &lt;/p&gt;

&lt;p&gt;Almost everyone is focused on honing their technical skills, which by the way is a great pursuit. But most ignore the art of basic persuasion and charm which keeps them from getting truly satisfying roles and opportunities.  &lt;/p&gt;

&lt;p&gt;So how does one gets perceived the right way…?  &lt;/p&gt;

&lt;p&gt;It has to do with the way you communicate. Experts have a way of talking that automatically demands compliance, respect, and conviction from others. Think of your last visit to a doctor. Did you argue with the doctor, or just wondered in your head that they probably know nothing, or simply dismissed what they asked you to do? Instead, you complied with whatever you said.  &lt;/p&gt;

&lt;p&gt;Effective persuasion is a learnable skill. Once you make a few key changes to the way you communicate, the environment around you and the way people treat you changes dramatically. You will command respect and unquestionable trust that will get you elite opportunities in your industry. So you can later prove it through your capabilities.  &lt;/p&gt;

&lt;p&gt;When you present something, people will just tune in to your reality as you speak. In fact, they literally will come to help you with your work even when you don’t ask them. All of this happens subconsciously when you know basic charisma and persuasion.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhtly,True,,hummus_wolf,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvhtly/this_one_thing_could_be_holding_back_your_career/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhtly/this_one_thing_could_be_holding_back_your_career/,515405,1623195275.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Just wanted to gauge how much data scientists like working with their colleagues? I've been interviewing for various positions and have been disappointed in the people interviewing me. Most of the time I'm not treated respectfully and subsequently haven't been able to see myself working with the interviewers. It's turning me away from joining the profession.,t2_3mao2qc7,False,,0,False,People you work with,[],r/datascience,False,6,discussion,0,,,False,t3_nuw45a,False,dark,0.92,,public,33,0,{},,,False,[],,False,False,,{},Discussion,False,33,,False,False,self,False,,[],{},,True,,1623154683.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just wanted to gauge how much data scientists like working with their colleagues? I&amp;#39;ve been interviewing for various positions and have been disappointed in the people interviewing me. Most of the time I&amp;#39;m not treated respectfully and subsequently haven&amp;#39;t been able to see myself working with the interviewers. It&amp;#39;s turning me away from joining the profession.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuw45a,True,,Professional_Owl_819,,16,True,all_ads,False,[],False,,/r/datascience/comments/nuw45a/people_you_work_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuw45a/people_you_work_with/,515405,1623125883.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What I am looking to do:

1) Scrape data of SP 500 company names and prices at specific date and end date.

2) compile the data.

3) organize companies by return over a 10 year period.

my goal:

 see what % of all SP 500 companies end up with different returns.

For  example, what % of companies end up being 10x or being removed.

Thanks in advance. I haver zero skills related to this.",t2_ogpog,False,,0,False,knuckle Dragger seeking advice - On what courses to take so that I can analyze data on the SP500.,[],r/datascience,False,6,discussion,0,,,False,t3_nvgbn4,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623219770.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What I am looking to do:&lt;/p&gt;

&lt;p&gt;1) Scrape data of SP 500 company names and prices at specific date and end date.&lt;/p&gt;

&lt;p&gt;2) compile the data.&lt;/p&gt;

&lt;p&gt;3) organize companies by return over a 10 year period.&lt;/p&gt;

&lt;p&gt;my goal:&lt;/p&gt;

&lt;p&gt;see what % of all SP 500 companies end up with different returns.&lt;/p&gt;

&lt;p&gt;For  example, what % of companies end up being 10x or being removed.&lt;/p&gt;

&lt;p&gt;Thanks in advance. I haver zero skills related to this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvgbn4,True,,badtradesguy,,18,True,all_ads,False,[],False,,/r/datascience/comments/nvgbn4/knuckle_dragger_seeking_advice_on_what_courses_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvgbn4/knuckle_dragger_seeking_advice_on_what_courses_to/,515405,1623190970.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

I'm building a tool that asks personalized follow up questions in order to get detailed survey responses. It's powered in part by GPT-3 (created by OpenAI). If you're open to testing it, it takes a couple of minutes...and I welcome feedback.

It's imperfect, but that's why we're testing ;)  Please DM me for a link!

I'd also love to hear your experiences building standalone AI products like this.",t2_5ccn6,False,,0,False,Testing GPT-3 powered survey tool - feedback and thoughts?,[],r/datascience,False,6,projects,0,,,False,t3_nvfy81,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1623218797.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m building a tool that asks personalized follow up questions in order to get detailed survey responses. It&amp;#39;s powered in part by GPT-3 (created by OpenAI). If you&amp;#39;re open to testing it, it takes a couple of minutes...and I welcome feedback.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s imperfect, but that&amp;#39;s why we&amp;#39;re testing ;)  Please DM me for a link!&lt;/p&gt;

&lt;p&gt;I&amp;#39;d also love to hear your experiences building standalone AI products like this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvfy81,True,,gaga_loo,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/,515405,1623189997.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"
Hello all, I’m currently an undergraduate stats major who will be a junior in the fall. My goals is to apply to PhD programs in senior fall. If I wanted to look at opportunities for the summer prior to it, should I look into doing research within say the stats dept? Or should I be trying to look for an actual internship at a company? My research interests are within statistical learning, so I was thinking a research role would be better suited for me than some data analytics position at a company.

I feel that it will be hard for me to get on any papers or do research because I won’t have a ton of theory knowledge, but I’m hoping I can get on something more applied.

So what do you think? I feel like trying to get a research role would be better when applying then doing SQL all day at a company for a summer, chances are even if I expressed my case as I want to do some more data science and quantitative sort of internship it wouldn’t be as good as if I did research with a prof.

I will be applying to stats phd programs.",t2_5w4i5kd1,False,,0,False,Research or Internship in prep for phd applications?,[],r/datascience,False,6,discussion,0,,,False,t3_nv589x,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623191109.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I’m currently an undergraduate stats major who will be a junior in the fall. My goals is to apply to PhD programs in senior fall. If I wanted to look at opportunities for the summer prior to it, should I look into doing research within say the stats dept? Or should I be trying to look for an actual internship at a company? My research interests are within statistical learning, so I was thinking a research role would be better suited for me than some data analytics position at a company.&lt;/p&gt;

&lt;p&gt;I feel that it will be hard for me to get on any papers or do research because I won’t have a ton of theory knowledge, but I’m hoping I can get on something more applied.&lt;/p&gt;

&lt;p&gt;So what do you think? I feel like trying to get a research role would be better when applying then doing SQL all day at a company for a summer, chances are even if I expressed my case as I want to do some more data science and quantitative sort of internship it wouldn’t be as good as if I did research with a prof.&lt;/p&gt;

&lt;p&gt;I will be applying to stats phd programs.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv589x,True,,veeeerain,,7,True,all_ads,False,[],False,,/r/datascience/comments/nv589x/research_or_internship_in_prep_for_phd/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv589x/research_or_internship_in_prep_for_phd/,515405,1623162309.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"A friend of mine is going to make a presentation to an Indian state government on how it can better use data, and how data may be used to have more effective policy and decision making for government bureaucrats in the Indian state.

To achieve traction with the bureaucracy, the focus needs to be more on projects and topics that will lead to ""quick wins.""Show some quick results; make like for the bureaucrats/policy makers perform better; and enable a virtuous loop where they can see how data can help in an efficient manner…. rather than being long projects that rarely result in concrete measurable benefits.

What are some projects/ topics that you would suggest? Maybe you have worked on some for local govts in other countries or in India itself. Any links and suggestions will be helpful

Note that most of these bureaucrats/policy makers are not coders; and data they have access to is often not clean. Many data bases do not talk to each other and there are data inconsistencies as well.

Any ideas and suggestions are much appreciated!! Thanks in advance!!

&amp;#x200B;

EDIT:   Also: My friend's NGO will support with some data science volunteers…they will do the data cleaning and analysis… looking for some initial ideas that you think may lead to interest from bureaucrats/policy makers ",t2_3mcq6yhh,False,,0,False,Data Science projects for local governments,[],r/datascience,False,6,projects,0,,,False,t3_nvafx3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,1623179509.0,,[],{},,True,,1623204593.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A friend of mine is going to make a presentation to an Indian state government on how it can better use data, and how data may be used to have more effective policy and decision making for government bureaucrats in the Indian state.&lt;/p&gt;

&lt;p&gt;To achieve traction with the bureaucracy, the focus needs to be more on projects and topics that will lead to &amp;quot;quick wins.&amp;quot;Show some quick results; make like for the bureaucrats/policy makers perform better; and enable a virtuous loop where they can see how data can help in an efficient manner…. rather than being long projects that rarely result in concrete measurable benefits.&lt;/p&gt;

&lt;p&gt;What are some projects/ topics that you would suggest? Maybe you have worked on some for local govts in other countries or in India itself. Any links and suggestions will be helpful&lt;/p&gt;

&lt;p&gt;Note that most of these bureaucrats/policy makers are not coders; and data they have access to is often not clean. Many data bases do not talk to each other and there are data inconsistencies as well.&lt;/p&gt;

&lt;p&gt;Any ideas and suggestions are much appreciated!! Thanks in advance!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT:   Also: My friend&amp;#39;s NGO will support with some data science volunteers…they will do the data cleaning and analysis… looking for some initial ideas that you think may lead to interest from bureaucrats/policy makers &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvafx3,True,,kdas22,,11,True,all_ads,False,[],False,,/r/datascience/comments/nvafx3/data_science_projects_for_local_governments/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvafx3/data_science_projects_for_local_governments/,515405,1623175793.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,Need to analyse app data for work and showcase it on a dashboard. Don't have a lot of experience in it and would like to see how similar data has been analyzed before. Are there any open projects/dashboards that you are aware of that I could use as a point of reference? Any other resources on the same are also appreciated. Thanks in advance.,t2_47ledrng,False,,0,False,Open dashboards for app data analysis,[],r/datascience,False,6,discussion,0,,,False,t3_nva6p2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623203906.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Need to analyse app data for work and showcase it on a dashboard. Don&amp;#39;t have a lot of experience in it and would like to see how similar data has been analyzed before. Are there any open projects/dashboards that you are aware of that I could use as a point of reference? Any other resources on the same are also appreciated. Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nva6p2,True,,humanmetric,,1,True,all_ads,False,[],False,,/r/datascience/comments/nva6p2/open_dashboards_for_app_data_analysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/nva6p2/open_dashboards_for_app_data_analysis/,515405,1623175106.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know Tableau very well and is my preferred tool. But, I noticed there's a cheaper option from Amazon. Any input from your experience is appreciated.",t2_862rknk8,False,,0,False,Tableau vs Amazon QuickSights,[],r/datascience,False,6,tooling,0,,,False,t3_nuwn27,False,dark,0.93,,public,12,1,{},,,False,[],,False,False,,{},Tooling,False,12,,False,False,self,False,,[],{},,True,,1623156596.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know Tableau very well and is my preferred tool. But, I noticed there&amp;#39;s a cheaper option from Amazon. Any input from your experience is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuwn27,True,,ricke813,,12,True,all_ads,False,[],False,,/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/,515405,1623127796.0,1,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all, not sure if this is allowed here or not, but I received a DS Case Study from a big pharma company in USA. I just wanted to see if there is anyone experienced in doing case studies and might want to help me brainstorm the problem and discuss possible methodologies and aligning the solution.",t2_6bdw2d0v,False,,0,False,Brainstorming a DS Case Study from a big pharma company,[],r/datascience,False,6,projects,0,,,False,t3_nvcn3w,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1623210495.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, not sure if this is allowed here or not, but I received a DS Case Study from a big pharma company in USA. I just wanted to see if there is anyone experienced in doing case studies and might want to help me brainstorm the problem and discuss possible methodologies and aligning the solution.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvcn3w,True,,mboorlu,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvcn3w/brainstorming_a_ds_case_study_from_a_big_pharma/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvcn3w/brainstorming_a_ds_case_study_from_a_big_pharma/,515405,1623181695.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi everyone! I have been a lurker in this community and it has been super helpful in more ways than I can count. Recently, I spoke with a company for a DS position and they sent me a take home assignment a couple of days ago.

It involves building an full-fledged ML web app from scratch. The steps include:

1. Loading tables in a SQL database
2. Training a model that predicts an outcome, and
3. Building a REST API that would receive data and post predictions based on the model I trained above

**In addition they state that it should take only 3-4 hours to complete this. REALLY????**

I do not have any meaningful background in building web apps and servers. This is pretty clear from my resume. Also, the job description did not mention any such requirements or skills for this particular position. Although, the company has an interesting product, I feel I would be wasting my time working on this assignment given my lack of skills. I wonder if I should rather spend my time working on other applications/assignments/interviews rather than doing this.  I feel really uncomfortable and honestly a little angry that they've asked me to build an entire project from scratch.

Would love to hear if y'all have any recommendations and thoughts about what I should do. Thank you :)",t2_j6dye,False,,0,False,DS take home assignment requires building an entire project using skills I don't have,[],r/datascience,False,6,,0,,,False,t3_nurs3c,False,dark,0.81,,public,16,0,{},,,False,[],,False,False,,{},Job Search,False,16,,False,False,self,False,,[],{},,True,,1623140590.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I have been a lurker in this community and it has been super helpful in more ways than I can count. Recently, I spoke with a company for a DS position and they sent me a take home assignment a couple of days ago.&lt;/p&gt;

&lt;p&gt;It involves building an full-fledged ML web app from scratch. The steps include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Loading tables in a SQL database&lt;/li&gt;
&lt;li&gt;Training a model that predicts an outcome, and&lt;/li&gt;
&lt;li&gt;Building a REST API that would receive data and post predictions based on the model I trained above&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;In addition they state that it should take only 3-4 hours to complete this. REALLY????&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I do not have any meaningful background in building web apps and servers. This is pretty clear from my resume. Also, the job description did not mention any such requirements or skills for this particular position. Although, the company has an interesting product, I feel I would be wasting my time working on this assignment given my lack of skills. I wonder if I should rather spend my time working on other applications/assignments/interviews rather than doing this.  I feel really uncomfortable and honestly a little angry that they&amp;#39;ve asked me to build an entire project from scratch.&lt;/p&gt;

&lt;p&gt;Would love to hear if y&amp;#39;all have any recommendations and thoughts about what I should do. Thank you :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nurs3c,True,,restaremeredetails,,27,True,all_ads,False,[],False,,/r/datascience/comments/nurs3c/ds_take_home_assignment_requires_building_an/,all_ads,False,https://www.reddit.com/r/datascience/comments/nurs3c/ds_take_home_assignment_requires_building_an/,515405,1623111790.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"HI there all,

First time poster and so sorry in advance for misdemeanors. I work in a commodity trading company and have been tasked with building / sourcing a CSO option risk model, in this case the spread is a time spread between future contracts for Crude Oil.

I have a pretty good understanding on VBA and there are people at the company who can work with Python etc. We have a SQL database of future prices for the various contracts as well as the volatility / delta / price of each options contract, from outright contracts to the CSO options.

Is anyone able to give me advice  / help on how to go about building this? Seems to be a MC simulation, but my only experience with that is using Matlab (engineering degree) which I have been told is not an option due to licencing costs.

&amp;#x200B;

Thanks,

&amp;#x200B;

thpj20",t2_15gxf0,False,,0,False,CSO Option VAR model,[],r/datascience,False,6,projects,0,,,False,t3_nv5jk1,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1623191928.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;HI there all,&lt;/p&gt;

&lt;p&gt;First time poster and so sorry in advance for misdemeanors. I work in a commodity trading company and have been tasked with building / sourcing a CSO option risk model, in this case the spread is a time spread between future contracts for Crude Oil.&lt;/p&gt;

&lt;p&gt;I have a pretty good understanding on VBA and there are people at the company who can work with Python etc. We have a SQL database of future prices for the various contracts as well as the volatility / delta / price of each options contract, from outright contracts to the CSO options.&lt;/p&gt;

&lt;p&gt;Is anyone able to give me advice  / help on how to go about building this? Seems to be a MC simulation, but my only experience with that is using Matlab (engineering degree) which I have been told is not an option due to licencing costs.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thpj20&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv5jk1,True,,thpj20,,2,True,all_ads,False,[],False,,/r/datascience/comments/nv5jk1/cso_option_var_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv5jk1/cso_option_var_model/,515405,1623163128.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,We are running a kubernetes based development environment where data scientists are free to work on anything they deem appropriate. We need to ensure they are not introducing vulnerabilities. Does anyone have any tools they recommend for continuous scanning and reporting?,t2_9l51o08l,False,,0,False,Reccomendations on vulnerability scanners.,[],r/datascience,False,6,tooling,0,,,False,t3_nv4uuy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623190120.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We are running a kubernetes based development environment where data scientists are free to work on anything they deem appropriate. We need to ensure they are not introducing vulnerabilities. Does anyone have any tools they recommend for continuous scanning and reporting?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv4uuy,True,,Desperate-Walk1780,,2,True,all_ads,False,[],False,,/r/datascience/comments/nv4uuy/reccomendations_on_vulnerability_scanners/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv4uuy/reccomendations_on_vulnerability_scanners/,515405,1623161320.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I see this on Reddit every single day: someone saying something to the effect of ""data science is over-saturated and not that interesting, I'm transitioning into data engineering where there are more jobs."" I don't blame people for being burnt out on data science and looking to go where the fields look greener, but we all see what's happening here, right? 

Combine this with Reddit's (and the tech community in general) tendency to have a massive hard-on for anything ""engineering"" and I think we're seeing the beginning of a trend we've all seen before. In a few years we'll all be back here (or maybe in /r/dataengineering ) saying ""data engineering is oversaturated - that's why I'm moving into...quantum skunk wrangling"" or something.",t2_ad5yokml,False,,0,False,"Calling it now: a few years from now, ""data engineering"" will be just as overhyped and saturated as ""data science"" is now.",[],r/datascience,False,6,discussion,0,,,False,t3_nukktk,False,dark,0.77,,public,24,0,{},,,False,[],,False,False,,{},Discussion,False,24,,False,False,self,False,,[],{},,True,,1623121553.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see this on Reddit every single day: someone saying something to the effect of &amp;quot;data science is over-saturated and not that interesting, I&amp;#39;m transitioning into data engineering where there are more jobs.&amp;quot; I don&amp;#39;t blame people for being burnt out on data science and looking to go where the fields look greener, but we all see what&amp;#39;s happening here, right? &lt;/p&gt;

&lt;p&gt;Combine this with Reddit&amp;#39;s (and the tech community in general) tendency to have a massive hard-on for anything &amp;quot;engineering&amp;quot; and I think we&amp;#39;re seeing the beginning of a trend we&amp;#39;ve all seen before. In a few years we&amp;#39;ll all be back here (or maybe in &lt;a href=""/r/dataengineering""&gt;/r/dataengineering&lt;/a&gt; ) saying &amp;quot;data engineering is oversaturated - that&amp;#39;s why I&amp;#39;m moving into...quantum skunk wrangling&amp;quot; or something.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nukktk,True,,antichain,,10,True,all_ads,False,[],False,,/r/datascience/comments/nukktk/calling_it_now_a_few_years_from_now_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nukktk/calling_it_now_a_few_years_from_now_data/,515405,1623092753.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello all,

Im working as a process engineer in a process equipment manufacturing company. I’m still in the 2-3 year experience category.

Most of the decision making for which equipment to select in my company is done through experience. They say it’s important to also develop a feeling for how the product runs and machine behaves. I feel like this decision making can be replicated by analyzing the past data.

So I’m interested to learn data science and statistical analysis, I have experience in programming in VBA, bash scripting and can implement algorithms in any programming language, thanks to stack overflow.

Does it help to have this skill in addition to my general process engineering background? What benefits can someone like me have with this addition.",t2_on8sl,False,,0,False,"What benefits can I have as a Mechanical/process engineer, if I have knowledge/skills in data science?",[],r/datascience,False,6,career,0,,,False,t3_nuxm2s,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623132445.0,,[],{},,True,,1623160260.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Im working as a process engineer in a process equipment manufacturing company. I’m still in the 2-3 year experience category.&lt;/p&gt;

&lt;p&gt;Most of the decision making for which equipment to select in my company is done through experience. They say it’s important to also develop a feeling for how the product runs and machine behaves. I feel like this decision making can be replicated by analyzing the past data.&lt;/p&gt;

&lt;p&gt;So I’m interested to learn data science and statistical analysis, I have experience in programming in VBA, bash scripting and can implement algorithms in any programming language, thanks to stack overflow.&lt;/p&gt;

&lt;p&gt;Does it help to have this skill in addition to my general process engineering background? What benefits can someone like me have with this addition.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuxm2s,True,,new_clearProjekt,,11,True,all_ads,False,[],False,,/r/datascience/comments/nuxm2s/what_benefits_can_i_have_as_a_mechanicalprocess/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuxm2s/what_benefits_can_i_have_as_a_mechanicalprocess/,515405,1623131460.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello peepos,

I’ve started an internship with territorial bureau of statistics that has me doing broad statistical work. It is my intention to get into the environmental field with a strong statistical background when I’m done. I’m wondering if any of you beautiful folk work for Mother Earth and could offer some guidance. (Or any “green” company of sorts. :D) For now, I’ll be be honing my GIS skills, dashboarding, database management and intensive math, like survival rates and differential equations. Any big topics I should be studying as well? 

Ty",t2_2d0otfa7,False,,0,False,Environmental Work,[],r/datascience,False,6,career,0,,,False,t3_nus823,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623142010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello peepos,&lt;/p&gt;

&lt;p&gt;I’ve started an internship with territorial bureau of statistics that has me doing broad statistical work. It is my intention to get into the environmental field with a strong statistical background when I’m done. I’m wondering if any of you beautiful folk work for Mother Earth and could offer some guidance. (Or any “green” company of sorts. :D) For now, I’ll be be honing my GIS skills, dashboarding, database management and intensive math, like survival rates and differential equations. Any big topics I should be studying as well? &lt;/p&gt;

&lt;p&gt;Ty&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nus823,True,,DeneHero,,4,True,all_ads,False,[],False,,/r/datascience/comments/nus823/environmental_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nus823/environmental_work/,515405,1623113210.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"So you’re reporting out weekly kpis... you want to report out WoW change. The metric went from 4.7% to 6.3%. How would you show the change?

[View Poll](https://www.reddit.com/poll/nujq7q)",t2_dzkzihh,False,,0,False,Reporting change of percentages.,[],r/datascience,False,6,meta,0,,,False,t3_nujq7q,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Meta,False,3,,False,False,self,False,,[],{},,True,,1623119408.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So you’re reporting out weekly kpis... you want to report out WoW change. The metric went from 4.7% to 6.3%. How would you show the change?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nujq7q""&gt;View Poll&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nujq7q,True,,jbt209,,5,True,all_ads,False,[],False,,/r/datascience/comments/nujq7q/reporting_change_of_percentages/,all_ads,False,https://www.reddit.com/r/datascience/comments/nujq7q/reporting_change_of_percentages/,515405,1623090608.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,"{'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1623349808569, 'options': [{'text': '+34%', 'vote_count': 35, 'id': '8424514'}, {'text': '+1.6%', 'vote_count': 37, 'id': '8424515'}, {'text': '+1.6pp', 'vote_count': 36, 'id': '8424516'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 108, 'total_stake_amount': None}"
,datascience,"Hi!

Interviews are not only a way for the company to test candidates, it is also an opportunity for the candidate to decide if the company can be a good fit or not.

I am going to have the first round of interviews next week with a medium-sized financial service company (they're opening a new medior/senior data scientist position).

Besides questions around the job content,  I am thinking about probing the following ""meta"" dimensions to detect potential orange (or red) flags:

\- size/seniority/background of the team

\- company culture

\- how mature is the organization in terms of data storage and management?

\- where does the data science team fit in the organigram / how close you are to key stakeholders?

\- Where does the company see the data science team in 5 years?

And you, what are the key dimensions that you are probing when you talk to a company about a new job?

&amp;#x200B;

EDIT: Thank you so much for all the valuable answers. They are incredibly helpful!",t2_t0451a,False,,0,False,Which dimensions are you probing when talking to a company about a new job?,[],r/datascience,False,6,career,0,,,False,t3_ntuj7e,False,dark,0.99,,public,167,0,{},,,False,[],,False,False,,{},Career,False,167,,False,False,self,1623100686.0,,[],{},,True,,1623039332.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Interviews are not only a way for the company to test candidates, it is also an opportunity for the candidate to decide if the company can be a good fit or not.&lt;/p&gt;

&lt;p&gt;I am going to have the first round of interviews next week with a medium-sized financial service company (they&amp;#39;re opening a new medior/senior data scientist position).&lt;/p&gt;

&lt;p&gt;Besides questions around the job content,  I am thinking about probing the following &amp;quot;meta&amp;quot; dimensions to detect potential orange (or red) flags:&lt;/p&gt;

&lt;p&gt;- size/seniority/background of the team&lt;/p&gt;

&lt;p&gt;- company culture&lt;/p&gt;

&lt;p&gt;- how mature is the organization in terms of data storage and management?&lt;/p&gt;

&lt;p&gt;- where does the data science team fit in the organigram / how close you are to key stakeholders?&lt;/p&gt;

&lt;p&gt;- Where does the company see the data science team in 5 years?&lt;/p&gt;

&lt;p&gt;And you, what are the key dimensions that you are probing when you talk to a company about a new job?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: Thank you so much for all the valuable answers. They are incredibly helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntuj7e,True,,gemag,,43,True,all_ads,False,[],False,,/r/datascience/comments/ntuj7e/which_dimensions_are_you_probing_when_talking_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntuj7e/which_dimensions_are_you_probing_when_talking_to/,515405,1623010532.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,,t2_vblkuw9,False,,0,False,"Apart from kaggle, where can I find data science challenges/projects?",[],r/datascience,False,6,discussion,0,,,False,t3_nu8eui,False,dark,0.87,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,False,,[],{},,True,,1623087707.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu8eui,True,,b15985,,12,True,all_ads,False,[],False,,/r/datascience/comments/nu8eui/apart_from_kaggle_where_can_i_find_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu8eui/apart_from_kaggle_where_can_i_find_data_science/,515405,1623058907.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_1g9552cc,False,,0,False,Is it okay to forget a language if you haven’t used it in a while?,[],r/datascience,False,6,discussion,0,,,False,t3_nufkw3,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623109222.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nufkw3,True,,MrDrPatrickStar,,8,True,all_ads,False,[],False,,/r/datascience/comments/nufkw3/is_it_okay_to_forget_a_language_if_you_havent/,all_ads,False,https://www.reddit.com/r/datascience/comments/nufkw3/is_it_okay_to_forget_a_language_if_you_havent/,515405,1623080422.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The shap package has been great when it works, but I would like an alternative package that has similar functionality. I mostly use gradient boosting, so any package that can use the tree-path methods (interventional is nice too, but not as important) would be a life saver.",t2_i8ujh,False,,0,False,Best alternatives to 'shap' package?,[],r/datascience,False,6,discussion,0,,,False,t3_nubt22,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623099208.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The shap package has been great when it works, but I would like an alternative package that has similar functionality. I mostly use gradient boosting, so any package that can use the tree-path methods (interventional is nice too, but not as important) would be a life saver.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nubt22,True,,suspicious_gardener,,9,True,all_ads,False,[],False,,/r/datascience/comments/nubt22/best_alternatives_to_shap_package/,all_ads,False,https://www.reddit.com/r/datascience/comments/nubt22/best_alternatives_to_shap_package/,515405,1623070408.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know there's a lot of different tasks people do, but I've generally worked on code bases that utilize many models, and our data has hundreds of variables. I've seen some guidance that if you've written any more than 10 lines of code it should be its own function, and that just seems insane to me.   


Obviously it's a balancing act and you shouldn't have functions with thousands of lines of code, but I've seen plenty of \~100-200 line functions that generally define either a clean group of variables (say defining the \~15 inputs to a specific model), or a business process that's a bit complex but one clear process as you'd explain in English. I've seen code that follows the paradigm of keeping functions short and I've ended up trying to search for how a variable is defined and literally following a path of 10+ functions to find the answer.   


I know it's a balancing act and you can say it always depends on details, but I didn't know if people could share their thoughts and whether in their actual day to day work they tried to follow the small function paradigm or whether the way I work is closer to how people handle their coding standards.

Edit:
Seen some posts about functions doing 1 thing and that's basically what I'm trying to figure out. Is ""prep data for model x"" considered one thing? Say you have 15 variables that all take 1-5 lines of code (and in my current project we have hundreds of models with ~15 variables each and not a ton of overlap). My opinion is that a function that defines those 15 variables is ""doing one thing"" and much easier to read/understand and maybe have to use the scroll wheel once or twice than to trace through different functions which are often factored into different code files. Do people tend to agree with this? If not how would you refactor such a function? Would you just have functions called prep_variable1, prep_variable2, etc some of which are one line long and called exactly once in your code base?",t2_541w9,False,,0,False,Data Science Coding Standards,[],r/datascience,False,6,discussion,0,,,False,t3_ntxx3j,False,dark,0.93,,public,34,0,{},,,False,[],,False,False,,{},Discussion,False,34,,False,False,self,1623021889.0,,[],{},,True,,1623048833.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know there&amp;#39;s a lot of different tasks people do, but I&amp;#39;ve generally worked on code bases that utilize many models, and our data has hundreds of variables. I&amp;#39;ve seen some guidance that if you&amp;#39;ve written any more than 10 lines of code it should be its own function, and that just seems insane to me.   &lt;/p&gt;

&lt;p&gt;Obviously it&amp;#39;s a balancing act and you shouldn&amp;#39;t have functions with thousands of lines of code, but I&amp;#39;ve seen plenty of ~100-200 line functions that generally define either a clean group of variables (say defining the ~15 inputs to a specific model), or a business process that&amp;#39;s a bit complex but one clear process as you&amp;#39;d explain in English. I&amp;#39;ve seen code that follows the paradigm of keeping functions short and I&amp;#39;ve ended up trying to search for how a variable is defined and literally following a path of 10+ functions to find the answer.   &lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s a balancing act and you can say it always depends on details, but I didn&amp;#39;t know if people could share their thoughts and whether in their actual day to day work they tried to follow the small function paradigm or whether the way I work is closer to how people handle their coding standards.&lt;/p&gt;

&lt;p&gt;Edit:
Seen some posts about functions doing 1 thing and that&amp;#39;s basically what I&amp;#39;m trying to figure out. Is &amp;quot;prep data for model x&amp;quot; considered one thing? Say you have 15 variables that all take 1-5 lines of code (and in my current project we have hundreds of models with ~15 variables each and not a ton of overlap). My opinion is that a function that defines those 15 variables is &amp;quot;doing one thing&amp;quot; and much easier to read/understand and maybe have to use the scroll wheel once or twice than to trace through different functions which are often factored into different code files. Do people tend to agree with this? If not how would you refactor such a function? Would you just have functions called prep_variable1, prep_variable2, etc some of which are one line long and called exactly once in your code base?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntxx3j,True,,zachvac,,35,True,all_ads,False,[],False,,/r/datascience/comments/ntxx3j/data_science_coding_standards/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntxx3j/data_science_coding_standards/,515405,1623020033.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm a first time hiring manager and I'm hiring a Data Analyst. I've reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.

Call me a purist, but I don't think it's possible to do good quantitative work if you don't understand how the models work from a basic mathematical perspective. For example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you're using? What does a p-value actually represent in mathematical terms? What are the assumptions around independence of your predictor variables for any given model you're using? What diagnostic tests should you run for any given model and what hypothesis are each of these testing? And more. If you don't understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.

The number of people I've talked to who have a MS in Business Analytics who say they're doing ML but then can't explain these basic underlying assumptions of the models they build is astounding and pitiful. It doesn't matter how prestigious the school they went to--I've talked to people with degrees like this from both Ivy League schools and from lesser known schools and it is all the same. I'm not trying to trick people with obscure trivia or anything; when I ask them to tell me about a project they worked on, they tell me ""I built \[insert model\] so I could \[solve whatever business problem\]"" and when I follow up with ""what are the underlying assumptions for \[insert model\]? What kinds of diagnostics did you run for that model?"", they get completely tongue tied, even for the basics like logistic and linear regression. The only things these people ever check for with diagnostics is ""sensitivity and specificity"" and ""area under the curve"", which barely scratches the surface in determining whether or not your model is good.

I've looked into the requirements on these programs (both when I was choosing a masters program a few years back and also more recently as I've been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like ""SQL for Analytics"", ""Tableau for Visualization"", and super tool specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.

So.... if you're thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a REAL ACADEMIC FIELD (ex: statistics, math, computer science, engineering) and not Some Buzzword That's Super Hot Right Now. No one will care about your bullshit buzzword degree 10 years from now. At the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into R or Python hoping that you'll be able to \~predict the future\~.",t2_7s1iybm6,False,,0,False,[rant from a hiring manager] MS in Buzzword programs are a waste of money,[],r/datascience,False,6,discussion,0,,,False,t3_nusz8e,False,dark,0.39,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623144392.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a first time hiring manager and I&amp;#39;m hiring a Data Analyst. I&amp;#39;ve reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.&lt;/p&gt;

&lt;p&gt;Call me a purist, but I don&amp;#39;t think it&amp;#39;s possible to do good quantitative work if you don&amp;#39;t understand how the models work from a basic mathematical perspective. For example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you&amp;#39;re using? What does a p-value actually represent in mathematical terms? What are the assumptions around independence of your predictor variables for any given model you&amp;#39;re using? What diagnostic tests should you run for any given model and what hypothesis are each of these testing? And more. If you don&amp;#39;t understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.&lt;/p&gt;

&lt;p&gt;The number of people I&amp;#39;ve talked to who have a MS in Business Analytics who say they&amp;#39;re doing ML but then can&amp;#39;t explain these basic underlying assumptions of the models they build is astounding and pitiful. It doesn&amp;#39;t matter how prestigious the school they went to--I&amp;#39;ve talked to people with degrees like this from both Ivy League schools and from lesser known schools and it is all the same. I&amp;#39;m not trying to trick people with obscure trivia or anything; when I ask them to tell me about a project they worked on, they tell me &amp;quot;I built [insert model] so I could [solve whatever business problem]&amp;quot; and when I follow up with &amp;quot;what are the underlying assumptions for [insert model]? What kinds of diagnostics did you run for that model?&amp;quot;, they get completely tongue tied, even for the basics like logistic and linear regression. The only things these people ever check for with diagnostics is &amp;quot;sensitivity and specificity&amp;quot; and &amp;quot;area under the curve&amp;quot;, which barely scratches the surface in determining whether or not your model is good.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve looked into the requirements on these programs (both when I was choosing a masters program a few years back and also more recently as I&amp;#39;ve been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like &amp;quot;SQL for Analytics&amp;quot;, &amp;quot;Tableau for Visualization&amp;quot;, and super tool specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.&lt;/p&gt;

&lt;p&gt;So.... if you&amp;#39;re thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a REAL ACADEMIC FIELD (ex: statistics, math, computer science, engineering) and not Some Buzzword That&amp;#39;s Super Hot Right Now. No one will care about your bullshit buzzword degree 10 years from now. At the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into R or Python hoping that you&amp;#39;ll be able to ~predict the future~.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nusz8e,True,,Apprehensive-Web-987,,46,True,all_ads,False,[],False,,/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/,all_ads,False,https://www.reddit.com/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/,515405,1623115592.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So suppose you have  different data sets that are not connected to each other via a ""primary ID, for example:

df 1:  Numerical data of medical records of patients (e.g BP, Sugar, etc)

df 2:  Objective and subjective records by the patient.

Now we have to match records from the df2 to that of df1. How can we match these two dfs without having any ID.

&amp;#x200B;

It can be data of anything, website data or exam data, etc. Let me know how would you approach this problem.",t2_34qgdyb6,False,,0,False,I was wondering about this problem and wanted to know if it makes any sense lol,[],r/datascience,False,6,discussion,0,,,False,t3_nu7uz0,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623085243.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So suppose you have  different data sets that are not connected to each other via a &amp;quot;primary ID, for example:&lt;/p&gt;

&lt;p&gt;df 1:  Numerical data of medical records of patients (e.g BP, Sugar, etc)&lt;/p&gt;

&lt;p&gt;df 2:  Objective and subjective records by the patient.&lt;/p&gt;

&lt;p&gt;Now we have to match records from the df2 to that of df1. How can we match these two dfs without having any ID.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;It can be data of anything, website data or exam data, etc. Let me know how would you approach this problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu7uz0,True,,yaakarsh1011,,6,True,all_ads,False,[],False,,/r/datascience/comments/nu7uz0/i_was_wondering_about_this_problem_and_wanted_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu7uz0/i_was_wondering_about_this_problem_and_wanted_to/,515405,1623056443.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm graduating next year from a CS engineering school in France.

I was contacted by a recruiter on LinkedIn for a Business/Data Analyst role. I applied to it, had a few tests in SQL/Excel, and video interviews. Got the internship.

Then, I had my first call with my manager. He told me that we would be doing some descriptive analytics, but also predictive and prescriptive (basically ML) analytics if I was willing to. He then asked me about my expectations for the internship, and I told him that I would like to do a lot of DS/ML work, instead of BA/DA. Luckily, since he's the one responsible for DS in the team, he agreed and offered me to do a DS oriented internship instead of a BA/DA one.

&amp;#x200B;

And that's how I got an Amazon DS internship, without even completing DS-specific tests.

I'm starting my project on Monday, I feel kinda scared lol. I don't have much experience and have only implemented a few clustering algorithms and linear regressions, but I have some theoretical knowledge on more complex stuff. Since it's going to be my first 'real' internship, I don't know how it's going to work, but I guess I'll just Google anything I don't know when I need it.

&amp;#x200B;

I still can't realize the opportunity it is, and hope it's going to launch my career in Data Science ! All of my friends are mad jealous",t2_lblkf,False,,0,False,"I got my first internship as a Data Scientist, at Amazon !",[],r/datascience,False,6,career,0,,,False,t3_nt8cg8,False,dark,0.95,,public,1119,7,{},,,False,[],,False,False,,{},Career,False,1119,,False,False,self,False,,[],{'gid_1': 3},,True,,1622964698.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m graduating next year from a CS engineering school in France.&lt;/p&gt;

&lt;p&gt;I was contacted by a recruiter on LinkedIn for a Business/Data Analyst role. I applied to it, had a few tests in SQL/Excel, and video interviews. Got the internship.&lt;/p&gt;

&lt;p&gt;Then, I had my first call with my manager. He told me that we would be doing some descriptive analytics, but also predictive and prescriptive (basically ML) analytics if I was willing to. He then asked me about my expectations for the internship, and I told him that I would like to do a lot of DS/ML work, instead of BA/DA. Luckily, since he&amp;#39;s the one responsible for DS in the team, he agreed and offered me to do a DS oriented internship instead of a BA/DA one.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s how I got an Amazon DS internship, without even completing DS-specific tests.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m starting my project on Monday, I feel kinda scared lol. I don&amp;#39;t have much experience and have only implemented a few clustering algorithms and linear regressions, but I have some theoretical knowledge on more complex stuff. Since it&amp;#39;s going to be my first &amp;#39;real&amp;#39; internship, I don&amp;#39;t know how it&amp;#39;s going to work, but I guess I&amp;#39;ll just Google anything I don&amp;#39;t know when I need it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I still can&amp;#39;t realize the opportunity it is, and hope it&amp;#39;s going to launch my career in Data Science ! All of my friends are mad jealous&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt8cg8,True,,Picetash,,87,True,all_ads,False,[],False,,/r/datascience/comments/nt8cg8/i_got_my_first_internship_as_a_data_scientist_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt8cg8/i_got_my_first_internship_as_a_data_scientist_at/,515405,1622935898.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have some data, mostly in excel that I would like to build some dashboards for and break down into understandable pieces.  But it seems like most tools like Tableau don’t have a free/personal tier/option.  Are any tools approachable for someone without deep pockets?",t2_p7u2k,False,,0,False,What tools are available for personal use?,[],r/datascience,False,6,tooling,0,,,False,t3_nu52fh,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Tooling,False,3,,False,False,self,False,,[],{},,True,,1623073317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have some data, mostly in excel that I would like to build some dashboards for and break down into understandable pieces.  But it seems like most tools like Tableau don’t have a free/personal tier/option.  Are any tools approachable for someone without deep pockets?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu52fh,True,,mjulson,,16,True,all_ads,False,[],False,,/r/datascience/comments/nu52fh/what_tools_are_available_for_personal_use/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu52fh/what_tools_are_available_for_personal_use/,515405,1623044517.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi! Looking for a suggestion on a visual cleaning tool for time series (or any) data. 

I am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. 

Trying to avoid writing a bespoke thing in dash but I can ultimately do that if it comes to it. 

Not bound to a platform but python is preferred, or even something electron based. 

Thanks!",t2_ahu1o,False,,0,False,Visual Cleaning tool - Time Series,[],r/datascience,False,6,tooling,0,,,False,t3_nub8j8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,1623069562.0,,[],{},,True,,1623097506.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! Looking for a suggestion on a visual cleaning tool for time series (or any) data. &lt;/p&gt;

&lt;p&gt;I am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. &lt;/p&gt;

&lt;p&gt;Trying to avoid writing a bespoke thing in dash but I can ultimately do that if it comes to it. &lt;/p&gt;

&lt;p&gt;Not bound to a platform but python is preferred, or even something electron based. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nub8j8,True,,Namur007,,0,True,all_ads,False,[],False,,/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/,all_ads,False,https://www.reddit.com/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/,515405,1623068706.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi all, been learning Factor Analysis for the first time using datasets from Kaggle. I’ve been using Factor Analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with Parallel Analysis (other than Kaiser Criterion and Scree Plot).

There’s literally nothing I can find on Parallel Analysis (PA) in Python, so I read a paper called: ‘Parallel Analysis: a method for determining significant principal components’. It suggests generating a random matrix with the same number of variables and samples. After standardising my dataset, I randomly generated normally-distributed numbers with mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform Parallel Analysis. My end Scree Plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically I would be doing a glorified Kaiser Criterion comparison).

So have I done something wrong? Are there any resources on PA in Python?",t2_wsxt9,False,,0,False,Horn’s Parallel Analysis in Python: Am I doing it correctly?,[],r/datascience,False,6,projects,0,,,False,t3_nu4deh,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1623070743.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, been learning Factor Analysis for the first time using datasets from Kaggle. I’ve been using Factor Analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with Parallel Analysis (other than Kaiser Criterion and Scree Plot).&lt;/p&gt;

&lt;p&gt;There’s literally nothing I can find on Parallel Analysis (PA) in Python, so I read a paper called: ‘Parallel Analysis: a method for determining significant principal components’. It suggests generating a random matrix with the same number of variables and samples. After standardising my dataset, I randomly generated normally-distributed numbers with mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform Parallel Analysis. My end Scree Plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically I would be doing a glorified Kaiser Criterion comparison).&lt;/p&gt;

&lt;p&gt;So have I done something wrong? Are there any resources on PA in Python?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu4deh,True,,Myzziah,,0,True,all_ads,False,[],False,,/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/,515405,1623041943.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm planning to become a SWE but have been developing carpal tunnel syndrome symptoms, and I don't want to risk wasting my time learning software programming if I'll still always have recurring CTS in the end and can't code for hours a day. I'm also interested in DS but I understand that there is a coding aspect to this job as well.",t2_3rjdpsxd,False,,0,False,How much coding do data scientists do in a day?,[],r/datascience,False,6,meta,0,,,False,t3_nu1go7,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Meta,False,4,,False,False,self,False,,[],{},,True,,1623060477.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m planning to become a SWE but have been developing carpal tunnel syndrome symptoms, and I don&amp;#39;t want to risk wasting my time learning software programming if I&amp;#39;ll still always have recurring CTS in the end and can&amp;#39;t code for hours a day. I&amp;#39;m also interested in DS but I understand that there is a coding aspect to this job as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu1go7,True,,Dudeguybrochingo,,8,True,all_ads,False,[],False,,/r/datascience/comments/nu1go7/how_much_coding_do_data_scientists_do_in_a_day/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu1go7/how_much_coding_do_data_scientists_do_in_a_day/,515405,1623031677.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,
,datascience,"Are there any mainstream python libraries which, given data from some source, suggest normalization schemes for data (e.g., recommended table structures for a 3NF or star schema in an RDBMS), profile relationships between fields for cardinality (x% of the time this field has a 1:1 relationship with this other field, the remaining y% are missing data and 1:many relationships), or complete other data modeling-related tasks?

I could cobble together some of this functionality using builtins/pandas/numpy etc. but am looking for industry-standard tools that data scientists use for the kind of “lite” data modeling that comes up on the job. (People’s personal GitHub repos for these tasks are OK but not exactly what I am looking for.)

Here are some sample cases to clarify:
1. You received a huge, raw denormalized extract from somewhere and will continue to receive incremental files on a regular basis. You want to create a profile of the initial data, use said profile to make some tradeoffs to “tidy” the data into an RDBMS-suitable format (maybe force 1:1 relationships where they exist 99% of the time for instance or fix overlapping datespans), and then monitor subsequent incremental files to ensure the underlying data profile had not changed drastically.
2. You received access to a new database with no documentation and little support from DBAs or the business on structures. Assume there are not keys or constraints defined in the RDBMS itself to leverage (perhaps the database was created/maintained by a skilled business user without DBA-level skills). You would like to create an ERD or some other documentation on this database quickly.

The focus on RDBMS as the endgame is because the goal here is to support analysis by a BI team that is skilled in SQL but no other programming languages.",t2_enab6,False,,0,False,Standard Python Resources for Data Modeling,[],r/datascience,False,6,tooling,0,,,False,t3_ntptf0,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},Tooling,False,7,,False,False,self,False,,[],{},,True,,1623026455.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any mainstream python libraries which, given data from some source, suggest normalization schemes for data (e.g., recommended table structures for a 3NF or star schema in an RDBMS), profile relationships between fields for cardinality (x% of the time this field has a 1:1 relationship with this other field, the remaining y% are missing data and 1:many relationships), or complete other data modeling-related tasks?&lt;/p&gt;

&lt;p&gt;I could cobble together some of this functionality using builtins/pandas/numpy etc. but am looking for industry-standard tools that data scientists use for the kind of “lite” data modeling that comes up on the job. (People’s personal GitHub repos for these tasks are OK but not exactly what I am looking for.)&lt;/p&gt;

&lt;p&gt;Here are some sample cases to clarify:
1. You received a huge, raw denormalized extract from somewhere and will continue to receive incremental files on a regular basis. You want to create a profile of the initial data, use said profile to make some tradeoffs to “tidy” the data into an RDBMS-suitable format (maybe force 1:1 relationships where they exist 99% of the time for instance or fix overlapping datespans), and then monitor subsequent incremental files to ensure the underlying data profile had not changed drastically.
2. You received access to a new database with no documentation and little support from DBAs or the business on structures. Assume there are not keys or constraints defined in the RDBMS itself to leverage (perhaps the database was created/maintained by a skilled business user without DBA-level skills). You would like to create an ERD or some other documentation on this database quickly.&lt;/p&gt;

&lt;p&gt;The focus on RDBMS as the endgame is because the goal here is to support analysis by a BI team that is skilled in SQL but no other programming languages.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntptf0,True,,AMereRedditor,,4,True,all_ads,False,[],False,,/r/datascience/comments/ntptf0/standard_python_resources_for_data_modeling/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntptf0/standard_python_resources_for_data_modeling/,515405,1622997655.0,2,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all! So recently, I started working at HP as a product management intern. I know, that’s marketing, not data science, but I’ve spent the past two weeks learning all about it, and I find it very interesting. 

So, for context, I’m a senior and this is my first PAID internship. I’ve done 5 unpaid internships over the course of 3 semesters before this in hopes it would land me a real one, and it actually did. I spoke to a HP recruiter which gave me a contact at another company. After 3 months of speaking to several people at the other company, just to result in my intern application being rejected, I returned to said HP recruiter. Their website had no internship that fit me, so I spoke to her directly, which led to two interviews and an offer. And after viewing my work profile, I saw that said internship was meant for an MBA candidate, not an undergrad. Therefore, I’m guessing my experience made up for what level of education they were looking for.

Thus, I signed on to be a marketing manager/product management intern for the Z by HP division. And Z makes computers for data scientists, like high gigs of ram, more core processors, and intel gold (and probably some other stuff, but my tech knowledge isn’t super expansive). And while working with them, I’ve interviewed a lot of DS professionals about their computer needs and what type they use. Also, about what SW stacks they like, and what language they code in. And there has been quite a consistency amongst their answers.

Therefore, if you are a DS undergrad/graduate looking for a job or internship, I have some advice for you:

1. Diversify your skill set. Most people I spoke to didn’t have a DS degree, rather they started their careers, something went wrong, and they made a career shift to DS. It was easy to switch to because from what I can tell, DS is growing, all companies are dependent on some sort of cloud, and python is their preferred code language. So with the influx of people entering DS, if you started in DS, learn for than one coding language, and make sure you can use a variety of SW stacks so that you can separate yourself from the competition.

2. Take an unpaid internship (only if it is worth it). Like I said, I’ve had 5, but I quit one early. The reason is if you aren’t going to pay me, I’m not going to perform work I believe a paid person should (which I know, is technically any work, but just watch where I’m going with this). I quit one because there was no way I was going to be at the stadium 3 hours before the game starts, run around setting every event table up and it’s accessories, miss the entire game because I have to help people the entire time, not have a break, get off at 11 pm, not have a parking pass, and on top of that only be fed a mini Jimmy John’s sandwhich. 

But, there were others where I liked what I was doing, and they only required 10 hours or less work a week. I’d never advise anyone to work 40 hours a week for free, I wouldn’t advise even 15 hours a week for free. Just a few internships to pad the Resumé, but most importantly, actually provide you with some knowledge that will be applicable to your next job.

3. Apply even if you are not qualified. This applies to both internships and scholarships because I’ve gotten my way with both. These things provide too much for you not to try. I almost didn’t apply to the scholarship that takes care of all my tuition for my last two years of college, but I did. I almost reach back out to the recruiter that led to my internship now, at a company that will pay off the few student loans I do have. Hell, I almost didn’t apply to transfer to the university in at now after my freshman year at one I didn’t like, but was the epitome of “safe”. So believe in yourself and pull the trigger. This saying be be shot to hell but it’s true: you only live once (unless you believe in reincarnation, but you still only live THIS life once).",t2_31rlco6l,False,,0,False,I got a DS related internship at HP!,[],r/datascience,False,6,career,0,,,False,t3_ntnan1,False,dark,0.63,,public,5,0,{},,,False,[],,False,False,,{},Career,False,5,,False,False,self,False,,[],{},,True,,1623019436.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all! So recently, I started working at HP as a product management intern. I know, that’s marketing, not data science, but I’ve spent the past two weeks learning all about it, and I find it very interesting. &lt;/p&gt;

&lt;p&gt;So, for context, I’m a senior and this is my first PAID internship. I’ve done 5 unpaid internships over the course of 3 semesters before this in hopes it would land me a real one, and it actually did. I spoke to a HP recruiter which gave me a contact at another company. After 3 months of speaking to several people at the other company, just to result in my intern application being rejected, I returned to said HP recruiter. Their website had no internship that fit me, so I spoke to her directly, which led to two interviews and an offer. And after viewing my work profile, I saw that said internship was meant for an MBA candidate, not an undergrad. Therefore, I’m guessing my experience made up for what level of education they were looking for.&lt;/p&gt;

&lt;p&gt;Thus, I signed on to be a marketing manager/product management intern for the Z by HP division. And Z makes computers for data scientists, like high gigs of ram, more core processors, and intel gold (and probably some other stuff, but my tech knowledge isn’t super expansive). And while working with them, I’ve interviewed a lot of DS professionals about their computer needs and what type they use. Also, about what SW stacks they like, and what language they code in. And there has been quite a consistency amongst their answers.&lt;/p&gt;

&lt;p&gt;Therefore, if you are a DS undergrad/graduate looking for a job or internship, I have some advice for you:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Diversify your skill set. Most people I spoke to didn’t have a DS degree, rather they started their careers, something went wrong, and they made a career shift to DS. It was easy to switch to because from what I can tell, DS is growing, all companies are dependent on some sort of cloud, and python is their preferred code language. So with the influx of people entering DS, if you started in DS, learn for than one coding language, and make sure you can use a variety of SW stacks so that you can separate yourself from the competition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take an unpaid internship (only if it is worth it). Like I said, I’ve had 5, but I quit one early. The reason is if you aren’t going to pay me, I’m not going to perform work I believe a paid person should (which I know, is technically any work, but just watch where I’m going with this). I quit one because there was no way I was going to be at the stadium 3 hours before the game starts, run around setting every event table up and it’s accessories, miss the entire game because I have to help people the entire time, not have a break, get off at 11 pm, not have a parking pass, and on top of that only be fed a mini Jimmy John’s sandwhich. &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But, there were others where I liked what I was doing, and they only required 10 hours or less work a week. I’d never advise anyone to work 40 hours a week for free, I wouldn’t advise even 15 hours a week for free. Just a few internships to pad the Resumé, but most importantly, actually provide you with some knowledge that will be applicable to your next job.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Apply even if you are not qualified. This applies to both internships and scholarships because I’ve gotten my way with both. These things provide too much for you not to try. I almost didn’t apply to the scholarship that takes care of all my tuition for my last two years of college, but I did. I almost reach back out to the recruiter that led to my internship now, at a company that will pay off the few student loans I do have. Hell, I almost didn’t apply to transfer to the university in at now after my freshman year at one I didn’t like, but was the epitome of “safe”. So believe in yourself and pull the trigger. This saying be be shot to hell but it’s true: you only live once (unless you believe in reincarnation, but you still only live THIS life once).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntnan1,True,,KVthegreatest,,0,True,all_ads,False,[],False,,/r/datascience/comments/ntnan1/i_got_a_ds_related_internship_at_hp/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntnan1/i_got_a_ds_related_internship_at_hp/,515405,1622990636.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 06 Jun 2021 - 13 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_ntk7dk,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1623009631.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,ntk7dk,True,,datascience-bot,,199,False,all_ads,False,[],False,dark,/r/datascience/comments/ntk7dk/weekly_entering_transitioning_thread_06_jun_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntk7dk/weekly_entering_transitioning_thread_06_jun_2021/,515405,1622980831.0,0,,False,,,,,,,,
,datascience,Edit: Thanks a lot for the people who have replied to this post. I now got a brief idea.,t2_1lhrbnal,False,,0,False,Is there any correlation between Supply Chain Management and Data Science ?,[],r/datascience,False,6,discussion,0,,,False,t3_nthz8k,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,1623006786.0,,[],{},,True,,1623000711.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: Thanks a lot for the people who have replied to this post. I now got a brief idea.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nthz8k,True,,akshithjay,,12,True,all_ads,False,[],False,,/r/datascience/comments/nthz8k/is_there_any_correlation_between_supply_chain/,all_ads,False,https://www.reddit.com/r/datascience/comments/nthz8k/is_there_any_correlation_between_supply_chain/,515405,1622971911.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So far I've used only R and Python for my main projects, but I keep hearing about Julia as a much better solution (performance wise). Has anyone used it instead of Python in production. Do you think it could replace Python, (provided there is more support for libraries)?",t2_3s7gldef,False,,0,False,Thoughts on Julia Programming Language,[],r/datascience,False,6,tooling,0,,,False,t3_ntj7md,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},Tooling,False,5,,False,False,self,False,,[],{},,True,,1623005858.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So far I&amp;#39;ve used only R and Python for my main projects, but I keep hearing about Julia as a much better solution (performance wise). Has anyone used it instead of Python in production. Do you think it could replace Python, (provided there is more support for libraries)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntj7md,True,,XhoniShollaj,,33,False,all_ads,False,[],False,,/r/datascience/comments/ntj7md/thoughts_on_julia_programming_language/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntj7md/thoughts_on_julia_programming_language/,515405,1622977058.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"My current work is very lax and although I was hired to do data engineering work, I have not done much of that. My position seem to be a placeholder and I am given random tasks (clean up data, check if some mathematics can be done with that data, image processing, extracting data from weird S3 buckets). And my work is not even very frequent, I can go on days without practically doing anything. 

I need to switch jobs immediately for family matters. Can I use these experiences in my resume and switch to a data science position? My coding skill is nothing to brag about but my understanding of statistics, Analytics and model building should hold up.

Edit: Thank you everyone for your responses. I had some good ideas.",t2_6pycc2fm,False,,0,False,How to explain lack of meaningful work in your last job?,[],r/datascience,False,6,,0,,,False,t3_nswxtw,False,dark,0.96,,public,166,0,{},,,False,[],,False,False,,{},Job Search,False,166,,False,False,self,1622925370.0,,[],{},,True,,1622932125.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My current work is very lax and although I was hired to do data engineering work, I have not done much of that. My position seem to be a placeholder and I am given random tasks (clean up data, check if some mathematics can be done with that data, image processing, extracting data from weird S3 buckets). And my work is not even very frequent, I can go on days without practically doing anything. &lt;/p&gt;

&lt;p&gt;I need to switch jobs immediately for family matters. Can I use these experiences in my resume and switch to a data science position? My coding skill is nothing to brag about but my understanding of statistics, Analytics and model building should hold up.&lt;/p&gt;

&lt;p&gt;Edit: Thank you everyone for your responses. I had some good ideas.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nswxtw,True,,not_y0ur_business,,53,True,all_ads,False,[],False,,/r/datascience/comments/nswxtw/how_to_explain_lack_of_meaningful_work_in_your/,all_ads,False,https://www.reddit.com/r/datascience/comments/nswxtw/how_to_explain_lack_of_meaningful_work_in_your/,515405,1622903325.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"What's the latest senior data scientist salary range in EU, especially countries like Germany, France and the Netherlands? In US tech companies based here, high end consulting such as BCG and also the average European company? Glassdoor is so outdated and anyway does not have sufficient data points for senior roles.",t2_6m8l9x87,False,,0,False,Senior data scientist salaries in Eurozone,[],r/datascience,False,6,career,0,,,False,t3_nsvihd,False,dark,0.87,,public,46,0,{},,,False,[],,False,False,,{},Career,False,46,,False,False,self,False,,[],{},,True,,1622927741.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What&amp;#39;s the latest senior data scientist salary range in EU, especially countries like Germany, France and the Netherlands? In US tech companies based here, high end consulting such as BCG and also the average European company? Glassdoor is so outdated and anyway does not have sufficient data points for senior roles.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsvihd,True,,darn321,,58,True,all_ads,False,[],False,,/r/datascience/comments/nsvihd/senior_data_scientist_salaries_in_eurozone/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsvihd/senior_data_scientist_salaries_in_eurozone/,515405,1622898941.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Looking for suggestions on either books, courses, websites etc. that cover things like:

* Survival analysis
* Reliability testing 
* Lifecycle testing 

Would be great if implementations were based on python/R.",t2_7m1zlf41,False,,0,False,"Lifetime, reliability and performance testing",[],r/datascience,False,6,discussion,0,,,False,t3_nt85ct,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622964073.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking for suggestions on either books, courses, websites etc. that cover things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Survival analysis&lt;/li&gt;
&lt;li&gt;Reliability testing &lt;/li&gt;
&lt;li&gt;Lifecycle testing &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Would be great if implementations were based on python/R.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt85ct,True,,Limp-Ad-7289,,2,True,all_ads,False,[],False,,/r/datascience/comments/nt85ct/lifetime_reliability_and_performance_testing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt85ct/lifetime_reliability_and_performance_testing/,515405,1622935273.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

Few months ago, I started working on a project that requires me to use R to fetch, clean data, do some feature engineering. I'm able to do whatever is required but I'm not sure if my code or rather code snippets are ""good"" or ""bad"". I'm not even sure what good or bad means but I've seen these words thrown around. 

Can more experienced people of this sub explain to me what qualifies as a good code?

Thanks!",t2_bv171ji2,False,,0,False,How do you know if you're writing a good code or a bad one?,[],r/datascience,False,6,discussion,0,,,False,t3_nt7t2p,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1622963003.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Few months ago, I started working on a project that requires me to use R to fetch, clean data, do some feature engineering. I&amp;#39;m able to do whatever is required but I&amp;#39;m not sure if my code or rather code snippets are &amp;quot;good&amp;quot; or &amp;quot;bad&amp;quot;. I&amp;#39;m not even sure what good or bad means but I&amp;#39;ve seen these words thrown around. &lt;/p&gt;

&lt;p&gt;Can more experienced people of this sub explain to me what qualifies as a good code?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt7t2p,True,,quite--average,,15,True,all_ads,False,[],False,,/r/datascience/comments/nt7t2p/how_do_you_know_if_youre_writing_a_good_code_or_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt7t2p/how_do_you_know_if_youre_writing_a_good_code_or_a/,515405,1622934203.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

I recently found out that Carvana lets you use the internet while taking their technical test. They wrote something like this in the email invitation, ""We all know everybody googles the syntax on their job"". I'm sure there are many companies out there with similar mindset that I'm not aware of.

I found it interesting and was wondering what are your thoughts on this. Should more companies start allowing the use of internet in their coding tests?

Thanks!",t2_bv171ji2,False,,0,False,Carvana lets you google while taking a coding test. Do you think more companies need to do this?,[],r/datascience,False,6,discussion,0,,,False,t3_nsf633,False,dark,0.99,,public,373,1,{},,,False,[],,False,False,,{},Discussion,False,373,,False,False,self,False,,[],{},,True,,1622868622.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I recently found out that Carvana lets you use the internet while taking their technical test. They wrote something like this in the email invitation, &amp;quot;We all know everybody googles the syntax on their job&amp;quot;. I&amp;#39;m sure there are many companies out there with similar mindset that I&amp;#39;m not aware of.&lt;/p&gt;

&lt;p&gt;I found it interesting and was wondering what are your thoughts on this. Should more companies start allowing the use of internet in their coding tests?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsf633,True,,quite--average,,76,True,all_ads,False,[],False,,/r/datascience/comments/nsf633/carvana_lets_you_google_while_taking_a_coding/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsf633/carvana_lets_you_google_while_taking_a_coding/,515405,1622839822.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey, everyone! So I'm about to update my laptop that is long overdue to hit the trash, and I'm thinking about how, logistics wise, I do my schoolwork at work. The issue, though, is I can't use that laptop to connect to the internet. By go-to makeshift solution was to google or search stuff through the work computer and use my personal computer to do Jupyter Notebook stuff, which doesn't require the internet to work, but that doesn't work for things such as using StackOverflow, where they want screenshots or wanting to see the exact code used.

So my  question is, is there a way to bring Wi-Fi with you so that you can use your laptop on the go? Is there anything I can buy? Thanks!",t2_2wzu5n8,False,,0,False,Portable Laptop Wifi,[],r/datascience,False,6,tooling,0,,,False,t3_ntasnn,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1622972867.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, everyone! So I&amp;#39;m about to update my laptop that is long overdue to hit the trash, and I&amp;#39;m thinking about how, logistics wise, I do my schoolwork at work. The issue, though, is I can&amp;#39;t use that laptop to connect to the internet. By go-to makeshift solution was to google or search stuff through the work computer and use my personal computer to do Jupyter Notebook stuff, which doesn&amp;#39;t require the internet to work, but that doesn&amp;#39;t work for things such as using StackOverflow, where they want screenshots or wanting to see the exact code used.&lt;/p&gt;

&lt;p&gt;So my  question is, is there a way to bring Wi-Fi with you so that you can use your laptop on the go? Is there anything I can buy? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntasnn,True,,KyronAWF,,5,True,all_ads,False,[],False,,/r/datascience/comments/ntasnn/portable_laptop_wifi/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntasnn/portable_laptop_wifi/,515405,1622944067.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Disclaimer: not a data scientist but a policy and urban affairs researcher/consultant that uses DS/DA tools to do his job better. Thus, honest question, sort if it sounds stupid.
  The job I applied to required only Excel knowledge but right now I'm using SQL and Python (100% self taught) almost every day as my company is doing less traditional consultancy and more analytics stuff. Most of the times it's just data cleaning and wrangling and getting insights or designing the process when it involves geographic data so that the data engineering interns can do the proper ETL process. If I'm doing something more advanced (clustering, distance matrices, facility location...) I usually make do with out of the box solutions.
 I keep reading about data structures and algorithms in this sub and how important they are. However, I'm a bit mystified by the terminology and can't really see how it's useful. I know spatial indexing often uses trees and is used to make searching and performing operations faster (Union, intersection...). Other than that, I'm a at a loss and I'm a bit worried that whenever I need to do more advanced stuff or eventually interview for a more data-oriented role, I'll just make a fool of myself. 

Thanks so much!",t2_4xuiwc2u,False,,0,False,How useful is knowledge of data structures and algorithms and how to learn them best?,[],r/datascience,False,6,discussion,0,,,False,t3_nsqj14,False,dark,0.97,,public,27,0,{},,,False,[],,False,False,,{},Discussion,False,27,,False,False,self,False,,[],{},,True,,1622908072.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Disclaimer: not a data scientist but a policy and urban affairs researcher/consultant that uses DS/DA tools to do his job better. Thus, honest question, sort if it sounds stupid.
  The job I applied to required only Excel knowledge but right now I&amp;#39;m using SQL and Python (100% self taught) almost every day as my company is doing less traditional consultancy and more analytics stuff. Most of the times it&amp;#39;s just data cleaning and wrangling and getting insights or designing the process when it involves geographic data so that the data engineering interns can do the proper ETL process. If I&amp;#39;m doing something more advanced (clustering, distance matrices, facility location...) I usually make do with out of the box solutions.
 I keep reading about data structures and algorithms in this sub and how important they are. However, I&amp;#39;m a bit mystified by the terminology and can&amp;#39;t really see how it&amp;#39;s useful. I know spatial indexing often uses trees and is used to make searching and performing operations faster (Union, intersection...). Other than that, I&amp;#39;m a at a loss and I&amp;#39;m a bit worried that whenever I need to do more advanced stuff or eventually interview for a more data-oriented role, I&amp;#39;ll just make a fool of myself. &lt;/p&gt;

&lt;p&gt;Thanks so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsqj14,True,,tururut_tururut,,10,True,all_ads,False,[],False,,/r/datascience/comments/nsqj14/how_useful_is_knowledge_of_data_structures_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsqj14/how_useful_is_knowledge_of_data_structures_and/,515405,1622879272.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My methodology is just making lots of pivot tables. What do they want to see? 

More info: they gave me a list of a dozen questions to answer. All of which were easily answered by cleaning up the data and then making pivot tables. I submitted a report in excel and now I’ve been asked to present my findings. They said my presentation should include my methodology and process. I’m kind of stumped for how to include it in a PowerPoint presentation. 

Thanks for any insight.",t2_ai5p1w9o,False,,0,False,"I’m doing an excel project for a job interview, and I’m supposed to do a presentation that includes my “process and methodology.”",[],r/datascience,False,6,,0,,,False,t3_nt8i82,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622965217.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My methodology is just making lots of pivot tables. What do they want to see? &lt;/p&gt;

&lt;p&gt;More info: they gave me a list of a dozen questions to answer. All of which were easily answered by cleaning up the data and then making pivot tables. I submitted a report in excel and now I’ve been asked to present my findings. They said my presentation should include my methodology and process. I’m kind of stumped for how to include it in a PowerPoint presentation. &lt;/p&gt;

&lt;p&gt;Thanks for any insight.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nt8i82,True,,Enough_Blueberry_549,,9,True,all_ads,False,[],False,,/r/datascience/comments/nt8i82/im_doing_an_excel_project_for_a_job_interview_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt8i82/im_doing_an_excel_project_for_a_job_interview_and/,515405,1622936417.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"when you start a project with a problem and try to work towards a solution (which is what you should do to make sure your work is actually useful) then you arrive at this hurdle where you have the problem and an idea for the solution at hand, and they are your only lead to finding the specific data you need to train you models. Sometimes this data can be really hard to find using these search parameters. No matter how much I search, I don't find what I’m looking for

The data is probably out there and there is probably some search term that would make google put this data right at the top for you to see, but I've often found that the problem and prospective solution I have on hand is generally not it. Datasets online simply aren't indexed by their applications, they are probably most often indexed by their source. And that is something that I, in my experience, can’t really use to engineer a search term that gives good results (if the data even exists online).

I was wondering if you all had the same problem and whether you agreed with this idea. Is it the same case in your experience or am I just doing it wrong?",t2_7z7d7,False,,0,False,"For most of the problems I try to solve using data science, the biggest challenge surprisingly isn’t really the “science” part but the “data” part",[],r/datascience,False,6,discussion,0,,,False,t3_ns5lwu,False,dark,0.96,,public,315,0,{},,,False,[],,False,False,,{},Discussion,False,315,,False,False,self,False,,[],{},,True,,1622843477.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;when you start a project with a problem and try to work towards a solution (which is what you should do to make sure your work is actually useful) then you arrive at this hurdle where you have the problem and an idea for the solution at hand, and they are your only lead to finding the specific data you need to train you models. Sometimes this data can be really hard to find using these search parameters. No matter how much I search, I don&amp;#39;t find what I’m looking for&lt;/p&gt;

&lt;p&gt;The data is probably out there and there is probably some search term that would make google put this data right at the top for you to see, but I&amp;#39;ve often found that the problem and prospective solution I have on hand is generally not it. Datasets online simply aren&amp;#39;t indexed by their applications, they are probably most often indexed by their source. And that is something that I, in my experience, can’t really use to engineer a search term that gives good results (if the data even exists online).&lt;/p&gt;

&lt;p&gt;I was wondering if you all had the same problem and whether you agreed with this idea. Is it the same case in your experience or am I just doing it wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns5lwu,True,,samrus,,102,True,all_ads,False,[],False,,/r/datascience/comments/ns5lwu/for_most_of_the_problems_i_try_to_solve_using/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns5lwu/for_most_of_the_problems_i_try_to_solve_using/,515405,1622814677.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am new in Data Science so please excuse my limited knowledge. I am learning more about classification algorithms and how to appropriately selected algorithms for your task.

According to this website, [Classification Algorithms](https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/), the author says, ""When the classes are not linearly separable, a kernel trick can be used to map a non-linearly separable space into a higher dimension linearly separable space."" So do you just check if your class is linearly separable or the entire dataset? In any case, I could not find a proper source that would explain clearly how do you go about checking linearity of the data, with actual implementation, can you help how to achieve this?

Further, a lot of blogs and tutorials mention that when you are selecting a classification algorithm you have to consider dataset size, distribution, computation time, data type of attributes etc. but I came across a notebook which was authored by a university professor wherein he used almost all algorithms, like SVM, AdaBoost, DecisionTree, Random Forest, Bagging, Boosting etc for the same project. Does it make sense to use all of them straightaway? 

Is there like a resource that can help in making a decision, I did come across a few cheat sheets that have a flow chart but they don't seem to be in-depth.",t2_a4yvnttd,False,,0,False,Deciding a classification algorithm,[],r/datascience,False,6,discussion,0,,,False,t3_nt3g36,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622950336.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new in Data Science so please excuse my limited knowledge. I am learning more about classification algorithms and how to appropriately selected algorithms for your task.&lt;/p&gt;

&lt;p&gt;According to this website, &lt;a href=""https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/""&gt;Classification Algorithms&lt;/a&gt;, the author says, &amp;quot;When the classes are not linearly separable, a kernel trick can be used to map a non-linearly separable space into a higher dimension linearly separable space.&amp;quot; So do you just check if your class is linearly separable or the entire dataset? In any case, I could not find a proper source that would explain clearly how do you go about checking linearity of the data, with actual implementation, can you help how to achieve this?&lt;/p&gt;

&lt;p&gt;Further, a lot of blogs and tutorials mention that when you are selecting a classification algorithm you have to consider dataset size, distribution, computation time, data type of attributes etc. but I came across a notebook which was authored by a university professor wherein he used almost all algorithms, like SVM, AdaBoost, DecisionTree, Random Forest, Bagging, Boosting etc for the same project. Does it make sense to use all of them straightaway? &lt;/p&gt;

&lt;p&gt;Is there like a resource that can help in making a decision, I did come across a few cheat sheets that have a flow chart but they don&amp;#39;t seem to be in-depth.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt3g36,True,,DietMediocre8993,,5,True,all_ads,False,[],False,,/r/datascience/comments/nt3g36/deciding_a_classification_algorithm/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt3g36/deciding_a_classification_algorithm/,515405,1622921536.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone. I am currently self-learning Data Analysis and Data Science from scratch and I am really enjoying the process. I am still a begginer at the moment and still have a long way to go. I have just familliarised myself with python and the NumPy and Pandas Libraries. Just now diving deaper into data cleaning.

I was wondering if any freelancer out there would be willing to connect for the prospect of a future apprentice/assistant to help with freelance work. I could help by handling simple parts of projects at the start and escalate to harder ones as I go along, helping to speed up your work. I am not looking for any salary, just some mentorship and a reference in the future.

Anyone interested feel free to DM me so I can adress any questions.",t2_1k0s2zau,False,,0,False,Any freelancer looking for an appretice/assistant?,[],r/datascience,False,6,career,0,,,False,t3_nsnzqe,False,dark,0.77,,public,15,0,{},,,False,[],,False,False,,{},Career,False,15,,False,False,self,False,,[],{},,True,,1622897590.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone. I am currently self-learning Data Analysis and Data Science from scratch and I am really enjoying the process. I am still a begginer at the moment and still have a long way to go. I have just familliarised myself with python and the NumPy and Pandas Libraries. Just now diving deaper into data cleaning.&lt;/p&gt;

&lt;p&gt;I was wondering if any freelancer out there would be willing to connect for the prospect of a future apprentice/assistant to help with freelance work. I could help by handling simple parts of projects at the start and escalate to harder ones as I go along, helping to speed up your work. I am not looking for any salary, just some mentorship and a reference in the future.&lt;/p&gt;

&lt;p&gt;Anyone interested feel free to DM me so I can adress any questions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsnzqe,True,,Jeenghiskhan,,6,True,all_ads,False,[],False,,/r/datascience/comments/nsnzqe/any_freelancer_looking_for_an_appreticeassistant/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsnzqe/any_freelancer_looking_for_an_appreticeassistant/,515405,1622868790.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all, I'm interested in salaries in Spain for the mid-career to most senior positions (including managing/head of data science kind of positions) in data science/machine learning engineer/etc., specifically for Spain. Any information is welcome (and yes, I know they are much lower than in the US, and the post is not about that).   
I'm asking specifically about mid-career positions and beyond because I get offers for more junior positions from time to time in Linkedin (so I have that salary range covered), but for more senior positions I don't have that information first hand, and I don't trust the numbers I see on Linkedin salaries or Glassdoor. They seem outdated and, more importantly, biased to people willing to answer. Kudos if you have information specifically about Barcelona and Madrid, where the salaries seem to be highest.  


Best",t2_nqspn,False,,0,False,Salaries in Spain (yet another post like this),[],r/datascience,False,6,,0,,,False,t3_nt1s22,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,False,,[],{},,True,,1622945629.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I&amp;#39;m interested in salaries in Spain for the mid-career to most senior positions (including managing/head of data science kind of positions) in data science/machine learning engineer/etc., specifically for Spain. Any information is welcome (and yes, I know they are much lower than in the US, and the post is not about that).&lt;br/&gt;
I&amp;#39;m asking specifically about mid-career positions and beyond because I get offers for more junior positions from time to time in Linkedin (so I have that salary range covered), but for more senior positions I don&amp;#39;t have that information first hand, and I don&amp;#39;t trust the numbers I see on Linkedin salaries or Glassdoor. They seem outdated and, more importantly, biased to people willing to answer. Kudos if you have information specifically about Barcelona and Madrid, where the salaries seem to be highest.  &lt;/p&gt;

&lt;p&gt;Best&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nt1s22,True,,polidrupa,,2,True,all_ads,False,[],False,,/r/datascience/comments/nt1s22/salaries_in_spain_yet_another_post_like_this/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt1s22/salaries_in_spain_yet_another_post_like_this/,515405,1622916829.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I am working on a college project using streamlit for making a web app. Is it possible to get to this web app through some user authentication?
Moreover if possible to do so via a mobile application to take users credentials and verify it and then direct to this web app. Thank you in advance.",t2_71ommnq4,False,,0,False,Authorization in streamlit,[],r/datascience,False,6,projects,0,,,False,t3_nsr2l3,False,dark,0.99,,public,5,0,{},,,False,[],,False,False,,{},Projects,False,5,,False,False,self,False,,[],{},,True,,1622910393.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a college project using streamlit for making a web app. Is it possible to get to this web app through some user authentication?
Moreover if possible to do so via a mobile application to take users credentials and verify it and then direct to this web app. Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsr2l3,True,,Entcune,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsr2l3/authorization_in_streamlit/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsr2l3/authorization_in_streamlit/,515405,1622881593.0,3,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I have the option to study two different courses, one is to become a Data Scientist Jr and the other one is the Google Project Management course. Right now I work as a
PM but I also know how to code, so I don’t know what path to pursue now, I mean I can improve my skills as a PM but in the other hand Data Science looks a very cool and interesting thing to study. 

I have some experience coding with Python and Django, so I think my profile match, however I’m still wondering what to do... Do you guys like being data scientists? What do you hate about the role? Is there professional growth?  Do you consider that the demand for your profile is increasing?",t2_5216olhf,False,,0,False,Do you like being a data scientist?,[],r/datascience,False,6,career,0,,,False,t3_nsda9k,False,dark,0.91,,public,43,0,{},,,False,[],,False,False,,{},Career,False,43,,False,False,self,False,,[],{},,True,,1622863678.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have the option to study two different courses, one is to become a Data Scientist Jr and the other one is the Google Project Management course. Right now I work as a
PM but I also know how to code, so I don’t know what path to pursue now, I mean I can improve my skills as a PM but in the other hand Data Science looks a very cool and interesting thing to study. &lt;/p&gt;

&lt;p&gt;I have some experience coding with Python and Django, so I think my profile match, however I’m still wondering what to do... Do you guys like being data scientists? What do you hate about the role? Is there professional growth?  Do you consider that the demand for your profile is increasing?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsda9k,True,,nachoaddict19,,49,True,all_ads,False,[],False,,/r/datascience/comments/nsda9k/do_you_like_being_a_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsda9k/do_you_like_being_a_data_scientist/,515405,1622834878.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,,t2_2iu6mxmf,False,,0,False,Doing open source science for a living,[],r/datascience,False,6,discussion,0,,,False,t3_nswaiu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,default,False,,[],{},,False,,1622930226.0,text,6,,,text,self.Researcher,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nswaiu,True,,Paranoid_Bot_42,,5,False,all_ads,False,[],False,,/r/datascience/comments/nswaiu/doing_open_source_science_for_a_living/,all_ads,False,/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/,515405,1622901426.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'Researcher', 'selftext': ""Hey science people, I'd like to consult you for career advice. So I am currently a physics undergrad, who likes programming and would like to be a data scientist in scientific applications. Right now, I am also working an programming job at a big company. One would say that I am at the right path but with one thing is messing me up. \n\nI don't like corporations and the thought of being a data scientist for one kinda makes me sad.  That's  beause as a scientist my goal is to create technologies or discover scientific insights that benefit society as a whole, rather than being a means for a corporation to make profits. What really makes me not want to be in such an environment is that my work will be patented and sold rather than being available for anyone to use it as they please. Other than that, I'd rather not solve business problems that don't directly benefit the general public. I don't really give a shit about finding optimal marketing strategies through data, for example. I wants things like contributing to solving climate change through data science (atmospheric physics) or analysing the brain and developing our understanding of it or creating a smart city that actually benefits the people. \n\nMy concern is that since we live in ever more corporate world, I won't find a true open source science job, neither in industry (because of the profit motive) neither in academia (due to job vacancies being so few). \n\nSo I am asking: are my concerns valid? how do I deal with them? how do I find a job that fits my goals? do you feel like this, too?\n\nSorry for the long post"", 'author_fullname': 't2_2iu6mxmf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Doing open-source science for a living', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/Researcher', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nsvvo7', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 26, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 26, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622928938.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.Researcher', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey science people, I&amp;#39;d like to consult you for career advice. So I am currently a physics undergrad, who likes programming and would like to be a data scientist in scientific applications. Right now, I am also working an programming job at a big company. One would say that I am at the right path but with one thing is messing me up. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t like corporations and the thought of being a data scientist for one kinda makes me sad.  That&amp;#39;s  beause as a scientist my goal is to create technologies or discover scientific insights that benefit society as a whole, rather than being a means for a corporation to make profits. What really makes me not want to be in such an environment is that my work will be patented and sold rather than being available for anyone to use it as they please. Other than that, I&amp;#39;d rather not solve business problems that don&amp;#39;t directly benefit the general public. I don&amp;#39;t really give a shit about finding optimal marketing strategies through data, for example. I wants things like contributing to solving climate change through data science (atmospheric physics) or analysing the brain and developing our understanding of it or creating a smart city that actually benefits the people. &lt;/p&gt;\n\n&lt;p&gt;My concern is that since we live in ever more corporate world, I won&amp;#39;t find a true open source science job, neither in industry (because of the profit motive) neither in academia (due to job vacancies being so few). &lt;/p&gt;\n\n&lt;p&gt;So I am asking: are my concerns valid? how do I deal with them? how do I find a job that fits my goals? do you feel like this, too?&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2rg08', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nsvvo7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Paranoid_Bot_42', 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/', 'subreddit_subscribers': 739, 'created_utc': 1622900138.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/,t3_nsvvo7,
,datascience,"We can all agree that most of the job is boring data stuff, collecting, cleaning, designing, transforming and the list keeps going. This takes like 75% to 90% of our time. The rest of the time is powerpoint, excel, meetings, building models, ML and ML-dev ops.

So my question is why so many positions require Master or higher education? do you guys have this kind of education?

Do you guys do actual complex stuff? What do you do that you need to have a PhD to do it?

I got 3 years of experience and i've never needed to know anything that couldnt be learned in a basic blog post of some random dude.

I guess some positions do need very complex knowledge of NPL or DL but the rest?? no way",t2_bftt58l,False,,0,False,So what's the point of having a Masters or a PhD in this field?,[],r/datascience,False,6,discussion,0,,,False,t3_nshjld,False,dark,0.68,,public,17,0,{},,,False,[],,False,False,,{},Discussion,False,17,,False,False,self,False,,[],{},,True,,1622875383.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We can all agree that most of the job is boring data stuff, collecting, cleaning, designing, transforming and the list keeps going. This takes like 75% to 90% of our time. The rest of the time is powerpoint, excel, meetings, building models, ML and ML-dev ops.&lt;/p&gt;

&lt;p&gt;So my question is why so many positions require Master or higher education? do you guys have this kind of education?&lt;/p&gt;

&lt;p&gt;Do you guys do actual complex stuff? What do you do that you need to have a PhD to do it?&lt;/p&gt;

&lt;p&gt;I got 3 years of experience and i&amp;#39;ve never needed to know anything that couldnt be learned in a basic blog post of some random dude.&lt;/p&gt;

&lt;p&gt;I guess some positions do need very complex knowledge of NPL or DL but the rest?? no way&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nshjld,True,,felipecalderon1,,53,True,all_ads,False,[],False,,/r/datascience/comments/nshjld/so_whats_the_point_of_having_a_masters_or_a_phd/,all_ads,False,https://www.reddit.com/r/datascience/comments/nshjld/so_whats_the_point_of_having_a_masters_or_a_phd/,515405,1622846583.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am working on a dataset that has both categorical and numerical (continuous and discrete) features (26 columns, 30244 rows). Target is categorical (1, 2, 3) and I am performing EDA on this dataset.

My dataset is regarding hotel reservation status (Not cancelled(1), Cancelled(2), No show(3)) of customers in the span of 3 years (2015, 2016, 2017). Given data of the customer, my task is to predict if the customer will either cancel, not cancel or no-show for his reservation.

* The categorical features with numerical values (ex: gender has values 0 and 1) are also considered when taking the heatmap with seaborn. As per my knowledge, the heatmap is drawn to check the correlation between continuous numerical features right (correct me if I am wrong). Should I remove such features before taking the heatmap?
* The book-in date, expected check-in date, expected check-out date are given in the dataset. I extracted month and year for each feature separately. These month columns are also categorical right? (As they only have values between 1-12). I took screenshots of month distribution plots and uploaded them here.

[https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing](https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing)

* Should I do a test like the Chi-Square test on those features?",t2_4q1w55i3,False,,0,False,"(I am self learning data science. I asked this question on every platform I can think of, but still didn't get an answer. Please help me out if you know the answer) Should I remove features such as gender and birth month before drawing the heatmap because they are categorical?",[],r/datascience,False,6,education,0,,,False,t3_nsojd5,False,dark,0.57,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,1622913126.0,,[],{},,True,,1622899665.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a dataset that has both categorical and numerical (continuous and discrete) features (26 columns, 30244 rows). Target is categorical (1, 2, 3) and I am performing EDA on this dataset.&lt;/p&gt;

&lt;p&gt;My dataset is regarding hotel reservation status (Not cancelled(1), Cancelled(2), No show(3)) of customers in the span of 3 years (2015, 2016, 2017). Given data of the customer, my task is to predict if the customer will either cancel, not cancel or no-show for his reservation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The categorical features with numerical values (ex: gender has values 0 and 1) are also considered when taking the heatmap with seaborn. As per my knowledge, the heatmap is drawn to check the correlation between continuous numerical features right (correct me if I am wrong). Should I remove such features before taking the heatmap?&lt;/li&gt;
&lt;li&gt;The book-in date, expected check-in date, expected check-out date are given in the dataset. I extracted month and year for each feature separately. These month columns are also categorical right? (As they only have values between 1-12). I took screenshots of month distribution plots and uploaded them here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing""&gt;https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Should I do a test like the Chi-Square test on those features?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsojd5,True,,hirushi_wijesinghe,,12,True,all_ads,False,[],False,,/r/datascience/comments/nsojd5/i_am_self_learning_data_science_i_asked_this/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsojd5/i_am_self_learning_data_science_i_asked_this/,515405,1622870865.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YeJZEzkcnDwlNU58RXraG9gbKZcGZvvTbcjCIwwXCYY.jpg?auto=webp&amp;s=3869a1565f12b3477e8cff17ec64e94a6fb115e9', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/YeJZEzkcnDwlNU58RXraG9gbKZcGZvvTbcjCIwwXCYY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4d3a80353a2c205ca5b617d6fd3cf7c31b4d872', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'duj8xNL5l81WpIkn0ToMaJAlR5YyBTeJcZYytO0KWDs'}], 'enabled': False}",,,,,
,datascience,"From your experience, is it worth tailoring resumes to job openings? Or is it a better idea to have a one-size-fits-all resume and cast a wide net?",t2_m4ekm0b,False,,0,False,On the quality vs quantity of job applications,[],r/datascience,False,6,career,0,,,False,t3_nseegw,False,dark,0.88,,public,12,0,{},,,False,[],,False,False,,{},Career,False,12,,False,False,self,False,,[],{},,True,,1622866559.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From your experience, is it worth tailoring resumes to job openings? Or is it a better idea to have a one-size-fits-all resume and cast a wide net?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nseegw,True,,fool126,,21,True,all_ads,False,[],False,,/r/datascience/comments/nseegw/on_the_quality_vs_quantity_of_job_applications/,all_ads,False,https://www.reddit.com/r/datascience/comments/nseegw/on_the_quality_vs_quantity_of_job_applications/,515405,1622837759.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I got a data science internship and I've just started, I've been in the role for about two weeks. It's a research and development role for the company's product. I'm honestly really enjoying it. The role is capturing meta data from data science tech stacks like Google, AWS, Microsoft etc. We get to build projects with the technology so there's free reign to do whatever I want to which is a lot of fun. I get to use a lot of the new or popular tools data scientists use but I was wondering if it's detrimental that I'm not actually creating models for production. 

I'll hopefully be very proficient with popular cloud services offerings but it's more of knowing how to create pipelines, setup services and a lot of the auxiliary things surrounding models. Like I previously said I do honestly enjoy the role. It wasn't what I was expecting but I get to learn a lot at a manageable pace which is nice.",t2_68lcg7iq,False,,0,False,Will My Internship Influence My Career Trajectory?,[],r/datascience,False,6,discussion,0,,,False,t3_nsq3k6,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622906148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got a data science internship and I&amp;#39;ve just started, I&amp;#39;ve been in the role for about two weeks. It&amp;#39;s a research and development role for the company&amp;#39;s product. I&amp;#39;m honestly really enjoying it. The role is capturing meta data from data science tech stacks like Google, AWS, Microsoft etc. We get to build projects with the technology so there&amp;#39;s free reign to do whatever I want to which is a lot of fun. I get to use a lot of the new or popular tools data scientists use but I was wondering if it&amp;#39;s detrimental that I&amp;#39;m not actually creating models for production. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ll hopefully be very proficient with popular cloud services offerings but it&amp;#39;s more of knowing how to create pipelines, setup services and a lot of the auxiliary things surrounding models. Like I previously said I do honestly enjoy the role. It wasn&amp;#39;t what I was expecting but I get to learn a lot at a manageable pace which is nice.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsq3k6,True,,PowerTurtz,,8,True,all_ads,False,[],False,,/r/datascience/comments/nsq3k6/will_my_internship_influence_my_career_trajectory/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsq3k6/will_my_internship_influence_my_career_trajectory/,515405,1622877348.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"**The Approximately Correct Machine Intelligence (ACMI) Lab at Carnegie Mellon University** (CMU) has published a paper on **Randomly Assign, Train, and Track (RATT)**. RATT is an algorithm that uses noisy training data to put an upper bound on a deep-learning model’s actual error risk. Model developers can use **RATT to see how well a model generalizes to new input data**.

The researchers demonstrate mathematical proofs of RATT’s guarantees and conduct experiments on various datasets for computer vision (CV) and natural language processing (NLP) models in their publication. **When a trained model gets a high error rate on randomly labeled (or noisy) data but a low error rate on clean data, the model is assumed to have a low error rate on new data.**

Full Summary: [https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/](https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/) 

Paper: https://arxiv.org/pdf/2105.00303.pdf",t2_4wudjgid,False,,0,False,"CMU Researchers Propose RATT (Randomly Assign, Train and Track), A Method for Guaranteeing AI Model Generalization",[],r/datascience,False,6,discussion,0,,,False,t3_nsplyp,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622904016.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;The Approximately Correct Machine Intelligence (ACMI) Lab at Carnegie Mellon University&lt;/strong&gt; (CMU) has published a paper on &lt;strong&gt;Randomly Assign, Train, and Track (RATT)&lt;/strong&gt;. RATT is an algorithm that uses noisy training data to put an upper bound on a deep-learning model’s actual error risk. Model developers can use &lt;strong&gt;RATT to see how well a model generalizes to new input data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The researchers demonstrate mathematical proofs of RATT’s guarantees and conduct experiments on various datasets for computer vision (CV) and natural language processing (NLP) models in their publication. &lt;strong&gt;When a trained model gets a high error rate on randomly labeled (or noisy) data but a low error rate on clean data, the model is assumed to have a low error rate on new data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/""&gt;https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2105.00303.pdf""&gt;https://arxiv.org/pdf/2105.00303.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsplyp,True,,techsucker,,0,True,all_ads,False,[],False,,/r/datascience/comments/nsplyp/cmu_researchers_propose_ratt_randomly_assign/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsplyp/cmu_researchers_propose_ratt_randomly_assign/,515405,1622875216.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?auto=webp&amp;s=cca984f6c568e770cf55df1e6e1ace3fe886eb7a', 'width': 1374, 'height': 742}, 'resolutions': [{'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=84a8950ffe2808e8eea767fbf4ea02aa28d7b79e', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6afde8a34d3731c356a0c8e72585a3ad9d2bc3e2', 'width': 216, 'height': 116}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c2656c02948cf52d53dd73def5dd4b3f239bae', 'width': 320, 'height': 172}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a95044a19444ac21402b2e5e6313747341d7ec3', 'width': 640, 'height': 345}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8792baa88f2f45679005e6e80e180de5bc0b489', 'width': 960, 'height': 518}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f332720922825789e85c18411a698c87584cc41c', 'width': 1080, 'height': 583}], 'variants': {}, 'id': 'QjAMdGXZr6yeEFP_BInpH50tMe0fN8LXPut1apFYauA'}], 'enabled': False}",,,,,
,datascience,"I have a dataset for a bank where the objective is to predict whether a loan will end up as good where it's completely paid off, or a bad loan. The dataset has the target variable consisting of three labels: open (the loan is still ongoing), good loan and bad loan. The dataset is heavily skewed/imbalanced where majority of the rows/data points behind to good loan and the bad loans are a minority.

What I have done is filter the dataset for good and bad loans and trained different ML models on it which gives too good to be true performance. This makes it a binary classification. Accuracy, precision and recall are all 1.00 or 100% for the validation sets.

Then I want to use it to make predictions for the ""open"" rows/data points in the CSV file. But, the problem is that since these loans are still ongoing, we don't have the ground truth for them and therefore whatever predictions I get cannot be compared against the ground truth to get model metrics such as accuracy, precision, recall, etc.


Suggestions/help?

Thanks",t2_2mmql89p,False,,0,False,Unseen data prediction,[],r/datascience,False,6,discussion,0,,,False,t3_nspls7,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,1622946507.0,,[],{},,True,,1622903996.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset for a bank where the objective is to predict whether a loan will end up as good where it&amp;#39;s completely paid off, or a bad loan. The dataset has the target variable consisting of three labels: open (the loan is still ongoing), good loan and bad loan. The dataset is heavily skewed/imbalanced where majority of the rows/data points behind to good loan and the bad loans are a minority.&lt;/p&gt;

&lt;p&gt;What I have done is filter the dataset for good and bad loans and trained different ML models on it which gives too good to be true performance. This makes it a binary classification. Accuracy, precision and recall are all 1.00 or 100% for the validation sets.&lt;/p&gt;

&lt;p&gt;Then I want to use it to make predictions for the &amp;quot;open&amp;quot; rows/data points in the CSV file. But, the problem is that since these loans are still ongoing, we don&amp;#39;t have the ground truth for them and therefore whatever predictions I get cannot be compared against the ground truth to get model metrics such as accuracy, precision, recall, etc.&lt;/p&gt;

&lt;p&gt;Suggestions/help?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nspls7,True,,grid_world,,21,True,all_ads,False,[],False,,/r/datascience/comments/nspls7/unseen_data_prediction/,all_ads,False,https://www.reddit.com/r/datascience/comments/nspls7/unseen_data_prediction/,515405,1622875196.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey Forum. I wanted to get some help and potentially collaborate/hire someone to help me find / come up with a solution to the below problem and the correct methodology on how to go about it. Here is the background:

&amp;#x200B;

The use case is algorithmic trading. To make a simple example, I use indicators to trade forex/stocks / etc. I perform backtests on data (over 10 to 15 years) and my goal is to determine the best performing indicator input parameter combination. I do my backtesting using a process called ""walk forward analysis/walk forward optimization"" [https://en.wikipedia.org/wiki/Walk\_forward\_optimization](https://en.wikipedia.org/wiki/Walk_forward_optimization) . The goal is to determine how robust the trading algo is (the system) when it runs on ""out of sample data"". The goal is to select the best performing parameters (indicator parameters like stochastic, if that's what you are using), and carry that forward and use those input parameters on your out-of-sample data. so here is what a in sample results look like 

&amp;#x200B;

param1	param1	    CAGR/AvgDD

27	         140		10.661

27	        160		       10.236

29	         145	      9.633

31		 150	      12.927

33	        155	               3.952

35	       140	              3.214

37	        145	              5.977

&amp;#x200B;

CAGR/AVgDD is my performance metric. It can be anything really, Profit, or profit factor, etc. Basically, parameters 1, 2, X are inputs to the system. During an in-sample optimization run, for each input parameter that I want to optimize, i pick a start, end and a step. So if there are 3 parameters and each has a start value of 1, the end value of 4, and a step of 1, then you have 4\*4\*4 combinations/passes and each pass will generate a performance metric value. My in-sample runs have a statistically significant amount of data. For example, a run over 4 years (in-sample period) will have at least 5000 to 10000 trades) for each pass/parameter combination. 

&amp;#x200B;

So here is the problem that I want to solve. How do I know which single ""pass"" / ""parameter combination"" to select to run on my out-of-sample data set? How do I choose the best-performing input parameter combination? I know it won't always be the one with the highest performance metric value. For example, my highest value can be 10, and the one right below it can be 4.3. Clearly here 10 is an outlier. 

what algo or method should be used? 

some people have said KNN. Is that true? 

can this be done with more than just 2 parameter inputs? what if I am optimizing 3 or 4? is there a downside to optimizing 3, 4, 5, etc different parameters

&amp;#x200B;

I'm also looking for someone to help automate this for me. Looking for someone in math/data science background to help me with this.",t2_zqq2s,False,,0,False,choosing the best parameters in an optimization,[],r/datascience,False,6,discussion,0,,,False,t3_nspktt,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622903880.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Forum. I wanted to get some help and potentially collaborate/hire someone to help me find / come up with a solution to the below problem and the correct methodology on how to go about it. Here is the background:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The use case is algorithmic trading. To make a simple example, I use indicators to trade forex/stocks / etc. I perform backtests on data (over 10 to 15 years) and my goal is to determine the best performing indicator input parameter combination. I do my backtesting using a process called &amp;quot;walk forward analysis/walk forward optimization&amp;quot; &lt;a href=""https://en.wikipedia.org/wiki/Walk_forward_optimization""&gt;https://en.wikipedia.org/wiki/Walk_forward_optimization&lt;/a&gt; . The goal is to determine how robust the trading algo is (the system) when it runs on &amp;quot;out of sample data&amp;quot;. The goal is to select the best performing parameters (indicator parameters like stochastic, if that&amp;#39;s what you are using), and carry that forward and use those input parameters on your out-of-sample data. so here is what a in sample results look like &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;param1  param1      CAGR/AvgDD&lt;/p&gt;

&lt;p&gt;27           140        10.661&lt;/p&gt;

&lt;p&gt;27          160            10.236&lt;/p&gt;

&lt;p&gt;29           145          9.633&lt;/p&gt;

&lt;p&gt;31       150          12.927&lt;/p&gt;

&lt;p&gt;33          155                3.952&lt;/p&gt;

&lt;p&gt;35         140                3.214&lt;/p&gt;

&lt;p&gt;37          145               5.977&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;CAGR/AVgDD is my performance metric. It can be anything really, Profit, or profit factor, etc. Basically, parameters 1, 2, X are inputs to the system. During an in-sample optimization run, for each input parameter that I want to optimize, i pick a start, end and a step. So if there are 3 parameters and each has a start value of 1, the end value of 4, and a step of 1, then you have 4*4*4 combinations/passes and each pass will generate a performance metric value. My in-sample runs have a statistically significant amount of data. For example, a run over 4 years (in-sample period) will have at least 5000 to 10000 trades) for each pass/parameter combination. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So here is the problem that I want to solve. How do I know which single &amp;quot;pass&amp;quot; / &amp;quot;parameter combination&amp;quot; to select to run on my out-of-sample data set? How do I choose the best-performing input parameter combination? I know it won&amp;#39;t always be the one with the highest performance metric value. For example, my highest value can be 10, and the one right below it can be 4.3. Clearly here 10 is an outlier. &lt;/p&gt;

&lt;p&gt;what algo or method should be used? &lt;/p&gt;

&lt;p&gt;some people have said KNN. Is that true? &lt;/p&gt;

&lt;p&gt;can this be done with more than just 2 parameter inputs? what if I am optimizing 3 or 4? is there a downside to optimizing 3, 4, 5, etc different parameters&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m also looking for someone to help automate this for me. Looking for someone in math/data science background to help me with this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nspktt,True,,daproject85,,1,True,all_ads,False,[],False,,/r/datascience/comments/nspktt/choosing_the_best_parameters_in_an_optimization/,all_ads,False,https://www.reddit.com/r/datascience/comments/nspktt/choosing_the_best_parameters_in_an_optimization/,515405,1622875080.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/apkmo5zs_-YmFFDjmG62TyIS5jvdW18a790XKORnEu8.jpg?auto=webp&amp;s=d430e4efd42baf58b4bc71c9549944b7bdf40715', 'width': 50, 'height': 39}, 'resolutions': [], 'variants': {}, 'id': 'K8Lxcv5aHkjcbujO-2N7cVXtdXkwYe8BHCOouTCCjb8'}], 'enabled': False}",,,,,
,datascience,"Hello!

I am about to finish my first year of a data job after grad school. In the past year, I've found myself not being able to upskill much outside of new things I learnt at work. Reasons or excuses could be wanting time for my hobbies, not wanting to study on the weekend.

I know I need to put some hours every week to learn new stuff outside of my job but I am kind of struggling.

I was wondering how do you guys go about your upskilling and what are some tips would you like to give to someone like me.

Thank you!

P.S. I can't say my skill set is the same as it was a year ago but not doing much outside of work makes me a bit insecure.",t2_bv171ji2,False,,0,False,"In the early stage of your career, how many hours per week should be dedicated to upskilling?",[],r/datascience,False,6,discussion,0,,,False,t3_nsbf1f,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1622858694.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am about to finish my first year of a data job after grad school. In the past year, I&amp;#39;ve found myself not being able to upskill much outside of new things I learnt at work. Reasons or excuses could be wanting time for my hobbies, not wanting to study on the weekend.&lt;/p&gt;

&lt;p&gt;I know I need to put some hours every week to learn new stuff outside of my job but I am kind of struggling.&lt;/p&gt;

&lt;p&gt;I was wondering how do you guys go about your upskilling and what are some tips would you like to give to someone like me.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;P.S. I can&amp;#39;t say my skill set is the same as it was a year ago but not doing much outside of work makes me a bit insecure.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsbf1f,True,,quite--average,,11,True,all_ads,False,[],False,,/r/datascience/comments/nsbf1f/in_the_early_stage_of_your_career_how_many_hours/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsbf1f/in_the_early_stage_of_your_career_how_many_hours/,515405,1622829894.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"First DS job, working remotely, started about 3 months ago. It's a very small company in an industry that I was unfamiliar with, so there was a distinct learning curve in getting acquainted. There was also no onboarding, as is to be expected for very small companies.

I'm not sure they needed a data scientist and now I'm kind of scrambling to try and show value. Along with that, the part of the business that I'm working on does not generate any revenue, and has very little data (about 3000 sparse data points, maybe 1200 good ones, very few updates - maybe 5 per week, and very few insights that I am being asked to run on these actual data points).

With that, I'm being involved in the business development side of things (higher-ups do not know how this current 6-year old project should generate revenue), and every week my task changes. Usually it's about finding open-source datasets, but my boss has very little focus/patience, so each week is different. I struggle to maintain focus in my day-to-day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame (i.e. investigate a causal question that would be great for an entire econometric paper), and will likely not generate revenue anytime soon.

Are there any other DS folk who have been hired into small places with very little data, being the only DS? How did you handle the situation? How long does it take you to do open-source data collection? I don't mind the field, the work, nor wearing many hats, but I'm worried that I won't be generating enough value to justify staying on the team.",t2_5fbf7t,False,,0,False,Hired at a Small Company. My Job is... Shaky.,[],r/datascience,False,6,career,0,,,False,t3_nrp373,False,dark,0.97,,public,209,1,{},,,False,[],,False,False,,{},Career,False,209,,False,False,self,False,,[],{},,True,,1622786086.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;First DS job, working remotely, started about 3 months ago. It&amp;#39;s a very small company in an industry that I was unfamiliar with, so there was a distinct learning curve in getting acquainted. There was also no onboarding, as is to be expected for very small companies.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not sure they needed a data scientist and now I&amp;#39;m kind of scrambling to try and show value. Along with that, the part of the business that I&amp;#39;m working on does not generate any revenue, and has very little data (about 3000 sparse data points, maybe 1200 good ones, very few updates - maybe 5 per week, and very few insights that I am being asked to run on these actual data points).&lt;/p&gt;

&lt;p&gt;With that, I&amp;#39;m being involved in the business development side of things (higher-ups do not know how this current 6-year old project should generate revenue), and every week my task changes. Usually it&amp;#39;s about finding open-source datasets, but my boss has very little focus/patience, so each week is different. I struggle to maintain focus in my day-to-day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame (i.e. investigate a causal question that would be great for an entire econometric paper), and will likely not generate revenue anytime soon.&lt;/p&gt;

&lt;p&gt;Are there any other DS folk who have been hired into small places with very little data, being the only DS? How did you handle the situation? How long does it take you to do open-source data collection? I don&amp;#39;t mind the field, the work, nor wearing many hats, but I&amp;#39;m worried that I won&amp;#39;t be generating enough value to justify staying on the team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrp373,True,,DegenerateWaves,,56,False,all_ads,False,[],False,,/r/datascience/comments/nrp373/hired_at_a_small_company_my_job_is_shaky/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrp373/hired_at_a_small_company_my_job_is_shaky/,515405,1622757286.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm working on a dating app and was crunching some numbers and would love some feedback to see if there's any holes in my assumptions and calculations.

Thank you!

[https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing)",t2_xu50j,False,,0,False,Looks like the expected number of dating app matches one can get in Iceland is 187?,[],r/datascience,False,6,discussion,0,,,False,t3_nsiwd1,False,dark,0.56,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622879695.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a dating app and was crunching some numbers and would love some feedback to see if there&amp;#39;s any holes in my assumptions and calculations.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing""&gt;https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsiwd1,True,,broccolie,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsiwd1/looks_like_the_expected_number_of_dating_app/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsiwd1/looks_like_the_expected_number_of_dating_app/,515405,1622850895.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?auto=webp&amp;s=e1dc0ea8fe78cdb885562e5e0204bf72513bb78a', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3daacadd74ede240d61102a9ce86a8c8cfd4664', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5bb8e6ca5279aa5247f906cb60428a99f418f8d9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f74b085e7350ebfd6349e49e958678fecea3f12b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72fd61b4498b2b84a2faf01070a71abf0c4e5545', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c5fc9c42d0159a50dae732fada8b243db802e5', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8801cb5e5f1ac15c206ae0b7850bac557aac2c81', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'cYhDIwEt8ylpsOspk4tAJprS3M-jj1txWKnXf_kG7RM'}], 'enabled': False}",,,,,
,datascience,"Hi, I am doing my masters in Data Science in Berlin, but my background is from automotive and have work experience as PMO for 2.9 years in Digital Transformation. Now to switch my career to Data Science, I took masters in Berlin. Now I have done 2 Company projects with the help of University and it was a part of University curriculum. Can I add this as experience in my resume or I need to add this only as a side project?

Will it be misleading the recruiter or the company I am giving Interviews. Kindly let me know.",t2_97m7yfi9,False,,0,False,Can I add University sponsored company Projects as experience in my resume?,[],r/datascience,False,6,,0,,,False,t3_nsi4ng,False,dark,0.62,,public,2,1,{},,,False,[],,False,False,,{},Job Search,False,2,,False,False,self,False,,[],{},,True,,1622877178.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am doing my masters in Data Science in Berlin, but my background is from automotive and have work experience as PMO for 2.9 years in Digital Transformation. Now to switch my career to Data Science, I took masters in Berlin. Now I have done 2 Company projects with the help of University and it was a part of University curriculum. Can I add this as experience in my resume or I need to add this only as a side project?&lt;/p&gt;

&lt;p&gt;Will it be misleading the recruiter or the company I am giving Interviews. Kindly let me know.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nsi4ng,True,,Vinothd19,,7,True,all_ads,False,[],False,,/r/datascience/comments/nsi4ng/can_i_add_university_sponsored_company_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsi4ng/can_i_add_university_sponsored_company_projects/,515405,1622848378.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience," I'm currently in a role where I've solely been working on NLP for the entire time. Recently I was offered an opportunity at a new company where I would be working with satellite and time series data with the goal of doing combining time series and CV. The thing is I have zero experience working with both time series and image data, the company are aware of this though, and they are fine with it.

My main reason for posting this is to try and gather some training resources so I don't go in with zero knowledge. Looking for any interesting projects/courses involving those types of data that would be beginner friendly and go through the processes that are generally required for this work. Appreciate any help you can provide",t2_18opmol6,False,,0,False,Transitioning from NLP to satellite and image based CV,[],r/datascience,False,6,career,0,,,False,t3_ns4sz8,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Career,False,7,,False,False,self,False,,[],{},,True,,1622841148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently in a role where I&amp;#39;ve solely been working on NLP for the entire time. Recently I was offered an opportunity at a new company where I would be working with satellite and time series data with the goal of doing combining time series and CV. The thing is I have zero experience working with both time series and image data, the company are aware of this though, and they are fine with it.&lt;/p&gt;

&lt;p&gt;My main reason for posting this is to try and gather some training resources so I don&amp;#39;t go in with zero knowledge. Looking for any interesting projects/courses involving those types of data that would be beginner friendly and go through the processes that are generally required for this work. Appreciate any help you can provide&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns4sz8,True,,sjh3192,,3,True,all_ads,False,[],False,,/r/datascience/comments/ns4sz8/transitioning_from_nlp_to_satellite_and_image/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns4sz8/transitioning_from_nlp_to_satellite_and_image/,515405,1622812348.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am new in Data Science so excuse my limited knowledge. I am studying an overview of the different stages that a DS project goes through.

Currently, I am learning more about outliers detection and removal. So, I have not found a resource that talks about real world application of outlier removal. Say you are working on a small project that has a data set that depends highly on other external factors. Let's assume the dataset that I will work on is economic growth in a particular state. In this case, economic growth depends on hundred different factors, growth in business, population, market, jobs etc. 

Given that in your project you are just limited to the economic data, my question is should you perform an outlier removal (get a boxplot of different attributes in the dataset and and remove all the values that fall out of the 1.5 times interquartile range) or let go of the removal so as to assume the closest real world possibility?

In practice, what would be the ideal solution to work in this scenario?",t2_a4yvnttd,False,,0,False,Should removing outlier be based on the specific data?,[],r/datascience,False,6,discussion,0,,,False,t3_ns7l7m,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622848705.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new in Data Science so excuse my limited knowledge. I am studying an overview of the different stages that a DS project goes through.&lt;/p&gt;

&lt;p&gt;Currently, I am learning more about outliers detection and removal. So, I have not found a resource that talks about real world application of outlier removal. Say you are working on a small project that has a data set that depends highly on other external factors. Let&amp;#39;s assume the dataset that I will work on is economic growth in a particular state. In this case, economic growth depends on hundred different factors, growth in business, population, market, jobs etc. &lt;/p&gt;

&lt;p&gt;Given that in your project you are just limited to the economic data, my question is should you perform an outlier removal (get a boxplot of different attributes in the dataset and and remove all the values that fall out of the 1.5 times interquartile range) or let go of the removal so as to assume the closest real world possibility?&lt;/p&gt;

&lt;p&gt;In practice, what would be the ideal solution to work in this scenario?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns7l7m,True,,DietMediocre8993,,9,True,all_ads,False,[],False,,/r/datascience/comments/ns7l7m/should_removing_outlier_be_based_on_the_specific/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns7l7m/should_removing_outlier_be_based_on_the_specific/,515405,1622819905.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, not sure if this is the right place for this, but just stressing cos I'm getting to the end of the sprint and I've got no results to show for my work. 

&amp;#x200B;

So basically, I started at my first DS job just over a month ago, and I'm currently working on developing a small object detection model to pick out various symbols in these complex noisy diagrams (crowded with lots of superfluous lines and text). 

&amp;#x200B;

The data set we have is pretty small, and I've had to label it myself. I've chosen to break the symbols into around 8 classes that are all pretty distinct from each other, and I've picked out \~600 data points of all symbols that we need, and \~400 cases of the general background cases. 

&amp;#x200B;

I've trained a CNN model to classify each case, and the confusion matrix looks pretty good, but when I run the model on the sliding windows it just performs terribly. 

&amp;#x200B;

Basically, it's OK at picking out these things in the small image sections that I've picked out but is terrible at picking them out in the random windows generated by the sliding windows technique that I'm applying. 

&amp;#x200B;

This is basically my first time building a NN model, and my first time labelling my own data. I think I have all of the implementations working fine, but I just don't know how to boost the performance.

&amp;#x200B;

I feel like this is mostly just a lack of experience on my part, but it just feels like I've hit a wall in terms of what I can do with this model... I just don't want to have to say to my team that the project I've been working on all sprint is a no-go... I'd also mean that I'd have to come up with an alternative strategy to identify these things cos we kind of need this in the long run...

&amp;#x200B;

Sorry for the rant.. Any advice or resources would be really appreciated!!

&amp;#x200B;

Thanks!",t2_711grept,False,,0,False,ML advice needed!,[],r/datascience,False,6,projects,0,,,False,t3_nrzvrn,False,dark,0.92,,public,11,0,{},,,False,[],,False,False,,{},Projects,False,11,,False,False,self,False,,[],{},,True,,1622823264.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, not sure if this is the right place for this, but just stressing cos I&amp;#39;m getting to the end of the sprint and I&amp;#39;ve got no results to show for my work. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So basically, I started at my first DS job just over a month ago, and I&amp;#39;m currently working on developing a small object detection model to pick out various symbols in these complex noisy diagrams (crowded with lots of superfluous lines and text). &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The data set we have is pretty small, and I&amp;#39;ve had to label it myself. I&amp;#39;ve chosen to break the symbols into around 8 classes that are all pretty distinct from each other, and I&amp;#39;ve picked out ~600 data points of all symbols that we need, and ~400 cases of the general background cases. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve trained a CNN model to classify each case, and the confusion matrix looks pretty good, but when I run the model on the sliding windows it just performs terribly. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Basically, it&amp;#39;s OK at picking out these things in the small image sections that I&amp;#39;ve picked out but is terrible at picking them out in the random windows generated by the sliding windows technique that I&amp;#39;m applying. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is basically my first time building a NN model, and my first time labelling my own data. I think I have all of the implementations working fine, but I just don&amp;#39;t know how to boost the performance.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I feel like this is mostly just a lack of experience on my part, but it just feels like I&amp;#39;ve hit a wall in terms of what I can do with this model... I just don&amp;#39;t want to have to say to my team that the project I&amp;#39;ve been working on all sprint is a no-go... I&amp;#39;d also mean that I&amp;#39;d have to come up with an alternative strategy to identify these things cos we kind of need this in the long run...&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Sorry for the rant.. Any advice or resources would be really appreciated!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrzvrn,True,,the_constant_reddit,,14,True,all_ads,False,[],False,,/r/datascience/comments/nrzvrn/ml_advice_needed/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrzvrn/ml_advice_needed/,515405,1622794464.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"This week I've had an interview with a company that manufactures wooden floors. They want to hire a data analyst / software engineer to digitilize  their manufacturing processses within their factory.

They told md which improvements they had in mine and some of them seemed completely okay, like installing sensors around the factory and gather data from those sensors yo atart building some big data bank.

The second is that they want to build applications for internal consumption for other workers to be able to better visualize what's going on in the factory.

However, their main focus was some other thing, they wanted to do was to build from zero a computer vision app in which you could take a picture of raw material and it wpuld predict its prone to failure or not.

The problem is that they don't hsve any data for that and I dont think they are well aware of the effort it would take, not to make the app itself (if possible) but to build a dataset large enough to be able to train the model.

The contract they are offering is just 6 months so I assume they need to see results within that time-frame.

I don't their degree of expertise in the topic, but to mind it seemed that they are not being realistic about this and the contractual relation wpuld end up rather bad.

What do you guys think?",t2_1fmeqg5h,False,,0,False,"Made an interview for a company, their project does not seem doable",[],r/datascience,False,6,career,0,,,False,t3_nrwt90,False,dark,0.91,,public,17,0,{},,,False,[],,False,False,,{},Career,False,17,,False,False,self,False,,[],{},,True,,1622810806.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This week I&amp;#39;ve had an interview with a company that manufactures wooden floors. They want to hire a data analyst / software engineer to digitilize  their manufacturing processses within their factory.&lt;/p&gt;

&lt;p&gt;They told md which improvements they had in mine and some of them seemed completely okay, like installing sensors around the factory and gather data from those sensors yo atart building some big data bank.&lt;/p&gt;

&lt;p&gt;The second is that they want to build applications for internal consumption for other workers to be able to better visualize what&amp;#39;s going on in the factory.&lt;/p&gt;

&lt;p&gt;However, their main focus was some other thing, they wanted to do was to build from zero a computer vision app in which you could take a picture of raw material and it wpuld predict its prone to failure or not.&lt;/p&gt;

&lt;p&gt;The problem is that they don&amp;#39;t hsve any data for that and I dont think they are well aware of the effort it would take, not to make the app itself (if possible) but to build a dataset large enough to be able to train the model.&lt;/p&gt;

&lt;p&gt;The contract they are offering is just 6 months so I assume they need to see results within that time-frame.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t their degree of expertise in the topic, but to mind it seemed that they are not being realistic about this and the contractual relation wpuld end up rather bad.&lt;/p&gt;

&lt;p&gt;What do you guys think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrwt90,True,,marmaduque_is_back,,18,True,all_ads,False,[],False,,/r/datascience/comments/nrwt90/made_an_interview_for_a_company_their_project/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrwt90/made_an_interview_for_a_company_their_project/,515405,1622782006.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm currently doing a SQL query (SELECT * FROM ... WHERE ...) using spark.sql() and then writing the results into a table using df.saveAsTable. However, this take upwards of an hour or more to run. The table I am querying from has ~1 billion rows of data and it is in Hive. Is this to be expected? Or is there something inherently wrong with my configurations of Spark that lead to the slow runtime?",t2_ql2r5,False,,0,False,Is upward of 1 hour in runtime a normal occurence when querying data from large tables using PySpark?,[],r/datascience,False,6,tooling,0,,,False,t3_nsd4m7,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1622863273.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently doing a SQL query (SELECT * FROM ... WHERE ...) using spark.sql() and then writing the results into a table using df.saveAsTable. However, this take upwards of an hour or more to run. The table I am querying from has ~1 billion rows of data and it is in Hive. Is this to be expected? Or is there something inherently wrong with my configurations of Spark that lead to the slow runtime?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsd4m7,True,,SaxxyBeast298,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsd4m7/is_upward_of_1_hour_in_runtime_a_normal_occurence/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsd4m7/is_upward_of_1_hour_in_runtime_a_normal_occurence/,515405,1622834473.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"My apologies if this isn't the proper group to ask the question - but any assistance would be greatly appreciated. 

My company does weekly surveys to our membership base - we have approximately 4 years worth of data/results that we were looking for someone to assist in interpreting the data and possibly tying it all together for a comprehensive report sort of speak. I am struggling to find a company that offers this service or even what this service would be called. 

Anyone here know of a company or consultant that could help?",t2_5d1vuffx,False,,0,False,Question on Survey Data Results and writing,[],r/datascience,False,6,network,0,,,False,t3_nsc65l,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Networking,False,0,,False,False,self,False,,[],{},,True,,1622860776.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My apologies if this isn&amp;#39;t the proper group to ask the question - but any assistance would be greatly appreciated. &lt;/p&gt;

&lt;p&gt;My company does weekly surveys to our membership base - we have approximately 4 years worth of data/results that we were looking for someone to assist in interpreting the data and possibly tying it all together for a comprehensive report sort of speak. I am struggling to find a company that offers this service or even what this service would be called. &lt;/p&gt;

&lt;p&gt;Anyone here know of a company or consultant that could help?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsc65l,True,,Briand9595,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsc65l/question_on_survey_data_results_and_writing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsc65l/question_on_survey_data_results_and_writing/,515405,1622831976.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I’m currently researching different prosthetic heart valve replacements and how they affect patient outcomes in a certain region of California. I want to get my hands on county specific data that shows which type of valve was used and then follow up on the patients outcome. Is this possible? I have no idea where to acquire these data. 

Thanks for your help!",t2_o00jp,False,,0,False,What’s the best source for raw medical data?,[],r/datascience,False,6,education,0,,,False,t3_nryo30,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1622818137.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m currently researching different prosthetic heart valve replacements and how they affect patient outcomes in a certain region of California. I want to get my hands on county specific data that shows which type of valve was used and then follow up on the patients outcome. Is this possible? I have no idea where to acquire these data. &lt;/p&gt;

&lt;p&gt;Thanks for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nryo30,True,,BushDid9Eleven,,11,True,all_ads,False,[],False,,/r/datascience/comments/nryo30/whats_the_best_source_for_raw_medical_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nryo30/whats_the_best_source_for_raw_medical_data/,515405,1622789337.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"NB: this is NOT a rant post, I swear. I want to be proactive.

I'm writing here to ask some advice on how to tackle my next interview processes, I have a problem about this.

&amp;#x200B;

SOME CONTEXT, QUICKLY:

I am already a professional Data Scientist with almost 3 years of experience in a large company.

I have a PhD from a social science department. My main field of study has been application of statistical models. I spent four years studying (mostly) statistics and econometrics, and doing estimations. My final thesis was completely statistical in nature. Before that, I received good basics in CS.

I don't want to sound arrogant, but I think I'm good at my job. I have a good understanding of math, calculus, statistics, and algorithms. My colleagues with a background in STEM told me I'm good at Deep Learning. I am the reference guy in my company for the use of TensorFlow.

&amp;#x200B;

HERE'S THE PROBLEM:

I like my current job but I don't have faith in the future of my company. I have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon, that know nothing outside marketing. I'm sick of this and I want to leave.

However, every time I apply for a new job I feel that I'm not taken seriously because of my social science academic background. I can see how recruiters changed attitude when they found I come from a social science department. They believe I got there by mistake.

This is so frustrating. What can I do about this? How should I approach recruiters and companies when I apply for a new job?

&amp;#x200B;

Thank you people, love this sub.  


\-------  
EDIT:  
To make myself more clear, and give you an idea of why I wrote this post: I have JUST received an email (literally 1 minute ago!) by a company I applied for. They had cool DL projects, young data-savvy team, both interviews went great, we all liked each other. Now they just told me: listen, we liked you very much, but our company's policy is that no people with a social science background can be hired for this role. They literally told me that.

I hope you will now better understand the reason for this post, instead of calling my ""lack of humility"".

Again it's not a rant (partially now), but rather: tell me what to do to attenuate/bypass this problem.  
",t2_3fcw5pgq,False,,0,False,How to be taken seriously during a job interview when you don't have a STEM degree?,[],r/datascience,False,6,,0,,,False,t3_nr9b9x,False,dark,0.92,,public,254,0,{},,,False,[],,False,False,,{},Job Search,False,254,,False,False,self,1622822079.0,,[],{},,True,,1622741342.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;NB: this is NOT a rant post, I swear. I want to be proactive.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m writing here to ask some advice on how to tackle my next interview processes, I have a problem about this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;SOME CONTEXT, QUICKLY:&lt;/p&gt;

&lt;p&gt;I am already a professional Data Scientist with almost 3 years of experience in a large company.&lt;/p&gt;

&lt;p&gt;I have a PhD from a social science department. My main field of study has been application of statistical models. I spent four years studying (mostly) statistics and econometrics, and doing estimations. My final thesis was completely statistical in nature. Before that, I received good basics in CS.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t want to sound arrogant, but I think I&amp;#39;m good at my job. I have a good understanding of math, calculus, statistics, and algorithms. My colleagues with a background in STEM told me I&amp;#39;m good at Deep Learning. I am the reference guy in my company for the use of TensorFlow.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;HERE&amp;#39;S THE PROBLEM:&lt;/p&gt;

&lt;p&gt;I like my current job but I don&amp;#39;t have faith in the future of my company. I have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon, that know nothing outside marketing. I&amp;#39;m sick of this and I want to leave.&lt;/p&gt;

&lt;p&gt;However, every time I apply for a new job I feel that I&amp;#39;m not taken seriously because of my social science academic background. I can see how recruiters changed attitude when they found I come from a social science department. They believe I got there by mistake.&lt;/p&gt;

&lt;p&gt;This is so frustrating. What can I do about this? How should I approach recruiters and companies when I apply for a new job?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you people, love this sub.  &lt;/p&gt;

&lt;p&gt;-------&lt;br/&gt;
EDIT:&lt;br/&gt;
To make myself more clear, and give you an idea of why I wrote this post: I have JUST received an email (literally 1 minute ago!) by a company I applied for. They had cool DL projects, young data-savvy team, both interviews went great, we all liked each other. Now they just told me: listen, we liked you very much, but our company&amp;#39;s policy is that no people with a social science background can be hired for this role. They literally told me that.&lt;/p&gt;

&lt;p&gt;I hope you will now better understand the reason for this post, instead of calling my &amp;quot;lack of humility&amp;quot;.&lt;/p&gt;

&lt;p&gt;Again it&amp;#39;s not a rant (partially now), but rather: tell me what to do to attenuate/bypass this problem.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nr9b9x,True,,Le2vo,,85,True,all_ads,False,[],False,,/r/datascience/comments/nr9b9x/how_to_be_taken_seriously_during_a_job_interview/,all_ads,False,https://www.reddit.com/r/datascience/comments/nr9b9x/how_to_be_taken_seriously_during_a_job_interview/,515405,1622712542.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,Does anyone have experience implementing causal inference in data science? What exactly did you use it for and how effective was it? Did it actually provide some value?,t2_bb2sfju2,False,,0,False,Causal inference in Data Science,[],r/datascience,False,6,discussion,0,,,False,t3_nrzbg6,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622820934.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone have experience implementing causal inference in data science? What exactly did you use it for and how effective was it? Did it actually provide some value?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrzbg6,True,,mimeticaware,,2,True,all_ads,False,[],False,,/r/datascience/comments/nrzbg6/causal_inference_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrzbg6/causal_inference_in_data_science/,515405,1622792134.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone!

Many members of this subreddit want to brush up on data science or keep their skills sharp. Would anyone be interested in starting a community where we write each other challenge problems and get in the habit of solving problems daily? Think probability puzzles, coding problems, and questions about ML techniques. Research shows daily problem-solving can help you learn much quicker, boost recall, and prevent you from forgetting key concepts. Even with a small community of 20 members, writing 1 question means 20 questions to practice with every week.

Feel free to comment or DM me if you're interested!",t2_8epoa,False,,0,False,Interest in a Puzzle-Solving Community?,[],r/datascience,False,6,education,0,,,False,t3_nrgvaq,False,dark,0.91,,public,31,0,{},,,False,[],,False,False,,{},Education,False,31,,False,False,self,False,,[],{},,True,,1622764722.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;Many members of this subreddit want to brush up on data science or keep their skills sharp. Would anyone be interested in starting a community where we write each other challenge problems and get in the habit of solving problems daily? Think probability puzzles, coding problems, and questions about ML techniques. Research shows daily problem-solving can help you learn much quicker, boost recall, and prevent you from forgetting key concepts. Even with a small community of 20 members, writing 1 question means 20 questions to practice with every week.&lt;/p&gt;

&lt;p&gt;Feel free to comment or DM me if you&amp;#39;re interested!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrgvaq,True,,solanumtuberosum,,31,True,all_ads,False,[],False,,/r/datascience/comments/nrgvaq/interest_in_a_puzzlesolving_community/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrgvaq/interest_in_a_puzzlesolving_community/,515405,1622735922.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi  there, I am building a text classification model to match the name and  description of a customer's item (e.g. name: ""suction press nip"",  category: ""paper machine parts"") to a list of 10k basic items (name:  ""steel, unalloyed"", category: ""metals""). I have some initial matched  data to test and I will get more and more, hopefully.

I've build a sentiment analysis program in the past, this is a good example of what I used: [https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) (Spacy, Scikitlearn).

This  current problem is more complex though, it's 1 to 10k+ match and not  binary (or max 5, 6 values), the string for the item is short and  absolutely at the discretion of the source (client item log).

Which reads/tutorials/examples would you suggest to take a look at? (in Python please)",t2_10p8w7,False,,0,False,"Text classification for item matching, best setup?",[],r/datascience,False,6,projects,0,,,False,t3_ns1l29,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1622830110.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi  there, I am building a text classification model to match the name and  description of a customer&amp;#39;s item (e.g. name: &amp;quot;suction press nip&amp;quot;,  category: &amp;quot;paper machine parts&amp;quot;) to a list of 10k basic items (name:  &amp;quot;steel, unalloyed&amp;quot;, category: &amp;quot;metals&amp;quot;). I have some initial matched  data to test and I will get more and more, hopefully.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve build a sentiment analysis program in the past, this is a good example of what I used: &lt;a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/""&gt;https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/&lt;/a&gt; (Spacy, Scikitlearn).&lt;/p&gt;

&lt;p&gt;This  current problem is more complex though, it&amp;#39;s 1 to 10k+ match and not  binary (or max 5, 6 values), the string for the item is short and  absolutely at the discretion of the source (client item log).&lt;/p&gt;

&lt;p&gt;Which reads/tutorials/examples would you suggest to take a look at? (in Python please)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns1l29,True,,lele-canfora,,1,True,all_ads,False,[],False,,/r/datascience/comments/ns1l29/text_classification_for_item_matching_best_setup/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns1l29/text_classification_for_item_matching_best_setup/,515405,1622801310.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi All,

I am working on a problem for a client wherein they want to streamline sourcing of their products based on 3 factors, Cost of goods, Lead Time &amp; Risk Factor to fulfill the demand.

&amp;#x200B;

I tried approaching it in the conventional Linear Programming model specifying constraints and minimizing Cost , but this method has a drawback that it is optimizing on 1 function i.e Cost and doesn't take it into account Risk/Lead , so I will get lowest value but my risk/lead time  is high I am looking for an approach which gives  balanced answer .

&amp;#x200B;

I am thinking I should create a new objective function which is a mix of all the factors like 

Objective = Qntity \* Country1 + Qntity \* Country2 + Risk\*Country1 + Risk\* Country2 

But I cant understand if I should maximize or minimize this.

&amp;#x200B;

Any thoughts on how can I proceed ?",t2_6ys5mu5,False,,0,False,Optimizing Based on all multiple counteracting factors,[],r/datascience,False,6,discussion,0,,,False,t3_ns15kc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622828412.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;I am working on a problem for a client wherein they want to streamline sourcing of their products based on 3 factors, Cost of goods, Lead Time &amp;amp; Risk Factor to fulfill the demand.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I tried approaching it in the conventional Linear Programming model specifying constraints and minimizing Cost , but this method has a drawback that it is optimizing on 1 function i.e Cost and doesn&amp;#39;t take it into account Risk/Lead , so I will get lowest value but my risk/lead time  is high I am looking for an approach which gives  balanced answer .&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am thinking I should create a new objective function which is a mix of all the factors like &lt;/p&gt;

&lt;p&gt;Objective = Qntity * Country1 + Qntity * Country2 + Risk*Country1 + Risk* Country2 &lt;/p&gt;

&lt;p&gt;But I cant understand if I should maximize or minimize this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any thoughts on how can I proceed ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns15kc,True,,user19911506,,4,True,all_ads,False,[],False,,/r/datascience/comments/ns15kc/optimizing_based_on_all_multiple_counteracting/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns15kc/optimizing_based_on_all_multiple_counteracting/,515405,1622799612.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in my first real data science job at a F500 med device company. The team I am supporting is looking to implement smart features for a web application. The team is all software developers with zero experience/understanding of data science. The previous work/proof of concept for the work was a bunch of Juptyer notebooks using static log data as inputs, and we are working through which features to implement.

I'm working to frame the steps of using data science/ML in production to crawl/walk/run (i.e. start small and work up from there, considering there is currently zero infrastructure). Anyone been in a similar situation and have advice on how to frame the crawl/walk/run steps for a team with zero experience?",t2_bkhm1,False,,0,False,Team with no data science infrastructure/knowledge (crawl/walk/run),[],r/datascience,False,6,projects,0,,,False,t3_nrj0i1,False,dark,0.92,,public,11,0,{},,,False,[],,False,False,,{},Projects,False,11,,False,False,self,1622742336.0,,[],{},,True,,1622770287.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in my first real data science job at a F500 med device company. The team I am supporting is looking to implement smart features for a web application. The team is all software developers with zero experience/understanding of data science. The previous work/proof of concept for the work was a bunch of Juptyer notebooks using static log data as inputs, and we are working through which features to implement.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working to frame the steps of using data science/ML in production to crawl/walk/run (i.e. start small and work up from there, considering there is currently zero infrastructure). Anyone been in a similar situation and have advice on how to frame the crawl/walk/run steps for a team with zero experience?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrj0i1,True,,getbuckets41,,19,True,all_ads,False,[],False,,/r/datascience/comments/nrj0i1/team_with_no_data_science_infrastructureknowledge/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrj0i1/team_with_no_data_science_infrastructureknowledge/,515405,1622741487.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"https://imgur.com/a/wz1Zwv3

I drew this picture - a regression model is being used to estimate the y-value for some points, and error bars that look like mini normal distributions are shown for each point.

Can anyone please recommend a higher quality version of this image? Something from google images, etc? I spent some time looking, but I couldn't find anything that exactly matches what I am looking for.

Thanks",t2_xtuyc,False,,0,False,Can anyone recommend a higher quality version of this picture?,[],r/datascience,False,6,discussion,0,,,False,t3_nrxu53,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622814662.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://imgur.com/a/wz1Zwv3""&gt;https://imgur.com/a/wz1Zwv3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I drew this picture - a regression model is being used to estimate the y-value for some points, and error bars that look like mini normal distributions are shown for each point.&lt;/p&gt;

&lt;p&gt;Can anyone please recommend a higher quality version of this image? Something from google images, etc? I spent some time looking, but I couldn&amp;#39;t find anything that exactly matches what I am looking for.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrxu53,True,,ottawalanguages,,1,True,all_ads,False,[],False,,/r/datascience/comments/nrxu53/can_anyone_recommend_a_higher_quality_version_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrxu53/can_anyone_recommend_a_higher_quality_version_of/,515405,1622785862.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?auto=webp&amp;s=6e979e1e776c978d6ca5092399e1ee7404b7712a', 'width': 773, 'height': 592}, 'resolutions': [{'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dcd1a65491af55c10c641b0e324c0bab026a03', 'width': 108, 'height': 82}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5bac6171b8774da9d7075bc0892f2cba02aef261', 'width': 216, 'height': 165}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd18ba2690980e382713ad18b71fc58f8b67ab08', 'width': 320, 'height': 245}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8fab84b4c851411dc3b06825e5a1bcb7d672aa2', 'width': 640, 'height': 490}], 'variants': {}, 'id': 'QFKxKl03GqHcaGfbTpFNPTy1WANFEYTH3LBnHoSPBhI'}], 'enabled': False}",,,,,
,datascience,"Hello, this just merely an interesting thought I’ve had. I’ve noticed there’s an interesting niche within the field of Bayesian statistics that goes into probabilistic programming, building Bayesian models, Bayesian deep learning etc. This area seems like a big topic in research as well. My question is more so geared towards industry, but what is the trend recently when it comes to using Bayesian statistics and probabilistic programming in a company? My intuition tells me that  Bayesian methods are really interpretable to stats/DS/math folks, but to those outside of that in industry, say stakeholders or upper level management it may not be as interpretable. With most baseline statistics classes starting off at the frequentist perspective, it seems that these are the methods which are really interpretable to management in industry, and thus there is not much of a use case for probabilistic programming and Bayesian methods other than research.

Can anyone speak to this? I’m curious to see how much of an acceptance there is to probabilistic programming  in the industry and if it is really only limited to research?",t2_5w4i5kd1,False,,0,False,How much has probabilistic programming been adopted in industry?,[],r/datascience,False,6,discussion,0,,,False,t3_nridri,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1622768658.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, this just merely an interesting thought I’ve had. I’ve noticed there’s an interesting niche within the field of Bayesian statistics that goes into probabilistic programming, building Bayesian models, Bayesian deep learning etc. This area seems like a big topic in research as well. My question is more so geared towards industry, but what is the trend recently when it comes to using Bayesian statistics and probabilistic programming in a company? My intuition tells me that  Bayesian methods are really interpretable to stats/DS/math folks, but to those outside of that in industry, say stakeholders or upper level management it may not be as interpretable. With most baseline statistics classes starting off at the frequentist perspective, it seems that these are the methods which are really interpretable to management in industry, and thus there is not much of a use case for probabilistic programming and Bayesian methods other than research.&lt;/p&gt;

&lt;p&gt;Can anyone speak to this? I’m curious to see how much of an acceptance there is to probabilistic programming  in the industry and if it is really only limited to research?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nridri,True,,veeeerain,,19,True,all_ads,False,[],False,,/r/datascience/comments/nridri/how_much_has_probabilistic_programming_been/,all_ads,False,https://www.reddit.com/r/datascience/comments/nridri/how_much_has_probabilistic_programming_been/,515405,1622739858.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello data scientists,

I am but a mere engineer, trying to pretend that I know what I’m doing. I have a small set of labelled greyscale images (2 classes), and a lot of unlabelled images. I want to understand what *qualitatively* differentiates the sets of labelled images.

My engineer brain understands how to train something to do a prediction, but absolutely fails to understand how to *analyze* the image data with the tools at my disposal. I would love to be able to tell someone, “*this* is the difference between these images”.

What’s the data science-y way to look at images?",t2_rsqm8,False,,0,False,Tools for analyzing images,[],r/datascience,False,6,tooling,0,,,False,t3_nrhzbw,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Tooling,False,4,,False,False,self,False,,[],{},,True,,1622767619.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello data scientists,&lt;/p&gt;

&lt;p&gt;I am but a mere engineer, trying to pretend that I know what I’m doing. I have a small set of labelled greyscale images (2 classes), and a lot of unlabelled images. I want to understand what &lt;em&gt;qualitatively&lt;/em&gt; differentiates the sets of labelled images.&lt;/p&gt;

&lt;p&gt;My engineer brain understands how to train something to do a prediction, but absolutely fails to understand how to &lt;em&gt;analyze&lt;/em&gt; the image data with the tools at my disposal. I would love to be able to tell someone, “&lt;em&gt;this&lt;/em&gt; is the difference between these images”.&lt;/p&gt;

&lt;p&gt;What’s the data science-y way to look at images?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrhzbw,True,,Num1DeathEater,,3,True,all_ads,False,[],False,,/r/datascience/comments/nrhzbw/tools_for_analyzing_images/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrhzbw/tools_for_analyzing_images/,515405,1622738819.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,I have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool. Should I add web development to my data science toolbox?,t2_bnc9e5y2,False,,0,False,Is learning web development worth it?,[],r/datascience,False,6,discussion,0,,,False,t3_nrs4r6,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622794914.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool. Should I add web development to my data science toolbox?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrs4r6,True,,daytoniano,,5,True,all_ads,False,[],False,,/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/,515405,1622766114.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Unlimited PTO (paid-time-off). Some love it, others think it’s a scam.

But it’s worth exploring why this policy was implemented in the first place. And for that, we go back to the early days at Netflix.

It’s 2003. Netflix is galloping along in pursuit of Blockbuster. There’s a buzz around the office. The chase is on and an employee asks:

*""'We are all working online some weekends, responding to emails at odd hours, taking off an afternoon for personal time. We don't track hours worked per day or week. Why are we tracking days of vacation per year?""*

Reed Hastings, CEO of Netflix, doesn’t really have a great answer. After all, he’s always judged performance without looking at hours. Get the job done in 1 hour or 10 hours? Doesn’t matter as long as you're doing good work.

Hastings also realizes that some of the best ideas at work come after someone’s just taken vacation. They’ve got the mental bandwidth to think about their work in a fresh, creative manner. Something that’s not possible if you’re clocking in and out without any rest.

So Hastings decides to pull the trigger. He introduces Netflix’s *No Vacation Policy* which puts the onus on their employees to decide when and how much vacation they need to take.

In his book, *No Rules Rules*, Hastings describes getting nightmares when he first introduced this policy. In one of these nightmares, he’d drive to the office, park his car, and walk into a completely empty building.

Those nightmares, minus a few blips which we’ll get to in a bit, never really materialized. The policy was a success and soon other companies in the Valley started copying Netflix. Everybody wanted the best talent and implementing a no rules vacation policy seemed like a great differentiator.

Except that the same policy which worked so well for Netflix...wasn’t working for anyone else.

Other companies found that after implementing an unlimited PTO type policy, employees paradoxically started to take *less* vacation. They would worry that their co-workers would think they were slacking off or that they would get left behind come promotion time.

Hastings was surprised. After a bit of digging, he realized the reason behind why these policies had failed.

The leaders at these companies were not modelling big vacation taking.

Indeed, if the execs were only taking 10 days off, then the unlimited plan would deter other employees from taking anywhere near that amount or more than that.

As Hastings put it:

*“In the absence of a policy, the amount of vacation people take largely reflects what they see their boss and colleagues taking.”*

**Modelling others around you**

This concept of modelling others around us applies not only to vacation taking, but to all sorts of behaviors. As we continue to move towards a new distributed, remote-first workforce, there’s going to be a lot of ambiguity in the decisions that we need to make.

The companies that are able to best adapt to this changing environment will be the ones in which leaders model the right set of behaviors.

A big one will be written communication. As the ability to just randomly walk up to someone at the office and ask them a question subsides, we’ll need to document our practices much better and be able to communicate much more efficiently.

The more we see others, especially our leaders, invest in written communication and take the time to get better at it, the more we will do it.

And never mind us seeing them do this. Reed Hastings wants them to shout loud and clear just how much vacation they’re taking or just how much they’re investing in themselves, so as to encourage everyone else to do it.

An example of good modelling in practice is Evernote. The company, which also doesn’t limit employee vacation days, actually gives a $1,000 stipend to anyone who takes an entire week off in order to encourage vacation taking ([source](https://www.washingtonpost.com/news/on-leadership/wp/2013/08/13/the-catch-of-having-an-unlimited-vacation-policy/)).

**Other Things**

Okay, so there was one more thing that Reed Hastings found out. It wasn’t enough for leaders to just model the right behavior. They also had to set context and guidelines.

Reed realized this when it was the end of quarter and his accounting team was supposed to be closing up their financial books. But a member of the team, in an attempt to avoid the annual crunch period, took off the first two weeks of January. No bueno.

So Reed decided to put in place clear parameters and guidelines on what was acceptable within the context of taking time off. For example, it was imperative to mention things like how many people taking time off at the same time is acceptable and how managers must be notified well in advance of any such long vacations.

This would help prevent blows like the one above in the accounting department.

**Conclusion**

In the end, it seems like Unlimited PTO can work, but it also needs to be supported with strong management. Individuals need to model big vacation taking and put into place the right guidelines.

But I think the lessons here go beyond just vacation.

The behaviors we see and notice from those around us eventually have a strong impact on the type of people that we become. This is especially true at the managerial level, where the impact is 1 to N and can result in considerable [cultural debt](https://www.careerfair.io/reviews/cultural-debt).

So just like this question of unlimited vacation, the answer usually lies in its implementation. Context is king. But that does't always make for good headlines, now, does it. 

\--------

Hope that was useful.

*If you liked this post, you might like* [*my newsletter*](https://www.careerfair.io/subscribe)*. It's my best content delivered to your inbox once every two weeks. And if Twitter is more your thing, I would love it if you* [retweeted the thread](https://twitter.com/OGCareerFair/status/1400161823299604481)*!!*",t2_qr5uf,False,,0,False,I researched the origin of Unlimited PTO (at Netflix) and wrote up a case study :),[],r/datascience,False,6,career,0,,,False,t3_nqnrs6,False,dark,0.93,,public,380,2,{},,,False,[],,False,False,,{},Career,False,380,,False,True,self,1622659648.0,,[],{},,True,,1622674872.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Unlimited PTO (paid-time-off). Some love it, others think it’s a scam.&lt;/p&gt;

&lt;p&gt;But it’s worth exploring why this policy was implemented in the first place. And for that, we go back to the early days at Netflix.&lt;/p&gt;

&lt;p&gt;It’s 2003. Netflix is galloping along in pursuit of Blockbuster. There’s a buzz around the office. The chase is on and an employee asks:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;quot;&amp;#39;We are all working online some weekends, responding to emails at odd hours, taking off an afternoon for personal time. We don&amp;#39;t track hours worked per day or week. Why are we tracking days of vacation per year?&amp;quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Reed Hastings, CEO of Netflix, doesn’t really have a great answer. After all, he’s always judged performance without looking at hours. Get the job done in 1 hour or 10 hours? Doesn’t matter as long as you&amp;#39;re doing good work.&lt;/p&gt;

&lt;p&gt;Hastings also realizes that some of the best ideas at work come after someone’s just taken vacation. They’ve got the mental bandwidth to think about their work in a fresh, creative manner. Something that’s not possible if you’re clocking in and out without any rest.&lt;/p&gt;

&lt;p&gt;So Hastings decides to pull the trigger. He introduces Netflix’s &lt;em&gt;No Vacation Policy&lt;/em&gt; which puts the onus on their employees to decide when and how much vacation they need to take.&lt;/p&gt;

&lt;p&gt;In his book, &lt;em&gt;No Rules Rules&lt;/em&gt;, Hastings describes getting nightmares when he first introduced this policy. In one of these nightmares, he’d drive to the office, park his car, and walk into a completely empty building.&lt;/p&gt;

&lt;p&gt;Those nightmares, minus a few blips which we’ll get to in a bit, never really materialized. The policy was a success and soon other companies in the Valley started copying Netflix. Everybody wanted the best talent and implementing a no rules vacation policy seemed like a great differentiator.&lt;/p&gt;

&lt;p&gt;Except that the same policy which worked so well for Netflix...wasn’t working for anyone else.&lt;/p&gt;

&lt;p&gt;Other companies found that after implementing an unlimited PTO type policy, employees paradoxically started to take &lt;em&gt;less&lt;/em&gt; vacation. They would worry that their co-workers would think they were slacking off or that they would get left behind come promotion time.&lt;/p&gt;

&lt;p&gt;Hastings was surprised. After a bit of digging, he realized the reason behind why these policies had failed.&lt;/p&gt;

&lt;p&gt;The leaders at these companies were not modelling big vacation taking.&lt;/p&gt;

&lt;p&gt;Indeed, if the execs were only taking 10 days off, then the unlimited plan would deter other employees from taking anywhere near that amount or more than that.&lt;/p&gt;

&lt;p&gt;As Hastings put it:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In the absence of a policy, the amount of vacation people take largely reflects what they see their boss and colleagues taking.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modelling others around you&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This concept of modelling others around us applies not only to vacation taking, but to all sorts of behaviors. As we continue to move towards a new distributed, remote-first workforce, there’s going to be a lot of ambiguity in the decisions that we need to make.&lt;/p&gt;

&lt;p&gt;The companies that are able to best adapt to this changing environment will be the ones in which leaders model the right set of behaviors.&lt;/p&gt;

&lt;p&gt;A big one will be written communication. As the ability to just randomly walk up to someone at the office and ask them a question subsides, we’ll need to document our practices much better and be able to communicate much more efficiently.&lt;/p&gt;

&lt;p&gt;The more we see others, especially our leaders, invest in written communication and take the time to get better at it, the more we will do it.&lt;/p&gt;

&lt;p&gt;And never mind us seeing them do this. Reed Hastings wants them to shout loud and clear just how much vacation they’re taking or just how much they’re investing in themselves, so as to encourage everyone else to do it.&lt;/p&gt;

&lt;p&gt;An example of good modelling in practice is Evernote. The company, which also doesn’t limit employee vacation days, actually gives a $1,000 stipend to anyone who takes an entire week off in order to encourage vacation taking (&lt;a href=""https://www.washingtonpost.com/news/on-leadership/wp/2013/08/13/the-catch-of-having-an-unlimited-vacation-policy/""&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Things&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Okay, so there was one more thing that Reed Hastings found out. It wasn’t enough for leaders to just model the right behavior. They also had to set context and guidelines.&lt;/p&gt;

&lt;p&gt;Reed realized this when it was the end of quarter and his accounting team was supposed to be closing up their financial books. But a member of the team, in an attempt to avoid the annual crunch period, took off the first two weeks of January. No bueno.&lt;/p&gt;

&lt;p&gt;So Reed decided to put in place clear parameters and guidelines on what was acceptable within the context of taking time off. For example, it was imperative to mention things like how many people taking time off at the same time is acceptable and how managers must be notified well in advance of any such long vacations.&lt;/p&gt;

&lt;p&gt;This would help prevent blows like the one above in the accounting department.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the end, it seems like Unlimited PTO can work, but it also needs to be supported with strong management. Individuals need to model big vacation taking and put into place the right guidelines.&lt;/p&gt;

&lt;p&gt;But I think the lessons here go beyond just vacation.&lt;/p&gt;

&lt;p&gt;The behaviors we see and notice from those around us eventually have a strong impact on the type of people that we become. This is especially true at the managerial level, where the impact is 1 to N and can result in considerable &lt;a href=""https://www.careerfair.io/reviews/cultural-debt""&gt;cultural debt&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So just like this question of unlimited vacation, the answer usually lies in its implementation. Context is king. But that does&amp;#39;t always make for good headlines, now, does it. &lt;/p&gt;

&lt;p&gt;--------&lt;/p&gt;

&lt;p&gt;Hope that was useful.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, you might like&lt;/em&gt; &lt;a href=""https://www.careerfair.io/subscribe""&gt;&lt;em&gt;my newsletter&lt;/em&gt;&lt;/a&gt;&lt;em&gt;. It&amp;#39;s my best content delivered to your inbox once every two weeks. And if Twitter is more your thing, I would love it if you&lt;/em&gt; &lt;a href=""https://twitter.com/OGCareerFair/status/1400161823299604481""&gt;retweeted the thread&lt;/a&gt;&lt;em&gt;!!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqnrs6,True,,ibsurvivors,,92,True,all_ads,False,[],False,,/r/datascience/comments/nqnrs6/i_researched_the_origin_of_unlimited_pto_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqnrs6/i_researched_the_origin_of_unlimited_pto_at/,515405,1622646072.0,2,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?auto=webp&amp;s=e8ad4fb2df20393ca4d7f1227812e12c5a42670f', 'width': 1484, 'height': 779}, 'resolutions': [{'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1590fc33abbbaa194a00f3db7057fe9f78d642c6', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=899e1e217514a5db15d356a906d0cd5cea3ab8a1', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7034972a438497c9cb80db4b506b80b5b262e117', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc8c4b6068f0861a1df004c98412463f2007b9b6', 'width': 640, 'height': 335}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=82e19e786f176070f692b1ff243b50193bd31887', 'width': 960, 'height': 503}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4bc22529c62f391f36c2a0fc6f7cbe7c7421f75', 'width': 1080, 'height': 566}], 'variants': {}, 'id': 'LLVQ2Y4uZDDEJ_QONX_Uk5Se66vJWi-oOjaVm579-YU'}], 'enabled': False}",,,,,
,datascience,"I'm moving into a new role at my school next year--Data Strategist (yay!) and I will be responsible for managing the school's data. At the moment, data is not accessible and we have struggled as a result. 

To my question, what do you think the best tool is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. 

My first thought was google data studio since it's pretty straight forward, free, and easily works with sheets. However, I'm also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. 

Further information.. this is for a high school that has about 600 students and I need dashboards for the entire school, each grade level, and each content area. 

Thanks!",t2_969d1sbr,False,,0,False,Best platform for a school data dashboard?,[],r/datascience,False,6,career,0,,,False,t3_nrou0l,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1622785392.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m moving into a new role at my school next year--Data Strategist (yay!) and I will be responsible for managing the school&amp;#39;s data. At the moment, data is not accessible and we have struggled as a result. &lt;/p&gt;

&lt;p&gt;To my question, what do you think the best tool is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. &lt;/p&gt;

&lt;p&gt;My first thought was google data studio since it&amp;#39;s pretty straight forward, free, and easily works with sheets. However, I&amp;#39;m also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. &lt;/p&gt;

&lt;p&gt;Further information.. this is for a high school that has about 600 students and I need dashboards for the entire school, each grade level, and each content area. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrou0l,True,,buttchiquesybobs,,6,True,all_ads,False,[],False,,/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/,515405,1622756592.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I want to use the recently published VQGAN+CLIP implementation of transformers to generate images based on a text description, and then sell those images in any way I can, maybe through a website. I would use very specific seeds and text, so these images would be almost impossible for anybody to replicate with the same network. Is this legal? As I understand, [VQGAN](https://github.com/CompVis/taming-transformers) and [CLIP](https://github.com/openai/CLIP) are both open-source.

Thank you",t2_5d6zezk,False,,0,False,Is it legal to create images with an open-source NN implementation and sell them?,[],r/datascience,False,6,discussion,0,,,False,t3_nrnxka,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622782966.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to use the recently published VQGAN+CLIP implementation of transformers to generate images based on a text description, and then sell those images in any way I can, maybe through a website. I would use very specific seeds and text, so these images would be almost impossible for anybody to replicate with the same network. Is this legal? As I understand, &lt;a href=""https://github.com/CompVis/taming-transformers""&gt;VQGAN&lt;/a&gt; and &lt;a href=""https://github.com/openai/CLIP""&gt;CLIP&lt;/a&gt; are both open-source.&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrnxka,True,,dokasov,,3,True,all_ads,False,[],False,,/r/datascience/comments/nrnxka/is_it_legal_to_create_images_with_an_opensource/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrnxka/is_it_legal_to_create_images_with_an_opensource/,515405,1622754166.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?auto=webp&amp;s=162fa63b063c05585b3367a9deca69265fafef99', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e27f3bc325ee962cbbfed579f5e9a281ae894bd', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3a1784dece0168a30340af97b4e24c15e69d2d', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4be0a7838e5a5faf4215ea4d6f8ac1e7e12cbf5c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f755a6353b6feb1e26777181dd5c78f40d5f80b2', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=145706f614fff027dd1e4761b15d12d281be83c7', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=760caa5c3ced375b8b513fa536257d8b446a25ae', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'SSo8Flx8Vqp1pRHNXHsKjDrKDSsGdGmeQ23z-_8rN1w'}], 'enabled': False}",,,,,
,datascience," Hi all. I was hoping for some insight. I haven't worked with Red Hat or OpenShift before, and so I was wondering if you could give me the Cliffs Notes version of how the platform differs to a traditional enterprise data architecture? Or is it meant to slot in to the gap to provide cloud services for the other tools being used?

Would appreciate a bit of insight there. Hope that isn't too silly a question.

I was also wondering if there is an open source Enterprise Data Architecture system (with all the components) that I could play with as I develop a set of tools for my company, to get a feel for what the various components do and how they fit together.",t2_vv6vm,False,,0,False,Explanation of Enterprise Data Architecture vs tools,[],r/datascience,False,6,discussion,0,,,False,t3_nrmhcn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622779225.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I was hoping for some insight. I haven&amp;#39;t worked with Red Hat or OpenShift before, and so I was wondering if you could give me the Cliffs Notes version of how the platform differs to a traditional enterprise data architecture? Or is it meant to slot in to the gap to provide cloud services for the other tools being used?&lt;/p&gt;

&lt;p&gt;Would appreciate a bit of insight there. Hope that isn&amp;#39;t too silly a question.&lt;/p&gt;

&lt;p&gt;I was also wondering if there is an open source Enterprise Data Architecture system (with all the components) that I could play with as I develop a set of tools for my company, to get a feel for what the various components do and how they fit together.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrmhcn,True,,ryanblumenow,,0,True,all_ads,False,[],False,,/r/datascience/comments/nrmhcn/explanation_of_enterprise_data_architecture_vs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrmhcn/explanation_of_enterprise_data_architecture_vs/,515405,1622750425.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"During this summer season, I hope to conduct a research project with my discord friend HonoredTarget. The goal of this project is to compile a giant database of resumes sent to FANG companies and compare those that were accepted/rejected to try and ""crack"" the screening process. In other words, we are essentially trying to figure out if there are certain keywords, phrases, or wordings that increase your chances of getting an interview. However, in order to do this, we need a large pool of resumes (then from this pool, we focus on the ""skills"" section of each resume).

Our initial plan was to create a form and post it on various subreddits but it seems that is not going well. I have spoken to various moderators of other CS-related reddits about permission to post the survey and they have flat out said no or just not responded, so it appears I am in a bit of a predicament. How do you guys think I should go about collecting a large amount of this sort of data? I have looked online and have been unable to find any public databases of resumes.

Let me know what you guys think I should do!",t2_83iz6ye1,False,,0,False,Advice On Resume Data Collection,[],r/datascience,False,6,projects,0,,,False,t3_nrpd22,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1622786828.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;During this summer season, I hope to conduct a research project with my discord friend HonoredTarget. The goal of this project is to compile a giant database of resumes sent to FANG companies and compare those that were accepted/rejected to try and &amp;quot;crack&amp;quot; the screening process. In other words, we are essentially trying to figure out if there are certain keywords, phrases, or wordings that increase your chances of getting an interview. However, in order to do this, we need a large pool of resumes (then from this pool, we focus on the &amp;quot;skills&amp;quot; section of each resume).&lt;/p&gt;

&lt;p&gt;Our initial plan was to create a form and post it on various subreddits but it seems that is not going well. I have spoken to various moderators of other CS-related reddits about permission to post the survey and they have flat out said no or just not responded, so it appears I am in a bit of a predicament. How do you guys think I should go about collecting a large amount of this sort of data? I have looked online and have been unable to find any public databases of resumes.&lt;/p&gt;

&lt;p&gt;Let me know what you guys think I should do!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrpd22,True,,Nervous_Ad_9620,,4,True,all_ads,False,[],False,,/r/datascience/comments/nrpd22/advice_on_resume_data_collection/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrpd22/advice_on_resume_data_collection/,515405,1622758028.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I tried fitting a GLM style regression model to some data and it resulted in all the regression coefficients being estimated as 0 (i.e. model failed). Yet when I tried a random forest model on the same data, the model worked well and I was even able to get 70% accuracy on the test set.

My dataset has continuous and categorical variables, as well as a lot of ""naturally occurring zeros"".

I was just wondering: is this a common problem? I spent a whole day trying to tweak the regression model to work, but the random forest instantly outperformed it?

Thanks",t2_3f0i9m72,False,,0,False,Is this a common problem?,[],r/datascience,False,6,discussion,0,,,False,t3_nrecu2,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622758043.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I tried fitting a GLM style regression model to some data and it resulted in all the regression coefficients being estimated as 0 (i.e. model failed). Yet when I tried a random forest model on the same data, the model worked well and I was even able to get 70% accuracy on the test set.&lt;/p&gt;

&lt;p&gt;My dataset has continuous and categorical variables, as well as a lot of &amp;quot;naturally occurring zeros&amp;quot;.&lt;/p&gt;

&lt;p&gt;I was just wondering: is this a common problem? I spent a whole day trying to tweak the regression model to work, but the random forest instantly outperformed it?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrecu2,True,,SQL_beginner,,13,True,all_ads,False,[],False,,/r/datascience/comments/nrecu2/is_this_a_common_problem/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrecu2/is_this_a_common_problem/,515405,1622729243.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I was brought on to pretty large multinational organization as a Data Scientist thinking I would do Data Science work right?

Now let me preface with, this issue might just be because the client really has no idea what they’re doing and has horrible data management and data engineering. And there’s just a lot of Pre-work that needs to be done.

But every time my group encounters a Data Science problem, the first though is “Let’s just throw it in one of the AWS Services (Reckognize, Comprehend…etc) and close out the project”. I actually don’t think any of the other “Data Scientist” on the team are doing any Data Science. More of Data Engineering perhaps.

I’m all for fast and efficient solutions, but I’m not doing any Data Science work. I get it, like why train an NLP model when AWS has already done that for you, and you can make slight tweaks to it. My last job I was able to build models, pass them off to the Engineering team and move on to the next.

Has anyone encountered this? What are your thoughts and how would you respond?",t2_4hpiqt08,False,,0,False,"Hired as a Data Scientist, not doing Data Science work.",[],r/datascience,False,6,discussion,0,,,False,t3_nqlcp6,False,dark,0.93,,public,183,1,{},,,False,[],,False,False,,{},Discussion,False,183,,False,False,self,False,,[],{},,True,,1622667945.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was brought on to pretty large multinational organization as a Data Scientist thinking I would do Data Science work right?&lt;/p&gt;

&lt;p&gt;Now let me preface with, this issue might just be because the client really has no idea what they’re doing and has horrible data management and data engineering. And there’s just a lot of Pre-work that needs to be done.&lt;/p&gt;

&lt;p&gt;But every time my group encounters a Data Science problem, the first though is “Let’s just throw it in one of the AWS Services (Reckognize, Comprehend…etc) and close out the project”. I actually don’t think any of the other “Data Scientist” on the team are doing any Data Science. More of Data Engineering perhaps.&lt;/p&gt;

&lt;p&gt;I’m all for fast and efficient solutions, but I’m not doing any Data Science work. I get it, like why train an NLP model when AWS has already done that for you, and you can make slight tweaks to it. My last job I was able to build models, pass them off to the Engineering team and move on to the next.&lt;/p&gt;

&lt;p&gt;Has anyone encountered this? What are your thoughts and how would you respond?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqlcp6,True,,CmdrAstroNaughty,,79,True,all_ads,False,[],False,,/r/datascience/comments/nqlcp6/hired_as_a_data_scientist_not_doing_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqlcp6/hired_as_a_data_scientist_not_doing_data_science/,515405,1622639145.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[RANT]

Hey gang, stand back, it’s rant time. 

Analytics is a new field at my work, and I’m here to pioneer it. I work In corporate at a large medical devices company. 

I’ve had the luxury of an amazing boss, some amazing colleagues, and decent budget. 

But for the love of fucking god... I am so sick of being thrown responsibility or projects because good ol mary in sales watched a video on “gesture recognition”. The ideas are a great, and I have a framework for filtering them, but the fucking pressure, the initiation of projects with 0 data, no aim at data collection, no quality assurance or risk management and the icing on the cake, “we should roll out an MVP in 2 months”. What in gods name is that shit? 

I’m the asshole. I’m always the asshole. 
“Here are my requirements if we wish to complete this project in the given time frame.” 
“So... why can’t you develop it now?”
Bro... for starters, I’m not a full fledged software engineer / deep learning god. 

I ask for resources or a relaxed time, and I get 0. 


I don’t need advice. I know what I need to do. I just love this community and felt the need to rant.",t2_5e34w9d2,False,,0,False,I’m so sick of corporate morons,[],r/datascience,False,6,discussion,0,,,False,t3_npurud,False,dark,0.97,,public,948,4,{},,,False,[],,False,False,,{},Discussion,False,948,,False,False,self,False,,[],{},,True,,1622585023.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[RANT]&lt;/p&gt;

&lt;p&gt;Hey gang, stand back, it’s rant time. &lt;/p&gt;

&lt;p&gt;Analytics is a new field at my work, and I’m here to pioneer it. I work In corporate at a large medical devices company. &lt;/p&gt;

&lt;p&gt;I’ve had the luxury of an amazing boss, some amazing colleagues, and decent budget. &lt;/p&gt;

&lt;p&gt;But for the love of fucking god... I am so sick of being thrown responsibility or projects because good ol mary in sales watched a video on “gesture recognition”. The ideas are a great, and I have a framework for filtering them, but the fucking pressure, the initiation of projects with 0 data, no aim at data collection, no quality assurance or risk management and the icing on the cake, “we should roll out an MVP in 2 months”. What in gods name is that shit? &lt;/p&gt;

&lt;p&gt;I’m the asshole. I’m always the asshole. 
“Here are my requirements if we wish to complete this project in the given time frame.” 
“So... why can’t you develop it now?”
Bro... for starters, I’m not a full fledged software engineer / deep learning god. &lt;/p&gt;

&lt;p&gt;I ask for resources or a relaxed time, and I get 0. &lt;/p&gt;

&lt;p&gt;I don’t need advice. I know what I need to do. I just love this community and felt the need to rant.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npurud,True,,expatwithajetpack,,252,True,all_ads,False,[],False,,/r/datascience/comments/npurud/im_so_sick_of_corporate_morons/,all_ads,False,https://www.reddit.com/r/datascience/comments/npurud/im_so_sick_of_corporate_morons/,515405,1622556223.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

I'm trying to use a Jupyter Notebook and pandas with a large dataset, but it keeps crashing and freezing my computer. I've also tried Google Colab, and a friend's computer with double the RAM, to no avail.

Any recommendations of what to use when handling really large sets of data?

Thank you!",t2_57lqzcyf,False,,0,False,How do you handle large datasets?,[],r/datascience,False,6,tooling,0,,,False,t3_nqcl3k,False,dark,0.77,,public,13,0,{},,,False,[],,False,False,,{},Tooling,False,13,,False,False,self,False,,[],{},,True,,1622634365.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to use a Jupyter Notebook and pandas with a large dataset, but it keeps crashing and freezing my computer. I&amp;#39;ve also tried Google Colab, and a friend&amp;#39;s computer with double the RAM, to no avail.&lt;/p&gt;

&lt;p&gt;Any recommendations of what to use when handling really large sets of data?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqcl3k,True,,GirlyWorly,,31,True,all_ads,False,[],False,,/r/datascience/comments/nqcl3k/how_do_you_handle_large_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqcl3k/how_do_you_handle_large_datasets/,515405,1622605565.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,Hey all. I've been tasked with figuring out the best way to implement record linkage between multiple data sources at my job. My suspicion is that supervised approaches will increase accuracy over unsupervised approaches as long as we are willing to do a clerical review to create a training data set that is large enough to be representative. Admittedly this would be very time consuming but I think it might be worthwhile in the long run. After reviewing many papers/blogs etc I can't seem to find many comparisons of current supervised vs unsupervised algorithms. Has anyone seen any work on this? Any links or guidance is appreciated. Or just general record linkage insight is appreciated also. Thanks!,t2_1gii1xta,False,,0,False,Record Linkage - Supervised vs Unsupervised,[],r/datascience,False,6,discussion,0,,,False,t3_nqprvb,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622680017.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all. I&amp;#39;ve been tasked with figuring out the best way to implement record linkage between multiple data sources at my job. My suspicion is that supervised approaches will increase accuracy over unsupervised approaches as long as we are willing to do a clerical review to create a training data set that is large enough to be representative. Admittedly this would be very time consuming but I think it might be worthwhile in the long run. After reviewing many papers/blogs etc I can&amp;#39;t seem to find many comparisons of current supervised vs unsupervised algorithms. Has anyone seen any work on this? Any links or guidance is appreciated. Or just general record linkage insight is appreciated also. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqprvb,True,,drmantist123,,4,True,all_ads,False,[],False,,/r/datascience/comments/nqprvb/record_linkage_supervised_vs_unsupervised/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqprvb/record_linkage_supervised_vs_unsupervised/,515405,1622651217.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have maybe 1000 hours of audio recordings I want to convert to text with timestamps to match diarization timestamps. Or at the minimum, at least convert to text without diarization. The files are a few hours each and add up to maybe 200 5hr sessions. Quality isn't always great but a human can clearly understand what is being said. Approaches I have tried:

Mozilla freespeech: convoluted installed, no diarization 

Kaldi: also somewhat convoluted install, could revisit 

SpeechBrain with Huggingface pretrained: got working, but attention model may need 30 second or less inputs, worried about splitting 6 hour session into 30 seconds and the information loss.",t2_blel3,False,,0,False,What is the best package for combined speech recognition and diarization on long conversation audio files?,[],r/datascience,False,6,discussion,0,,,False,t3_nqenba,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622641781.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have maybe 1000 hours of audio recordings I want to convert to text with timestamps to match diarization timestamps. Or at the minimum, at least convert to text without diarization. The files are a few hours each and add up to maybe 200 5hr sessions. Quality isn&amp;#39;t always great but a human can clearly understand what is being said. Approaches I have tried:&lt;/p&gt;

&lt;p&gt;Mozilla freespeech: convoluted installed, no diarization &lt;/p&gt;

&lt;p&gt;Kaldi: also somewhat convoluted install, could revisit &lt;/p&gt;

&lt;p&gt;SpeechBrain with Huggingface pretrained: got working, but attention model may need 30 second or less inputs, worried about splitting 6 hour session into 30 seconds and the information loss.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqenba,True,,po-handz,,0,True,all_ads,False,[],False,,/r/datascience/comments/nqenba/what_is_the_best_package_for_combined_speech/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqenba/what_is_the_best_package_for_combined_speech/,515405,1622612981.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My company isn't too large but there is starting to be a disconnect between different groups on what data is being used to make decisions, how it is being used, who knows what data exists, etc. As a result I'm working on putting a business case together for getting a centralized data team consisting of data scientists and analyst. I was hoping you all may be able to provide more insight into the benefits and drawbacks of doing this based on your experience.",t2_69ab049y,False,,0,False,What pros or cons have you all seen by centralizing data science and data analysis operations across your organization?,[],r/datascience,False,6,discussion,0,,,False,t3_nq3kne,False,dark,0.91,,public,22,0,{},,,False,[],,False,False,,{},Discussion,False,22,,False,False,self,False,,[],{},,True,,1622607696.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My company isn&amp;#39;t too large but there is starting to be a disconnect between different groups on what data is being used to make decisions, how it is being used, who knows what data exists, etc. As a result I&amp;#39;m working on putting a business case together for getting a centralized data team consisting of data scientists and analyst. I was hoping you all may be able to provide more insight into the benefits and drawbacks of doing this based on your experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nq3kne,True,,MysteriousArmadillo5,,19,True,all_ads,False,[],False,,/r/datascience/comments/nq3kne/what_pros_or_cons_have_you_all_seen_by/,all_ads,False,https://www.reddit.com/r/datascience/comments/nq3kne/what_pros_or_cons_have_you_all_seen_by/,515405,1622578896.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I assume at least 70% will say Jupyter, probably mostly with Python, maybe a few with Julia. Another 20% will say R with RStudio, etc. A handful might say Spyder or something.

I just want to say that I've found nearly every stack, at least as a beginner looking to 'get started fast', is horribly convoluted. I say that as a software developer who writes JavaScript applications (a well-known convoluted platform).

I just want to find something as simple as basic markdown that also lets me execute code, and it doesn't require a special IDE or a command for the code to run (ie, it continuously runs, or it creates live updates on changes). 

- I really like the R Markdown format, but I don't love R itself, and I hate having to use R Studio. It also doesn't seem trivial to set up live updates while editing.
- I like Python more as a data science language, and it's cleanliness as a language also has a similar philosophy as markdown, but there doesn't seem to be a document format that I like. I don't want to write in cells (a la Jupyter) or use an IDE that controls where my cursor can go. I want to edit raw code.
- I even looked into writing code in JavaScript. Yes, Javascript math is UGLY, but I thought oh well, at least I can easily write documents and upload it to the internet, if that is my goal. I tried MDX engines, and you can't even write JS in MDX. At most, you can write a component in a separate file and import it into the MDX file, which is super bloaty.

So, what are your thoughts on this?

Is there any stack that has:

1. Markdown as a base document (not cells). I could be persuaded to something other than markdown, but it would have to be something that is easy to write books and author blogposts with. Markdown seems like the best game in town for that.
2. uses a decent high level language (I'm pretty flexible) that can be executed within the document and ideally create visuals too
3. live updates to the executed code (not just the markdown code) whenever the file is saved (or by some other similar mechanism)
4. no major requirements on IDE (I am a big time VS Code user, and I don't want to change my IDE just to write a little math)",t2_6pptpbhr,False,,0,False,What is your preferred workflow for working with documents and code at the same time?,[],r/datascience,False,6,tooling,0,,,False,t3_nqhdm6,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,1622624702.0,,[],{},,True,,1622653132.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I assume at least 70% will say Jupyter, probably mostly with Python, maybe a few with Julia. Another 20% will say R with RStudio, etc. A handful might say Spyder or something.&lt;/p&gt;

&lt;p&gt;I just want to say that I&amp;#39;ve found nearly every stack, at least as a beginner looking to &amp;#39;get started fast&amp;#39;, is horribly convoluted. I say that as a software developer who writes JavaScript applications (a well-known convoluted platform).&lt;/p&gt;

&lt;p&gt;I just want to find something as simple as basic markdown that also lets me execute code, and it doesn&amp;#39;t require a special IDE or a command for the code to run (ie, it continuously runs, or it creates live updates on changes). &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I really like the R Markdown format, but I don&amp;#39;t love R itself, and I hate having to use R Studio. It also doesn&amp;#39;t seem trivial to set up live updates while editing.&lt;/li&gt;
&lt;li&gt;I like Python more as a data science language, and it&amp;#39;s cleanliness as a language also has a similar philosophy as markdown, but there doesn&amp;#39;t seem to be a document format that I like. I don&amp;#39;t want to write in cells (a la Jupyter) or use an IDE that controls where my cursor can go. I want to edit raw code.&lt;/li&gt;
&lt;li&gt;I even looked into writing code in JavaScript. Yes, Javascript math is UGLY, but I thought oh well, at least I can easily write documents and upload it to the internet, if that is my goal. I tried MDX engines, and you can&amp;#39;t even write JS in MDX. At most, you can write a component in a separate file and import it into the MDX file, which is super bloaty.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, what are your thoughts on this?&lt;/p&gt;

&lt;p&gt;Is there any stack that has:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Markdown as a base document (not cells). I could be persuaded to something other than markdown, but it would have to be something that is easy to write books and author blogposts with. Markdown seems like the best game in town for that.&lt;/li&gt;
&lt;li&gt;uses a decent high level language (I&amp;#39;m pretty flexible) that can be executed within the document and ideally create visuals too&lt;/li&gt;
&lt;li&gt;live updates to the executed code (not just the markdown code) whenever the file is saved (or by some other similar mechanism)&lt;/li&gt;
&lt;li&gt;no major requirements on IDE (I am a big time VS Code user, and I don&amp;#39;t want to change my IDE just to write a little math)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqhdm6,True,,CallSignNovember,,21,True,all_ads,False,[],False,,/r/datascience/comments/nqhdm6/what_is_your_preferred_workflow_for_working_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqhdm6/what_is_your_preferred_workflow_for_working_with/,515405,1622624332.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys. Outside of one statistics class for stem majors that I took which involves python/jupyter notebooks and the likes, the rest of my statistics courses have been using R as the main programming language for our homework and projects. I'm a senior majoring in applied mathematics. I constantly see here that python is the ""now"" and R is being utilized less. 


Is this true? Should I just derp my way through the rest of these classes without much thought to learn it better so I can focus on getting better at python?",t2_20qhf3tk,False,,0,False,"Am I wasting my time learning ""R""",[],r/datascience,False,6,discussion,0,,,False,t3_npnf3s,False,dark,0.89,,public,134,0,{},,,False,[],,False,False,,{},Discussion,False,134,,False,False,self,False,,[],{},,True,,1622557443.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Outside of one statistics class for stem majors that I took which involves python/jupyter notebooks and the likes, the rest of my statistics courses have been using R as the main programming language for our homework and projects. I&amp;#39;m a senior majoring in applied mathematics. I constantly see here that python is the &amp;quot;now&amp;quot; and R is being utilized less. &lt;/p&gt;

&lt;p&gt;Is this true? Should I just derp my way through the rest of these classes without much thought to learn it better so I can focus on getting better at python?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npnf3s,True,,GeminiDavid,,132,True,all_ads,False,[],False,,/r/datascience/comments/npnf3s/am_i_wasting_my_time_learning_r/,all_ads,False,https://www.reddit.com/r/datascience/comments/npnf3s/am_i_wasting_my_time_learning_r/,515405,1622528643.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If a choice is given to work as a Data Analyst at a rapidly growing startup which would have responsibilities of an analytics+engineering as well as future progression into becoming second data scientist in the company, or to work as a Senior Data Scientist at an analytics and measurement company, what would you think would be a good choice?

If compensation is a factor in deciding, the data analyst offer could fetch somewhere around 80k+ (no data as the company was recently founded) and the senior data scientist would be around $120k (as per published data)",t2_a4yvnttd,False,,0,False,How to finalize job offer?,[],r/datascience,False,6,career,0,,,False,t3_nqcfya,False,dark,0.66,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1622633879.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If a choice is given to work as a Data Analyst at a rapidly growing startup which would have responsibilities of an analytics+engineering as well as future progression into becoming second data scientist in the company, or to work as a Senior Data Scientist at an analytics and measurement company, what would you think would be a good choice?&lt;/p&gt;

&lt;p&gt;If compensation is a factor in deciding, the data analyst offer could fetch somewhere around 80k+ (no data as the company was recently founded) and the senior data scientist would be around $120k (as per published data)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqcfya,True,,DietMediocre8993,,15,True,all_ads,False,[],False,,/r/datascience/comments/nqcfya/how_to_finalize_job_offer/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqcfya/how_to_finalize_job_offer/,515405,1622605079.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm looking for a variety of opinions on the following:

&amp;#x200B;

I'm looking to build a website that brings together a variety of data sources and presents them as dashboards. The business model is paid subscription for access to the dashboards.

&amp;#x200B;

For the Dashboards I'm trying to decide whether to use Plotly/Dash or PowerBI.

&amp;#x200B;

I feel that Dash gives me more flexibility but the time taken to build fully interactive dashboards is higher compared with PowerBI. Also the user authorization side is more complex (sticking to the free open source version).

&amp;#x200B;

On the flip side, I don't necessarily want a business that is reliant on a product of another company like Microsoft.

&amp;#x200B;

Anyone have any thoughts on this?",t2_a3797,False,,0,False,Setup for a Dashboard-based business,[],r/datascience,False,6,tooling,0,,,False,t3_nqd3gh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1622636118.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a variety of opinions on the following:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking to build a website that brings together a variety of data sources and presents them as dashboards. The business model is paid subscription for access to the dashboards.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For the Dashboards I&amp;#39;m trying to decide whether to use Plotly/Dash or PowerBI.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I feel that Dash gives me more flexibility but the time taken to build fully interactive dashboards is higher compared with PowerBI. Also the user authorization side is more complex (sticking to the free open source version).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On the flip side, I don&amp;#39;t necessarily want a business that is reliant on a product of another company like Microsoft.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyone have any thoughts on this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqd3gh,True,,yourfutureyesterday,,11,True,all_ads,False,[],False,,/r/datascience/comments/nqd3gh/setup_for_a_dashboardbased_business/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqd3gh/setup_for_a_dashboardbased_business/,515405,1622607318.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"What do you do when you have finished most (all) your tasks for the day?

I have been wondering how much actual work data analysts do per day on their 9-5 job. I know that some days will be very busy, but do you frequently have calm workdays?

How should/do you feel about it when you're supposedly working but have nothing to do. Is that a bad thing or is it normal to most companies (average company, not a FAANG or something)?",t2_7mkrswyv,False,,0,False,How do you feel when you have nothing to do at work.,[],r/datascience,False,6,discussion,0,,,False,t3_npuger,False,dark,0.91,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1622584089.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you do when you have finished most (all) your tasks for the day?&lt;/p&gt;

&lt;p&gt;I have been wondering how much actual work data analysts do per day on their 9-5 job. I know that some days will be very busy, but do you frequently have calm workdays?&lt;/p&gt;

&lt;p&gt;How should/do you feel about it when you&amp;#39;re supposedly working but have nothing to do. Is that a bad thing or is it normal to most companies (average company, not a FAANG or something)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npuger,True,,Ecstatic_Tooth_1096,,20,True,all_ads,False,[],False,,/r/datascience/comments/npuger/how_do_you_feel_when_you_have_nothing_to_do_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/npuger/how_do_you_feel_when_you_have_nothing_to_do_at/,515405,1622555289.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, I am going crazy trying to figure out the relationship between these big data technologies. I understand what they do but I cannot find anything that tells me the relevance of a particular one to a field or task.

Are they technologies used in combination? Is one better for a particular type of data? Are there limitations that means the choice is budget dependent? Or is it just matter of preference?

Many thanks to anyone who can point me in the right direction!",t2_6inkacm6,False,,0,False,"Relationship between no NoSQL, Hadoop and Data lakes.",[],r/datascience,False,6,discussion,0,,,False,t3_nq1fj2,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622602220.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am going crazy trying to figure out the relationship between these big data technologies. I understand what they do but I cannot find anything that tells me the relevance of a particular one to a field or task.&lt;/p&gt;

&lt;p&gt;Are they technologies used in combination? Is one better for a particular type of data? Are there limitations that means the choice is budget dependent? Or is it just matter of preference?&lt;/p&gt;

&lt;p&gt;Many thanks to anyone who can point me in the right direction!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nq1fj2,True,,Bumblebee-Impressive,,1,True,all_ads,False,[],False,,/r/datascience/comments/nq1fj2/relationship_between_no_nosql_hadoop_and_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nq1fj2/relationship_between_no_nosql_hadoop_and_data/,515405,1622573420.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey everyone,

after doing online courses I'm working my first guided project and I hope that I can transfer to unguided ones in Anaconda soon. My current project for practising is guided I find myself not remembering some solutions which I had in the courses eventhough I know I've solved a similar issue before.

I know it's common to have some kind of library for code for different problems. How do you store and manage these? I was thinking to maybe make a OneNote (Windows) while studying but I can imagine there are better ways? Especially when I switch to Linux I will need a new solution.",t2_zu1og,False,,0,False,How do you save and manage code for reuse?,[],r/datascience,False,6,discussion,0,,,False,t3_nptfed,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,1622552593.0,,[],{},,True,,1622581204.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;

&lt;p&gt;after doing online courses I&amp;#39;m working my first guided project and I hope that I can transfer to unguided ones in Anaconda soon. My current project for practising is guided I find myself not remembering some solutions which I had in the courses eventhough I know I&amp;#39;ve solved a similar issue before.&lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s common to have some kind of library for code for different problems. How do you store and manage these? I was thinking to maybe make a OneNote (Windows) while studying but I can imagine there are better ways? Especially when I switch to Linux I will need a new solution.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nptfed,True,,hugg3rs,,17,True,all_ads,False,[],False,,/r/datascience/comments/nptfed/how_do_you_save_and_manage_code_for_reuse/,all_ads,False,https://www.reddit.com/r/datascience/comments/nptfed/how_do_you_save_and_manage_code_for_reuse/,515405,1622552404.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[https://www.crosstab.io/articles/professionalizing-machine-learning](https://www.crosstab.io/articles/professionalizing-machine-learning)

I'm curious to hear what people think, especially about things I missed in my list. I'm sure there are some...",t2_arhctu9v,False,,0,False,A checklist for professionalizing machine learning models,[],r/datascience,False,6,discussion,0,,,False,t3_npyxks,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622595993.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.crosstab.io/articles/professionalizing-machine-learning""&gt;https://www.crosstab.io/articles/professionalizing-machine-learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious to hear what people think, especially about things I missed in my list. I&amp;#39;m sure there are some...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npyxks,True,,ctk_brian,,0,True,all_ads,False,[],False,,/r/datascience/comments/npyxks/a_checklist_for_professionalizing_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/npyxks/a_checklist_for_professionalizing_machine/,515405,1622567193.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?auto=webp&amp;s=49f8e795fd63a3d25b5e33174f01a405c1f73f08', 'width': 1160, 'height': 857}, 'resolutions': [{'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5c1aca4f9788a474b84ade5b600b930b984395f', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e6a9bf5b59cb97091af987cf5f2f821e264f96e', 'width': 216, 'height': 159}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12b3d78e03042038047e724746755fbe14d6d7f4', 'width': 320, 'height': 236}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbad8e01c9d5e74e5f9b7c5b806823da39d29eb2', 'width': 640, 'height': 472}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=81c84814d8f8fb06e841ecf106863c965f764300', 'width': 960, 'height': 709}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcdf632bd9ed3ca381f068b296b1c48c8048648f', 'width': 1080, 'height': 797}], 'variants': {}, 'id': 'DgIqH2aQ26rHZyiVgA8GCE9GS6FiEWAzhckDaNXuMu8'}], 'enabled': False}",,,,,
,datascience,"So I'm a data scientist with 2 years of experience, but I work only with traditional  ML, i.e., multiple regression, logistics regressions, regularized regression and clustering algorithms (kmeans and hierarchical for the most part.) I also don't have the opportunity to work with big data.

Since that is not going to change any time soon, I've decided to try to look for a better, less limited, DS opportunity. However, I'm struggling to find a role that doesn't require 5-10 years of experience using DL/AI and big data spec. I'm just wondering if anyone here has gone through a similar process, and if so, what advice would you give to someone in a similar position?

To add a bit to my background, I am currently completing my masters in computer science with a specialization in machine learning, have a lot of personal experience working with random forest (with boosting), SVMs, and some personal experience in DL. I am also currently completing the MLOps coursera course, which hopefully would give me some more experience in working in deploying ML models.",t2_1eoopt,False,,0,False,Any advice as to how to best transition into a more advanced data science role?,[],r/datascience,False,6,career,0,,,False,t3_npv75z,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1622586220.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m a data scientist with 2 years of experience, but I work only with traditional  ML, i.e., multiple regression, logistics regressions, regularized regression and clustering algorithms (kmeans and hierarchical for the most part.) I also don&amp;#39;t have the opportunity to work with big data.&lt;/p&gt;

&lt;p&gt;Since that is not going to change any time soon, I&amp;#39;ve decided to try to look for a better, less limited, DS opportunity. However, I&amp;#39;m struggling to find a role that doesn&amp;#39;t require 5-10 years of experience using DL/AI and big data spec. I&amp;#39;m just wondering if anyone here has gone through a similar process, and if so, what advice would you give to someone in a similar position?&lt;/p&gt;

&lt;p&gt;To add a bit to my background, I am currently completing my masters in computer science with a specialization in machine learning, have a lot of personal experience working with random forest (with boosting), SVMs, and some personal experience in DL. I am also currently completing the MLOps coursera course, which hopefully would give me some more experience in working in deploying ML models.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npv75z,True,,scun1995,,8,True,all_ads,False,[],False,,/r/datascience/comments/npv75z/any_advice_as_to_how_to_best_transition_into_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/npv75z/any_advice_as_to_how_to_best_transition_into_a/,515405,1622557420.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," Cheer everyone,

I just moved from Python to SAS for 4 months due to new job requirements. I wonder how you think SAS compared with other languages, any future.

Mine:

\- SAS is not so complex. The only problem is we have to memorize weird syntax

\- Lots of problem can be solved with proc sql. Unfortunately, proc sql has some different characteristics compared with standard SQL (e.g. why row\_number is missing in proc sql????). I likely use SAS mainly for practicing SQL.

\- The syntax is unique and not transferrable. If you're in SAS industry for too long, then it's likely hard to move to other jobs with different tool. Unlike if you know MATLAB or Python, you can easily move to R, or even C/C++ (They're interconnected with each other very well, SAS is a standalone hero)

\- Company uses SAS likely for security purposes (need an organization who is responsible for the tool if anything bad happened)

\- Then SAS Visual Analytics is another story and if you program for Advanced Filter in SAS Viya, then again it's more or less different systax compared with SAS Guide.

What's your thought?",t2_5czjyjhi,False,,0,False,What is your thought on SAS as a tool for data science,[],r/datascience,False,6,tooling,0,,,False,t3_np8uqk,False,dark,0.91,,public,137,0,{},,,False,[],,False,False,,{},Tooling,False,137,,False,False,self,False,,[],{},,True,,1622511481.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Cheer everyone,&lt;/p&gt;

&lt;p&gt;I just moved from Python to SAS for 4 months due to new job requirements. I wonder how you think SAS compared with other languages, any future.&lt;/p&gt;

&lt;p&gt;Mine:&lt;/p&gt;

&lt;p&gt;- SAS is not so complex. The only problem is we have to memorize weird syntax&lt;/p&gt;

&lt;p&gt;- Lots of problem can be solved with proc sql. Unfortunately, proc sql has some different characteristics compared with standard SQL (e.g. why row_number is missing in proc sql????). I likely use SAS mainly for practicing SQL.&lt;/p&gt;

&lt;p&gt;- The syntax is unique and not transferrable. If you&amp;#39;re in SAS industry for too long, then it&amp;#39;s likely hard to move to other jobs with different tool. Unlike if you know MATLAB or Python, you can easily move to R, or even C/C++ (They&amp;#39;re interconnected with each other very well, SAS is a standalone hero)&lt;/p&gt;

&lt;p&gt;- Company uses SAS likely for security purposes (need an organization who is responsible for the tool if anything bad happened)&lt;/p&gt;

&lt;p&gt;- Then SAS Visual Analytics is another story and if you program for Advanced Filter in SAS Viya, then again it&amp;#39;s more or less different systax compared with SAS Guide.&lt;/p&gt;

&lt;p&gt;What&amp;#39;s your thought?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,np8uqk,True,,vietlinh12hoa,,141,True,all_ads,False,[],False,,/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/,515405,1622482681.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi guys, 

I'm curious what websites you guys have used in the past for data science tutoring? What has been your experience with particular websites/services?

-Andrew",t2_1ns77nex,False,,0,False,Tutoring for Data Science,[],r/datascience,False,6,education,0,,,False,t3_npt96d,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Education,False,4,,False,False,self,False,seniorflair,[],{},,True,,1622580655.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious what websites you guys have used in the past for data science tutoring? What has been your experience with particular websites/services?&lt;/p&gt;

&lt;p&gt;-Andrew&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,MBA/MS (Candidate) | Student,[],False,,,,t5_2sptq,,,,npt96d,True,,DJAlaskaAndrew,,2,True,all_ads,False,[],False,dark,/r/datascience/comments/npt96d/tutoring_for_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/npt96d/tutoring_for_data_science/,515405,1622551855.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hey all! I am trying to draw bounding boxes on arrays I have. For a 2-D set it's pretty simple (I follow a process detailed here: https://stackoverflow.com/a/67784869/9345615). However, my problem is now I may have much higher-dimension data. 

Is there an accepted method of drawing bounding boxes on N-dim data, that scales well to high N?

For example, if I had a 5D array, that was filled with 1's, but with 2 separate areas filled with 100's, how can I find and draw boundary boxes around them?

I'm really open to any idea. So far I'm using image filters for the 2D examples, but for the N-dim case I'm not sure how well this will apply? Or if those filters really make sense to be used on Dim&gt;2. It's hard for me to imagine as well an NN-esque approach (think Yolo), since I will only have a few arrays, so training would be an issue.",t2_dt0g2,False,,0,False,How to find and draw a high dimensional bounding box,[],r/datascience,False,6,discussion,0,,,False,t3_npuamu,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622583600.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all! I am trying to draw bounding boxes on arrays I have. For a 2-D set it&amp;#39;s pretty simple (I follow a process detailed here: &lt;a href=""https://stackoverflow.com/a/67784869/9345615""&gt;https://stackoverflow.com/a/67784869/9345615&lt;/a&gt;). However, my problem is now I may have much higher-dimension data. &lt;/p&gt;

&lt;p&gt;Is there an accepted method of drawing bounding boxes on N-dim data, that scales well to high N?&lt;/p&gt;

&lt;p&gt;For example, if I had a 5D array, that was filled with 1&amp;#39;s, but with 2 separate areas filled with 100&amp;#39;s, how can I find and draw boundary boxes around them?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m really open to any idea. So far I&amp;#39;m using image filters for the 2D examples, but for the N-dim case I&amp;#39;m not sure how well this will apply? Or if those filters really make sense to be used on Dim&amp;gt;2. It&amp;#39;s hard for me to imagine as well an NN-esque approach (think Yolo), since I will only have a few arrays, so training would be an issue.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npuamu,True,,vaaalbara,,3,True,all_ads,False,[],False,,/r/datascience/comments/npuamu/how_to_find_and_draw_a_high_dimensional_bounding/,all_ads,False,https://www.reddit.com/r/datascience/comments/npuamu/how_to_find_and_draw_a_high_dimensional_bounding/,515405,1622554800.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",,,,,
,datascience,"I got this assessment today and just wanted to get an opinion on the difficulty level. Is it similar to LeetCode SQL Easy/Medium/Hard  or is it a totally different level. Trying to get a feel for what to expect in the 75 minute period.

TIA",t2_118kaw,False,,0,False,Have any of you taked the 'Qualified' SQL challenge as part of interview process for Zoom,[],r/datascience,False,6,,0,,,False,t3_npzaky,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622596904.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got this assessment today and just wanted to get an opinion on the difficulty level. Is it similar to LeetCode SQL Easy/Medium/Hard  or is it a totally different level. Trying to get a feel for what to expect in the 75 minute period.&lt;/p&gt;

&lt;p&gt;TIA&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,npzaky,True,,paglaindian,,0,True,all_ads,False,[],False,,/r/datascience/comments/npzaky/have_any_of_you_taked_the_qualified_sql_challenge/,all_ads,False,https://www.reddit.com/r/datascience/comments/npzaky/have_any_of_you_taked_the_qualified_sql_challenge/,515405,1622568104.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_q7fwt,False,,0,False,[Q] Bayesian statistics: how to introduce posterior-probability interpretation in the body an article?,[],r/datascience,False,6,discussion,0,,,False,t3_npxh8b,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,True,default,False,,[],{},,False,,1622592328.0,text,6,,,text,self.statistics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npxh8b,True,,vvvvalvalval,,0,True,all_ads,False,[],False,,/r/datascience/comments/npxh8b/q_bayesian_statistics_how_to_introduce/,all_ads,False,/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/,515405,1622563528.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'statistics', 'selftext': 'I\'m in the process of writing various Bayesian data analyses, and I feel it is required to **add a quick guide to Bayesian statistics within each article**, to help readers in their interpretation (especially if they have little or no statistical training).\n\nI was wondering if people here know good resources for that. It would be good to have some ready-to-use prose to include in each analysis.\n\n&amp;#x200B;\n\nIdeally, the content I\'d add to a data analysis would be something like this:\n\n**A- Below each major posterior-based result (e.g a 99% credible interval), a few sentences to help interpret the 99%.** For instance:\n\n&gt;***How to interpret:*** according to our model and prior assumptions, we have inferred that **with 99% credibility, the Effect Size lies between 1.8 and 7.8 points.** The Effect Size is the treatment\'s average reduction in perceived pain level compared to a placebo.  \n&gt;  \n&gt;**-** The phrase *\'with 99% credibility\'* may be understood as follows: given the results, a rational person who believes in the model and prior assumptions would be willing to bet 99-to-1 that the Effect Size lies in interval \\[1.8, 7.8\\]\n\n&amp;#x200B;\n\nB- As an appendix, a brief **motivation of the use of Bayesian statistics** rather than classical frequentist statistics, featuring:\n\n1. Evidence and explanation for frequentist results such as p-values and confidence intervals being commonly misinterpreted (in particular, being interpreted as Bayesian posterior probabilities).\n2. Brief explanation / references for grasping the philosophical implications of Bayesian probability (along the lines of: ""you have to be OK with using probability as a measure of subjective uncertainty and beliefs. As we saw above, most people do that intuitively.""), and the related tradeoffs and controversy.\n3. References for learning Bayesian statistics at various technical depths.\n\n&amp;#x200B;\n\nIf I\'m being facetious:\n\n*""Most people should interpret posterior probabilities like they usually interpret p-values. Except that this time they\'ll be correct.""*', 'author_fullname': 't2_q7fwt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Q] Bayesian statistics: how to introduce posterior-probability interpretation in the body an article?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/statistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_npu58g', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622583166.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.statistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of writing various Bayesian data analyses, and I feel it is required to &lt;strong&gt;add a quick guide to Bayesian statistics within each article&lt;/strong&gt;, to help readers in their interpretation (especially if they have little or no statistical training).&lt;/p&gt;\n\n&lt;p&gt;I was wondering if people here know good resources for that. It would be good to have some ready-to-use prose to include in each analysis.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ideally, the content I&amp;#39;d add to a data analysis would be something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A- Below each major posterior-based result (e.g a 99% credible interval), a few sentences to help interpret the 99%.&lt;/strong&gt; For instance:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How to interpret:&lt;/em&gt;&lt;/strong&gt; according to our model and prior assumptions, we have inferred that &lt;strong&gt;with 99% credibility, the Effect Size lies between 1.8 and 7.8 points.&lt;/strong&gt; The Effect Size is the treatment&amp;#39;s average reduction in perceived pain level compared to a placebo.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; The phrase &lt;em&gt;&amp;#39;with 99% credibility&amp;#39;&lt;/em&gt; may be understood as follows: given the results, a rational person who believes in the model and prior assumptions would be willing to bet 99-to-1 that the Effect Size lies in interval [1.8, 7.8]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;B- As an appendix, a brief &lt;strong&gt;motivation of the use of Bayesian statistics&lt;/strong&gt; rather than classical frequentist statistics, featuring:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Evidence and explanation for frequentist results such as p-values and confidence intervals being commonly misinterpreted (in particular, being interpreted as Bayesian posterior probabilities).&lt;/li&gt;\n&lt;li&gt;Brief explanation / references for grasping the philosophical implications of Bayesian probability (along the lines of: &amp;quot;you have to be OK with using probability as a measure of subjective uncertainty and beliefs. As we saw above, most people do that intuitively.&amp;quot;), and the related tradeoffs and controversy.&lt;/li&gt;\n&lt;li&gt;References for learning Bayesian statistics at various technical depths.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m being facetious:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;&amp;quot;Most people should interpret posterior probabilities like they usually interpret p-values. Except that this time they&amp;#39;ll be correct.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'npu58g', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'vvvvalvalval', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/', 'subreddit_subscribers': 310857, 'created_utc': 1622554366.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/,t3_npu58g,
,datascience,"Hey, I'm looking for a solution, open source docker deployable service.

What I need to do is stream data from several mysql db's to a central postgres db. Polling once every 30-min. So performance and query size should matter. 

Some of the tables in the mysql db do not have a date on them.

Now initially I planned to join the 6 tables in one of the db's to make a view of 130 columns, in order to get the date data in. Then SELECT * FROM table WHERE date=last 1 day. This should give me an option to limit the query by filtering it. 

One can quickly see how the massive view will lag the origin server, especially if I poll it every 30 mins. There are plenty of NULL values in there but that's beside the point. Its the best solution I can think of at the moment. Correct me if I'm wrong about the filtering performance and if its actually sensible. 

Alternatively I can stream individual tables but as mentioned above, there isn't any date column to filter, so it'd cause even more of a performance bottleneck to select all rows for each table at every update. 

I initially used airflow to achieve this, by writing the data to a csv as a staging area for loading on to pg.

I've also looked at airbyte but they seem to be pretty early stage, though correct me if I'm wrong.

Could be difficult to use a mysql slave db as the two db's are different types. I also don't think there's probably sufficient infrastructure currently to handle several additional slave db's. 

What would you do with the scenario above?",t2_xrizd,False,,0,False,Data streaming question,[],r/datascience,False,6,discussion,0,,,False,t3_npwyg0,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622590979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;m looking for a solution, open source docker deployable service.&lt;/p&gt;

&lt;p&gt;What I need to do is stream data from several mysql db&amp;#39;s to a central postgres db. Polling once every 30-min. So performance and query size should matter. &lt;/p&gt;

&lt;p&gt;Some of the tables in the mysql db do not have a date on them.&lt;/p&gt;

&lt;p&gt;Now initially I planned to join the 6 tables in one of the db&amp;#39;s to make a view of 130 columns, in order to get the date data in. Then SELECT * FROM table WHERE date=last 1 day. This should give me an option to limit the query by filtering it. &lt;/p&gt;

&lt;p&gt;One can quickly see how the massive view will lag the origin server, especially if I poll it every 30 mins. There are plenty of NULL values in there but that&amp;#39;s beside the point. Its the best solution I can think of at the moment. Correct me if I&amp;#39;m wrong about the filtering performance and if its actually sensible. &lt;/p&gt;

&lt;p&gt;Alternatively I can stream individual tables but as mentioned above, there isn&amp;#39;t any date column to filter, so it&amp;#39;d cause even more of a performance bottleneck to select all rows for each table at every update. &lt;/p&gt;

&lt;p&gt;I initially used airflow to achieve this, by writing the data to a csv as a staging area for loading on to pg.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also looked at airbyte but they seem to be pretty early stage, though correct me if I&amp;#39;m wrong.&lt;/p&gt;

&lt;p&gt;Could be difficult to use a mysql slave db as the two db&amp;#39;s are different types. I also don&amp;#39;t think there&amp;#39;s probably sufficient infrastructure currently to handle several additional slave db&amp;#39;s. &lt;/p&gt;

&lt;p&gt;What would you do with the scenario above?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npwyg0,True,,ApocalypseAce,,2,True,all_ads,False,[],False,,/r/datascience/comments/npwyg0/data_streaming_question/,all_ads,False,https://www.reddit.com/r/datascience/comments/npwyg0/data_streaming_question/,515405,1622562179.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let me start this off with a disclaimer that I'm still a beginner in data science, and I haven't been exposed to many projects. So far, the real life projects I've worked on concerned time-series data (energy demand and price forcasting + supply optimization, quantitative analysis for algorithmic trading), which I imagined to be heavy on stats before I began.

However, after I've worked on those projects, I feel like knowing or not knowing stats doesn't really affect my ability to complete the projects. I'm still able to analyze the data well, gather actionable insights and build models around those insights to optimize processes.

For example, linear regression is constrained by its assumptions, so what? We want stationary time-series data for ARIMA-based models so sample statistics are not time-dependent, so what? Bayesian methods like MCMC allow us to sample a distribution similar to the actual distribution due to the law of large numbers, so what?

Don't get me wrong, I really like learning about statistics as it fills me with a wondrous sense of appreciation every time I understand the underlying reasons behind why certain models work. But I'm just really curious as to why statistical knowledge is so valued in this field when (based on my experience so far) it doesn't really affect the quality of your work as long as you know WHEN and HOW to use statistical tools/models (even if you don't know WHY).",t2_l7hoj,False,,0,False,"How important is knowledge of statistics, really?",[],r/datascience,False,6,discussion,0,,,False,t3_npupwg,False,dark,0.47,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1622557969.0,,[],{},,True,,1622584876.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let me start this off with a disclaimer that I&amp;#39;m still a beginner in data science, and I haven&amp;#39;t been exposed to many projects. So far, the real life projects I&amp;#39;ve worked on concerned time-series data (energy demand and price forcasting + supply optimization, quantitative analysis for algorithmic trading), which I imagined to be heavy on stats before I began.&lt;/p&gt;

&lt;p&gt;However, after I&amp;#39;ve worked on those projects, I feel like knowing or not knowing stats doesn&amp;#39;t really affect my ability to complete the projects. I&amp;#39;m still able to analyze the data well, gather actionable insights and build models around those insights to optimize processes.&lt;/p&gt;

&lt;p&gt;For example, linear regression is constrained by its assumptions, so what? We want stationary time-series data for ARIMA-based models so sample statistics are not time-dependent, so what? Bayesian methods like MCMC allow us to sample a distribution similar to the actual distribution due to the law of large numbers, so what?&lt;/p&gt;

&lt;p&gt;Don&amp;#39;t get me wrong, I really like learning about statistics as it fills me with a wondrous sense of appreciation every time I understand the underlying reasons behind why certain models work. But I&amp;#39;m just really curious as to why statistical knowledge is so valued in this field when (based on my experience so far) it doesn&amp;#39;t really affect the quality of your work as long as you know WHEN and HOW to use statistical tools/models (even if you don&amp;#39;t know WHY).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npupwg,True,,ReaperJr,,19,True,all_ads,False,[],False,,/r/datascience/comments/npupwg/how_important_is_knowledge_of_statistics_really/,all_ads,False,https://www.reddit.com/r/datascience/comments/npupwg/how_important_is_knowledge_of_statistics_really/,515405,1622556076.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been using R, python for data science for about 10 years, and I have a phd in industrial engineering. 

We all know this is a field requires constant learning. 

For example, I am starting to work on a time-series clusterings project, which is totally new to me. Concept like dynamic time warping is so great, however I found it difficult to dig into the algorithm. I  ended up just calling some package to solve the problem rather than trying to understand it inside out. 

Science is progressing fast and it's very difficult for me to stay on top of everything. What's your experience on keep yourself relevant on the latest and greatest things?",t2_12x43b,False,,0,False,continuous learning advice,[],r/datascience,False,6,discussion,0,,,False,t3_npa881,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622515219.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been using R, python for data science for about 10 years, and I have a phd in industrial engineering. &lt;/p&gt;

&lt;p&gt;We all know this is a field requires constant learning. &lt;/p&gt;

&lt;p&gt;For example, I am starting to work on a time-series clusterings project, which is totally new to me. Concept like dynamic time warping is so great, however I found it difficult to dig into the algorithm. I  ended up just calling some package to solve the problem rather than trying to understand it inside out. &lt;/p&gt;

&lt;p&gt;Science is progressing fast and it&amp;#39;s very difficult for me to stay on top of everything. What&amp;#39;s your experience on keep yourself relevant on the latest and greatest things?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npa881,True,,janicewa,,7,True,all_ads,False,[],False,,/r/datascience/comments/npa881/continuous_learning_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/npa881/continuous_learning_advice/,515405,1622486419.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Title basically says it all. I'm wrapping up a PhD in [computational biology field] and starting to think about what's next for me. I don't really want to stay in academia at this point: the odds of getting the fabled tenure track jobs are low and I'm pushing 30 so I haven less interest in bouncing around post-doc to post-doc until getting a TT or burning out. 

A lot of my friends who graduated before me went the Data Science route - they're making good money (much better then we made as graduate students or would make as Tenure Track Profs) but the work just seems so *boring.* Instead of wrangling with interesting data types and trying to solve interesting problems, a lot of it seems to be basically financial or behavioral user data, and the goal is to deliver ""actionable business insights"", which always seems to boil down to optimizing profit-to-cost ratio. Far less of the interesting questions about mathematics and inference that pulled me into computational modeling and a lot more focus on business, learning how to pitch ideas to managers, etc. 

I don't give a d*mn about that, and kind of chafe at the idea of using skills I spent 6 years developing at the cutting edge of scientific research to help make already-wealthy investors in a company richer. For context, my thesis research involves developing a very niche kind of computational model to explore distributed information processing in biological systems that I know has absolutely no relevance to anything in the world of business or finance.",t2_ad5yokml,False,,0,False,Wrapping up a data-intensive PhD but most industry data science seems really boring. Are there interesting jobs?,[],r/datascience,False,6,career,0,,,False,t3_nobqjn,False,dark,0.91,,public,274,1,{},,,False,[],,False,False,,{},Career,False,274,,False,False,self,False,,[],{},,True,,1622411878.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title basically says it all. I&amp;#39;m wrapping up a PhD in [computational biology field] and starting to think about what&amp;#39;s next for me. I don&amp;#39;t really want to stay in academia at this point: the odds of getting the fabled tenure track jobs are low and I&amp;#39;m pushing 30 so I haven less interest in bouncing around post-doc to post-doc until getting a TT or burning out. &lt;/p&gt;

&lt;p&gt;A lot of my friends who graduated before me went the Data Science route - they&amp;#39;re making good money (much better then we made as graduate students or would make as Tenure Track Profs) but the work just seems so &lt;em&gt;boring.&lt;/em&gt; Instead of wrangling with interesting data types and trying to solve interesting problems, a lot of it seems to be basically financial or behavioral user data, and the goal is to deliver &amp;quot;actionable business insights&amp;quot;, which always seems to boil down to optimizing profit-to-cost ratio. Far less of the interesting questions about mathematics and inference that pulled me into computational modeling and a lot more focus on business, learning how to pitch ideas to managers, etc. &lt;/p&gt;

&lt;p&gt;I don&amp;#39;t give a d*mn about that, and kind of chafe at the idea of using skills I spent 6 years developing at the cutting edge of scientific research to help make already-wealthy investors in a company richer. For context, my thesis research involves developing a very niche kind of computational model to explore distributed information processing in biological systems that I know has absolutely no relevance to anything in the world of business or finance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nobqjn,True,,antichain,,144,True,all_ads,False,[],False,,/r/datascience/comments/nobqjn/wrapping_up_a_dataintensive_phd_but_most_industry/,all_ads,False,https://www.reddit.com/r/datascience/comments/nobqjn/wrapping_up_a_dataintensive_phd_but_most_industry/,515405,1622383078.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Currently I'm working in civil engineering, and it has a pretty conservative culture. I'm a young queer lady and I had been closeted at work but I'm about to get married and I can imagine uncomfortable questions. 

I have the opportunity to get a masters in data science largely paid for. I'm wondering if a career pivot would increase the chances of having a queer friendly work place. What's the culture like in a data science career?

And yes I know this should be protected but with at will employment laws its harder to build a case. Also I think I'd enjoy working in an environment without racists, transphobes, climate change deniers and antivaxers.",t2_aycyec16,False,,0,False,What's the culture like in data science? Progressive?,[],r/datascience,False,6,career,0,,,False,t3_nompja,False,dark,0.62,,public,21,0,{},,,False,[],,False,False,,{},Career,False,21,,False,False,self,False,,[],{},,True,,1622443998.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I&amp;#39;m working in civil engineering, and it has a pretty conservative culture. I&amp;#39;m a young queer lady and I had been closeted at work but I&amp;#39;m about to get married and I can imagine uncomfortable questions. &lt;/p&gt;

&lt;p&gt;I have the opportunity to get a masters in data science largely paid for. I&amp;#39;m wondering if a career pivot would increase the chances of having a queer friendly work place. What&amp;#39;s the culture like in a data science career?&lt;/p&gt;

&lt;p&gt;And yes I know this should be protected but with at will employment laws its harder to build a case. Also I think I&amp;#39;d enjoy working in an environment without racists, transphobes, climate change deniers and antivaxers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nompja,True,,happybabylizard,,64,True,all_ads,False,[],False,,/r/datascience/comments/nompja/whats_the_culture_like_in_data_science_progressive/,all_ads,False,https://www.reddit.com/r/datascience/comments/nompja/whats_the_culture_like_in_data_science_progressive/,515405,1622415198.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am interested to know how often do people participate in Kaggle competitions, and when do they get to work on the competition's code.

I have a full-time data science/research position so I am usually occupied during the weekdays. In the weekends I just like to take sometime off-the-screen and do outdoor activities (hiking, cycling, chilling out). I am keen on getting more engaged in Kaggle competitions so I can have a broader knowledge and more career opportunities in the future. However, I am concerned that I will burn out, having to stay coding behind the computer screen 7 days a week. My position is a priority for sure, and I don't want my work to be affected because I was not properly taking a time off when I am supposed to. How do other people handle such a situation?

Tl;dr: I am interested in becoming more active on Kaggle, but I am concerned that I will burn myself out. What are your tips?",t2_3vsv0v1p,False,,0,False,Kaggle and burnouts,[],r/datascience,False,6,discussion,0,,,False,t3_no7s9w,False,dark,0.96,,public,177,1,{},,,False,[],,False,False,,{},Discussion,False,177,,False,False,self,False,,[],{},,True,,1622396718.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am interested to know how often do people participate in Kaggle competitions, and when do they get to work on the competition&amp;#39;s code.&lt;/p&gt;

&lt;p&gt;I have a full-time data science/research position so I am usually occupied during the weekdays. In the weekends I just like to take sometime off-the-screen and do outdoor activities (hiking, cycling, chilling out). I am keen on getting more engaged in Kaggle competitions so I can have a broader knowledge and more career opportunities in the future. However, I am concerned that I will burn out, having to stay coding behind the computer screen 7 days a week. My position is a priority for sure, and I don&amp;#39;t want my work to be affected because I was not properly taking a time off when I am supposed to. How do other people handle such a situation?&lt;/p&gt;

&lt;p&gt;Tl;dr: I am interested in becoming more active on Kaggle, but I am concerned that I will burn myself out. What are your tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no7s9w,True,,straightbackward,,63,True,all_ads,False,[],False,,/r/datascience/comments/no7s9w/kaggle_and_burnouts/,all_ads,False,https://www.reddit.com/r/datascience/comments/no7s9w/kaggle_and_burnouts/,515405,1622367918.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,Just curious!,t2_5qn6m6wc,False,,0,False,"Did anyone here get their company to pay for their masters in a field of data science? If so, where did you work and what did you do?",[],r/datascience,False,6,education,0,,,False,t3_noe1fg,False,dark,0.9,,public,57,0,{},,,False,[],,False,False,,{},Education,False,57,,False,False,self,False,,[],{},,True,,1622418826.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just curious!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,noe1fg,True,,questions2067,,61,True,all_ads,False,[],False,,/r/datascience/comments/noe1fg/did_anyone_here_get_their_company_to_pay_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/noe1fg/did_anyone_here_get_their_company_to_pay_for/,515405,1622390026.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming. In fact my peers too seem like years ahead of me when it comes to knowledge and implementation. 

Curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something. Or if you still feel this way. I want to know your experience.",t2_7mksj1z1,False,,0,False,Data Science field is overwhelming,[],r/datascience,False,6,discussion,0,,,False,t3_no7t46,False,dark,0.81,,public,50,0,{},,,False,[],,False,False,,{},Discussion,False,50,,False,False,self,False,,[],{},,True,,1622396821.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming. In fact my peers too seem like years ahead of me when it comes to knowledge and implementation. &lt;/p&gt;

&lt;p&gt;Curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something. Or if you still feel this way. I want to know your experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no7t46,True,,HyunjinsPeach,,28,True,all_ads,False,[],False,,/r/datascience/comments/no7t46/data_science_field_is_overwhelming/,all_ads,False,https://www.reddit.com/r/datascience/comments/no7t46/data_science_field_is_overwhelming/,515405,1622368021.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_onq0,False,,0,False,ProteinBERT: A universal deep-learning model of protein sequence and function,[],r/datascience,False,6,projects,0,,,False,t3_nofgvu,False,dark,0.67,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,default,False,seniorflair,[],{},,False,,1622423034.0,text,6,,,text,self.bioinformatics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,MSC | Data Scientist | Bioinformatics &amp; AI,[],False,,,,t5_2sptq,,,,nofgvu,True,,ddofer,,0,True,all_ads,False,[],False,dark,/r/datascience/comments/nofgvu/proteinbert_a_universal_deeplearning_model_of/,all_ads,False,/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/,515405,1622394234.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?auto=webp&amp;s=a01d53e28ea2703748a93f24b9c36be28502973a', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9535b1ec3074f9be1d5ad488f32afcfa7fa4917f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11e566015051d306cd191c9c83b55533591d7c07', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'ZEtHsU9C8_lJyLY_pv_-HcDEP3BCZ770IN4DQxggJJs'}], 'enabled': False}",,"[{'approved_at_utc': None, 'subreddit': 'bioinformatics', 'selftext': ""# ProteinBERT: A universal deep-learning model of protein sequence and function\n\n&gt;Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal\n\nPaper: [https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1](https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1)\n\nTL;DR:\n\n&gt;Deep learning language models (like BERT in NLP) but for proteins!  \n&gt;  \n&gt;We trained a model on over 100 million proteins to predict their sequence and GO annotations (i.e their functions and properties). We show \\~SOTA performance on a wide range of benchmarks. Our model is much smaller and faster than comparable works (TAPE, ESM), and is quite interpretable thanks to our global attention. We provide the pretrained models and code, in a simple Keras/Tensorflow Python package.\n\nCode &amp; pretrained models:\n\n[https://github.com/nadavbra/protein\\_bert](https://github.com/nadavbra/protein_bert)\n\n&amp;#x200B;\n\nI'm one of the authors, AMA! :)"", 'author_fullname': 't2_onq0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ProteinBERT: A universal deep-learning model of protein sequence and function', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/bioinformatics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'academic', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_no76jp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 91, 'total_awards_received': 3, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'academic', 'can_mod_post': False, 'score': 91, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {'gid_1': 2}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622394001.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.bioinformatics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;ProteinBERT: A universal deep-learning model of protein sequence and function&lt;/h1&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Paper: &lt;a href=""https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1""&gt;https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Deep learning language models (like BERT in NLP) but for proteins!  &lt;/p&gt;\n\n&lt;p&gt;We trained a model on over 100 million proteins to predict their sequence and GO annotations (i.e their functions and properties). We show ~SOTA performance on a wide range of benchmarks. Our model is much smaller and faster than comparable works (TAPE, ESM), and is quite interpretable thanks to our global attention. We provide the pretrained models and code, in a simple Keras/Tensorflow Python package.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Code &amp;amp; pretrained models:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/nadavbra/protein_bert""&gt;https://github.com/nadavbra/protein_bert&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m one of the authors, AMA! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?auto=webp&amp;s=a01d53e28ea2703748a93f24b9c36be28502973a', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9535b1ec3074f9be1d5ad488f32afcfa7fa4917f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11e566015051d306cd191c9c83b55533591d7c07', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'ZEtHsU9C8_lJyLY_pv_-HcDEP3BCZ770IN4DQxggJJs'}], 'enabled': False}, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}], 'awarders': [], 'media_only': False, 'link_flair_template_id': '12168aa0-7f51-11e4-8866-22000b3396c4', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh0x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'no76jp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ddofer', 'discussion_type': None, 'num_comments': 36, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/', 'subreddit_subscribers': 65941, 'created_utc': 1622365201.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/,t3_no76jp,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 30 May 2021 - 06 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_no9q3m,False,dark,0.92,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1622404831.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,no9q3m,True,,datascience-bot,,153,False,all_ads,False,[],False,dark,/r/datascience/comments/no9q3m/weekly_entering_transitioning_thread_30_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/no9q3m/weekly_entering_transitioning_thread_30_may_2021/,515405,1622376031.0,0,,False,,,,,,,,
,datascience,"I've been lurking around this subreddit since I started my final year project, a facial recognition project. I fell in love with data science overall and the stuff I was discovering every day. I lost interest in software engineering and embedded engineering a year ago, and I've been looking for the field that I would be happy to go into, and DS was the one. I've started applying for graduate jobs to become a data scientist, and I'm starting on a side project soon to boost my profile a bit more. I wondered how hard it would be when looking for jobs in DS, primarily when you haven't studied the course directly.",t2_nrpje,False,,0,False,Finally graduated from computer engineering,[],r/datascience,False,6,career,0,,,False,t3_nnqrta,False,dark,0.95,,public,230,1,{},,,False,[],,False,False,,{},Career,False,230,,False,False,self,False,,[],{},,True,,1622333848.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been lurking around this subreddit since I started my final year project, a facial recognition project. I fell in love with data science overall and the stuff I was discovering every day. I lost interest in software engineering and embedded engineering a year ago, and I&amp;#39;ve been looking for the field that I would be happy to go into, and DS was the one. I&amp;#39;ve started applying for graduate jobs to become a data scientist, and I&amp;#39;m starting on a side project soon to boost my profile a bit more. I wondered how hard it would be when looking for jobs in DS, primarily when you haven&amp;#39;t studied the course directly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnqrta,True,,JeffTheSpider,,34,True,all_ads,False,[],False,,/r/datascience/comments/nnqrta/finally_graduated_from_computer_engineering/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnqrta/finally_graduated_from_computer_engineering/,515405,1622305048.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Is there any open source work for someone who has no prior experience in making tools or any contributions? Someone who welcomes beginners and helps them or guides them to get better in contributing and improve collaboration skills.,t2_34qgdyb6,False,,0,False,Open-source work in data science for a newbie.,[],r/datascience,False,6,projects,0,,,False,t3_no77f5,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},Projects,False,6,,False,False,self,False,,[],{},,True,,1622394085.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any open source work for someone who has no prior experience in making tools or any contributions? Someone who welcomes beginners and helps them or guides them to get better in contributing and improve collaboration skills.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no77f5,True,,yaakarsh1011,,8,True,all_ads,False,[],False,,/r/datascience/comments/no77f5/opensource_work_in_data_science_for_a_newbie/,all_ads,False,https://www.reddit.com/r/datascience/comments/no77f5/opensource_work_in_data_science_for_a_newbie/,515405,1622365285.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I've started working as a data analyst at a company where I used to work in a non-techical role. They didn't have an established data team. Instead myself and my manager (we both graduated last year). Great opportunity for me, but I'm being asked to write a pretty big app in Python and I'm really struggling with it from a technical ""know-how"" standpoint.   
I feel like maybe I'm not cut out for data work, but also understand that imposter syndrome is a thing. Just looking for advice from people who have spent a longer time in the field. Are there moments where you're completely over your head? Do you just try and work through it?   
Thanks team.",t2_7qw6e,False,,0,False,When is it imposter syndrome?,[],r/datascience,False,6,career,0,,,False,t3_no9tq4,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1622405200.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve started working as a data analyst at a company where I used to work in a non-techical role. They didn&amp;#39;t have an established data team. Instead myself and my manager (we both graduated last year). Great opportunity for me, but I&amp;#39;m being asked to write a pretty big app in Python and I&amp;#39;m really struggling with it from a technical &amp;quot;know-how&amp;quot; standpoint.&lt;br/&gt;
I feel like maybe I&amp;#39;m not cut out for data work, but also understand that imposter syndrome is a thing. Just looking for advice from people who have spent a longer time in the field. Are there moments where you&amp;#39;re completely over your head? Do you just try and work through it?&lt;br/&gt;
Thanks team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no9tq4,True,,Gusdafamon,,10,True,all_ads,False,[],False,,/r/datascience/comments/no9tq4/when_is_it_imposter_syndrome/,all_ads,False,https://www.reddit.com/r/datascience/comments/no9tq4/when_is_it_imposter_syndrome/,515405,1622376400.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi there.

I am building a website with a rating system and I was wondering if there are any highly recommendable books that could help with designing it.

I'm interested in anything ranging from details such as designing algorithms to deal with items with fewer ratings having higher average scores (as in, an item with a single 10 rating being ranked higher than an item with a thousand 9 votes) or the arguments behind different rating systems (10 stars, 5 stars, thumbs up/down, subcategory ratings, etc) to even more disperse topics such as adapting the math to the user psychology behind it all (such as how to adapt the system to  nostalgia votes or how to separate evaluating how impactful or revolutionary a product was when released from how good or recommendable it is today).

I am also really interested in the implications that those different rating system designs would have for a inter-user recommender/affinity system, but I understand that might be a tad specific \^\^

&amp;#x200B;

I am ok with technical reads and know some machine learning (I've done the Stanford course on Coursera), but I am not a data scientist, so I'd prefer it if the books/resources were reasonably accessible (though if a dense book is an absolute staple it would be good to know about it as well).

Cheers =)",t2_r3pn6,False,,0,False,Recommended resources for designing an item rating system?,[],r/datascience,False,6,education,0,,,False,t3_no33kc,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,1622352991.0,,[],{},,True,,1622376469.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there.&lt;/p&gt;

&lt;p&gt;I am building a website with a rating system and I was wondering if there are any highly recommendable books that could help with designing it.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in anything ranging from details such as designing algorithms to deal with items with fewer ratings having higher average scores (as in, an item with a single 10 rating being ranked higher than an item with a thousand 9 votes) or the arguments behind different rating systems (10 stars, 5 stars, thumbs up/down, subcategory ratings, etc) to even more disperse topics such as adapting the math to the user psychology behind it all (such as how to adapt the system to  nostalgia votes or how to separate evaluating how impactful or revolutionary a product was when released from how good or recommendable it is today).&lt;/p&gt;

&lt;p&gt;I am also really interested in the implications that those different rating system designs would have for a inter-user recommender/affinity system, but I understand that might be a tad specific ^^&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am ok with technical reads and know some machine learning (I&amp;#39;ve done the Stanford course on Coursera), but I am not a data scientist, so I&amp;#39;d prefer it if the books/resources were reasonably accessible (though if a dense book is an absolute staple it would be good to know about it as well).&lt;/p&gt;

&lt;p&gt;Cheers =)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no33kc,True,,HigoChumbo,,4,True,all_ads,False,[],False,,/r/datascience/comments/no33kc/recommended_resources_for_designing_an_item/,all_ads,False,https://www.reddit.com/r/datascience/comments/no33kc/recommended_resources_for_designing_an_item/,515405,1622347669.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"These are two areas that broadly, I’m interested in applying ML and DS techniques and skills to. I was wondering if some people could shed some light on some of the pros and cons of working in a machine learning or data science capacity in either of these areas. Thanks!",t2_7ciqyc7g,False,,0,False,Applying ML/DS for cybersecurity vs finance,[],r/datascience,False,6,discussion,0,,,False,t3_nnwyn7,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622353367.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;These are two areas that broadly, I’m interested in applying ML and DS techniques and skills to. I was wondering if some people could shed some light on some of the pros and cons of working in a machine learning or data science capacity in either of these areas. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnwyn7,True,,m4mancy,,2,True,all_ads,False,[],False,,/r/datascience/comments/nnwyn7/applying_mlds_for_cybersecurity_vs_finance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnwyn7/applying_mlds_for_cybersecurity_vs_finance/,515405,1622324567.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"As part of a bigger project, I'm looking to put some effort into open sourcing a data sourcing and data collaboration cli tool.  The utility of the tool has been great limited enterprise and research settings as a sort of a shadow IT tool that replaces ""emailing CSVs"" around"".

Please note, I'm not a data scientists or engineer so I'm looking to understand this use case further.  Thanks.

Edit: 
If possible, when commenting, could you include organztion context. For instance, enterprise, startup, tech, research or other.",t2_dnelb,False,,0,False,"What is your current process like to source, clean/prepare, and collaborate with datasets?",[],r/datascience,False,6,discussion,0,,,False,t3_nnlunt,False,dark,0.96,,public,26,0,{},,,False,[],,False,False,,{},Discussion,False,26,,False,False,self,1622293522.0,,[],{},,True,,1622317844.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As part of a bigger project, I&amp;#39;m looking to put some effort into open sourcing a data sourcing and data collaboration cli tool.  The utility of the tool has been great limited enterprise and research settings as a sort of a shadow IT tool that replaces &amp;quot;emailing CSVs&amp;quot; around&amp;quot;.&lt;/p&gt;

&lt;p&gt;Please note, I&amp;#39;m not a data scientists or engineer so I&amp;#39;m looking to understand this use case further.  Thanks.&lt;/p&gt;

&lt;p&gt;Edit: 
If possible, when commenting, could you include organztion context. For instance, enterprise, startup, tech, research or other.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnlunt,True,,adgezaza87,,14,True,all_ads,False,[],False,,/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/,515405,1622289044.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Today, I got my first paycheck from my first internship and I am shocked about the entire situation. I come from a poor family, I am the first of my family to college (and grad-school) and the first to have a real professional work experience. I honestly feel blessed to be able to improve on my data science abilities and get paid for it! 

I have been working with the lead data scientist and have learned so much in these past two weeks. I enjoy coming to work and even more so now that I saw the paycheck. 

Sorry for the weird post, but I am just in a good mood right now. 

P.s. My boss asked me if I want to continue my internship for the Fall",t2_51tyvidz,False,,0,False,First two weeks of my first internship,[],r/datascience,False,6,career,0,,,False,t3_nmyg3i,False,dark,0.97,,public,901,8,{},,,False,[],,False,False,,{},Career,False,901,,False,False,self,False,,[],{'gid_1': 1},,True,,1622239176.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Today, I got my first paycheck from my first internship and I am shocked about the entire situation. I come from a poor family, I am the first of my family to college (and grad-school) and the first to have a real professional work experience. I honestly feel blessed to be able to improve on my data science abilities and get paid for it! &lt;/p&gt;

&lt;p&gt;I have been working with the lead data scientist and have learned so much in these past two weeks. I enjoy coming to work and even more so now that I saw the paycheck. &lt;/p&gt;

&lt;p&gt;Sorry for the weird post, but I am just in a good mood right now. &lt;/p&gt;

&lt;p&gt;P.s. My boss asked me if I want to continue my internship for the Fall&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 75, 'id': 'award_9663243a-e77f-44cf-abc6-850ead2cd18d', 'penny_donate': 0, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'For an especially amazing showing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Bravo Grande!', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=16&amp;height=16&amp;auto=webp&amp;s=3459bdf1d1777821a831c5bf9834f4365263fcff', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=32&amp;height=32&amp;auto=webp&amp;s=9181d68065ccfccf2b1074e499cd7c1103aa2ce8', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=48&amp;height=48&amp;auto=webp&amp;s=339b368d395219120abc50d54fb3e2cdcad8ca4f', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=64&amp;height=64&amp;auto=webp&amp;s=de4ebbe92f9019de05aaa77f88810d44adbe1e50', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=128&amp;height=128&amp;auto=webp&amp;s=ba6c1add5204ea43e5af010bd9622392a42140e3', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmyg3i,True,,royal-Brwn,,76,True,all_ads,False,[],False,,/r/datascience/comments/nmyg3i/first_two_weeks_of_my_first_internship/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmyg3i/first_two_weeks_of_my_first_internship/,515405,1622210376.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"In hope that this post won't get removed, I will take you through the steps of getting an internship at a Big4 in Belgium (Yes, mentioning the country is important because some things aren't the same in all the countries). The position was in data analytics\*.\* (the [full blog](https://dataanalystlife.blogspot.com/2021/06/working-at-big-4.html) if anyone is interested)

**The interview**

I had three interviews to pass.

* Technical (with one of the nicest recruiters/Data analyst/scientist)
* HR (also a very nice person)
* Manager (very serious guy, he scared the shit out of me, but once I started working with them he was super nice)

***Technical***

1. SQL = What would you write to do x, y, z. (Small question to test if I know how to use SQL or not at all)
2. Business question to test my business acumen
3. Statistics questions regarding outliers and robust preprocessing, median and mean in skewed data

Yes overall, the interview was not that hard, because I am a Civil Engineer + Master of AI. I truly think that having a good educational gives a nice push. The interviewer will think, if he made it that far, he won't be a dumb fuck (unless i cheated all the way). Yes I know, good grades does not mean intelligence. But good grades/good education means hard work and that's what most company want (hard workers) in my opinion.

***HR***

1. Testing my French skills (I am supposed to be bilingual but once you start talking English everyday you start losing the French vocab, ""le science de data"" I said it like 30 times (FFS)) mainly because in Belgium the languages are French, English, Dutch and German.
2. General storytelling, my cv, why i came to europe, why i chose my masters, what i did before coming
3. Motivation: on why I found the position interesting. BTW Deals Analytics is one of the most fun position if you check the YouTube videos.
4. \*\*Me saying some jokes to gain points :p\*\*

**Manager**

1. Same as HR but with a lot more pressure. I got scared to death no joke (I bet many people know what I am talking about)

**First Week as an Intern**

Right away, I had to learn the software they use, which is **Alteryx**. Since I have already done my Pandas and SQL on DataCamp, I did not require lots of time to get used to it. **VERY** fun no code software for data cleaning and prepping.

**First Project**

I directly started working on an RED financial data. The goal was to create insight from the financial data to direct the next investments in the right path (most lucrative).

**Second Project**

Geospatial analysis. Oh boiii, Oh BOIIIII ! This one was amazing. I had no idea how good is creating a map and understanding the effect of the surroundings on the business. This one was huge! Learned how to webscrape, how to deal with JSONS and much more.

**Third Project**

Computer Vision project (what the actual fuck? are we still in the same internship?). Haha. Matter of fact, I had learned a course on CV and I was like: ""guys i studied this a few months ago, I can solve it with CV"". Everyone liked it so much (even though i copy pasted the code from my old projects \*\*evil laugh\*\*)

**Fourth Project**

Geospatial Analysis again. But this was one very tough and stressing.

**Fifth Project**

At that point I had to work on my thesis because I had an intermediate presentation. However, the project was about traffic analysis (traffic as in cars). A dream coming true because i am a civil engineer specialized in traffic engineering/transportation. We did amazing stuff but they were very limited :(.

**The End**

One thing to add, for however gets an internship at a big4, I hope you get a supervisor similar to the one I had. I loved him. Always supportive and encouraging. He cared about me on the personal level.

**To the readers**

I hope you benefit from this. Either motivates you to apply or to learn something new.

hope you enjoyed it♥",t2_7mkrswyv,False,,0,False,Working as a Data Analyst at a Big4 (Europe),[],r/datascience,False,6,career,0,,,False,t3_nn8v5a,False,dark,0.74,,public,14,0,{},,,False,[],,False,False,,{},Career,False,14,,False,False,self,1622660382.0,,[],{},,True,,1622268152.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In hope that this post won&amp;#39;t get removed, I will take you through the steps of getting an internship at a Big4 in Belgium (Yes, mentioning the country is important because some things aren&amp;#39;t the same in all the countries). The position was in data analytics*.* (the &lt;a href=""https://dataanalystlife.blogspot.com/2021/06/working-at-big-4.html""&gt;full blog&lt;/a&gt; if anyone is interested)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The interview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I had three interviews to pass.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Technical (with one of the nicest recruiters/Data analyst/scientist)&lt;/li&gt;
&lt;li&gt;HR (also a very nice person)&lt;/li&gt;
&lt;li&gt;Manager (very serious guy, he scared the shit out of me, but once I started working with them he was super nice)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Technical&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;SQL = What would you write to do x, y, z. (Small question to test if I know how to use SQL or not at all)&lt;/li&gt;
&lt;li&gt;Business question to test my business acumen&lt;/li&gt;
&lt;li&gt;Statistics questions regarding outliers and robust preprocessing, median and mean in skewed data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Yes overall, the interview was not that hard, because I am a Civil Engineer + Master of AI. I truly think that having a good educational gives a nice push. The interviewer will think, if he made it that far, he won&amp;#39;t be a dumb fuck (unless i cheated all the way). Yes I know, good grades does not mean intelligence. But good grades/good education means hard work and that&amp;#39;s what most company want (hard workers) in my opinion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;HR&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Testing my French skills (I am supposed to be bilingual but once you start talking English everyday you start losing the French vocab, &amp;quot;le science de data&amp;quot; I said it like 30 times (FFS)) mainly because in Belgium the languages are French, English, Dutch and German.&lt;/li&gt;
&lt;li&gt;General storytelling, my cv, why i came to europe, why i chose my masters, what i did before coming&lt;/li&gt;
&lt;li&gt;Motivation: on why I found the position interesting. BTW Deals Analytics is one of the most fun position if you check the YouTube videos.&lt;/li&gt;
&lt;li&gt;**Me saying some jokes to gain points :p**&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Manager&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Same as HR but with a lot more pressure. I got scared to death no joke (I bet many people know what I am talking about)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;First Week as an Intern&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Right away, I had to learn the software they use, which is &lt;strong&gt;Alteryx&lt;/strong&gt;. Since I have already done my Pandas and SQL on DataCamp, I did not require lots of time to get used to it. &lt;strong&gt;VERY&lt;/strong&gt; fun no code software for data cleaning and prepping.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I directly started working on an RED financial data. The goal was to create insight from the financial data to direct the next investments in the right path (most lucrative).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Geospatial analysis. Oh boiii, Oh BOIIIII ! This one was amazing. I had no idea how good is creating a map and understanding the effect of the surroundings on the business. This one was huge! Learned how to webscrape, how to deal with JSONS and much more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Computer Vision project (what the actual fuck? are we still in the same internship?). Haha. Matter of fact, I had learned a course on CV and I was like: &amp;quot;guys i studied this a few months ago, I can solve it with CV&amp;quot;. Everyone liked it so much (even though i copy pasted the code from my old projects **evil laugh**)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Geospatial Analysis again. But this was one very tough and stressing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fifth Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At that point I had to work on my thesis because I had an intermediate presentation. However, the project was about traffic analysis (traffic as in cars). A dream coming true because i am a civil engineer specialized in traffic engineering/transportation. We did amazing stuff but they were very limited :(.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The End&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One thing to add, for however gets an internship at a big4, I hope you get a supervisor similar to the one I had. I loved him. Always supportive and encouraging. He cared about me on the personal level.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To the readers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hope you benefit from this. Either motivates you to apply or to learn something new.&lt;/p&gt;

&lt;p&gt;hope you enjoyed it♥&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn8v5a,True,,Ecstatic_Tooth_1096,,17,True,all_ads,False,[],False,,/r/datascience/comments/nn8v5a/working_as_a_data_analyst_at_a_big4_europe/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn8v5a/working_as_a_data_analyst_at_a_big4_europe/,515405,1622239352.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?auto=webp&amp;s=8b97c0888f7fbb5d3636b1de1a3003d29752a279', 'width': 1100, 'height': 619}, 'resolutions': [{'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7712bda390a6d3e6ebaef44fff0553937c1f1f7b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b17b8c73ced12433e61e8e5870b3a5a4baf5b0e', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d0705e3f6e6c114b3b2051175dfe66bb13cb958', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f912cf9b331ae1037ac26c2e4eeda7f9c9b7488', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=025eb5700d54827954185f7d78b2703abce0be0b', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2435b9f1684b896f6d480f0d2484f9aecb535e97', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'RfmbTLUIFPHuunMFk36ftpkiWcluXrN-0jeTfuGUHu4'}], 'enabled': False}",,,,,
,datascience,"Hi everyone,

I had an unusual situation happen in the past few days, and I'd like some advice.

A staffing agency in the Bay Area offered me the opportunity to interview for a DS role on a FAANG team that would directly impact a product that is popular worldwide (think 100m+ users). I like the role, but am hesitant about it being a contract position, considering I have a full-time job lined up post-MS in the Bay Area that is paying 135k (\~150k if you include benefits, 170k if equity options aren't worth crap) with a team I like, though at a much smaller scale (more relatively unknown) company with far fewer DS to learn from.

The staffing agency told me the team wants to bring me in for an additional 7 interviews, testing me on everything (statistics, ML, product sense, python, SQL, behavioral), but that the position would only be paying 120k. I told her that is ridiculous, since this is just a contract position, and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role, considering I have a full-time offer already. I was told today they would match the 180k.

Was I being extremely low-balled initially? The staffing agency is well known, and I've heard decent things about it. For context, this team has been looking for nearly a year for someone and I'm the only person to make it to the final stage (as far as I know). Do you think it is worthwhile to continue the interview process? Would you?

Any advice is appreciated. Thanks!

Edit #1: I'd be a W2 employee of the staffing agency. They'd have the contract with the FAANG.",t2_1100xc,False,,0,False,Lowballed for FAANG DS Contracting as New Grad? Advice Needed,[],r/datascience,False,6,,0,,,False,t3_nnfeb4,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,1622266646.0,,[],{},,True,,1622290979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I had an unusual situation happen in the past few days, and I&amp;#39;d like some advice.&lt;/p&gt;

&lt;p&gt;A staffing agency in the Bay Area offered me the opportunity to interview for a DS role on a FAANG team that would directly impact a product that is popular worldwide (think 100m+ users). I like the role, but am hesitant about it being a contract position, considering I have a full-time job lined up post-MS in the Bay Area that is paying 135k (~150k if you include benefits, 170k if equity options aren&amp;#39;t worth crap) with a team I like, though at a much smaller scale (more relatively unknown) company with far fewer DS to learn from.&lt;/p&gt;

&lt;p&gt;The staffing agency told me the team wants to bring me in for an additional 7 interviews, testing me on everything (statistics, ML, product sense, python, SQL, behavioral), but that the position would only be paying 120k. I told her that is ridiculous, since this is just a contract position, and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role, considering I have a full-time offer already. I was told today they would match the 180k.&lt;/p&gt;

&lt;p&gt;Was I being extremely low-balled initially? The staffing agency is well known, and I&amp;#39;ve heard decent things about it. For context, this team has been looking for nearly a year for someone and I&amp;#39;m the only person to make it to the final stage (as far as I know). Do you think it is worthwhile to continue the interview process? Would you?&lt;/p&gt;

&lt;p&gt;Any advice is appreciated. Thanks!&lt;/p&gt;

&lt;p&gt;Edit #1: I&amp;#39;d be a W2 employee of the staffing agency. They&amp;#39;d have the contract with the FAANG.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nnfeb4,True,,CustardEnigma,,30,True,all_ads,False,[],False,,/r/datascience/comments/nnfeb4/lowballed_for_faang_ds_contracting_as_new_grad/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnfeb4/lowballed_for_faang_ds_contracting_as_new_grad/,515405,1622262179.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hello!

I am doing a project at work to predict On time delivery percentage in a manufacturing process.
I recently discovered Quantile Random Forest and I like the idea of it. I am thinking of using Quantile 0.5 as a point estimator and 0.1 and 0.9 quantile as prediction interval.

So far the results have been good but since I'm new to the real world project setting and new to quantile random forest, I was wondering is there something I should keep in mind while using this algorithm?

I read an article at Medium where they showed a use case of QRF at Instacart to predict On time delivery percentage but I was thinking why this algorithm is not so popular (maybe I just don't know about it).

What have your personal experience been using QRF?

Thank you!",t2_7ckcfm6o,False,,0,False,What are your thoughts on Quantile Random Forest?,[],r/datascience,False,6,discussion,0,,,False,t3_nn5z43,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1622260096.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am doing a project at work to predict On time delivery percentage in a manufacturing process.
I recently discovered Quantile Random Forest and I like the idea of it. I am thinking of using Quantile 0.5 as a point estimator and 0.1 and 0.9 quantile as prediction interval.&lt;/p&gt;

&lt;p&gt;So far the results have been good but since I&amp;#39;m new to the real world project setting and new to quantile random forest, I was wondering is there something I should keep in mind while using this algorithm?&lt;/p&gt;

&lt;p&gt;I read an article at Medium where they showed a use case of QRF at Instacart to predict On time delivery percentage but I was thinking why this algorithm is not so popular (maybe I just don&amp;#39;t know about it).&lt;/p&gt;

&lt;p&gt;What have your personal experience been using QRF?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn5z43,True,,ysharm10,,15,True,all_ads,False,[],False,,/r/datascience/comments/nn5z43/what_are_your_thoughts_on_quantile_random_forest/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn5z43/what_are_your_thoughts_on_quantile_random_forest/,515405,1622231296.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"No disrespect to Ph'd's,  just an interesting analogy.

lots of internal validation and creds,  but poor performance in the wild.",t2_6hh47hwj,False,,0,False,A lot of people entering this field are like over-fitted models,[],r/datascience,False,6,discussion,0,,,False,t3_nmaguz,False,dark,0.91,,public,644,3,{},,,False,[],,False,False,,{},Discussion,False,644,,False,False,self,False,,[],{'gid_1': 1},,True,,1622159474.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;No disrespect to Ph&amp;#39;d&amp;#39;s,  just an interesting analogy.&lt;/p&gt;

&lt;p&gt;lots of internal validation and creds,  but poor performance in the wild.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmaguz,True,,redmoon_reddit,,161,True,all_ads,False,[],False,,/r/datascience/comments/nmaguz/a_lot_of_people_entering_this_field_are_like/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmaguz/a_lot_of_people_entering_this_field_are_like/,515405,1622130674.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"i'm a software developer first, so getting into AI it's hard to get used to handling so much binary data. how do people in this field do data and model versioning in general?

ninja edit: my first instinct is git LFS",t2_8hhmk,False,,0,False,How does version control work for ML stuff?,[],r/datascience,False,6,tooling,0,,,False,t3_nn53za,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},Tooling,False,5,,False,False,self,False,,[],{},,True,,1622257620.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i&amp;#39;m a software developer first, so getting into AI it&amp;#39;s hard to get used to handling so much binary data. how do people in this field do data and model versioning in general?&lt;/p&gt;

&lt;p&gt;ninja edit: my first instinct is git LFS&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn53za,True,,covercash2,,9,True,all_ads,False,[],False,,/r/datascience/comments/nn53za/how_does_version_control_work_for_ml_stuff/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn53za/how_does_version_control_work_for_ml_stuff/,515405,1622228820.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I'd really like to learn more about data science through practice and maybe build a portfolio, but I'm really uncreative and can't think of any projects to try. Does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience? Maybe something w a machine learning component.",t2_y46sv,False,,0,False,How do I think of data science projects?,[],r/datascience,False,6,projects,0,,,False,t3_nn4j8q,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,self,False,,[],{},,True,,1622256004.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;d really like to learn more about data science through practice and maybe build a portfolio, but I&amp;#39;m really uncreative and can&amp;#39;t think of any projects to try. Does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience? Maybe something w a machine learning component.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn4j8q,True,,JBizzle07,,2,True,all_ads,False,[],False,,/r/datascience/comments/nn4j8q/how_do_i_think_of_data_science_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn4j8q/how_do_i_think_of_data_science_projects/,515405,1622227204.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'll keep this short, the start up were impressed with me when I was a software developer for them. I have a mathematical background and my masters was on a data science working with AI PhD students. I have a lot of knowledge in algorithms, cloud and best CI/CD practises. And I was previously a PhD student in A.I (didn't finish for valid reasons).

&amp;#x200B;

The company has asked me to take the lead on projects despite my lack of experience. And due to the nature of startups, there's not much structure and mentorship involved, but they believe in me.  I'm extremely motivated to do well and constantly learn for me and my company. So I was wondering the data science journey might be more smooth if I make this post.

&amp;#x200B;

I know this is kind of vague so if you need more information about me...let me know!

&amp;#x200B;

Thanks a lot everyone.",t2_1tvv8r2v,False,,0,False,Experienced data scientists....what advice could you give to a Junior data scientist working at a start up?,[],r/datascience,False,6,discussion,0,,,False,t3_nn210e,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622249236.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ll keep this short, the start up were impressed with me when I was a software developer for them. I have a mathematical background and my masters was on a data science working with AI PhD students. I have a lot of knowledge in algorithms, cloud and best CI/CD practises. And I was previously a PhD student in A.I (didn&amp;#39;t finish for valid reasons).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The company has asked me to take the lead on projects despite my lack of experience. And due to the nature of startups, there&amp;#39;s not much structure and mentorship involved, but they believe in me.  I&amp;#39;m extremely motivated to do well and constantly learn for me and my company. So I was wondering the data science journey might be more smooth if I make this post.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I know this is kind of vague so if you need more information about me...let me know!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot everyone.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn210e,True,,onechamp27,,9,True,all_ads,False,[],False,,/r/datascience/comments/nn210e/experienced_data_scientistswhat_advice_could_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn210e/experienced_data_scientistswhat_advice_could_you/,515405,1622220436.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_21s9nfge,False,,0,False,"I held &gt;120 office hour sessions with aspiring data scientists, picked the best ones, and turned them into a free course on getting hired in DS",[],r/datascience,False,6,,0,,,False,t3_nmb3ff,False,dark,0.97,,public,264,5,{},,,False,[],,False,False,,{},Job Search,False,264,,False,False,default,False,,[],{'gid_1': 2},,False,,1622161147.0,text,6,,,text,sharpestminds.com,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmb3ff,True,,jeremie-harris,,35,True,all_ads,False,[],False,,/r/datascience/comments/nmb3ff/i_held_120_office_hour_sessions_with_aspiring/,all_ads,False,https://www.sharpestminds.com/landing-a-data-job-the-course,515405,1622132347.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,https://www.sharpestminds.com/landing-a-data-job-the-course,,
,datascience,"Let's say I have a binary classification model where one of the discrete variable has 3 values, for example, male, female, unidentified. Does it make sense to use different probability thresholds for each value?

It seems intuitive to me but I'm trying to think of potential downfalls.

To give more context, our model generates a lot of false positives but performs well on class 0 (TN/FN). Does it make sense to have one threshold optimizing for class 0 prediction, then accept class 1 prediction only if the probability score is high? 

Since high here means class 1 precision, if I want to fix this (at say 75%), each gender would have a different probability threshold. 

The rest of the predictions will have prob score above class 0 threshold but lower than precision@.75 threshold. These will trigger manual intervention and we won't rely on model prediction.",t2_qinw9,False,,0,False,Binary Classification with Pick-and-Choose Threshold?,[],r/datascience,False,6,discussion,0,,,False,t3_nn5wfs,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622259880.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I have a binary classification model where one of the discrete variable has 3 values, for example, male, female, unidentified. Does it make sense to use different probability thresholds for each value?&lt;/p&gt;

&lt;p&gt;It seems intuitive to me but I&amp;#39;m trying to think of potential downfalls.&lt;/p&gt;

&lt;p&gt;To give more context, our model generates a lot of false positives but performs well on class 0 (TN/FN). Does it make sense to have one threshold optimizing for class 0 prediction, then accept class 1 prediction only if the probability score is high? &lt;/p&gt;

&lt;p&gt;Since high here means class 1 precision, if I want to fix this (at say 75%), each gender would have a different probability threshold. &lt;/p&gt;

&lt;p&gt;The rest of the predictions will have prob score above class 0 threshold but lower than &lt;a href=""mailto:precision@.75""&gt;precision@.75&lt;/a&gt; threshold. These will trigger manual intervention and we won&amp;#39;t rely on model prediction.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn5wfs,True,,monkeyunited,,1,True,all_ads,False,[],False,,/r/datascience/comments/nn5wfs/binary_classification_with_pickandchoose_threshold/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn5wfs/binary_classification_with_pickandchoose_threshold/,515405,1622231080.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[Source Code](https://github.com/tstewart161/Reddit_Sentiment_Trader) (mine)

[Article](https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664) (not mine but it's an amazing look into how this works)

**HOW I DID THIS**

Scraped WSB sentiment, got the top + most positively mentioned stocks on WSB (for the better part of this year, that's been $GME and $AMC, recently some $SPCE and $NVDA, and about 13 other stocks. I have the strategy rebalancing monthly.

Right now I'm up 60% YTD, compared to the SP500's 13% (the recent spikes in GME and AMC have helped tremendously)

**Some stats (and a** [**picture of a card**](https://preview.redd.it/62exai0wik171.png?width=620&amp;format=png&amp;auto=webp&amp;s=1f8ec63b65c84e0e28da6d9164edfbb618ff09d0) **I made giving more info about the strategy):**

\- The strategy is **backtested** only to the beginning of 2020, but I'm working on it. It's got an annualized return of 33% (compared to 16% for the SP500)

\- **Max drawdown of -8.7%** (thought this was pretty interesting - WSB would be a very cool hedge for financial markets at large. Rode COVID like a wave)

Happy to answer any more questions about the process/results. I think doing stuff like this is pretty cool as someone with a foot in algo trading and traditional financial markets",t2_9myqz8vx,False,,0,False,"I used VADER sentiment analysis to track and invest based on WallStreetBets stock sentiment -- I'm up 33% annually $16k) -- here's source code, process, result, and an article",[],r/datascience,False,6,projects,0,,,False,t3_nmiq15,False,dark,0.88,,public,51,0,{},,,False,[],,False,False,,{},Projects,False,51,,False,False,self,False,,[],{},,True,,1622181396.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/tstewart161/Reddit_Sentiment_Trader""&gt;Source Code&lt;/a&gt; (mine)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664""&gt;Article&lt;/a&gt; (not mine but it&amp;#39;s an amazing look into how this works)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HOW I DID THIS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Scraped WSB sentiment, got the top + most positively mentioned stocks on WSB (for the better part of this year, that&amp;#39;s been $GME and $AMC, recently some $SPCE and $NVDA, and about 13 other stocks. I have the strategy rebalancing monthly.&lt;/p&gt;

&lt;p&gt;Right now I&amp;#39;m up 60% YTD, compared to the SP500&amp;#39;s 13% (the recent spikes in GME and AMC have helped tremendously)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some stats (and a&lt;/strong&gt; &lt;a href=""https://preview.redd.it/62exai0wik171.png?width=620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f8ec63b65c84e0e28da6d9164edfbb618ff09d0""&gt;&lt;strong&gt;picture of a card&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;I made giving more info about the strategy):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;- The strategy is &lt;strong&gt;backtested&lt;/strong&gt; only to the beginning of 2020, but I&amp;#39;m working on it. It&amp;#39;s got an annualized return of 33% (compared to 16% for the SP500)&lt;/p&gt;

&lt;p&gt;- &lt;strong&gt;Max drawdown of -8.7%&lt;/strong&gt; (thought this was pretty interesting - WSB would be a very cool hedge for financial markets at large. Rode COVID like a wave)&lt;/p&gt;

&lt;p&gt;Happy to answer any more questions about the process/results. I think doing stuff like this is pretty cool as someone with a foot in algo trading and traditional financial markets&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmiq15,True,,notjimryan,,14,True,all_ads,False,[],False,,/r/datascience/comments/nmiq15/i_used_vader_sentiment_analysis_to_track_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmiq15/i_used_vader_sentiment_analysis_to_track_and/,515405,1622152596.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?auto=webp&amp;s=39a158f450dc1b27762cdfcaf0746d4206b517fe', 'width': 620, 'height': 1126}, 'resolutions': [{'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=487122150616a702dcf667e5e973f55727150c03', 'width': 108, 'height': 196}, {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=013f4c76756dc0deecd91287debe9358d39ed64c', 'width': 216, 'height': 392}, {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=668b911d284b536bf93a748c469d26adbe46b842', 'width': 320, 'height': 581}], 'variants': {}, 'id': 'I75ug1AxTprL5FUnkUsdNjsyRhowuFtELPEwsV_aEGA'}], 'enabled': False}",,,,,
,datascience,"Hey all. I'm working on a project to extract machine-generated text and table data from PDFs.

We got part of this using libraries such as PyPDF2, PyMuPDF, and extracting tables using Camelot. 

But you know PDFs can be a pain to read, sometimes have weird encoding or are scanned so we need an OCR solution. I've been researching and playing with OpenCV, but most resources involve neural networks on images. 

I need to learn how to read text via an OCR solution. Can you recommend any resources or methods? Do they require NNs? Any insights would be appreciated. Thank you!",t2_2o0q5m4h,False,,0,False,what does the path to computer vision/OpenCV for text look like?,[],r/datascience,False,6,discussion,0,,,False,t3_nn0f57,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622244800.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all. I&amp;#39;m working on a project to extract machine-generated text and table data from PDFs.&lt;/p&gt;

&lt;p&gt;We got part of this using libraries such as PyPDF2, PyMuPDF, and extracting tables using Camelot. &lt;/p&gt;

&lt;p&gt;But you know PDFs can be a pain to read, sometimes have weird encoding or are scanned so we need an OCR solution. I&amp;#39;ve been researching and playing with OpenCV, but most resources involve neural networks on images. &lt;/p&gt;

&lt;p&gt;I need to learn how to read text via an OCR solution. Can you recommend any resources or methods? Do they require NNs? Any insights would be appreciated. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn0f57,True,,rotterdamn8,,1,True,all_ads,False,[],False,,/r/datascience/comments/nn0f57/what_does_the_path_to_computer_visionopencv_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn0f57/what_does_the_path_to_computer_visionopencv_for/,515405,1622216000.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"We all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying, which are more related to software engineering than math/statistics. If one wants to be as well-rounded as possible, what are some of the best software engineering practices (things like version control) that are a must?",t2_3q66js10,False,,0,False,What are some good software engineering practices that all data scientists must know?,[],r/datascience,False,6,discussion,0,,,False,t3_nmgfgb,False,dark,0.97,,public,25,0,{},,,False,[],,False,False,,{},Discussion,False,25,,False,False,self,False,,[],{},,True,,1622175148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying, which are more related to software engineering than math/statistics. If one wants to be as well-rounded as possible, what are some of the best software engineering practices (things like version control) that are a must?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmgfgb,True,,Pedro9870,,15,True,all_ads,False,[],False,,/r/datascience/comments/nmgfgb/what_are_some_good_software_engineering_practices/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmgfgb/what_are_some_good_software_engineering_practices/,515405,1622146348.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi , 

I am switching from PMO to a Data Science role and got one interview today for a part-time role.

The Manager asked me, are you working on any ML projects, I told them I am working on 2 projects one is with a company and another one is with a university.  

Manager asked me what models you use and I told Linear regression to predict the values and we are still in Data preprocessing like cleaning the data, imputing the values and stuff.

Then, later manager didn't ask any question and asked me whether I have any question for them

then the interview was done in 15 mins. now I feel I screwed up

What to do in future to prevent this scenario?

Please help me.",t2_97m7yfi9,False,,0,False,How to explain your projects?,[],r/datascience,False,6,,0,,,False,t3_nmt4pp,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622219188.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi , &lt;/p&gt;

&lt;p&gt;I am switching from PMO to a Data Science role and got one interview today for a part-time role.&lt;/p&gt;

&lt;p&gt;The Manager asked me, are you working on any ML projects, I told them I am working on 2 projects one is with a company and another one is with a university.  &lt;/p&gt;

&lt;p&gt;Manager asked me what models you use and I told Linear regression to predict the values and we are still in Data preprocessing like cleaning the data, imputing the values and stuff.&lt;/p&gt;

&lt;p&gt;Then, later manager didn&amp;#39;t ask any question and asked me whether I have any question for them&lt;/p&gt;

&lt;p&gt;then the interview was done in 15 mins. now I feel I screwed up&lt;/p&gt;

&lt;p&gt;What to do in future to prevent this scenario?&lt;/p&gt;

&lt;p&gt;Please help me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmt4pp,True,,Vinothd19,,5,True,all_ads,False,[],False,,/r/datascience/comments/nmt4pp/how_to_explain_your_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmt4pp/how_to_explain_your_projects/,515405,1622190388.0,2,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Just had an interview task for a data engineer role where I had to filter by date, find some codes in different columns etc. I read after that I was not allowed to use pandas or any external library? Is this a ridiculous ask?  I feel like this doesn’t represent the working environment at all.",t2_5z43s,False,,0,False,"Technical interview timed task, no pandas?",[],r/datascience,False,6,,0,,,False,t3_nmkw02,False,dark,0.61,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1622188086.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just had an interview task for a data engineer role where I had to filter by date, find some codes in different columns etc. I read after that I was not allowed to use pandas or any external library? Is this a ridiculous ask?  I feel like this doesn’t represent the working environment at all.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmkw02,True,,boboshoes,,24,True,all_ads,False,[],False,,/r/datascience/comments/nmkw02/technical_interview_timed_task_no_pandas/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmkw02/technical_interview_timed_task_no_pandas/,515405,1622159286.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"In my spare time, I do some freelance work as a side hustle on some of the bigger platforms out there. The amount of DA, DS, Stat and etc. students that are posting their university homework and projects is preposterous. It ranges from basic stuff like running simple models and cleaning datasets to big capstone projects. I  decline every interview from these types of students that really ramp me up and beat the whole purpose of studying. They don't know the basics and have gone into the field just because they've heard that you can make money there. Not saying that they can't learn it, but going into a field without a deep interest or passion for it, on average, breeds bad practitioners.

Have you run into people like these? What is your opinion on this matter?",t2_4o6wucq4,False,,0,False,"Paying for ""Personal"" Projects and Homework",[],r/datascience,False,6,discussion,0,,,False,t3_nmblj9,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1622162494.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my spare time, I do some freelance work as a side hustle on some of the bigger platforms out there. The amount of DA, DS, Stat and etc. students that are posting their university homework and projects is preposterous. It ranges from basic stuff like running simple models and cleaning datasets to big capstone projects. I  decline every interview from these types of students that really ramp me up and beat the whole purpose of studying. They don&amp;#39;t know the basics and have gone into the field just because they&amp;#39;ve heard that you can make money there. Not saying that they can&amp;#39;t learn it, but going into a field without a deep interest or passion for it, on average, breeds bad practitioners.&lt;/p&gt;

&lt;p&gt;Have you run into people like these? What is your opinion on this matter?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmblj9,True,,Ingvariuss,,5,True,all_ads,False,[],False,,/r/datascience/comments/nmblj9/paying_for_personal_projects_and_homework/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmblj9/paying_for_personal_projects_and_homework/,515405,1622133694.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Sorry if this is not allowed in this sub - 

Was wondering if anyone had experience working as a Data Scientist at a Big 4...What should I expect?",t2_7hcxrfsi,False,,0,False,Data Science at a Big 4,[],r/datascience,False,6,,0,,,False,t3_nmn22u,False,dark,0.44,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,False,,[],{},,True,,1622195419.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this is not allowed in this sub - &lt;/p&gt;

&lt;p&gt;Was wondering if anyone had experience working as a Data Scientist at a Big 4...What should I expect?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmn22u,True,,StringAbject7049,,10,True,all_ads,False,[],False,,/r/datascience/comments/nmn22u/data_science_at_a_big_4/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmn22u/data_science_at_a_big_4/,515405,1622166619.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"What’s everyone’s favorite data science related podcasts, YouTube channels and audiobooks?

I spend a lot of time in the car and would love to learn while I drive! Thanks.",t2_846wudmb,False,,0,False,Favorite podcasts and other audio resources for learning and staying up to date?,[],r/datascience,False,6,discussion,0,,,False,t3_nm8k0y,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622154114.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What’s everyone’s favorite data science related podcasts, YouTube channels and audiobooks?&lt;/p&gt;

&lt;p&gt;I spend a lot of time in the car and would love to learn while I drive! Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm8k0y,True,,Homura_A,,6,True,all_ads,False,[],False,,/r/datascience/comments/nm8k0y/favorite_podcasts_and_other_audio_resources_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm8k0y/favorite_podcasts_and_other_audio_resources_for/,515405,1622125314.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello!

Sorry this might be a very stupid question but I've seen many people saying, ""Never submit your task before the deadline"". What's the thought behind this statement?

Asking this because, I'm a junior at a company and my manager often overestimates the time required to do certain tasks he gives me. And right now I happened to be in a situation where he gave me a task with a deadline of a week and I finished it within a few hours.

In your experience, what's the best thing to do?

Thanks!",t2_bv171ji2,False,,0,False,"New to corporate, should you submit your task way before deadline?",[],r/datascience,False,6,discussion,0,,,False,t3_nmbicr,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622162261.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Sorry this might be a very stupid question but I&amp;#39;ve seen many people saying, &amp;quot;Never submit your task before the deadline&amp;quot;. What&amp;#39;s the thought behind this statement?&lt;/p&gt;

&lt;p&gt;Asking this because, I&amp;#39;m a junior at a company and my manager often overestimates the time required to do certain tasks he gives me. And right now I happened to be in a situation where he gave me a task with a deadline of a week and I finished it within a few hours.&lt;/p&gt;

&lt;p&gt;In your experience, what&amp;#39;s the best thing to do?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmbicr,True,,quite--average,,20,True,all_ads,False,[],False,,/r/datascience/comments/nmbicr/new_to_corporate_should_you_submit_your_task_way/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmbicr/new_to_corporate_should_you_submit_your_task_way/,515405,1622133461.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

 I work for a large wholesale company, and I am trying to create a way to forecast when specific items will go out of stock at a specific store. I have access to sales data by store-item for the past 5 years as well the past 5 years of out of stock data (also on a store-item level). 

How would you approach this? 

FYI I code in Python mainly, and I'm pretty familiar with most ML tools and models. 

Just trying to get a variety of ideas. 

Thank you.",t2_4r0mnaaz,False,,0,False,Forecasting Out of Stock items on a store-item level,[],r/datascience,False,6,projects,0,,,False,t3_nm9155,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1622155470.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I work for a large wholesale company, and I am trying to create a way to forecast when specific items will go out of stock at a specific store. I have access to sales data by store-item for the past 5 years as well the past 5 years of out of stock data (also on a store-item level). &lt;/p&gt;

&lt;p&gt;How would you approach this? &lt;/p&gt;

&lt;p&gt;FYI I code in Python mainly, and I&amp;#39;m pretty familiar with most ML tools and models. &lt;/p&gt;

&lt;p&gt;Just trying to get a variety of ideas. &lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm9155,True,,kking1122,,11,True,all_ads,False,[],False,,/r/datascience/comments/nm9155/forecasting_out_of_stock_items_on_a_storeitem/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm9155/forecasting_out_of_stock_items_on_a_storeitem/,515405,1622126670.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Ive been at this job for about three months now and I feel like I’m not being challenged. I get assigned work that takes me about four hours to complete then the rest of the day I’m not doing anything except maybe 3-4 meetings that last 20 mins and then Im done. 

I also spend most of my time just cleaning and pulling data in Excel. Which often makes me think why I’m being pay well when this stuff is so easy.

Is this the norm for a jr data analyst role or should i find another job?",t2_qpxlk,False,,0,False,How was your first job out of college?,[],r/datascience,False,6,career,0,,,False,t3_nlwviq,False,dark,0.94,,public,16,0,{},,,False,[],,False,False,,{},Career,False,16,,False,False,self,False,,[],{},,True,,1622108708.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ive been at this job for about three months now and I feel like I’m not being challenged. I get assigned work that takes me about four hours to complete then the rest of the day I’m not doing anything except maybe 3-4 meetings that last 20 mins and then Im done. &lt;/p&gt;

&lt;p&gt;I also spend most of my time just cleaning and pulling data in Excel. Which often makes me think why I’m being pay well when this stuff is so easy.&lt;/p&gt;

&lt;p&gt;Is this the norm for a jr data analyst role or should i find another job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlwviq,True,,ForeignMate,,10,True,all_ads,False,[],False,,/r/datascience/comments/nlwviq/how_was_your_first_job_out_of_college/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlwviq/how_was_your_first_job_out_of_college/,515405,1622079908.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I work at a data science consulting firm and there I was faced with the established procedure that, if the team presents some forecast to a client and the client thinks that the forecast is too high or too low, instead of changing hyperparameters or even changing models, the team basically changes the forecast values manually using ""market insights"" (they basically look for other specialists forecasts in the internet and subjectively choose a value close to those). This makes me incredibly uncomfortable. Is this a normal procedure to be used at data science consultancies? I never worked at another consultancy, so I don't know if this is normal or not.",t2_10ftnyis,False,,0,False,"Is it normal to manually ""adjust"" forecasts?",[],r/datascience,False,6,discussion,0,,,False,t3_nm00ol,False,dark,1.0,,public,7,1,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{'gid_1': 1},,True,,1622120537.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work at a data science consulting firm and there I was faced with the established procedure that, if the team presents some forecast to a client and the client thinks that the forecast is too high or too low, instead of changing hyperparameters or even changing models, the team basically changes the forecast values manually using &amp;quot;market insights&amp;quot; (they basically look for other specialists forecasts in the internet and subjectively choose a value close to those). This makes me incredibly uncomfortable. Is this a normal procedure to be used at data science consultancies? I never worked at another consultancy, so I don&amp;#39;t know if this is normal or not.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm00ol,True,,hi_fi_v,,16,True,all_ads,False,[],False,,/r/datascience/comments/nm00ol/is_it_normal_to_manually_adjust_forecasts/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm00ol/is_it_normal_to_manually_adjust_forecasts/,515405,1622091737.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a dataset that is pre-computed into a distance matrix, fully connected between all points. This made sense as it was originally intended for clustering but now I am wondering if I could use it for classification, i.e. given unlabelled points and their distances to all labelled points, assign soft labels. Being able to do regression in the same manner would also be a plus.

I tried Googling this but I could not come up with the appropriate terms to search. The best I could find is graph-based semi-supervised techniques like Label Propogation in sklearn but I was wondering if anyone had any insight into additional models &amp; techniques I could look into. Thanks!",t2_8nyuj3,False,,0,False,Classification/Regression just from a distance matrix?,[],r/datascience,False,6,discussion,0,,,False,t3_nlj5j8,False,dark,0.94,,public,48,0,{},,,False,[],,False,False,,{},Discussion,False,48,,False,False,self,False,,[],{},,True,,1622070193.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset that is pre-computed into a distance matrix, fully connected between all points. This made sense as it was originally intended for clustering but now I am wondering if I could use it for classification, i.e. given unlabelled points and their distances to all labelled points, assign soft labels. Being able to do regression in the same manner would also be a plus.&lt;/p&gt;

&lt;p&gt;I tried Googling this but I could not come up with the appropriate terms to search. The best I could find is graph-based semi-supervised techniques like Label Propogation in sklearn but I was wondering if anyone had any insight into additional models &amp;amp; techniques I could look into. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlj5j8,True,,metsfan1025,,19,True,all_ads,False,[],False,,/r/datascience/comments/nlj5j8/classificationregression_just_from_a_distance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlj5j8/classificationregression_just_from_a_distance/,515405,1622041393.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been a data scientist for a bit over a year and a half. But I don't feel ""Senior"" at all.

If anything, I feel like what I assumed data scientists who got entry-level roles felt like.

I look at the interview questions for Uber, Amazon, MSFT and others and I don't really see myself learning them from my peers. I'll need to study ISLR or other materials.

But it's the other materials where I'm a bit stuck.

Wondering if folks who made it to Senior roles or FAANG roles (not counting analytics/product roles) could share tips on how to learn to be a better data scientist.",t2_a3t5z3gn,False,,0,False,Data scientists who moved to ML data science roles: How did you get senior level skills?,[],r/datascience,False,6,career,0,,,False,t3_nlwczv,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Career,False,7,,False,False,self,False,,[],{},,True,,1622106883.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been a data scientist for a bit over a year and a half. But I don&amp;#39;t feel &amp;quot;Senior&amp;quot; at all.&lt;/p&gt;

&lt;p&gt;If anything, I feel like what I assumed data scientists who got entry-level roles felt like.&lt;/p&gt;

&lt;p&gt;I look at the interview questions for Uber, Amazon, MSFT and others and I don&amp;#39;t really see myself learning them from my peers. I&amp;#39;ll need to study ISLR or other materials.&lt;/p&gt;

&lt;p&gt;But it&amp;#39;s the other materials where I&amp;#39;m a bit stuck.&lt;/p&gt;

&lt;p&gt;Wondering if folks who made it to Senior roles or FAANG roles (not counting analytics/product roles) could share tips on how to learn to be a better data scientist.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlwczv,True,,latticeprep,,2,True,all_ads,False,[],False,,/r/datascience/comments/nlwczv/data_scientists_who_moved_to_ml_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlwczv/data_scientists_who_moved_to_ml_data_science/,515405,1622078083.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm interested in a side project that looks at grocery store shopping habits and health outcomes. For the shopper history, I was hoping for loyalty number-based transaction data that would give me a good picture on how ""healthy"" their trips look. I'm not as interested in price as much as types of food (i.e. fruits vs candy). Haven't come across how to buy these data, but this [tweet thread](https://twitter.com/RobertGReeve/status/1397034344833748992?s=20) made me think a data aggregator may have it. Has anyone purchased grocery shopper transaction data before?",t2_13dmdo,False,,0,False,Grocery store purchase records,[],r/datascience,False,6,projects,0,,,False,t3_nlmcr8,False,dark,0.75,,public,8,0,{},,,False,[],,False,False,,{},Projects,False,8,,False,False,self,False,,[],{},,True,,1622078475.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m interested in a side project that looks at grocery store shopping habits and health outcomes. For the shopper history, I was hoping for loyalty number-based transaction data that would give me a good picture on how &amp;quot;healthy&amp;quot; their trips look. I&amp;#39;m not as interested in price as much as types of food (i.e. fruits vs candy). Haven&amp;#39;t come across how to buy these data, but this &lt;a href=""https://twitter.com/RobertGReeve/status/1397034344833748992?s=20""&gt;tweet thread&lt;/a&gt; made me think a data aggregator may have it. Has anyone purchased grocery shopper transaction data before?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlmcr8,True,,capitolcustomer,,18,True,all_ads,False,[],False,,/r/datascience/comments/nlmcr8/grocery_store_purchase_records/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlmcr8/grocery_store_purchase_records/,515405,1622049675.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yeWln_oOQKCfHMVv-wDnoPLgmZsjdgW7gQlWuKsChz8.jpg?auto=webp&amp;s=dc5e44b524b4a5f204f4421083b51e300c808f68', 'width': 140, 'height': 140}, 'resolutions': [{'url': 'https://external-preview.redd.it/yeWln_oOQKCfHMVv-wDnoPLgmZsjdgW7gQlWuKsChz8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d66ec268c453746be133d7cdd842fb83853fcbac', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'gnPwJIajQwm335Rwf7irkj5bPUrc3fg0DECiwlg-xs8'}], 'enabled': False}",,,,,
,datascience,"I just got tossed an interesting problem by management.   There's a lot of a certain type of document laying out agreements between labor/mgmt at the local branches, and a poor understanding at the top over what topics occur regularly in these documents.  It's over a thousand .txt documents, up to about 15k characters in each, so there's a lot to look through, and therefore, text analytics to the rescue.

I just got finished reading them all into Hadoop and turning them into a big table of the form:

Row = LineOfText,OriginalDocumentName

&amp;#x200B;

Before I get too far into the modeling part, I'm wondering if this is the best form to have the data in, or whether I'd do better with a format of:

Row = EntireDocument, OriginalDocumentName

&amp;#x200B;

My tools will be SAS Viya's text analytics primarily because I like the concept modeling part, and some R package like tidytext and whatever I can find for concept formation (R is my real go-to language, but I'm a bit of a noob in text analytics, and don't fully understand those packages yet.)  I've done some with Python's nltk and similar packages, although I'm expert level in R, and struggle some in Python.

Any thoughts about what's the best general format to use -- one line at a time, or the entire document at a time?   By the way, the documents come from a variety of sources like Word, PDF, and OCR input, and are pretty low quality in terms of misread or mangled words, white space, and weird control characters.  I'm not sure there's an obvious way to skip the line feeds in a Hive import to get to the second document-as-a-row format, but I can cross that bridge later, and have a few databases whizzes on my team that could help me figure it out if needed.",t2_87whm2mh,False,,0,False,"Ideal Text Analytics Data Structure -- One Document at a time, or one Row at a Time?",[],r/datascience,False,6,projects,0,,,False,t3_nlh1wv,False,dark,0.95,,public,18,0,{},,,False,[],,False,False,,{},Projects,False,18,,False,False,self,False,,[],{},,True,,1622064533.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just got tossed an interesting problem by management.   There&amp;#39;s a lot of a certain type of document laying out agreements between labor/mgmt at the local branches, and a poor understanding at the top over what topics occur regularly in these documents.  It&amp;#39;s over a thousand .txt documents, up to about 15k characters in each, so there&amp;#39;s a lot to look through, and therefore, text analytics to the rescue.&lt;/p&gt;

&lt;p&gt;I just got finished reading them all into Hadoop and turning them into a big table of the form:&lt;/p&gt;

&lt;p&gt;Row = LineOfText,OriginalDocumentName&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Before I get too far into the modeling part, I&amp;#39;m wondering if this is the best form to have the data in, or whether I&amp;#39;d do better with a format of:&lt;/p&gt;

&lt;p&gt;Row = EntireDocument, OriginalDocumentName&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My tools will be SAS Viya&amp;#39;s text analytics primarily because I like the concept modeling part, and some R package like tidytext and whatever I can find for concept formation (R is my real go-to language, but I&amp;#39;m a bit of a noob in text analytics, and don&amp;#39;t fully understand those packages yet.)  I&amp;#39;ve done some with Python&amp;#39;s nltk and similar packages, although I&amp;#39;m expert level in R, and struggle some in Python.&lt;/p&gt;

&lt;p&gt;Any thoughts about what&amp;#39;s the best general format to use -- one line at a time, or the entire document at a time?   By the way, the documents come from a variety of sources like Word, PDF, and OCR input, and are pretty low quality in terms of misread or mangled words, white space, and weird control characters.  I&amp;#39;m not sure there&amp;#39;s an obvious way to skip the line feeds in a Hive import to get to the second document-as-a-row format, but I can cross that bridge later, and have a few databases whizzes on my team that could help me figure it out if needed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlh1wv,True,,BullCityPicker,,19,True,all_ads,False,[],False,,/r/datascience/comments/nlh1wv/ideal_text_analytics_data_structure_one_document/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlh1wv/ideal_text_analytics_data_structure_one_document/,515405,1622035733.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm not sure if this is the right place to ask but here it goes.

I started my Etsy sales dataset in the daily format:
https://imgur.com/CXmWQuE

Then I grouped them by 'year-month': 
https://imgur.com/YFBODfA

I ended up with the following:
https://imgur.com/Lu0HlRa

For months where I don't have any sales for that specific listing_id, I want to have the month and zero next to it. 

For example:
listing #902533496 from the last screenshot, I want to add these rows: 

* 2021-01            0
* 2021-02            0
* 2021-05            0

Anybody has an idea on how to do this?

Thank you!",t2_7zm6p,False,,0,False,Working on a project analyzing my Etsy sales data,[],r/datascience,False,6,projects,0,,,False,t3_nlroo8,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1622092431.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m not sure if this is the right place to ask but here it goes.&lt;/p&gt;

&lt;p&gt;I started my Etsy sales dataset in the daily format:
&lt;a href=""https://imgur.com/CXmWQuE""&gt;https://imgur.com/CXmWQuE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then I grouped them by &amp;#39;year-month&amp;#39;: 
&lt;a href=""https://imgur.com/YFBODfA""&gt;https://imgur.com/YFBODfA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I ended up with the following:
&lt;a href=""https://imgur.com/Lu0HlRa""&gt;https://imgur.com/Lu0HlRa&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For months where I don&amp;#39;t have any sales for that specific listing_id, I want to have the month and zero next to it. &lt;/p&gt;

&lt;p&gt;For example:
listing #902533496 from the last screenshot, I want to add these rows: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2021-01            0&lt;/li&gt;
&lt;li&gt;2021-02            0&lt;/li&gt;
&lt;li&gt;2021-05            0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anybody has an idea on how to do this?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlroo8,True,,maxcaliburx,,5,True,all_ads,False,[],False,,/r/datascience/comments/nlroo8/working_on_a_project_analyzing_my_etsy_sales_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlroo8/working_on_a_project_analyzing_my_etsy_sales_data/,515405,1622063631.0,1,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?auto=webp&amp;s=eb0be617377d1035b74e223904049f299e6f0ccb', 'width': 708, 'height': 654}, 'resolutions': [{'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d811f8726df5091374ed527b1135bbba2cee885a', 'width': 108, 'height': 99}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b43e5b18443a2380de3d08b2b2bbce0ae3bd941', 'width': 216, 'height': 199}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=582e4989d8166e59dfe0579b75b9382ca9dfa42e', 'width': 320, 'height': 295}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da68722d1c6172e7c824fc9a2b10ed6ae5ed6643', 'width': 640, 'height': 591}], 'variants': {}, 'id': 'eDCvs1jx9Yhw3OQOadWU8ScsnvqaDC7lATDXidwDFG4'}], 'enabled': False}",,,,,
,datascience,"Hello, 

I am looking for some suggestions or ideas surrounding the most onerous and irksome data formats you’ve had to work with. Here’s the skinny: a very large, and in my humble opinion, just straight up malevolent company X is conducting retaliatory data requests at small public offices and institutions in a US state. One of my friends happens to work there. Now, I am all for free and open public data, 100%, but this is a nefarious attempt to drain and waste public resources and to intimidate. They are making entirely useless requests. 
 
I am a senior data analyst with a few skills here and there and would like to help my friend stick it to evil corp while still operating well within the law and legal requirements. If you were tasked with handling hundreds of thousands of emails, meeting notes, minutes, powerpoints etc.. what would be the most onerous and cumbersome file formats or organization? Like, I was thinking of converting all text to binary and writing a little python program to generate folder hierarchies and compress every single email etc.. somehow scramble things up a bit while still ensuring the data has integrity etc.. any suggestions?",t2_17fdw9,False,,0,False,Most onerous data formats?,[],r/datascience,False,6,discussion,0,,,False,t3_nlnpcx,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622081961.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, &lt;/p&gt;

&lt;p&gt;I am looking for some suggestions or ideas surrounding the most onerous and irksome data formats you’ve had to work with. Here’s the skinny: a very large, and in my humble opinion, just straight up malevolent company X is conducting retaliatory data requests at small public offices and institutions in a US state. One of my friends happens to work there. Now, I am all for free and open public data, 100%, but this is a nefarious attempt to drain and waste public resources and to intimidate. They are making entirely useless requests. &lt;/p&gt;

&lt;p&gt;I am a senior data analyst with a few skills here and there and would like to help my friend stick it to evil corp while still operating well within the law and legal requirements. If you were tasked with handling hundreds of thousands of emails, meeting notes, minutes, powerpoints etc.. what would be the most onerous and cumbersome file formats or organization? Like, I was thinking of converting all text to binary and writing a little python program to generate folder hierarchies and compress every single email etc.. somehow scramble things up a bit while still ensuring the data has integrity etc.. any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlnpcx,True,,pythagorasshat,,25,True,all_ads,False,[],False,,/r/datascience/comments/nlnpcx/most_onerous_data_formats/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlnpcx/most_onerous_data_formats/,515405,1622053161.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_bc606vut,False,,0,False,The Economist's excess deaths model,[],r/datascience,False,6,projects,0,70.0,,False,t3_nkz602,False,dark,0.97,,public,276,0,{},140.0,,False,[],,False,False,,{},Projects,False,276,,False,False,https://b.thumbs.redditmedia.com/iKrJJ8HNG7JckPGp-pF1WOw4vG5d6YE1OSA8Bp5xZdA.jpg,False,,[],{},,False,,1622003060.0,text,6,,,text,github.com,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkz602,True,,beleeee_dat,,49,True,all_ads,False,[],False,,/r/datascience/comments/nkz602/the_economists_excess_deaths_model/,all_ads,False,https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model,515405,1621974260.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?auto=webp&amp;s=717238cd6a5f5bd38e6d1f4af2dc19afe13a4b10', 'width': 2400, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3266601f149b7853aa79b9471c064805d61ed750', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=812e850b90195dfe1a1620e3635fff178022630e', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d40fbc7a742a8568c10b2f612678d0fe8dad6b9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=741a630cbb893fa0f06e0d63ace37f73bc2da231', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1259206dc74b91d5ef844fd4fefe356520b8800', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1801bcf4a2ee87e4dbdf6688d29d6125abe1cb1', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'eLGUN6fbVeJpOidBYzQIkX1RdE1KmMiaCOnXvdBkyBY'}], 'enabled': False}",,,https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model,,
,datascience,,t2_cc4pi6l7,False,,0,False,"Join Chief/ Heads of Data &amp; Analytics from Grab; Pfizer; Reckitt; Unistar Credit &amp; Finance; TymeGlobal. Talking AI, Big Data, Data Ethics",[],r/datascience,False,6,career,0,73.0,,False,t3_nltp3t,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Career,False,1,,False,False,https://a.thumbs.redditmedia.com/JkH_0qOdibB758RrS4n8caqAqE7eBjKu36-sIwtf4l4.jpg,False,,[],{},,False,,1622098324.0,text,6,,,text,cdao-asia.coriniumintelligence.com,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nltp3t,True,,Corinium_APAC,,0,True,all_ads,False,[],False,,/r/datascience/comments/nltp3t/join_chief_heads_of_data_analytics_from_grab/,all_ads,False,https://cdao-asia.coriniumintelligence.com/?utm_source=Linkedin%20Conv&amp;utm_medium=Linkedin%20Conv&amp;utm_campaign=0659%20CDAO%20ASEAN#CDAOREG,515405,1622069524.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?auto=webp&amp;s=883d06ad0202e142f3260ef55e6ea583f3fb1fd5', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fefb6c9a51af3391d0a5bd2b7bfcd618fb0441d5', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=09df3b86b117d6bb513a3062d1f00b871313e46d', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cbc189b38a9836883f326594278d043e77712f7', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=567456f0d359b33f70cab852e146790ad8b85ba6', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcd9d3d5bfd68946d63a542297bd93e314ee8763', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa88640a5572eed141b7a62e23009fee06ebbd54', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'G4c7GSDz80CFFMKvgq_y-d4c6yzJ_Z6eS3POUeNN6SI'}], 'enabled': False}",,,https://cdao-asia.coriniumintelligence.com/?utm_source=Linkedin%20Conv&amp;utm_medium=Linkedin%20Conv&amp;utm_campaign=0659%20CDAO%20ASEAN#CDAOREG,,
,datascience,"Hi all,

I posted a question yesterday asking if anyone had any thoughts on how I could predict the rate at which employees in my would the company. 

Thank you so much for the help! I've manage to get a working model that does it based on my current employee base, but when new employee's join, I haven't quite got it working there.

What my analysis showed is that, on average, 5% of all new employees we get will leave in the first week. Due to this, we should onboard an additional 5%. Here is where the circular reference starts. We therefore onboard 5% more and so our attrition rate increases, meaning we need to onboard more.

Has anyone come across an issue like this before?  


What assumptions can I take to stop the constant circle?

Any thoughts/advice would be really appreciated all!",t2_n8vyt,False,,0,False,Forecasting employee turnover with circular reference,[],r/datascience,False,6,discussion,0,,,False,t3_nlryy8,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622093289.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I posted a question yesterday asking if anyone had any thoughts on how I could predict the rate at which employees in my would the company. &lt;/p&gt;

&lt;p&gt;Thank you so much for the help! I&amp;#39;ve manage to get a working model that does it based on my current employee base, but when new employee&amp;#39;s join, I haven&amp;#39;t quite got it working there.&lt;/p&gt;

&lt;p&gt;What my analysis showed is that, on average, 5% of all new employees we get will leave in the first week. Due to this, we should onboard an additional 5%. Here is where the circular reference starts. We therefore onboard 5% more and so our attrition rate increases, meaning we need to onboard more.&lt;/p&gt;

&lt;p&gt;Has anyone come across an issue like this before?  &lt;/p&gt;

&lt;p&gt;What assumptions can I take to stop the constant circle?&lt;/p&gt;

&lt;p&gt;Any thoughts/advice would be really appreciated all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlryy8,True,,claret_n_blue,,3,True,all_ads,False,[],False,,/r/datascience/comments/nlryy8/forecasting_employee_turnover_with_circular/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlryy8/forecasting_employee_turnover_with_circular/,515405,1622064489.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"A recruiter has sent me a paper to review including “The six primary data quality assessment” before an online assessment I will go through with him, in which he will send me a dataset to find data quality issues using Python/Pandas within an hour.

- Where to find datasets to practice on?

- What do I expect to be doing in this hour?

- How to practice and be ready in the next couple of days?

Link to the paper: https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf",t2_ryrcq,False,,0,False,How can I practice finding data quality issues before an interview assessment?,[],r/datascience,False,6,,0,,,False,t3_nlql25,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,1622063095.0,,[],{},,True,,1622089461.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A recruiter has sent me a paper to review including “The six primary data quality assessment” before an online assessment I will go through with him, in which he will send me a dataset to find data quality issues using Python/Pandas within an hour.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where to find datasets to practice on?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What do I expect to be doing in this hour?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to practice and be ready in the next couple of days?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Link to the paper: &lt;a href=""https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf""&gt;https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nlql25,True,,Abdullah_super,,1,True,all_ads,False,[],False,,/r/datascience/comments/nlql25/how_can_i_practice_finding_data_quality_issues/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlql25/how_can_i_practice_finding_data_quality_issues/,515405,1622060661.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I've been reading a lot about Palantir lately and how they are creating software that analyzes data and creates models for you. So naturally, i've been concerned that this new technology will eventually replace data analysts/scientists. I know right now they are only working with the government but from what i've read, they hope to eventually move into the private sector as well. Anyone have any thoughts on this and how this might affect the future of data science jobs? I'm worried the job will be made redundant by this tech and the more I read the more worried I get- it just makes me sad tbh",t2_6adzpsau,False,,0,False,Will Palantir replace data science teams?,[],r/datascience,False,6,career,0,,,False,t3_nm0lab,False,dark,0.29,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1622122861.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been reading a lot about Palantir lately and how they are creating software that analyzes data and creates models for you. So naturally, i&amp;#39;ve been concerned that this new technology will eventually replace data analysts/scientists. I know right now they are only working with the government but from what i&amp;#39;ve read, they hope to eventually move into the private sector as well. Anyone have any thoughts on this and how this might affect the future of data science jobs? I&amp;#39;m worried the job will be made redundant by this tech and the more I read the more worried I get- it just makes me sad tbh&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm0lab,True,,Lanky_Seaworthiness8,,8,True,all_ads,False,[],False,,/r/datascience/comments/nm0lab/will_palantir_replace_data_science_teams/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm0lab/will_palantir_replace_data_science_teams/,515405,1622094061.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm seeing this quite a lot, is this normal or are they just throwing in some buzz words they've seen for data science skills?

I've always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist's preference.",t2_4mk1e,False,,0,False,Data science job postings asking for both Python and R?,[],r/datascience,False,6,discussion,0,,,False,t3_nle5jq,False,dark,0.65,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1622055545.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m seeing this quite a lot, is this normal or are they just throwing in some buzz words they&amp;#39;ve seen for data science skills?&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist&amp;#39;s preference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nle5jq,True,,Jimbobmij,,27,True,all_ads,False,[],False,,/r/datascience/comments/nle5jq/data_science_job_postings_asking_for_both_python/,all_ads,False,https://www.reddit.com/r/datascience/comments/nle5jq/data_science_job_postings_asking_for_both_python/,515405,1622026745.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm currently an engineering student, and I started my data science journey about a year ago when I discovered my passion for data. Since then, I have been self-taught all the way from MOOCs and books. I tried to apply what I learnt through personal projects and internships, but the entire time I feel like I've only been searching up, copying code and modifying it for my own needs. 

Most of the time I don't write any original code except for some simple functions or if I'm working on a platform with custom syntax. I am mostly familiar with the libraries I need, but only to the extent where I know which specific library can help me with a particular task. 

I know it's still pretty early in my DS journey, but I can't help but feel incompetent sometimes. Does anyone else have the same problem? If not, how do I overcome this and become better at writing original code?",t2_l7hoj,False,,0,False,Does anyone else feel like an incompetent programmer?,[],r/datascience,False,6,discussion,0,,,False,t3_nknmvd,False,dark,0.91,,public,136,2,{},,,False,[],,False,False,,{},Discussion,False,136,,False,False,self,False,,[],{},,True,,1621971504.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently an engineering student, and I started my data science journey about a year ago when I discovered my passion for data. Since then, I have been self-taught all the way from MOOCs and books. I tried to apply what I learnt through personal projects and internships, but the entire time I feel like I&amp;#39;ve only been searching up, copying code and modifying it for my own needs. &lt;/p&gt;

&lt;p&gt;Most of the time I don&amp;#39;t write any original code except for some simple functions or if I&amp;#39;m working on a platform with custom syntax. I am mostly familiar with the libraries I need, but only to the extent where I know which specific library can help me with a particular task. &lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s still pretty early in my DS journey, but I can&amp;#39;t help but feel incompetent sometimes. Does anyone else have the same problem? If not, how do I overcome this and become better at writing original code?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nknmvd,True,,ReaperJr,,52,True,all_ads,False,[],False,,/r/datascience/comments/nknmvd/does_anyone_else_feel_like_an_incompetent/,all_ads,False,https://www.reddit.com/r/datascience/comments/nknmvd/does_anyone_else_feel_like_an_incompetent/,515405,1621942704.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"&amp;#x200B;

So I have been working overall for around 6 months as a data analyst. The first 4 months were a full-time internship at a big4 and the last 2 months have been a full-time position at a 6 years old startup. The experience at both firms is amazing and enjoyable. However, the tasks themselves can sometimes be frustrating. 

**Problem**

The frustration comes mainly from hard to clean datasets (data inconsistency) or tasks that are not at my level of experience yet (aka challenges). We also receive in some cases datasets that are slightly modified (for no reason) which causes the script to give an error...

**Context**

We receive generally sales data from retailers, we clean them and push them to the database, so we can later on visualize, analyze, create predictive models.

**Discussion**

I would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them (stackoverflow, asked your supervisor ...). 

I wrote a [small blog here](https://dataanalystlife.blogspot.com/2021/05/data-inconsistency-in-real-life.html) about my thoughts  and stories so far (a 2 mins read). However, I would be much more interested in the real stories in the comment section than the traffic to my blog, because it will help me deal with the situations in a better way and with less frustrations on the long run.",t2_7mkrswyv,False,,0,False,Job frustrations (Data science/analysis),[],r/datascience,False,6,discussion,0,,,False,t3_nky5ei,False,dark,0.87,,public,31,0,{},,,False,[],,False,False,,{},Discussion,False,31,,False,False,self,False,,[],{},,True,,1622000320.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I have been working overall for around 6 months as a data analyst. The first 4 months were a full-time internship at a big4 and the last 2 months have been a full-time position at a 6 years old startup. The experience at both firms is amazing and enjoyable. However, the tasks themselves can sometimes be frustrating. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The frustration comes mainly from hard to clean datasets (data inconsistency) or tasks that are not at my level of experience yet (aka challenges). We also receive in some cases datasets that are slightly modified (for no reason) which causes the script to give an error...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We receive generally sales data from retailers, we clean them and push them to the database, so we can later on visualize, analyze, create predictive models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them (stackoverflow, asked your supervisor ...). &lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=""https://dataanalystlife.blogspot.com/2021/05/data-inconsistency-in-real-life.html""&gt;small blog here&lt;/a&gt; about my thoughts  and stories so far (a 2 mins read). However, I would be much more interested in the real stories in the comment section than the traffic to my blog, because it will help me deal with the situations in a better way and with less frustrations on the long run.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nky5ei,True,,Ecstatic_Tooth_1096,,18,True,all_ads,False,[],False,,/r/datascience/comments/nky5ei/job_frustrations_data_scienceanalysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/nky5ei/job_frustrations_data_scienceanalysis/,515405,1621971520.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm head of sales for a relatively small company (~20m annual sales) and do all of the sales data analysis via Power BI and I'm learning Python.  Just curious how many other people in similar positions and how you're dealing with it.  Any insight or guidance is appreciated.

I love working with data but have 20 years of sales specific experience.  Previously wrote programs/whatever you'd like to call it in Excel VBA.  Currently pulling from SQL into Power BI realizing I can't do what I want without Python/Seaborn etc.",t2_1f2mdynj,False,,0,False,How many of you are in a Sales position but doing data science for your company?,[],r/datascience,False,6,discussion,0,,,False,t3_nl8wmt,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622034386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m head of sales for a relatively small company (~20m annual sales) and do all of the sales data analysis via Power BI and I&amp;#39;m learning Python.  Just curious how many other people in similar positions and how you&amp;#39;re dealing with it.  Any insight or guidance is appreciated.&lt;/p&gt;

&lt;p&gt;I love working with data but have 20 years of sales specific experience.  Previously wrote programs/whatever you&amp;#39;d like to call it in Excel VBA.  Currently pulling from SQL into Power BI realizing I can&amp;#39;t do what I want without Python/Seaborn etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl8wmt,True,,neededtowrite,,4,True,all_ads,False,[],False,,/r/datascience/comments/nl8wmt/how_many_of_you_are_in_a_sales_position_but_doing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl8wmt/how_many_of_you_are_in_a_sales_position_but_doing/,515405,1622005586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I find it demeaning,t2_gw1hd,False,,0,False,I'm offended by having to scale my data,[],r/datascience,False,6,fun,0,,,False,t3_nkbqx6,False,dark,0.94,,public,606,4,{},,,False,[],,False,False,,{},Fun/Trivia,False,606,,False,False,self,False,,[],{'gid_1': 3},,True,,1621928644.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I find it demeaning&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkbqx6,True,,beepbloopbloop,,40,True,all_ads,False,[],False,,/r/datascience/comments/nkbqx6/im_offended_by_having_to_scale_my_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkbqx6/im_offended_by_having_to_scale_my_data/,515405,1621899844.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_gafbm9o,False,,0,False,"I find sometimes that the most interesting or unique analysis comes from ""amateurs"". What cool projects you have seen or worked on?",[],r/datascience,False,6,discussion,0,,,False,t3_nlhan4,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,default,False,,[],{},,False,,1622065205.0,text,6,,,text,self.TheAnalystEconomy,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlhan4,True,,Kobedoggg,,1,True,all_ads,False,[],False,,/r/datascience/comments/nlhan4/i_find_sometimes_that_the_most_interesting_or/,all_ads,False,/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/,515405,1622036405.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'TheAnalystEconomy', 'selftext': 'By ""amateur"", I do not mean inexperienced, less skilled or bad necessarily. More that the analysis came from an area of passion, rather than a professional environment.\n\nThe origin of the word ""amateur"" is French and means ""one who loves"". Amateur analysts, to me, are those who spend time on problems for their own sake because they are intrinsically motivating, beautiful or fascinating.\n\nI would love to hear about any cool projects people have worked on in their spare time, regardless of the topic. Comment below.', 'author_fullname': 't2_gafbm9o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I find sometimes that the most interesting or unique analysis comes from ""amateurs"". What cool projects you have seen or worked on?', 'link_flair_richtext': [{'e': 'text', 't': 'General discussion'}], 'subreddit_name_prefixed': 'r/TheAnalystEconomy', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nlf6ih', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'General discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622059011.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.TheAnalystEconomy', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;By &amp;quot;amateur&amp;quot;, I do not mean inexperienced, less skilled or bad necessarily. More that the analysis came from an area of passion, rather than a professional environment.&lt;/p&gt;\n\n&lt;p&gt;The origin of the word &amp;quot;amateur&amp;quot; is French and means &amp;quot;one who loves&amp;quot;. Amateur analysts, to me, are those who spend time on problems for their own sake because they are intrinsically motivating, beautiful or fascinating.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear about any cool projects people have worked on in their spare time, regardless of the topic. Comment below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dcd9e614-ad4a-11eb-9428-0e37c2f586d7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_4bo841', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#dadada', 'id': 'nlf6ih', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Kobedoggg', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/', 'subreddit_subscribers': 1661, 'created_utc': 1622030211.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/,t3_nlf6ih,
,datascience,"TLDR: As a data scientist how important are the assumptions for the statistical methods of analysis?

Currently I am working on my MS in data science, and I've been thinking a lot about how my data science CS courses practically ignore basic statistical practices that I was taught in undergrad as a stats major. For example: the last two semesters I have taken two courses offered through the college of computer science, taught by the same professor, one: data preparation and analysis, and two: data mining. Having taken a data analysis class in undergad (a different university and offered under the college of mathematics as a statistics course) I was very well equipped for this course but I noticed that the professor overlooked a lot of things that I was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met. I didn't think much of it, this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods (though not all of the methods in this course are addressed in the prerequisite courses).

The next semester I take Data Mining, an undergraduate level course. Like I said, same professor, and a CS course. I understand that data mining might not be heavily as heavily based in stats as CS with its basis in machine learning and AI, but the stats is a piece. During our final project I was discussing with some friends the trouble I was having meeting assumptions with the dataset given (same project and dataset for everyone) and asked them how they were handling it. My friends, no stats majors, could not understand what my issues were. When I was explaining to them the assumptions for a principle component analysis (PCA) (a large part of the project) they said that I was making it a bigger deal than it needs to be, and I should just run the PCA with no check on the assumptions and move on like the example the prof provided us. Unable to get any help on my problem I did just that, turned in my project and to my surprise I got a 100%. I couldn't believe that I got no points deducted for not checking the assumptions. The previous semester I had a project too, my partner dropped the class in the middle of the semester so I did the project, assumption checks and all, on my own with little problems so there wasn't an issue.

As a statistician running analyses without checking assumptions raises huge ethical flags. How can I know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met? I went on Kaggle to look at other people's code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed. It made me wonder if in data science it was enough to run the analysis and predict, as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data.",t2_4oa8ltdj,False,,0,False,How much of a role should assumptions for statistical methods of analysis play in data science?,[],r/datascience,False,6,discussion,0,,,False,t3_nkwlgu,False,dark,0.76,,public,15,0,{},,,False,[],,False,False,,{},Discussion,False,15,,False,False,self,False,,[],{},,True,,1621996159.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;TLDR: As a data scientist how important are the assumptions for the statistical methods of analysis?&lt;/p&gt;

&lt;p&gt;Currently I am working on my MS in data science, and I&amp;#39;ve been thinking a lot about how my data science CS courses practically ignore basic statistical practices that I was taught in undergrad as a stats major. For example: the last two semesters I have taken two courses offered through the college of computer science, taught by the same professor, one: data preparation and analysis, and two: data mining. Having taken a data analysis class in undergad (a different university and offered under the college of mathematics as a statistics course) I was very well equipped for this course but I noticed that the professor overlooked a lot of things that I was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met. I didn&amp;#39;t think much of it, this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods (though not all of the methods in this course are addressed in the prerequisite courses).&lt;/p&gt;

&lt;p&gt;The next semester I take Data Mining, an undergraduate level course. Like I said, same professor, and a CS course. I understand that data mining might not be heavily as heavily based in stats as CS with its basis in machine learning and AI, but the stats is a piece. During our final project I was discussing with some friends the trouble I was having meeting assumptions with the dataset given (same project and dataset for everyone) and asked them how they were handling it. My friends, no stats majors, could not understand what my issues were. When I was explaining to them the assumptions for a principle component analysis (PCA) (a large part of the project) they said that I was making it a bigger deal than it needs to be, and I should just run the PCA with no check on the assumptions and move on like the example the prof provided us. Unable to get any help on my problem I did just that, turned in my project and to my surprise I got a 100%. I couldn&amp;#39;t believe that I got no points deducted for not checking the assumptions. The previous semester I had a project too, my partner dropped the class in the middle of the semester so I did the project, assumption checks and all, on my own with little problems so there wasn&amp;#39;t an issue.&lt;/p&gt;

&lt;p&gt;As a statistician running analyses without checking assumptions raises huge ethical flags. How can I know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met? I went on Kaggle to look at other people&amp;#39;s code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed. It made me wonder if in data science it was enough to run the analysis and predict, as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkwlgu,True,,rProgs,,34,True,all_ads,False,[],False,,/r/datascience/comments/nkwlgu/how_much_of_a_role_should_assumptions_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkwlgu/how_much_of_a_role_should_assumptions_for/,515405,1621967359.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been wracking my brains on trying to do this efficiently.  I want to learn a probability distribution of a bunch of values and sample new data points from that curve

Given a numpy array/matrix of integers/floats, I want to be able to learn the probability density function of those values(per array in matrix) and sample or generate new values or columns for the matrix as per the probability distribution of that row

If possible, group similar rows in the matrix or pick a subset and then do a signal to noise kind of thing  to generate more data points as features or columns. 

Is there any prebuilt package in numpy, sklearn or scipy that allows me to achieve this or apply it to a whole matrix at a time to scale quickly?  Even if some part of this is achievable through a package that would be a huge help",t2_69fak7f,False,,0,False,Data Augmentation idea help,[],r/datascience,False,6,discussion,0,,,False,t3_nl595z,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,True,self,False,,[],{},,True,,1622021640.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been wracking my brains on trying to do this efficiently.  I want to learn a probability distribution of a bunch of values and sample new data points from that curve&lt;/p&gt;

&lt;p&gt;Given a numpy array/matrix of integers/floats, I want to be able to learn the probability density function of those values(per array in matrix) and sample or generate new values or columns for the matrix as per the probability distribution of that row&lt;/p&gt;

&lt;p&gt;If possible, group similar rows in the matrix or pick a subset and then do a signal to noise kind of thing  to generate more data points as features or columns. &lt;/p&gt;

&lt;p&gt;Is there any prebuilt package in numpy, sklearn or scipy that allows me to achieve this or apply it to a whole matrix at a time to scale quickly?  Even if some part of this is achievable through a package that would be a huge help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl595z,True,,Nike_Zoldyck,,7,True,all_ads,False,[],False,,/r/datascience/comments/nl595z/data_augmentation_idea_help/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl595z/data_augmentation_idea_help/,515405,1621992840.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I read an interesting article on Medium that claims the Data Scientist title will go the way of the dodo in a decade. 

The TLDR is: data science tools will become as ubiquitous as MS Office products, and everyone will be expected to be skilled in using them. Titles will transition back to reflecting domain knowledge rather than skills will data.

What are your thoughts? Do you agree? Will this impact how you approach your career path?

It reminds me of Chandler making a lot of money for simple data entry. I never understood why something so easy would pay so well, but maybe the next generation will say the same of us...

 https://link.medium.com/hiXbXHrDzgb",t2_2ncqmqa0,False,,0,False,Data Scientists extinct within 10 years?,[],r/datascience,False,6,discussion,0,,,False,t3_nlgnd6,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622063386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read an interesting article on Medium that claims the Data Scientist title will go the way of the dodo in a decade. &lt;/p&gt;

&lt;p&gt;The TLDR is: data science tools will become as ubiquitous as MS Office products, and everyone will be expected to be skilled in using them. Titles will transition back to reflecting domain knowledge rather than skills will data.&lt;/p&gt;

&lt;p&gt;What are your thoughts? Do you agree? Will this impact how you approach your career path?&lt;/p&gt;

&lt;p&gt;It reminds me of Chandler making a lot of money for simple data entry. I never understood why something so easy would pay so well, but maybe the next generation will say the same of us...&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/hiXbXHrDzgb""&gt;https://link.medium.com/hiXbXHrDzgb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlgnd6,True,,kdawgovich,,50,True,all_ads,False,[],False,,/r/datascience/comments/nlgnd6/data_scientists_extinct_within_10_years/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlgnd6/data_scientists_extinct_within_10_years/,515405,1622034586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This is a little confusing, so I'll add all the details that I know so far. We have Facebook Ads running for a product that is displayed on Amazon, but my manager wants me to create a website for this product and then analyze which Ad people are coming from.

At the same time, there's Influencer Marketing for this product as well. Unfortunately, Adobe Analytics would be an easy way to track where people are coming from, but my company doesn't use that. There are a lot of clicks on the Ads, but no one is buying the product, so I'm trying to see exactly where on the website they stop and how to optimize the site layout. My manager really wants me to create a website in order to track all of the information instead of using Amazon, and I'm not so sure if that step is needed at all.

The product started out as really successful, but now almost no one is buying it, so it seems like a weird project. 

The main issue is that I'm not sure how to even start collecting data. After I'm able to collect the data, I can then analyze it and make a model, but I don't have a background in marketing or marketing analytics, so I'm pretty lost. ",t2_7lgf0u9e,False,,0,False,Marketing Analytics - how do I track where customers are coming from and how far on the website they go to?,[],r/datascience,False,6,projects,0,,,False,t3_nl0fz7,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,self,False,,[],{},,True,,1622006690.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a little confusing, so I&amp;#39;ll add all the details that I know so far. We have Facebook Ads running for a product that is displayed on Amazon, but my manager wants me to create a website for this product and then analyze which Ad people are coming from.&lt;/p&gt;

&lt;p&gt;At the same time, there&amp;#39;s Influencer Marketing for this product as well. Unfortunately, Adobe Analytics would be an easy way to track where people are coming from, but my company doesn&amp;#39;t use that. There are a lot of clicks on the Ads, but no one is buying the product, so I&amp;#39;m trying to see exactly where on the website they stop and how to optimize the site layout. My manager really wants me to create a website in order to track all of the information instead of using Amazon, and I&amp;#39;m not so sure if that step is needed at all.&lt;/p&gt;

&lt;p&gt;The product started out as really successful, but now almost no one is buying it, so it seems like a weird project. &lt;/p&gt;

&lt;p&gt;The main issue is that I&amp;#39;m not sure how to even start collecting data. After I&amp;#39;m able to collect the data, I can then analyze it and make a model, but I don&amp;#39;t have a background in marketing or marketing analytics, so I&amp;#39;m pretty lost. &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl0fz7,True,,SnooWalruses4775,,3,True,all_ads,False,[],False,,/r/datascience/comments/nl0fz7/marketing_analytics_how_do_i_track_where/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl0fz7/marketing_analytics_how_do_i_track_where/,515405,1621977890.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hey there! Long time lurker, first time poster. I love this sub, I learned quite a lot and I am very thankful for all the interactions. 
About my post, I work as a DS in the risk area of a bank, nonetheless, lately my boss has been taking away my projects and giving them to someone else, also cancelling my meetings and engaging less with me. I fear he is trying to make me quit, management has used this tactic to force other employees to quit before.
I decided to look for other jobs, no luck so far, and I read a bunch of articles suggesting me to do freelance work. My github portfolio is wanting and I am asking for any and all advice from the community:
What is your experience doing freelance? I am setting my account on upwork atm.
How may I get noticed? Any experience you can give a n00b like me? Can I get a living wage from freelance?
Much appreciated!",t2_4oymsu8g,False,,0,False,Freelance experience as a DS you may share,[],r/datascience,False,6,,0,,,False,t3_nky62p,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1622000370.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there! Long time lurker, first time poster. I love this sub, I learned quite a lot and I am very thankful for all the interactions. 
About my post, I work as a DS in the risk area of a bank, nonetheless, lately my boss has been taking away my projects and giving them to someone else, also cancelling my meetings and engaging less with me. I fear he is trying to make me quit, management has used this tactic to force other employees to quit before.
I decided to look for other jobs, no luck so far, and I read a bunch of articles suggesting me to do freelance work. My github portfolio is wanting and I am asking for any and all advice from the community:
What is your experience doing freelance? I am setting my account on upwork atm.
How may I get noticed? Any experience you can give a n00b like me? Can I get a living wage from freelance?
Much appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nky62p,True,,another_dissapoint,,2,True,all_ads,False,[],False,,/r/datascience/comments/nky62p/freelance_experience_as_a_ds_you_may_share/,all_ads,False,https://www.reddit.com/r/datascience/comments/nky62p/freelance_experience_as_a_ds_you_may_share/,515405,1621971570.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I just started a lead role with my company and have was curious if anyone else has done something similar. Basically, I will be leading (and helping develop) efforts towards curating a use case list, choosing, of that list, what we'll be working on, based on business value, and executing those solutions. This effort started a couple of weeks before I began the new role, so I am catching up. Our executive leader has some passion around developing and using some tools we do not already have, so there is a bit of focus on graph analytics right now, but other ML and AI solutions are not out of the question, I just need work with my team to decide on the use cases and create a document for our plan. 

I've done this for other projects that were not DS related, but was curious if anyone has any sort of template or guidance for a format that worked really well for them. Maybe even an outline of what should be included and how it should be organized. I don't have a lot of direction, since I am being asked to run with this, so I thought this community might have some good suggestions. Thanks so much!",t2_v7o60,False,,0,False,Roadmap / Plan Documentation for DS Projects,[],r/datascience,False,6,discussion,0,,,False,t3_nkzz3e,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622005439.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just started a lead role with my company and have was curious if anyone else has done something similar. Basically, I will be leading (and helping develop) efforts towards curating a use case list, choosing, of that list, what we&amp;#39;ll be working on, based on business value, and executing those solutions. This effort started a couple of weeks before I began the new role, so I am catching up. Our executive leader has some passion around developing and using some tools we do not already have, so there is a bit of focus on graph analytics right now, but other ML and AI solutions are not out of the question, I just need work with my team to decide on the use cases and create a document for our plan. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve done this for other projects that were not DS related, but was curious if anyone has any sort of template or guidance for a format that worked really well for them. Maybe even an outline of what should be included and how it should be organized. I don&amp;#39;t have a lot of direction, since I am being asked to run with this, so I thought this community might have some good suggestions. Thanks so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkzz3e,True,,azdatasci,,2,True,all_ads,False,[],False,,/r/datascience/comments/nkzz3e/roadmap_plan_documentation_for_ds_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkzz3e/roadmap_plan_documentation_for_ds_projects/,515405,1621976639.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

At work, I am looking to predict how many employees will leave the company, on a weekly basis.

We contract a lot of our staff, so every week, we can have new joiners and those that leave (either due to their contracts ending, poor performance, etc).

Depending how work is going, we may be looking to hire 10 new contractors every week, with around 3/4 leaving. However in times when we get particularly busy, I want to be able to predict that more employees will leave and so we can tell HR to hire more contractors.

The data I have available (but can try and get more) is:

\- Total number of employees that have left every week

\- Total number of employees we hire every week

\- Current end of contracts/rating of contractors (those with particularly poor ratings, we will look to terminate).

&amp;#x200B;

Can someone help me understand which direction I need to go in to do my forecasting? Just to confirm, I'm asking for you to point me in the direction, as I have never done this before!  


I've read a few articles on Python that try and predict which of your current crop of employees will leave, but as there is a chance that our contractors can change, I need this model to be future proof.

&amp;#x200B;

My initial thoughts were to create a basic model in Excel, where I calculate a baseline using the past 4 weeks actuals, then apply an uplift in weeks with a high end of contract, and when we see high number of low rated contractors. 

This is quite manual though, so I was wondering if there was a more clever and robust way to do  this.

&amp;#x200B;

Any advice/information would hugely appreciated.

&amp;#x200B;

Thanks all",t2_n8vyt,False,,0,False,Predicting number of employees that will leave for a high turnover workforce,[],r/datascience,False,6,discussion,0,,,False,t3_nkurf1,False,dark,0.72,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1621991350.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;At work, I am looking to predict how many employees will leave the company, on a weekly basis.&lt;/p&gt;

&lt;p&gt;We contract a lot of our staff, so every week, we can have new joiners and those that leave (either due to their contracts ending, poor performance, etc).&lt;/p&gt;

&lt;p&gt;Depending how work is going, we may be looking to hire 10 new contractors every week, with around 3/4 leaving. However in times when we get particularly busy, I want to be able to predict that more employees will leave and so we can tell HR to hire more contractors.&lt;/p&gt;

&lt;p&gt;The data I have available (but can try and get more) is:&lt;/p&gt;

&lt;p&gt;- Total number of employees that have left every week&lt;/p&gt;

&lt;p&gt;- Total number of employees we hire every week&lt;/p&gt;

&lt;p&gt;- Current end of contracts/rating of contractors (those with particularly poor ratings, we will look to terminate).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can someone help me understand which direction I need to go in to do my forecasting? Just to confirm, I&amp;#39;m asking for you to point me in the direction, as I have never done this before!  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read a few articles on Python that try and predict which of your current crop of employees will leave, but as there is a chance that our contractors can change, I need this model to be future proof.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My initial thoughts were to create a basic model in Excel, where I calculate a baseline using the past 4 weeks actuals, then apply an uplift in weeks with a high end of contract, and when we see high number of low rated contractors. &lt;/p&gt;

&lt;p&gt;This is quite manual though, so I was wondering if there was a more clever and robust way to do  this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any advice/information would hugely appreciated.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks all&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkurf1,True,,claret_n_blue,,16,True,all_ads,False,[],False,,/r/datascience/comments/nkurf1/predicting_number_of_employees_that_will_leave/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkurf1/predicting_number_of_employees_that_will_leave/,515405,1621962550.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"These last couple of years, I've spent a lot of time writing SQL. I put together some lessons learned to use it effectively for Data Science projects.

Small things like using CTEs, auto-formatting, and jinja have made a huge difference for me.

What other recommendations you have to master SQL for Data Science?

[https://ploomber.io/posts/sql/](https://ploomber.io/posts/sql/)",t2_5r54hksr,False,,0,False,Effective SQL for Data Science,[],r/datascience,False,6,discussion,0,,,False,t3_nka0nu,False,dark,0.94,,public,121,0,{},,,False,[],,False,False,,{},Discussion,False,121,,False,True,self,False,,[],{},,True,,1621923595.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;These last couple of years, I&amp;#39;ve spent a lot of time writing SQL. I put together some lessons learned to use it effectively for Data Science projects.&lt;/p&gt;

&lt;p&gt;Small things like using CTEs, auto-formatting, and jinja have made a huge difference for me.&lt;/p&gt;

&lt;p&gt;What other recommendations you have to master SQL for Data Science?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://ploomber.io/posts/sql/""&gt;https://ploomber.io/posts/sql/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nka0nu,True,,ploomber-io,,55,True,all_ads,False,[],False,,/r/datascience/comments/nka0nu/effective_sql_for_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nka0nu/effective_sql_for_data_science/,515405,1621894795.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am using programs like node2vec, graph2vec, doc2vec etc to build a knowledge graph. The output of all of them comes in 128 length vectors.

Is it feasible to try and compress the length to 2 or 3 dimensions so that I might visualize what any of the above methods have accomplished? If it it feasible than can I get pointed in the right direction for python libraries, packages, etc? Does not need to be perfect... just needs to give more than a csv file lol.

Thank you!",t2_wwwre7,False,,0,False,Visualizing Multidimensional Data... 128 Dimensions ---&gt; 2D or 3D,[],r/datascience,False,6,tooling,0,,,False,t3_nl3j1f,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1622015961.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using programs like node2vec, graph2vec, doc2vec etc to build a knowledge graph. The output of all of them comes in 128 length vectors.&lt;/p&gt;

&lt;p&gt;Is it feasible to try and compress the length to 2 or 3 dimensions so that I might visualize what any of the above methods have accomplished? If it it feasible than can I get pointed in the right direction for python libraries, packages, etc? Does not need to be perfect... just needs to give more than a csv file lol.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl3j1f,True,,smok1naces,,5,True,all_ads,False,[],False,,/r/datascience/comments/nl3j1f/visualizing_multidimensional_data_128_dimensions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl3j1f/visualizing_multidimensional_data_128_dimensions/,515405,1621987161.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys,

I recently got a position of a PO of a data science team. Previously I always worked with classic engineering teams. Needless to say, I quickly come to realize that 1) data science team != engineering team in terms of way of working and approach to work 2) I probably could use some development of how to be a better PO for such a team.

&amp;#x200B;

What I would like to ask you guys about:

1. Could you recommend a good book / training / materials to become better in working with a data science team?
2. Could you recommend a good blog / web-site where I, as a PO can read updates in data science community that might be relevant for me? (by now I typically find resources which are more focused on tech issues / updates which would be more relevant for the ML engineers, data scientists) 

Thank you!",t2_45py034y,False,,0,False,Advice needed - PO for data science team,[],r/datascience,False,6,education,0,,,False,t3_nkqyll,False,dark,0.77,,public,5,0,{},,,False,[],,False,False,,{},Education,False,5,,False,False,self,False,,[],{},,True,,1621981461.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;I recently got a position of a PO of a data science team. Previously I always worked with classic engineering teams. Needless to say, I quickly come to realize that 1) data science team != engineering team in terms of way of working and approach to work 2) I probably could use some development of how to be a better PO for such a team.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I would like to ask you guys about:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Could you recommend a good book / training / materials to become better in working with a data science team?&lt;/li&gt;
&lt;li&gt;Could you recommend a good blog / web-site where I, as a PO can read updates in data science community that might be relevant for me? (by now I typically find resources which are more focused on tech issues / updates which would be more relevant for the ML engineers, data scientists) &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkqyll,True,,ohshouldi,,8,False,all_ads,False,[],False,,/r/datascience/comments/nkqyll/advice_needed_po_for_data_science_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkqyll/advice_needed_po_for_data_science_team/,515405,1621952661.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi datascience,
My team at work has been asked to share some of our work at an upcoming event. from a best-practices perspective, how much of our methodology should we share? 

My concern is that much of our data is publicly available and the rest can be purchased easily. Our models are largely off the shelf or require minimal tuning. Our innovations are really elegant and not hard to reproduce. Should I be at all worried that a larger, more experienced DS team will piggyback off of our efforts and compete for our lucrative contracts? What parts of data + model + configuration should be kept obscure?",t2_4hepb,False,,0,False,How much to share in a public presentation,[],r/datascience,False,6,discussion,0,,,False,t3_nkyiwc,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622001312.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi datascience,
My team at work has been asked to share some of our work at an upcoming event. from a best-practices perspective, how much of our methodology should we share? &lt;/p&gt;

&lt;p&gt;My concern is that much of our data is publicly available and the rest can be purchased easily. Our models are largely off the shelf or require minimal tuning. Our innovations are really elegant and not hard to reproduce. Should I be at all worried that a larger, more experienced DS team will piggyback off of our efforts and compete for our lucrative contracts? What parts of data + model + configuration should be kept obscure?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkyiwc,True,,theinexplicablefuzz,,2,True,all_ads,False,[],False,,/r/datascience/comments/nkyiwc/how_much_to_share_in_a_public_presentation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkyiwc/how_much_to_share_in_a_public_presentation/,515405,1621972512.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Edit: Disclaimer, not 'stress' burnout, more like, feels like a total dead end role now, can't go on much longer, like Peter Gibbons from Office Space, that's how I feel.

Ok, context in a nutshell, been at this tech company for about 4yrs in a mixture of roles from product to market research, but been working as a data analyst for the last 2yrs, nearly 2.5

My goal had been (and agreed with HR) to progress towards a data scientist role when I originally signed up to it. But this has not materialised.

Fast forward 2.5yrs, I've had absolutely nobody in the business to learn from(never had), everything has been self-taught, my day to day work has been much more BI related (dashboards, product stats etc) as opposed to core data science tasks. I really have to go out of my way if I want to write and code, even SQL (data is all maintained by gatekeepers, mostly inefficient ones, so I have to request it mostly, then it's just an out the box plug in to PowerBI/Tableau or similar)

I've absolutely maxed out my patience with this company and their talk the talk grand plans. My technical skills have totally stagnated, and if I was to compare it to when I left uni, my skills regressed in areas such as advanced mathematical/statistical techniques. My day to day tasks typically involve adhoc requests solved with Tableau (with absolutely horrendous data quality). There are some back end things involving SQL/Mongo/AWS but these are probably 10% overall if even that. When it comes to addressing the data, I'm more like a project manager, because I'm not allowed to fix or implement things myself. So I end up making requests for things and gathering requirements like a PO/BA/ProjMan hybrid role, but even that is a waste of space, e.g. simple request for consumer product usage? Dead in the water for over a year despite weekly meetings because the development team won't implement changes needed to track them (new POs can be seen mentally questioning life choices when the answer to 'so where can I see the usage stats' is, ""they don't exist"")

The only thing it has that appeals is a degree of autonomy with what I do once I finish the adhoc requests. I easily have 50% of my working hours to address anything I like (in my more motivated, enthusiastic days this was spent addressing the data quality issues, data warehouse etc, but the way these ideas and suggestions were valued/met just killed my soul; options and recommendations closed due to 'time', 'cost', 'effort' etc

I'm thinking of just self teaching the core data science content I wanted to initially learn (I'm already doing this, just not applying it to anything at work), and finding things to apply this to at work. 

The problem is, I'm all alone. Nobody in this company has a data science background, so I can't learn from anyone here. I'll have no idea if what I am doing is good practice or even straight up wrong. Even if I did, the data is such bad quality and held in rigid, closed off systems (I am always refused direct access to it) that I'm not convinced there's a lot of real insight to get from it

Given that I have no real world data science experience, I'll be competing for entry roles in a competitive job market. So I'm really not sure if I should stick at this place and do my own thing on the job (flying blind, probably with low output) or just quit, spend summer selfteaching and practicing on projects I actually care about personally, while applying for jobs (have some master's degree offers in September if shit hits fan)",t2_1w1o79i7,False,,0,False,"Completely and totally 'burnt out' at work, unsure if I should quit or just go rouge and do my own core data science based projects?",[],r/datascience,False,6,career,0,,,False,t3_njyxnj,False,dark,0.94,,public,193,0,{},,,False,[],,False,False,,{},Career,False,193,,False,False,self,1621867899.0,,[],{},,True,,1621894438.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: Disclaimer, not &amp;#39;stress&amp;#39; burnout, more like, feels like a total dead end role now, can&amp;#39;t go on much longer, like Peter Gibbons from Office Space, that&amp;#39;s how I feel.&lt;/p&gt;

&lt;p&gt;Ok, context in a nutshell, been at this tech company for about 4yrs in a mixture of roles from product to market research, but been working as a data analyst for the last 2yrs, nearly 2.5&lt;/p&gt;

&lt;p&gt;My goal had been (and agreed with HR) to progress towards a data scientist role when I originally signed up to it. But this has not materialised.&lt;/p&gt;

&lt;p&gt;Fast forward 2.5yrs, I&amp;#39;ve had absolutely nobody in the business to learn from(never had), everything has been self-taught, my day to day work has been much more BI related (dashboards, product stats etc) as opposed to core data science tasks. I really have to go out of my way if I want to write and code, even SQL (data is all maintained by gatekeepers, mostly inefficient ones, so I have to request it mostly, then it&amp;#39;s just an out the box plug in to PowerBI/Tableau or similar)&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve absolutely maxed out my patience with this company and their talk the talk grand plans. My technical skills have totally stagnated, and if I was to compare it to when I left uni, my skills regressed in areas such as advanced mathematical/statistical techniques. My day to day tasks typically involve adhoc requests solved with Tableau (with absolutely horrendous data quality). There are some back end things involving SQL/Mongo/AWS but these are probably 10% overall if even that. When it comes to addressing the data, I&amp;#39;m more like a project manager, because I&amp;#39;m not allowed to fix or implement things myself. So I end up making requests for things and gathering requirements like a PO/BA/ProjMan hybrid role, but even that is a waste of space, e.g. simple request for consumer product usage? Dead in the water for over a year despite weekly meetings because the development team won&amp;#39;t implement changes needed to track them (new POs can be seen mentally questioning life choices when the answer to &amp;#39;so where can I see the usage stats&amp;#39; is, &amp;quot;they don&amp;#39;t exist&amp;quot;)&lt;/p&gt;

&lt;p&gt;The only thing it has that appeals is a degree of autonomy with what I do once I finish the adhoc requests. I easily have 50% of my working hours to address anything I like (in my more motivated, enthusiastic days this was spent addressing the data quality issues, data warehouse etc, but the way these ideas and suggestions were valued/met just killed my soul; options and recommendations closed due to &amp;#39;time&amp;#39;, &amp;#39;cost&amp;#39;, &amp;#39;effort&amp;#39; etc&lt;/p&gt;

&lt;p&gt;I&amp;#39;m thinking of just self teaching the core data science content I wanted to initially learn (I&amp;#39;m already doing this, just not applying it to anything at work), and finding things to apply this to at work. &lt;/p&gt;

&lt;p&gt;The problem is, I&amp;#39;m all alone. Nobody in this company has a data science background, so I can&amp;#39;t learn from anyone here. I&amp;#39;ll have no idea if what I am doing is good practice or even straight up wrong. Even if I did, the data is such bad quality and held in rigid, closed off systems (I am always refused direct access to it) that I&amp;#39;m not convinced there&amp;#39;s a lot of real insight to get from it&lt;/p&gt;

&lt;p&gt;Given that I have no real world data science experience, I&amp;#39;ll be competing for entry roles in a competitive job market. So I&amp;#39;m really not sure if I should stick at this place and do my own thing on the job (flying blind, probably with low output) or just quit, spend summer selfteaching and practicing on projects I actually care about personally, while applying for jobs (have some master&amp;#39;s degree offers in September if shit hits fan)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njyxnj,True,,tea_horse,,49,True,all_ads,False,[],False,,/r/datascience/comments/njyxnj/completely_and_totally_burnt_out_at_work_unsure/,all_ads,False,https://www.reddit.com/r/datascience/comments/njyxnj/completely_and_totally_burnt_out_at_work_unsure/,515405,1621865638.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi,

I'm working on a project where I need to automatically create a large number of decision tree, and to prune them.

The only way to do that is a function like that :

    def do_best_tree(Xtrain, ytrain, Xtest, ytest):
        clf = DecisionTreeClassifier()
        clf.fit(Xtrain, ytrain)
        path = clf.cost_complexity_pruning_path(Xtrain, ytrain)
        ccp_alphas = path.ccp_alphas
        clfs = []
        if len(ccp_alphas) &gt; 100:
            nb = 2
        else:
            nb=1
        for ccp_alpha in tqdm(ccp_alphas[::nb]):
            clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
            clf.fit(Xtrain, ytrain)
            clfs.append(clf)
        return max(clfs, key=lambda x:x.score(Xtest, ytest))
    

So it take a huge amount of time, as it fit a lot of trees. See the

       if len(ccp_alphas) &gt; 100:
            nb = 2

I added in order to divide by two the number of trees, but it still very slow.

Isn't there another way to do that ? For example by really pruning a single tree ? I'm not limited to scikit-learn, but didn't find anything that is doing that....

Thank you in advance !",t2_1lbcpzsa,False,,0,False,Efficient Decision Tree Pruning in Python,[],r/datascience,False,6,tooling,0,,,False,t3_nkpidl,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1621977344.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a project where I need to automatically create a large number of decision tree, and to prune them.&lt;/p&gt;

&lt;p&gt;The only way to do that is a function like that :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def do_best_tree(Xtrain, ytrain, Xtest, ytest):
    clf = DecisionTreeClassifier()
    clf.fit(Xtrain, ytrain)
    path = clf.cost_complexity_pruning_path(Xtrain, ytrain)
    ccp_alphas = path.ccp_alphas
    clfs = []
    if len(ccp_alphas) &amp;gt; 100:
        nb = 2
    else:
        nb=1
    for ccp_alpha in tqdm(ccp_alphas[::nb]):
        clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
        clf.fit(Xtrain, ytrain)
        clfs.append(clf)
    return max(clfs, key=lambda x:x.score(Xtest, ytest))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So it take a huge amount of time, as it fit a lot of trees. See the&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   if len(ccp_alphas) &amp;gt; 100:
        nb = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I added in order to divide by two the number of trees, but it still very slow.&lt;/p&gt;

&lt;p&gt;Isn&amp;#39;t there another way to do that ? For example by really pruning a single tree ? I&amp;#39;m not limited to scikit-learn, but didn&amp;#39;t find anything that is doing that....&lt;/p&gt;

&lt;p&gt;Thank you in advance !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkpidl,True,,ez613,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/,515405,1621948544.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I graduated undergrad in late 2000’s, grad school in early 2010’s. I’m a statistician focused mostly in the social science, medical, health insurance, and public health spheres. 

Around 2010-2012, all I heard was “you gotta learn R” so I did and I’m pretty proficient with it now. Then I heard you gotta learn SQL, and now I’m an intermediate SQL user, I mostly just use it in SAS commands.  It’s great with large datasets. 

Then in the mid 2010’s you gotta learn Tableau, check easy enough. Then the tidyverse package,... which has a lot of overlap with SQL skills anyways. Then other packages then a few job ads wanted SPSS-Amos and/or Mplus. 

Now almost every job posting for statistical / data analytics type roles I’ve come across wants me to know python. I’m sure I’ll pick it up eventually. But I can do most of what I need to do in R (and other tools). Hiring committees can’t explain why they want me to know python. I think they just stick it on there because it sounds cool. Wow snake programming language?

What’s up with these committees and job ads and their affinity towards python, yet they don’t know the value or applicability or necessity of it in these roles?",t2_5dpllzer,False,,0,False,What’s the deal with python becoming more popular in job ads?,[],r/datascience,False,6,,0,,,False,t3_nl02h9,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,True,self,1622030721.0,,[],{},,True,,1622005676.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I graduated undergrad in late 2000’s, grad school in early 2010’s. I’m a statistician focused mostly in the social science, medical, health insurance, and public health spheres. &lt;/p&gt;

&lt;p&gt;Around 2010-2012, all I heard was “you gotta learn R” so I did and I’m pretty proficient with it now. Then I heard you gotta learn SQL, and now I’m an intermediate SQL user, I mostly just use it in SAS commands.  It’s great with large datasets. &lt;/p&gt;

&lt;p&gt;Then in the mid 2010’s you gotta learn Tableau, check easy enough. Then the tidyverse package,... which has a lot of overlap with SQL skills anyways. Then other packages then a few job ads wanted SPSS-Amos and/or Mplus. &lt;/p&gt;

&lt;p&gt;Now almost every job posting for statistical / data analytics type roles I’ve come across wants me to know python. I’m sure I’ll pick it up eventually. But I can do most of what I need to do in R (and other tools). Hiring committees can’t explain why they want me to know python. I think they just stick it on there because it sounds cool. Wow snake programming language?&lt;/p&gt;

&lt;p&gt;What’s up with these committees and job ads and their affinity towards python, yet they don’t know the value or applicability or necessity of it in these roles?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nl02h9,True,,Weird_Surname,,25,True,all_ads,False,[],False,,/r/datascience/comments/nl02h9/whats_the_deal_with_python_becoming_more_popular/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl02h9/whats_the_deal_with_python_becoming_more_popular/,515405,1621976876.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I’m wondering if I should stay a generalist (there’s always stuff to learn) or if I should hone in one niche area (NLP, computer vision, causal inference or bayesian stats)

It seems like the ideal data scientist is an expert in all of those areas, even though in reality it’s quite difficult. We all sorta learn what we need depending on the problem we’re tryna solve

Update: seems like most of y’all are recommending be a generalist unless I’m passionate about a specific area",t2_7nbpziqx,False,,0,False,Specialize in one area or be a generalist?,[],r/datascience,False,6,career,0,,,False,t3_nkg3d9,False,dark,0.78,,public,8,1,{},,,False,[],,False,False,,{},Career,False,8,,False,False,self,1621954676.0,,[],{},,True,,1621942284.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m wondering if I should stay a generalist (there’s always stuff to learn) or if I should hone in one niche area (NLP, computer vision, causal inference or bayesian stats)&lt;/p&gt;

&lt;p&gt;It seems like the ideal data scientist is an expert in all of those areas, even though in reality it’s quite difficult. We all sorta learn what we need depending on the problem we’re tryna solve&lt;/p&gt;

&lt;p&gt;Update: seems like most of y’all are recommending be a generalist unless I’m passionate about a specific area&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 20, 'id': 'award_abb865cf-620b-4219-8777-3658cf9091fb', 'penny_donate': 0, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Can't stop seeing stars"", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Starstruck', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=16&amp;height=16&amp;auto=webp&amp;s=800ea0775a3f25602bfab03058d64d25352c04d2', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=32&amp;height=32&amp;auto=webp&amp;s=1d4be9117f8e389c54e0a7e23918355d7d2df185', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=48&amp;height=48&amp;auto=webp&amp;s=45443e65acd1cf76585f7c9d904e1484f89db521', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=64&amp;height=64&amp;auto=webp&amp;s=f4164f96ab1df25de2024b8b65e9ce91d3424c86', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=128&amp;height=128&amp;auto=webp&amp;s=b09819f49e26e5ad518dbdf1aa69f0916d514c6e', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkg3d9,True,,Affectionate_Shine55,,10,True,all_ads,False,[],False,,/r/datascience/comments/nkg3d9/specialize_in_one_area_or_be_a_generalist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkg3d9/specialize_in_one_area_or_be_a_generalist/,515405,1621913484.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I’m a data science n00b and was recently asked to look into graph2vec... I ran the ex. program and it outputs a csv file with 128 dimensions... is their anyway to visualize this so that one can roughly see what is happening?

Thanks!",t2_wwwre7,False,,0,False,Visualizing graph2vec,[],r/datascience,False,6,tooling,0,,,False,t3_nkptwy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1621978282.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a data science n00b and was recently asked to look into graph2vec... I ran the ex. program and it outputs a csv file with 128 dimensions... is their anyway to visualize this so that one can roughly see what is happening?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkptwy,True,,smok1naces,,1,True,all_ads,False,[],False,,/r/datascience/comments/nkptwy/visualizing_graph2vec/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkptwy/visualizing_graph2vec/,515405,1621949482.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"In statistics, we are always warned about the ""bias-variance tradeoff"":  simple statistical models are reliable, but are generally unable to sufficiently capture the complexity within the data (i.e. high bias, low variance) ; complex statistical models are able to capture complexity within the data, but are generally not as reliable when generalizing to new data (i.e. high variance, low bias).

This leads me to my questions:

1) Are successful statistical models able to defy the ""bias-variance tradeoff""? As a simple example, consider the famous ""iris dataset"".  Kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data. Are these statistical models defying the ""bias-variance tradeoff""? Now, let's imagine a far more complicated problem and dataset - but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data : are we again defying the ""bias-variance tradeoff""?

2) I have seen proofs that show how the MSE (mean squared error) can be decomposed into a ""bias term"" and a ""variance term"". Thus, for a given statistical model : for a fixed value of this model's MSE, if the variance is high then the bias must be low in order to compensate (and vice versa).

My question relates to the following : when people discuss the variance in the ""bias-variance tradeoff"", they are generally interested in the variance of a statistical model's performance when dealing with unseen data. Since this unseen data might not even exist at the moment, how is the ""bias-variance tradeoff"" able to make claims about unseen data? Is the ""bias-variance tradeoff"" a general idea with some theoretical foundations? Or is it mainly empirical?

3) Finally, how does the ""bias-variance tradeoff"" apply to real world models such as the ""self driving car"" , ""alpha go"" and computers playing tetris? Or in the case of reinforcement learning models, the ""bias-variance tradeoff"" does not apply the same way it does in supervised learning models?

Thanks",t2_o4xj9,False,,0,False,"Do successful models defy the ""bias-variance tradeoff""?",[],r/datascience,False,6,discussion,0,,,False,t3_nkfa84,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1621939663.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In statistics, we are always warned about the &amp;quot;bias-variance tradeoff&amp;quot;:  simple statistical models are reliable, but are generally unable to sufficiently capture the complexity within the data (i.e. high bias, low variance) ; complex statistical models are able to capture complexity within the data, but are generally not as reliable when generalizing to new data (i.e. high variance, low bias).&lt;/p&gt;

&lt;p&gt;This leads me to my questions:&lt;/p&gt;

&lt;p&gt;1) Are successful statistical models able to defy the &amp;quot;bias-variance tradeoff&amp;quot;? As a simple example, consider the famous &amp;quot;iris dataset&amp;quot;.  Kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data. Are these statistical models defying the &amp;quot;bias-variance tradeoff&amp;quot;? Now, let&amp;#39;s imagine a far more complicated problem and dataset - but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data : are we again defying the &amp;quot;bias-variance tradeoff&amp;quot;?&lt;/p&gt;

&lt;p&gt;2) I have seen proofs that show how the MSE (mean squared error) can be decomposed into a &amp;quot;bias term&amp;quot; and a &amp;quot;variance term&amp;quot;. Thus, for a given statistical model : for a fixed value of this model&amp;#39;s MSE, if the variance is high then the bias must be low in order to compensate (and vice versa).&lt;/p&gt;

&lt;p&gt;My question relates to the following : when people discuss the variance in the &amp;quot;bias-variance tradeoff&amp;quot;, they are generally interested in the variance of a statistical model&amp;#39;s performance when dealing with unseen data. Since this unseen data might not even exist at the moment, how is the &amp;quot;bias-variance tradeoff&amp;quot; able to make claims about unseen data? Is the &amp;quot;bias-variance tradeoff&amp;quot; a general idea with some theoretical foundations? Or is it mainly empirical?&lt;/p&gt;

&lt;p&gt;3) Finally, how does the &amp;quot;bias-variance tradeoff&amp;quot; apply to real world models such as the &amp;quot;self driving car&amp;quot; , &amp;quot;alpha go&amp;quot; and computers playing tetris? Or in the case of reinforcement learning models, the &amp;quot;bias-variance tradeoff&amp;quot; does not apply the same way it does in supervised learning models?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkfa84,True,,blueest,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkfa84/do_successful_models_defy_the_biasvariance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkfa84/do_successful_models_defy_the_biasvariance/,515405,1621910863.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Prior Post for context: https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/

Sadly I think it was an air raid for the most part... and I don't think I did very well.

I didn't really get many rapid fire trivia questions, although I got some, and I had to answer a couple with ""I don't know."" For example I was asked what is the mathematical equation to calculate p-values... I answered with ""I don't remember? I would have to consult my college notes from 11 years ago"". In my head I was thinking, why does it matter when scikit learn or R does that all under the hood? There was another trivia question I got which I answered I don't know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.

I got some data engineering questions(not my strong suit) and I don't believe I gave good answers. Everywhere I have been I have worked along side with data engineering teams but have never actually done the data engineering work. When I got to the point of the interview where I could ask questions I asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so I have no idea why I was even asked these questions.

I think my strong suit was using my past work experiences to outline how I got things done, but even then I am not sure if my responses were good enough because I felt like I was on defense from the get go. I do better in interviews when I am ""on offense"", so to speak. For example I was asked for an example of how I segmented customers in the past and used that to drive revenue growth. I said I would look at various segmentation techniques and choose the best one based on model performance against test data, and how I determined that to be K-Means clustering after using PCA to reduce the number of features and using a scree plot to determine the number of segments. And how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. I don't think I was precise or deep enough in this answer and was too vague and I didn't project enough confidence. But, in an hour long verbal interview, how deep am I supposed to go? I struggled with finding the right balance of depth and respecting the time constraints.

I'll be surprised if I don't get rejected... but I feel like this format was tough to succeed in. I believe I am a strong data scientist, but I am at my best when I know exactly what scenario I am facing. Questions like ""Here is what we want to accomplish, how would you do it?"" are what I do best, I didn't get any of those questions. It was more open ended broad questions such as ""What is your go to machine learning model"" and I literally could have given anything. It depends on the task at hand. I said Random Forest to go with the question and how I like the built in OOB functionality in R. Although if I could answer the question again I'd go with Gradient Boosting. 

Another question I hated was ""How do you stay on top of data science industry trends"". I didn't really know how to answer that one. I said I stay engaged with publications and gave a personal example of what I do in my free time(that I won't say since I don't want to out myself incase those who interviewed lurk here) but I don't like this because I believe you stick with what works and you don't always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you'd probably know about it without having to be proactive to learn about it.

Another question was ""Do you follow up on your own models performance against real world data?"". I didn't have a problem with this question because of course I do. Who doesn't? I said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. Therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. Then they followed up with ""What if you gained more responsibilities on your plate and could no longer follow up on all of your models?"", which I think was a silly follow up, because I feel that is putting the cart before the horse. I said in a hypothetical scenario where my bandwidth was overextended to where I could not do such a thing, as I hope it would not get to that point, I would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.

I got asked ""How do you perform feature selection"" and I simply explained various ways to do it based on the model at hand(i.e. p-Values for linear regression, feature score for Random Forest, etc.). I didn't like this question because its something you would ask an entry level candidate, not someone interviewing for director. I answered it anyway but with only an hour, I feel like better questions could have been asked.

The interview was solely to gauge my technical abilities, not my leadership or management abilities, so I understand that the interview was aimed to do that. With that being said I don't think I answered the questions given to me with enough confidence or precision. I am good at gauging body language, facial expressions and tone of voice of those interviewing me and I don't think they were impressed. If its up to these 2 guys I don't have a chance. The only way I have a chance is if all I needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. 

Comparing this interview to the 3 bad ones I had before I think this one was definitely the worst of the bunch in terms of how I gauged my performance. I am starting to lose confidence in my interviewing skills, as I believe I have a hard time translating my actual 10 year track record into verbal competency. I know how to do things, I just have a hard time coherently explaining how those things are done.

Fortunately... I have another interview starting in 10 minutes... and 5 others lined up this week. But this was the job I really wanted out of all of them.",t2_y5gxz,False,,0,False,"UPDATE: Just had my interview, was it an ""air raid"" style of interview that I was afraid it was going to be?",[],r/datascience,False,6,,0,,,False,t3_nk5dny,False,dark,0.77,,public,12,0,{},,,False,[],,False,False,,{},Job Search,False,12,,False,False,self,False,,[],{},,True,,1621911457.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Prior Post for context: &lt;a href=""https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/""&gt;https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Sadly I think it was an air raid for the most part... and I don&amp;#39;t think I did very well.&lt;/p&gt;

&lt;p&gt;I didn&amp;#39;t really get many rapid fire trivia questions, although I got some, and I had to answer a couple with &amp;quot;I don&amp;#39;t know.&amp;quot; For example I was asked what is the mathematical equation to calculate p-values... I answered with &amp;quot;I don&amp;#39;t remember? I would have to consult my college notes from 11 years ago&amp;quot;. In my head I was thinking, why does it matter when scikit learn or R does that all under the hood? There was another trivia question I got which I answered I don&amp;#39;t know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.&lt;/p&gt;

&lt;p&gt;I got some data engineering questions(not my strong suit) and I don&amp;#39;t believe I gave good answers. Everywhere I have been I have worked along side with data engineering teams but have never actually done the data engineering work. When I got to the point of the interview where I could ask questions I asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so I have no idea why I was even asked these questions.&lt;/p&gt;

&lt;p&gt;I think my strong suit was using my past work experiences to outline how I got things done, but even then I am not sure if my responses were good enough because I felt like I was on defense from the get go. I do better in interviews when I am &amp;quot;on offense&amp;quot;, so to speak. For example I was asked for an example of how I segmented customers in the past and used that to drive revenue growth. I said I would look at various segmentation techniques and choose the best one based on model performance against test data, and how I determined that to be K-Means clustering after using PCA to reduce the number of features and using a scree plot to determine the number of segments. And how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. I don&amp;#39;t think I was precise or deep enough in this answer and was too vague and I didn&amp;#39;t project enough confidence. But, in an hour long verbal interview, how deep am I supposed to go? I struggled with finding the right balance of depth and respecting the time constraints.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be surprised if I don&amp;#39;t get rejected... but I feel like this format was tough to succeed in. I believe I am a strong data scientist, but I am at my best when I know exactly what scenario I am facing. Questions like &amp;quot;Here is what we want to accomplish, how would you do it?&amp;quot; are what I do best, I didn&amp;#39;t get any of those questions. It was more open ended broad questions such as &amp;quot;What is your go to machine learning model&amp;quot; and I literally could have given anything. It depends on the task at hand. I said Random Forest to go with the question and how I like the built in OOB functionality in R. Although if I could answer the question again I&amp;#39;d go with Gradient Boosting. &lt;/p&gt;

&lt;p&gt;Another question I hated was &amp;quot;How do you stay on top of data science industry trends&amp;quot;. I didn&amp;#39;t really know how to answer that one. I said I stay engaged with publications and gave a personal example of what I do in my free time(that I won&amp;#39;t say since I don&amp;#39;t want to out myself incase those who interviewed lurk here) but I don&amp;#39;t like this because I believe you stick with what works and you don&amp;#39;t always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you&amp;#39;d probably know about it without having to be proactive to learn about it.&lt;/p&gt;

&lt;p&gt;Another question was &amp;quot;Do you follow up on your own models performance against real world data?&amp;quot;. I didn&amp;#39;t have a problem with this question because of course I do. Who doesn&amp;#39;t? I said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. Therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. Then they followed up with &amp;quot;What if you gained more responsibilities on your plate and could no longer follow up on all of your models?&amp;quot;, which I think was a silly follow up, because I feel that is putting the cart before the horse. I said in a hypothetical scenario where my bandwidth was overextended to where I could not do such a thing, as I hope it would not get to that point, I would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.&lt;/p&gt;

&lt;p&gt;I got asked &amp;quot;How do you perform feature selection&amp;quot; and I simply explained various ways to do it based on the model at hand(i.e. p-Values for linear regression, feature score for Random Forest, etc.). I didn&amp;#39;t like this question because its something you would ask an entry level candidate, not someone interviewing for director. I answered it anyway but with only an hour, I feel like better questions could have been asked.&lt;/p&gt;

&lt;p&gt;The interview was solely to gauge my technical abilities, not my leadership or management abilities, so I understand that the interview was aimed to do that. With that being said I don&amp;#39;t think I answered the questions given to me with enough confidence or precision. I am good at gauging body language, facial expressions and tone of voice of those interviewing me and I don&amp;#39;t think they were impressed. If its up to these 2 guys I don&amp;#39;t have a chance. The only way I have a chance is if all I needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. &lt;/p&gt;

&lt;p&gt;Comparing this interview to the 3 bad ones I had before I think this one was definitely the worst of the bunch in terms of how I gauged my performance. I am starting to lose confidence in my interviewing skills, as I believe I have a hard time translating my actual 10 year track record into verbal competency. I know how to do things, I just have a hard time coherently explaining how those things are done.&lt;/p&gt;

&lt;p&gt;Fortunately... I have another interview starting in 10 minutes... and 5 others lined up this week. But this was the job I really wanted out of all of them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nk5dny,True,,dothingsright_,,35,True,all_ads,False,[],False,,/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/,515405,1621882657.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience," Hi  Guys, I have a problem statement where there is a need for fire  detection which is usually handled by Computer Vision Object Detection  models - YOLO, Faster R-CNN, etc. However, I was thinking about using  Multimodal DL for this to take inputs from heat/thermal sensor, etc.  apart from video feeds.

Any practical blog/tutorial you can point me to?

Thanks!",t2_2mmql89p,False,,0,False,Multimodal Deep Learning,[],r/datascience,False,6,discussion,0,,,False,t3_nkgeop,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621943380.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi  Guys, I have a problem statement where there is a need for fire  detection which is usually handled by Computer Vision Object Detection  models - YOLO, Faster R-CNN, etc. However, I was thinking about using  Multimodal DL for this to take inputs from heat/thermal sensor, etc.  apart from video feeds.&lt;/p&gt;

&lt;p&gt;Any practical blog/tutorial you can point me to?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkgeop,True,,grid_world,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkgeop/multimodal_deep_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkgeop/multimodal_deep_learning/,515406,1621914580.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_c4iedttd,False,,0,False,Solving Data Reproducibility,[],r/datascience,False,6,education,0,93.0,,False,t3_njxxzw,False,dark,0.71,,public,10,0,{},140.0,,False,[],,False,False,,{},Education,False,10,,False,False,https://b.thumbs.redditmedia.com/kQjLYt3j4bPKUiWntV_75P7wTm0H3VgISa0L3s6svxU.jpg,False,,[],{},,False,,1621891607.0,text,6,,,text,lakefs.io,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njxxzw,True,,MagicianPutrid5245,,4,True,all_ads,False,[],False,,/r/datascience/comments/njxxzw/solving_data_reproducibility/,all_ads,False,https://lakefs.io/solving-data-reproducibility/,515406,1621862807.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?auto=webp&amp;s=160916fedf27f4f97c389fc4741035317f00640d', 'width': 2560, 'height': 1707}, 'resolutions': [{'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e443e72eb3104417395966097d6607b657e35c8', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50dba762021fa1f52248195ea9dbee85d3027b2d', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2756ba7f45530b436539d7b6246b16dcb16477e7', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c523a73e78e25b72fc7b43b929403c7f628a443', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f019a1459aa3fcff13e328a1581fb303b2b818c1', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a38b68951b8fc6c1424888acbc7895b106cefef8', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'rp0Ipk9UJAk0VSXiYJVzIGk2P5crNNykj8vSL74E2-M'}], 'enabled': False}",,,https://lakefs.io/solving-data-reproducibility/,,
,datascience,"Hi , Looking at [metaflow](https://docs.metaflow.org/) tutorials and trying to see how different/ better it is from the setup below that can work in a SWE based pipeline; And if any reason to switch to metaflow , or all its features listed below are covered adequately by this setup:

&amp;#x200B;

1. **DataWarehouse** \- Source of data (pumped in from dbt pipeline)
2. **Compute resources** \- Covered by autoscaling in aws;
3. **Job Scheduler**:

* Quartz/Spring application reads configuration from a job database with a UI for entering schedules &amp; jobs;
* Schedules are decoupled from jobs; can evolve independently without touching code.
* Most jobs are to send a message to a kafka queue to kick off spring batch or python ML code.

**4. Architecture**:

* Orchestration handled by Java Spring cloud  stream / data flows plus kafka topics / producers/consumers.
* ML jobs are python classes /in-house python libraries so mostly in the ML pipeline only parameters are read from db and applied to the appropriate ML class;
* Emphasis on avoiding one-off ML python procedural scripts
* Data transfer between steps done using dbt (sql is understood by more folks). Only ML steps done in python

&amp;#x200B;

5.     **Versioning/Inspecting Results &amp; Organizing Results:**

* dbt  &amp; python ML jobs  results histories  based on batch/run\_id are stored and can be queried in sql
* Micrometer&amp;Prometheus metrics used to monitor success/ failed stages of all jobs.
* Sleuth distributed log tracing also used.
* Namespaces for development artifacts are simply git branches of each developer
* Namespaces isolation when inspecting results is based on run\_ids/user\_ids embedded in micrometer metrics and intermediate results of dbt &amp; python ML job histories.

**6. Loading and Storing Data:**

* Intermediate steps data is stored in database/s3

**7.  Loading and Storing Data:**

* Again no need to reinvent the wheel ; gradle for java code , pip aand requirements.txt for python

In all these steps using a mature framework like spring boot &amp; spring cloud stream makes life a lot easier

This setup will allow use common workflows that are already well defined in SWE rather than redefine new workflows in the existing infrastructure",t2_1j1zx80q,False,,0,False,Explain the need for non SWE ML pipeline vs existing SWE frameworks,[],r/datascience,False,6,tooling,0,,,False,t3_nkcehu,False,dark,1.0,,public,1,0,{},,,True,[],,False,False,,{},Tooling,False,1,,False,False,self,1621902065.0,,[],{},,True,,1621930670.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi , Looking at &lt;a href=""https://docs.metaflow.org/""&gt;metaflow&lt;/a&gt; tutorials and trying to see how different/ better it is from the setup below that can work in a SWE based pipeline; And if any reason to switch to metaflow , or all its features listed below are covered adequately by this setup:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;DataWarehouse&lt;/strong&gt; - Source of data (pumped in from dbt pipeline)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute resources&lt;/strong&gt; - Covered by autoscaling in aws;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Job Scheduler&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Quartz/Spring application reads configuration from a job database with a UI for entering schedules &amp;amp; jobs;&lt;/li&gt;
&lt;li&gt;Schedules are decoupled from jobs; can evolve independently without touching code.&lt;/li&gt;
&lt;li&gt;Most jobs are to send a message to a kafka queue to kick off spring batch or python ML code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Architecture&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Orchestration handled by Java Spring cloud  stream / data flows plus kafka topics / producers/consumers.&lt;/li&gt;
&lt;li&gt;ML jobs are python classes /in-house python libraries so mostly in the ML pipeline only parameters are read from db and applied to the appropriate ML class;&lt;/li&gt;
&lt;li&gt;Emphasis on avoiding one-off ML python procedural scripts&lt;/li&gt;
&lt;li&gt;Data transfer between steps done using dbt (sql is understood by more folks). Only ML steps done in python&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;    &lt;strong&gt;Versioning/Inspecting Results &amp;amp; Organizing Results:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;dbt  &amp;amp; python ML jobs  results histories  based on batch/run_id are stored and can be queried in sql&lt;/li&gt;
&lt;li&gt;Micrometer&amp;amp;Prometheus metrics used to monitor success/ failed stages of all jobs.&lt;/li&gt;
&lt;li&gt;Sleuth distributed log tracing also used.&lt;/li&gt;
&lt;li&gt;Namespaces for development artifacts are simply git branches of each developer&lt;/li&gt;
&lt;li&gt;Namespaces isolation when inspecting results is based on run_ids/user_ids embedded in micrometer metrics and intermediate results of dbt &amp;amp; python ML job histories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;6. Loading and Storing Data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Intermediate steps data is stored in database/s3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;7.  Loading and Storing Data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Again no need to reinvent the wheel ; gradle for java code , pip aand requirements.txt for python&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In all these steps using a mature framework like spring boot &amp;amp; spring cloud stream makes life a lot easier&lt;/p&gt;

&lt;p&gt;This setup will allow use common workflows that are already well defined in SWE rather than redefine new workflows in the existing infrastructure&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkcehu,True,,dev-1773,,1,True,all_ads,False,[],False,,/r/datascience/comments/nkcehu/explain_the_need_for_non_swe_ml_pipeline_vs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkcehu/explain_the_need_for_non_swe_ml_pipeline_vs/,515406,1621901870.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?auto=webp&amp;s=4ebad4b6de1c70b5aee591b6b982b03dff86935a', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce3cc02960dc84452f80dfbec0265763836ff49', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fdc21aef33f177a9577a165595113c249702379f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a9ac6e65b2724a701a5068591e3e8bb16670497', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f6b83391a3e86f843e0d2a1c89ac3353fbbda72', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b34e9ae2037f47b501ddf1b35223c5bdc3f8ece0', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69bda3cac43d547ca62eec68f643502de1a64d95', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'vhUSJAG4C7ac0HmwHNqVfs5T8E-4KHs5MskFwWHQXjw'}], 'enabled': False}",,,,,
,datascience,"I see this as an example interview question all the time.

But I never see great answers.

So far as I can tell, a good answer might be..

\- L2 norm is continuously differentiable  
\- L1 has closed-form solution  


\-L0.5 or L3 are possible   
\-but L3 norm will tend to overregularize and bring coefficients towards each other.  
\-L0.5 norm would induce some irregular sparsity; not even sure how to clarify this one.

&amp;#x200B;

What else would people mention? I know there are answers involving Hilbert spaces with the L2 norm but I don't really understand that.",t2_a3t5z3gn,False,,0,False,What is a good answer for: Why use l1/l2 norms but not L0.5 or L3?,[],r/datascience,False,6,discussion,0,,,False,t3_nk7hs9,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621916860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see this as an example interview question all the time.&lt;/p&gt;

&lt;p&gt;But I never see great answers.&lt;/p&gt;

&lt;p&gt;So far as I can tell, a good answer might be..&lt;/p&gt;

&lt;p&gt;- L2 norm is continuously differentiable&lt;br/&gt;
- L1 has closed-form solution  &lt;/p&gt;

&lt;p&gt;-L0.5 or L3 are possible&lt;br/&gt;
-but L3 norm will tend to overregularize and bring coefficients towards each other.&lt;br/&gt;
-L0.5 norm would induce some irregular sparsity; not even sure how to clarify this one.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What else would people mention? I know there are answers involving Hilbert spaces with the L2 norm but I don&amp;#39;t really understand that.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk7hs9,True,,latticeprep,,2,True,all_ads,False,[],False,,/r/datascience/comments/nk7hs9/what_is_a_good_answer_for_why_use_l1l2_norms_but/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk7hs9/what_is_a_good_answer_for_why_use_l1l2_norms_but/,515406,1621888060.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"LinkedIn recently opened-sourced [Greykite](https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library), a Python library originally built for LinkedIn’s forecasting needs. Greykite’s main algorithm is Silverkite, which delivers automated forecasting, which LinkedIn uses for resource planning, performance management, optimization, and ecosystem insight.

While using predictive models to estimate consumer behavior, data drift has proven to be a great challenge during the pandemic in 2020. In such a situation, predicting future expectations is challenging as well as necessarily helpful to any business. Automation, which allows for repeatability, can increase accuracy and can be used by algorithms to make decisions further down the line. According to LinkedIn, Silverkite has improved revenue forecasts for ‘1-day ahead’ and ‘7-day ahead’ and Weekly Active User forecasts for 2-week ahead.

Full Summary: [https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/](https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/?_ga=2.74959442.1924646600.1621739878-488125022.1618729090)

GitHub: [https://github.com/linkedin/greykite](https://github.com/linkedin/greykite)

PyPI: [https://pypi.org/project/greykite/](https://pypi.org/project/greykite/)

Paper: http://arxiv.org/abs/2105.01098",t2_4wudjgid,False,,0,False,"LinkedIn Open-Sources ‘Greykite’, A Time Series Forecasting Library",[],r/datascience,False,6,discussion,0,,,False,t3_nj9s57,False,dark,0.98,,public,311,1,{},,,False,[],,False,False,,{},Discussion,False,311,,False,False,self,False,,[],{},,True,,1621811802.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;LinkedIn recently opened-sourced &lt;a href=""https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library""&gt;Greykite&lt;/a&gt;, a Python library originally built for LinkedIn’s forecasting needs. Greykite’s main algorithm is Silverkite, which delivers automated forecasting, which LinkedIn uses for resource planning, performance management, optimization, and ecosystem insight.&lt;/p&gt;

&lt;p&gt;While using predictive models to estimate consumer behavior, data drift has proven to be a great challenge during the pandemic in 2020. In such a situation, predicting future expectations is challenging as well as necessarily helpful to any business. Automation, which allows for repeatability, can increase accuracy and can be used by algorithms to make decisions further down the line. According to LinkedIn, Silverkite has improved revenue forecasts for ‘1-day ahead’ and ‘7-day ahead’ and Weekly Active User forecasts for 2-week ahead.&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/?_ga=2.74959442.1924646600.1621739878-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/linkedin/greykite""&gt;https://github.com/linkedin/greykite&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyPI: &lt;a href=""https://pypi.org/project/greykite/""&gt;https://pypi.org/project/greykite/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""http://arxiv.org/abs/2105.01098""&gt;http://arxiv.org/abs/2105.01098&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nj9s57,True,,techsucker,,30,True,all_ads,False,[],False,,/r/datascience/comments/nj9s57/linkedin_opensources_greykite_a_time_series/,all_ads,False,https://www.reddit.com/r/datascience/comments/nj9s57/linkedin_opensources_greykite_a_time_series/,515406,1621783002.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?auto=webp&amp;s=7f878bb82e1d42a4f047eb9ea0ab2a672174f972', 'width': 700, 'height': 429}, 'resolutions': [{'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dd66d32bdeef9af4ebbe5b1cc5d795726d77c36', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37dbcb228d193c9c884cb7b66b8326ff5d206b70', 'width': 216, 'height': 132}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20a255678eac392685a8f7c5e5faca0bf19669d', 'width': 320, 'height': 196}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=220f2947bf099b6a546fba0b558a93cb51caaa83', 'width': 640, 'height': 392}], 'variants': {}, 'id': 'hmkDzPyA5ng1AHnJpwon-OFBITQk95iQy6nblLoR2Ag'}], 'enabled': False}",,,,,
,datascience,"I have a fairly large portfolio of houses (think thousands) that I want to cluster based on - proximity to neighboring houses and some house types (fuel source, detached / apartment ...).

The goal is to create clusters based on the distance to other houses and the types (e.g., cluster of 5 houses max. 50 meters from each other which are all on the same fuel source and are detached). Luckily, in my dataset it is most likely that houses next to each other will also be of the same type.

Do you have any tips on algorithms / approaches for this job? I am proficient in Python / R.

Thank you.",t2_14qnk0,False,,0,False,How to geo-cluster houses in a real estate dataset?,[],r/datascience,False,6,discussion,0,,,False,t3_njxwij,False,dark,0.7,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1621891483.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a fairly large portfolio of houses (think thousands) that I want to cluster based on - proximity to neighboring houses and some house types (fuel source, detached / apartment ...).&lt;/p&gt;

&lt;p&gt;The goal is to create clusters based on the distance to other houses and the types (e.g., cluster of 5 houses max. 50 meters from each other which are all on the same fuel source and are detached). Luckily, in my dataset it is most likely that houses next to each other will also be of the same type.&lt;/p&gt;

&lt;p&gt;Do you have any tips on algorithms / approaches for this job? I am proficient in Python / R.&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njxwij,True,,Mieleki,,14,True,all_ads,False,[],False,,/r/datascience/comments/njxwij/how_to_geocluster_houses_in_a_real_estate_dataset/,all_ads,False,https://www.reddit.com/r/datascience/comments/njxwij/how_to_geocluster_houses_in_a_real_estate_dataset/,515406,1621862683.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Happy Sunday!

I was hoping we'd have a light discussion before jumping in the week tomorrow. 

I'm 26 right now, when I was in my undergrad, I remember Android development or some kind of website development were pretty popular among the college students. Today I was wondering that apart from money, what makes a job ""hot""?

Would love to hear from you guys, especially who have been in the market before Data Science even existed.

Thank you!",t2_bv171ji2,False,,0,False,"Why do you think Data Science became this ""sexy"" job and what job was hot before this?",[],r/datascience,False,6,discussion,0,,,False,t3_njihmm,False,dark,0.88,,public,76,0,{},,,False,[],,False,False,,{},Discussion,False,76,,False,False,self,False,,[],{},,True,,1621836176.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Happy Sunday!&lt;/p&gt;

&lt;p&gt;I was hoping we&amp;#39;d have a light discussion before jumping in the week tomorrow. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m 26 right now, when I was in my undergrad, I remember Android development or some kind of website development were pretty popular among the college students. Today I was wondering that apart from money, what makes a job &amp;quot;hot&amp;quot;?&lt;/p&gt;

&lt;p&gt;Would love to hear from you guys, especially who have been in the market before Data Science even existed.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njihmm,True,,quite--average,,82,True,all_ads,False,[],False,,/r/datascience/comments/njihmm/why_do_you_think_data_science_became_this_sexy/,all_ads,False,https://www.reddit.com/r/datascience/comments/njihmm/why_do_you_think_data_science_became_this_sexy/,515406,1621807376.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The situation is like this:  We receive deliveries of data electronically, and let's say they are expected to arrive at 8 AM daily. But historically they can arrive at say 5-10 AM. So it's periodic but not really. The data is gathered only hourly at the top of the hour. 

The data we're gathering now is basically ""time since last arrival""  which is zero when something has arrived in the last hour, and ticks up hour by hour until the next delivery.  What I've been doing so far is fitting this with a sawtooth wave and whenever a delivery is late, the ""time since last"" goes above the threshold and alerts us.  I researched time series data and such but it's not really what I'm looking for. 

Here's the issue:  we have dozens of different ones of these per customer and hundreds of customers,  and my fitting works well most of the time, but there are still a large number of false alerts. Also, some of these are 7 days a week, others are M-F,  others are more than once per day.  All very different kinds of sawtooths, and a lot of manual adjusting. 

What I've started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day, this gives me just a set of times. I did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable. (Is this a Poisson process?) I have enough history to look back 6 weeks. This gives only 6 data points for e.g. a Tuesday, but for 7-day-per-week systems there is no problem aggregating all the days and looking just at the hour of the day (42 total data points)  But knowing when to combine 7 days, vs 5/2 split etc is a manual decision. I would prefer it it could fit each arrival event separately.

(I'm actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week, so I see the scatter plots overlayed and can guess at the amount of variation when I do these, so for example 8 AM Tuesday would be at T=56 hour and constructing the threshold sawtooth function on that axis.)

What I am looking for (perhaps) is a way to give say, based on the real history, give me the time of day that if the packet hasn't arrived by, there's a high % chance that it's late and set an alarm. 

Since the history can and does contain some that were late (or absent) I know those should factor in with less weight. I don't know what kind of distribution these should fit or how to output a cutoff value for a given % chance it's out of compliance.  And the reason I'm trying is so I can script this to analyze hundreds of such data sets and set thresholds automatically. 

Any suggestions where to look?    Online article?  videos?  
I would be interested in suggestions for textbooks as well. 

Thanks",t2_4hrqs,False,,0,False,"Need pointers on where to start on how to model this data that is essentially ""arrival times"" or ""arrival delays""",[],r/datascience,False,6,education,0,,,False,t3_nk8t4x,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,1621909656.0,,[],{},,True,,1621920228.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The situation is like this:  We receive deliveries of data electronically, and let&amp;#39;s say they are expected to arrive at 8 AM daily. But historically they can arrive at say 5-10 AM. So it&amp;#39;s periodic but not really. The data is gathered only hourly at the top of the hour. &lt;/p&gt;

&lt;p&gt;The data we&amp;#39;re gathering now is basically &amp;quot;time since last arrival&amp;quot;  which is zero when something has arrived in the last hour, and ticks up hour by hour until the next delivery.  What I&amp;#39;ve been doing so far is fitting this with a sawtooth wave and whenever a delivery is late, the &amp;quot;time since last&amp;quot; goes above the threshold and alerts us.  I researched time series data and such but it&amp;#39;s not really what I&amp;#39;m looking for. &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the issue:  we have dozens of different ones of these per customer and hundreds of customers,  and my fitting works well most of the time, but there are still a large number of false alerts. Also, some of these are 7 days a week, others are M-F,  others are more than once per day.  All very different kinds of sawtooths, and a lot of manual adjusting. &lt;/p&gt;

&lt;p&gt;What I&amp;#39;ve started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day, this gives me just a set of times. I did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable. (Is this a Poisson process?) I have enough history to look back 6 weeks. This gives only 6 data points for e.g. a Tuesday, but for 7-day-per-week systems there is no problem aggregating all the days and looking just at the hour of the day (42 total data points)  But knowing when to combine 7 days, vs 5/2 split etc is a manual decision. I would prefer it it could fit each arrival event separately.&lt;/p&gt;

&lt;p&gt;(I&amp;#39;m actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week, so I see the scatter plots overlayed and can guess at the amount of variation when I do these, so for example 8 AM Tuesday would be at T=56 hour and constructing the threshold sawtooth function on that axis.)&lt;/p&gt;

&lt;p&gt;What I am looking for (perhaps) is a way to give say, based on the real history, give me the time of day that if the packet hasn&amp;#39;t arrived by, there&amp;#39;s a high % chance that it&amp;#39;s late and set an alarm. &lt;/p&gt;

&lt;p&gt;Since the history can and does contain some that were late (or absent) I know those should factor in with less weight. I don&amp;#39;t know what kind of distribution these should fit or how to output a cutoff value for a given % chance it&amp;#39;s out of compliance.  And the reason I&amp;#39;m trying is so I can script this to analyze hundreds of such data sets and set thresholds automatically. &lt;/p&gt;

&lt;p&gt;Any suggestions where to look?    Online article?  videos?&lt;br/&gt;
I would be interested in suggestions for textbooks as well. &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk8t4x,True,,ggrieves,,2,True,all_ads,False,[],False,,/r/datascience/comments/nk8t4x/need_pointers_on_where_to_start_on_how_to_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk8t4x/need_pointers_on_where_to_start_on_how_to_model/,515406,1621891428.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Next year I will start my MBA and after that I am starting a MSc in Data Science. I am 31 years old and have worked as Marketing Manager and eCommerce Manager. All of these positions involve lots of data analysis and data visualization but none of data science (prediction models, machine learning, etc). 

I have experience using Tableau, PowerBi, Google Data Studio, Analytics, Ads... I have worked mostly on Analysis and Visualization. My biggest weakness would be the ""math"" side of data science.

My ultimate goal is to work at Google, it has always been my dream company. Other options are big companies such as Amazon, Facebook, Netflix, DiDi, Uber, or any other ""big"" company.

I don't know if I am the right thing by looking for a Data Science mentor, or if I should be looking for a different person, like a general career counselor? Any tips?",t2_618gmrpn,False,,0,False,Looking yo hire a Data Science mentor for career advice,[],r/datascience,False,6,career,0,,,False,t3_nk6noh,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1621914740.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Next year I will start my MBA and after that I am starting a MSc in Data Science. I am 31 years old and have worked as Marketing Manager and eCommerce Manager. All of these positions involve lots of data analysis and data visualization but none of data science (prediction models, machine learning, etc). &lt;/p&gt;

&lt;p&gt;I have experience using Tableau, PowerBi, Google Data Studio, Analytics, Ads... I have worked mostly on Analysis and Visualization. My biggest weakness would be the &amp;quot;math&amp;quot; side of data science.&lt;/p&gt;

&lt;p&gt;My ultimate goal is to work at Google, it has always been my dream company. Other options are big companies such as Amazon, Facebook, Netflix, DiDi, Uber, or any other &amp;quot;big&amp;quot; company.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if I am the right thing by looking for a Data Science mentor, or if I should be looking for a different person, like a general career counselor? Any tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk6noh,True,,diegoarmando50,,8,True,all_ads,False,[],False,,/r/datascience/comments/nk6noh/looking_yo_hire_a_data_science_mentor_for_career/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk6noh/looking_yo_hire_a_data_science_mentor_for_career/,515406,1621885940.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Some context, I'm gonna be speaker for a meetup and I want to touch the fundamentals for people who wants to start their career as data scientists.

Rather than talk about the usual stuff (develop math, biz and coding skills) **I want to people reflect on the mindset they need to build so they can become successful data scientists**.

Right now I have for things to share with them:

1. _Curiosity_: Data scientist are people with lot of curiosity and they are willing to research. If your stay curious you will have the mindset for ask questions, look for data, formulate hypothesis and test them.  


2. _Perseverance_: Usually, Data Science is a rough path. There is no magic bootcamp or 6 month course to become a good scientific. You need to stay focus and practice your skills pretty often (even by just listening a podcast or reading a paper).  


3. _Emphaty_: You have put yourself in other scenarios rather than just thinking inside the Data Science side. Often you need to collaborate with other disciplines and understand the context of unknown situations. You might analyze social events, business metrics, human behavior, etcetera.  


4. _Participation_: As data scientists, we need to get our hands dirty. No better way to learn (imo) than building real applications, solve real problems.  


**What other characteristics do you perceive in a successful data scientist?**",t2_c9ezux7k,False,,0,False,What kind of mindset a person should shape in order to bocome a good data scientist?,[],r/datascience,False,6,discussion,0,,,False,t3_nkbfil,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1621908574.0,,[],{},,True,,1621927685.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Some context, I&amp;#39;m gonna be speaker for a meetup and I want to touch the fundamentals for people who wants to start their career as data scientists.&lt;/p&gt;

&lt;p&gt;Rather than talk about the usual stuff (develop math, biz and coding skills) &lt;strong&gt;I want to people reflect on the mindset they need to build so they can become successful data scientists&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Right now I have for things to share with them:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Curiosity&lt;/em&gt;: Data scientist are people with lot of curiosity and they are willing to research. If your stay curious you will have the mindset for ask questions, look for data, formulate hypothesis and test them.  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Perseverance&lt;/em&gt;: Usually, Data Science is a rough path. There is no magic bootcamp or 6 month course to become a good scientific. You need to stay focus and practice your skills pretty often (even by just listening a podcast or reading a paper).  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Emphaty&lt;/em&gt;: You have put yourself in other scenarios rather than just thinking inside the Data Science side. Often you need to collaborate with other disciplines and understand the context of unknown situations. You might analyze social events, business metrics, human behavior, etcetera.  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Participation&lt;/em&gt;: As data scientists, we need to get our hands dirty. No better way to learn (imo) than building real applications, solve real problems.  &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;What other characteristics do you perceive in a successful data scientist?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkbfil,True,,donelianc,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkbfil/what_kind_of_mindset_a_person_should_shape_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkbfil/what_kind_of_mindset_a_person_should_shape_in/,515406,1621898885.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Are there any tools to make it easier to debug the model performance and evaluate the model errors? For example, I want to be able to able to dissect how my model is performing for certain classes etc",t2_1o26bmw3,False,,0,False,Tools for error Analysis,[],r/datascience,False,6,tooling,0,,,False,t3_njqqpw,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,seniorflair,[],{},,True,,1621864024.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any tools to make it easier to debug the model performance and evaluate the model errors? For example, I want to be able to able to dissect how my model is performing for certain classes etc&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,MS | Student,[],False,,,,t5_2sptq,,,,njqqpw,True,,da_chosen1,,1,True,all_ads,False,[],False,dark,/r/datascience/comments/njqqpw/tools_for_error_analysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/njqqpw/tools_for_error_analysis/,515406,1621835224.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I have noticed this in practice like in Kaggle comps. Sometimes I have done no hyperparameter tuning by just setting whatever seems reasonable ish in a ballpark or using defaults and it ends up performing better than doing computationally intensive and tuning every little hyperparameter. 

In the real world I also wonder whether cross validation for hyperparams can result in being more sensitive to things like data and concept drift. Because well if the future data doesn’t look like your validation set then the CV would have resulted in you overfitting the hyperparameters themselves.",t2_wmwkc,False,,0,False,Is cross-validation hyperparameter tuning sometimes not better than just setting reasonable values?,[],r/datascience,False,6,discussion,0,,,False,t3_njbini,False,dark,0.93,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,1621795785.0,,[],{},,True,,1621816890.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have noticed this in practice like in Kaggle comps. Sometimes I have done no hyperparameter tuning by just setting whatever seems reasonable ish in a ballpark or using defaults and it ends up performing better than doing computationally intensive and tuning every little hyperparameter. &lt;/p&gt;

&lt;p&gt;In the real world I also wonder whether cross validation for hyperparams can result in being more sensitive to things like data and concept drift. Because well if the future data doesn’t look like your validation set then the CV would have resulted in you overfitting the hyperparameters themselves.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njbini,True,,ice_shadow,,8,True,all_ads,False,[],False,,/r/datascience/comments/njbini/is_crossvalidation_hyperparameter_tuning/,all_ads,False,https://www.reddit.com/r/datascience/comments/njbini/is_crossvalidation_hyperparameter_tuning/,515406,1621788090.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let’s take Covid-19 global number of positive cases.

For each day, the global daily number is known by aggregating each country number. And each country cases can be further grouped into different variables like by virus variant, by country state, etc.

If someone looks at the global time series, it’s hard to know which variables are contributing to the number of cases without cutting and slicing the data.

Are there methods that allow us to surface the variable values that contribute to the number or cases for a given time interval?

This sounds somewhat easy, like I can calculate the contribution percentage of each variable value, and sort them.

Though I may also be interested in the contributing trend as well which can be positively contributing to the global number of cases and negatively contributing to the global number of cases.

From my cursory reading, I found topics such as aberration detection. Or generally anomaly detection. But these topics almost never explain what are the variable values that contribute to the anomaly. It max explain anomalous points due to trends and seasonalities but not so much on what are the explanatory variables given that we have aggregated time series.",t2_4bix301o,False,,0,False,What are methods that explain aggregated time series anomalies? For example what feature values contributes to number of Covid-19 cases at a given time interval?,[],r/datascience,False,6,discussion,0,,,False,t3_nja1gq,False,dark,0.9,,public,13,0,{},,,False,[],,False,False,,{},Discussion,False,13,,False,False,self,False,,[],{},,True,,1621812585.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let’s take Covid-19 global number of positive cases.&lt;/p&gt;

&lt;p&gt;For each day, the global daily number is known by aggregating each country number. And each country cases can be further grouped into different variables like by virus variant, by country state, etc.&lt;/p&gt;

&lt;p&gt;If someone looks at the global time series, it’s hard to know which variables are contributing to the number of cases without cutting and slicing the data.&lt;/p&gt;

&lt;p&gt;Are there methods that allow us to surface the variable values that contribute to the number or cases for a given time interval?&lt;/p&gt;

&lt;p&gt;This sounds somewhat easy, like I can calculate the contribution percentage of each variable value, and sort them.&lt;/p&gt;

&lt;p&gt;Though I may also be interested in the contributing trend as well which can be positively contributing to the global number of cases and negatively contributing to the global number of cases.&lt;/p&gt;

&lt;p&gt;From my cursory reading, I found topics such as aberration detection. Or generally anomaly detection. But these topics almost never explain what are the variable values that contribute to the anomaly. It max explain anomalous points due to trends and seasonalities but not so much on what are the explanatory variables given that we have aggregated time series.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nja1gq,True,,levenshteinn,,6,True,all_ads,False,[],False,,/r/datascience/comments/nja1gq/what_are_methods_that_explain_aggregated_time/,all_ads,False,https://www.reddit.com/r/datascience/comments/nja1gq/what_are_methods_that_explain_aggregated_time/,515406,1621783785.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello!

I an looking for a book that explains all the distributions, probability, Anova, p value, confidence and prediction interval and maybe linear regression too. 

Is there a book you like that explains this well?

Thank you!",t2_7ckcfm6o,False,,0,False,"Need to go back to the basics, what's your favorite Stats 101 book?",[],r/datascience,False,6,education,0,,,False,t3_nino7x,False,dark,0.99,,public,376,1,{},,,False,[],,False,False,,{},Education,False,376,,False,False,self,1621705520.0,,[],{},,True,,1621733528.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I an looking for a book that explains all the distributions, probability, Anova, p value, confidence and prediction interval and maybe linear regression too. &lt;/p&gt;

&lt;p&gt;Is there a book you like that explains this well?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nino7x,True,,ysharm10,,88,True,all_ads,False,[],False,,/r/datascience/comments/nino7x/need_to_go_back_to_the_basics_whats_your_favorite/,all_ads,False,https://www.reddit.com/r/datascience/comments/nino7x/need_to_go_back_to_the_basics_whats_your_favorite/,515406,1621704728.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I am giving a technical interview later this week.  My background is in statistics and I will be a question with coding on it.  But I also want to ask a machine learning question. 

Our company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  My own understanding is just that decision tree has a decent interpretability but less accurate than random forest or Xgboost.  I don’t think it is fair for me to ask the candidate their algorithms in details since I am not an expert myself.  

Does anyone have any suggestions?  Thanks.",t2_y7l57,False,,0,False,What is a good technical interview machine learning question?,[],r/datascience,False,6,discussion,0,,,False,t3_njd0ft,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621821094.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am giving a technical interview later this week.  My background is in statistics and I will be a question with coding on it.  But I also want to ask a machine learning question. &lt;/p&gt;

&lt;p&gt;Our company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  My own understanding is just that decision tree has a decent interpretability but less accurate than random forest or Xgboost.  I don’t think it is fair for me to ask the candidate their algorithms in details since I am not an expert myself.  &lt;/p&gt;

&lt;p&gt;Does anyone have any suggestions?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njd0ft,True,,sonicking12,,11,True,all_ads,False,[],False,,/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/,515406,1621792294.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm trying to find resources to learn more about a new subfield in Machine Learning called online learning.

The idea is beautiful and powerful: your model in production trains itself with new latest data to react to changes faster. However the classic ways to build the MLOps infrastructure and algorithms' maths won't do the job here, so I'm eager to learn more:

* I've found [this post](https://huyenchip.com/2020/12/27/real-time-machine-learning.html) by Standford's ML lecturer Chip Huyen to be a great introduction to the concept of Online Learning.
* I've found [river](https://github.com/online-ml/river) to be a promising python library for online learning.

Apart from that, I don't know many resources out there, do you? Any blogs to follow? Any ""Titanic"" equivalents (a simple problem to get going)?",t2_c4rvshhm,False,,0,False,Online machine learning (or how to automatically update your model in production),[],r/datascience,False,6,discussion,0,,,False,t3_njen7v,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1621800204.0,,[],{},,True,,1621825605.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to find resources to learn more about a new subfield in Machine Learning called online learning.&lt;/p&gt;

&lt;p&gt;The idea is beautiful and powerful: your model in production trains itself with new latest data to react to changes faster. However the classic ways to build the MLOps infrastructure and algorithms&amp;#39; maths won&amp;#39;t do the job here, so I&amp;#39;m eager to learn more:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I&amp;#39;ve found &lt;a href=""https://huyenchip.com/2020/12/27/real-time-machine-learning.html""&gt;this post&lt;/a&gt; by Standford&amp;#39;s ML lecturer Chip Huyen to be a great introduction to the concept of Online Learning.&lt;/li&gt;
&lt;li&gt;I&amp;#39;ve found &lt;a href=""https://github.com/online-ml/river""&gt;river&lt;/a&gt; to be a promising python library for online learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apart from that, I don&amp;#39;t know many resources out there, do you? Any blogs to follow? Any &amp;quot;Titanic&amp;quot; equivalents (a simple problem to get going)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njen7v,True,,JB__Quix,,5,True,all_ads,False,[],False,,/r/datascience/comments/njen7v/online_machine_learning_or_how_to_automatically/,all_ads,False,https://www.reddit.com/r/datascience/comments/njen7v/online_machine_learning_or_how_to_automatically/,515406,1621796805.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?auto=webp&amp;s=afc490b7ded3f15b261d944c6fada22a812f2b58', 'width': 1999, 'height': 1293}, 'resolutions': [{'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8aabc02673c1d710295046ce6abdc8a4ae70b99', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1502d89e7e95c23c9c9d44c88b1d8c286fa9638f', 'width': 216, 'height': 139}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f537ef5f39159f95f7e6f5e3b33ded65da91a20', 'width': 320, 'height': 206}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee994ae648a0e8a28cae01e81aa9e84187ca6ee9', 'width': 640, 'height': 413}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=41dce60ae2abd99411174cf179a6a3988658aa30', 'width': 960, 'height': 620}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59104f8f3e4f7fdcbb133574095d313209923a5b', 'width': 1080, 'height': 698}], 'variants': {}, 'id': 'W3GknR_Tnzbar4S08sfFzXqOVZNSULnboBhRnnDiRxc'}], 'enabled': False}",,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,1,False,Weekly Entering &amp; Transitioning Thread | 23 May 2021 - 30 May 2021,[],r/datascience,False,6,,0,,,False,t3_nj6cc2,False,dark,0.82,,public,7,1,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{'gid_2': 1},,True,,1621800030.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Gold', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png'}]",[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,nj6cc2,True,,datascience-bot,,172,False,all_ads,False,[],False,dark,/r/datascience/comments/nj6cc2/weekly_entering_transitioning_thread_23_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/nj6cc2/weekly_entering_transitioning_thread_23_may_2021/,515406,1621771230.0,0,,False,,,,,,,,
,datascience,"Hi All,

So I recently joined a firm with massive operations in the logistics &amp; delivery business. We are currently building a system that can automatically flag anomalous events based on the time-series nature of business KPIs across multiple cities &amp; zones. We are facing multiple roadblocks &amp; I feel completely stuck in a loop without any progress. Below are a few problems we're facing:

1. Prior to this system business never tried detecting anomalies manually, so they don't know what an anomaly is. This is my biggest concern as it makes the problem statement quite open-ended. Also, even after detecting an anomaly through time series models, we don't have any mechanism to evaluate them
2. While building a time series model, should we include the underlying variables as the predictors? Or just treat the target variable as a univariate variable and perform Root cause analysis on the detected anomaly events?

Please share any resources or case studies of a similar system that is implemented on a large scale. I know this problem is very business-specific but I assume the underlying techniques will be similar for such systems.",t2_dlxtbst,False,,0,False,Time Series Anomaly detection system,[],r/datascience,False,6,discussion,0,,,False,t3_njcmm2,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621820037.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;So I recently joined a firm with massive operations in the logistics &amp;amp; delivery business. We are currently building a system that can automatically flag anomalous events based on the time-series nature of business KPIs across multiple cities &amp;amp; zones. We are facing multiple roadblocks &amp;amp; I feel completely stuck in a loop without any progress. Below are a few problems we&amp;#39;re facing:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Prior to this system business never tried detecting anomalies manually, so they don&amp;#39;t know what an anomaly is. This is my biggest concern as it makes the problem statement quite open-ended. Also, even after detecting an anomaly through time series models, we don&amp;#39;t have any mechanism to evaluate them&lt;/li&gt;
&lt;li&gt;While building a time series model, should we include the underlying variables as the predictors? Or just treat the target variable as a univariate variable and perform Root cause analysis on the detected anomaly events?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Please share any resources or case studies of a similar system that is implemented on a large scale. I know this problem is very business-specific but I assume the underlying techniques will be similar for such systems.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njcmm2,True,,jkashish1818,,3,True,all_ads,False,[],False,,/r/datascience/comments/njcmm2/time_series_anomaly_detection_system/,all_ads,False,https://www.reddit.com/r/datascience/comments/njcmm2/time_series_anomaly_detection_system/,515406,1621791237.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone,

I was scrolling feeds of the group and did a quick search for Knime. It actually surprises me how unpopular as a platform is considering that the last post was a year ago. 

I have started to learn more about Knime (required for job) and wanted to see your thoughts on the platform based on the experience you had.

Is there any substitute that does a better job than Knime and this is the reason why it is not very popular.

Any opinion is helpful.",t2_7gvgv,False,,0,False,Your experience with Knime,[],r/datascience,False,6,tooling,0,,,False,t3_niieqi,False,dark,0.94,,public,55,0,{},,,False,[],,False,False,,{},Tooling,False,55,,False,False,self,False,,[],{},,True,,1621718131.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I was scrolling feeds of the group and did a quick search for Knime. It actually surprises me how unpopular as a platform is considering that the last post was a year ago. &lt;/p&gt;

&lt;p&gt;I have started to learn more about Knime (required for job) and wanted to see your thoughts on the platform based on the experience you had.&lt;/p&gt;

&lt;p&gt;Is there any substitute that does a better job than Knime and this is the reason why it is not very popular.&lt;/p&gt;

&lt;p&gt;Any opinion is helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niieqi,True,,salihveseli,,37,True,all_ads,False,[],False,,/r/datascience/comments/niieqi/your_experience_with_knime/,all_ads,False,https://www.reddit.com/r/datascience/comments/niieqi/your_experience_with_knime/,515406,1621689331.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I am currently a data scientist/analyst. My current job has a lot of analyses and code that could be functionalized and used as an internal Python package. I am interested in working in a role that is focused on creating packages, tools, and other processes to help streamline and create efficiency for an entire DS team... and less so a role that is actually DS itself. Curious if anyone knows if there is a certain job title that fits this description?",t2_yy1onqs,False,,0,False,What type of job would allow me to create useful tools for a data science/data analyst team?,[],r/datascience,False,6,career,0,,,False,t3_niuauk,False,dark,0.74,,public,10,0,{},,,False,[],,False,False,,{},Career,False,10,,False,False,self,False,,[],{},,True,,1621753022.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am currently a data scientist/analyst. My current job has a lot of analyses and code that could be functionalized and used as an internal Python package. I am interested in working in a role that is focused on creating packages, tools, and other processes to help streamline and create efficiency for an entire DS team... and less so a role that is actually DS itself. Curious if anyone knows if there is a certain job title that fits this description?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niuauk,True,,createAnAccount13,,10,True,all_ads,False,[],False,,/r/datascience/comments/niuauk/what_type_of_job_would_allow_me_to_create_useful/,all_ads,False,https://www.reddit.com/r/datascience/comments/niuauk/what_type_of_job_would_allow_me_to_create_useful/,515406,1621724222.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have 8 years of experience in data science, and was reached out to by a recruiter in regards to a Director of Marketing Analytics position for a company that is not a no-name company(many of you have probably heard of them). I have gone through the interview process and have advanced far in the process, so I would imagine I am coming to the end of the line.

The problem is I keep reading the job description and am wondering how qualified I truly am for this role. The last 5 years I have managed teams, with my most recent role managing a team of 8 as a senior data science manager(my team was laid off in December). I consider myself to be very well versed in data science and everything that comes with it.

HOWEVER... the job description(in terms of duties) has a lot of data engineering lingo in it. Something I really don't have experience in. Everywhere I have been, we have had a separate data engineering team that handles all of that, which I work parallel with. In all my roles I have been strictly data science, modeling, machine learning, deep learning, analytics and insights, client and executive presentations, and nothing to do with data warehousing and data pipelining. But, in the job requirements section, I meet all of the qualification criteria. 

I can send the exact job description, qualifications and responsibilities in private message if anyone asks. But, I feel like my data engineering skills are lacking, and you think the recruiters and those I have interviewed with so far would have been able to tell that, but I keep advancing. In my interviews, I talk up my technical skills, but most importantly I talk up my people skills. As I have advanced in this profession, the one thing I have learned is the higher you go, the less important your techinical skills become, and the more important your people skills become. Those I have spoke with so far are really glad to hear someone say that...

But, I don't want to set myself up to fail. Should someone with my level of experience in this industry be expected to have data engineering experience as well? I've always viewed the two as two separate discplines, and I believe a position marrying the two would be overkill. I believe in specialization in that regard. I believe in terms of pure data science and analytics I am a home run for this role, but all of the data engineering mumbo jumbo in the job description is scaring the bejeezus out of me. Surely in my next interview this week I could ask if there is a separate data engineering team or if its something I would have to handle. Or am I worrying over nothing? Surely a company this size would have enough resources to split the two disciplines out?",t2_260w6lk0,False,,0,False,Questioning my qualifications for an Analytics Director position I have been interviewing for(and have advanced far in the process for),[],r/datascience,False,6,,0,,,False,t3_nir1qm,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},Job Search,False,5,,False,False,self,False,,[],{},,True,,1621743270.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 8 years of experience in data science, and was reached out to by a recruiter in regards to a Director of Marketing Analytics position for a company that is not a no-name company(many of you have probably heard of them). I have gone through the interview process and have advanced far in the process, so I would imagine I am coming to the end of the line.&lt;/p&gt;

&lt;p&gt;The problem is I keep reading the job description and am wondering how qualified I truly am for this role. The last 5 years I have managed teams, with my most recent role managing a team of 8 as a senior data science manager(my team was laid off in December). I consider myself to be very well versed in data science and everything that comes with it.&lt;/p&gt;

&lt;p&gt;HOWEVER... the job description(in terms of duties) has a lot of data engineering lingo in it. Something I really don&amp;#39;t have experience in. Everywhere I have been, we have had a separate data engineering team that handles all of that, which I work parallel with. In all my roles I have been strictly data science, modeling, machine learning, deep learning, analytics and insights, client and executive presentations, and nothing to do with data warehousing and data pipelining. But, in the job requirements section, I meet all of the qualification criteria. &lt;/p&gt;

&lt;p&gt;I can send the exact job description, qualifications and responsibilities in private message if anyone asks. But, I feel like my data engineering skills are lacking, and you think the recruiters and those I have interviewed with so far would have been able to tell that, but I keep advancing. In my interviews, I talk up my technical skills, but most importantly I talk up my people skills. As I have advanced in this profession, the one thing I have learned is the higher you go, the less important your techinical skills become, and the more important your people skills become. Those I have spoke with so far are really glad to hear someone say that...&lt;/p&gt;

&lt;p&gt;But, I don&amp;#39;t want to set myself up to fail. Should someone with my level of experience in this industry be expected to have data engineering experience as well? I&amp;#39;ve always viewed the two as two separate discplines, and I believe a position marrying the two would be overkill. I believe in specialization in that regard. I believe in terms of pure data science and analytics I am a home run for this role, but all of the data engineering mumbo jumbo in the job description is scaring the bejeezus out of me. Surely in my next interview this week I could ask if there is a separate data engineering team or if its something I would have to handle. Or am I worrying over nothing? Surely a company this size would have enough resources to split the two disciplines out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nir1qm,True,,Beer_Makes_You_Fat,,17,True,all_ads,False,[],False,,/r/datascience/comments/nir1qm/questioning_my_qualifications_for_an_analytics/,all_ads,False,https://www.reddit.com/r/datascience/comments/nir1qm/questioning_my_qualifications_for_an_analytics/,515406,1621714470.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_jokwd,False,,0,False,"Currently a Data Scientist... Want to increase my skillset to expand into Data Engineering... Any great resources, courses etc that you guys can recommend. Thanks",[],r/datascience,False,6,education,0,,,False,t3_ni0b8j,False,dark,0.97,,public,266,1,{},,,False,[],,False,False,,{},Education,False,266,,False,False,self,False,,[],{'gid_1': 1},,True,,1621652985.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni0b8j,True,,Bosser7,,38,True,all_ads,False,[],False,,/r/datascience/comments/ni0b8j/currently_a_data_scientist_want_to_increase_my/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni0b8j/currently_a_data_scientist_want_to_increase_my/,515406,1621624185.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Does anyone know of any good tools for knowledge sharing post fact?

We've tried this but found it came up a bit short.

https://github.com/airbnb/knowledge-repo",t2_ur9jhrn,False,,0,False,How does everyone share their models etc. across teams for re-use effectively?,[],r/datascience,False,6,tooling,0,,,False,t3_niq3eb,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1621740450.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know of any good tools for knowledge sharing post fact?&lt;/p&gt;

&lt;p&gt;We&amp;#39;ve tried this but found it came up a bit short.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/airbnb/knowledge-repo""&gt;https://github.com/airbnb/knowledge-repo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niq3eb,True,,SimpleEnthusiasm,,1,True,all_ads,False,[],False,,/r/datascience/comments/niq3eb/how_does_everyone_share_their_models_etc_across/,all_ads,False,https://www.reddit.com/r/datascience/comments/niq3eb/how_does_everyone_share_their_models_etc_across/,515406,1621711650.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?auto=webp&amp;s=72f747c55187d86115126cf085f85ddafa20b95e', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f322bba42b689af77aa1a9465d6d2b8559d1620', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=417361eb84b420b2e14ec73e71f53c1fe2d478bc', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb2d707e4a56e2382dc9e65be394076814105d2', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db5b918a07af15c657a173593e5ea67fdba27f04', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=453be7f6124a043bd4e88fbb64ae2cee30b76187', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4402785456011f4ff77bb947faa96940e5957054', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'GFCwLI821B9LV6Lussb5HWKwu5cg_cho_mHtdCSct1c'}], 'enabled': False}",,,,,
,datascience,"I have a Master's of Public Policy and I generally would do data cleaning / ETL in Excel, and then import it into STATA for multivariate regressions. 

It was nothing fancy but it was insightful and I enjoyed it; now I want to expand my knowledge into SQL, but I am wondering if realistically so few people/places use STATA that I should start learning Python, or SPSS instead?",t2_8azmn3,False,,0,False,Does anyone use STATA?,[],r/datascience,False,6,education,0,,,False,t3_niljmw,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1621727511.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a Master&amp;#39;s of Public Policy and I generally would do data cleaning / ETL in Excel, and then import it into STATA for multivariate regressions. &lt;/p&gt;

&lt;p&gt;It was nothing fancy but it was insightful and I enjoyed it; now I want to expand my knowledge into SQL, but I am wondering if realistically so few people/places use STATA that I should start learning Python, or SPSS instead?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niljmw,True,,TimboCA,,21,True,all_ads,False,[],False,,/r/datascience/comments/niljmw/does_anyone_use_stata/,all_ads,False,https://www.reddit.com/r/datascience/comments/niljmw/does_anyone_use_stata/,515406,1621698711.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I've been back on the job market for the last month since being laid off and I have what may be a final interview on Monday for a job I really want, its kind of a dream job for me, Director of Data/Analytics for a company I've always wanted to work for. I didn't even apply, a recruiter sought me out personally. I have the experience and qualifications. I've been in this business since I graduated in 2011, I have management experience, and a successful track record. 

I've already had multiple calls with hiring managers, and those have all been great, hence why I have advanced so far. However my next interview they said they are bringing in 2 outside guys from a data firm they contract to ""gauge"" my technical experience.

This immediately brought in fear because I've been interviewing for many jobs over the past month and I've already had 3 interviews that I considered to be ""air raid"" interviews where it seemed like I was under attack from the panel rather than being interviewed by the panel.

The first ""air raid"" interview I was facing a panel of 3 in a final interview setting and all of a sudden we get to a part where it was rapid fire trivia style questions. I felt like I was on the fast money round of Family Feud because I was only given 30 seconds to answer each question. The thing is it was pure Python trivia, a lot of questions over stuff I'll automate anyway that I didn't know off the top of my head and would have to reference my work. The guy leading the interview, the entire time during this, had a smug, skeptical look on his face like that blonde dude from Napoleon Dynamite who is always making fun of Napoelon. Afterward the 2 other interviewers were trying to apologize without apologizing as you could tell they werent a fan of that part of the interview. I did fine on everything else, but that was a train wreck and I got the rejection email the next week.

Then about a month ago I had another panel style interview, going up against 3 guys who I felt were just trying to knock me down from the get go. Asking a lot of ""booby trapped"" questions that I was clever enough to ""figure out"" but there were a couple that got by me. Questions obviously designed to trip people up. It didn't help that the microphones one of the interviewers had was horrible and I kept having to ask him to repeat questions, but these guys were very smug and hostile and played off of each other as if it were some kind of game to ruin the dreams of anyone applying. I got rejected by these people too, and actually got a customized rejection email saying that my skillset was below their standards. No shit, if they are judging my skillset based on that interview they won't find anyone that meets their standards. Which is the case because I still get emails from LinkedIn telling me to apply for that job as they are still actively recruiting for it. Good luck!

Finally I had a take home task that took me about 5 hours and I had to present it to a panel, and they were really unimpressed, not because of my conclusions/findings/work, but because it wasn't ""visually appealing"" enough. They actually wanted me to present my findings in PowerPoint(I used R Markdown) with fancy infographics and such. I didn't realize I was being hired for a graphic design job. I did what I was tasked to do and infographics were not in the task instructions. It's not like my R Markdown graphs and charts were ugly or off, they told the story perfectly and accurately, but its clear they were looking for someone who wanted to be an artist as well. 

Anyway, I feel like these 3 experiences have prepared me for what I may be facing, and I suppose there is a chance that these 2 guys they are bringing in to ""gauge"" my technical skills won't bombard me and will be totally cool, but I am not getting my hopes up. These two have already reached out to me and asked for my GitHub account, something I was not prepared for since I was never asked about it prior nor was it in the job description. I sent it over anyway but a lot of my code was written for a one man audience(me) and I wasn't able to save all of my work from my past job as they blocked access to my work OneDrive as soon as I was laid off so I could only save my work samples that I had on my own machine. Therefore I don't feel like my GitHub is truly putting my best foot forward. 

I had a good panel interview experience 3 weeks ago, I made it to the final 2 but was not selected, but it was still a great experience, and I let the recruiters/hiring managers know it. They didn't try to pepper you with trivia or kaggle or leetcode, and instead just probed you about your conceptual understanding of concepts. 

When I was a hiring manager and was hiring people for my own teams, I always hired on potential, not pure ability. I didn't hire them based on what they could do now, but what I felt like they could ultimately become. I feel like a lot of these interviews are structured around finding candidates with high floors, rather than high ceilings. 

Anyway, just kind of ranting as I mentally prepare for what could be another air raid interview. Anyone have any tips in case it gets to that point?",t2_y5gxz,False,,0,False,"Has anyone here had the experience of what I call an ""air raid"" style of interview, where the panel of interviewers seem to be constantly trying to attack and poke holes in you?",[],r/datascience,False,6,,0,,,False,t3_nhi11p,False,dark,0.98,,public,372,3,{},,,False,[],,False,False,,{},Job Search,False,372,,False,False,self,False,,[],{},,True,,1621593266.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been back on the job market for the last month since being laid off and I have what may be a final interview on Monday for a job I really want, its kind of a dream job for me, Director of Data/Analytics for a company I&amp;#39;ve always wanted to work for. I didn&amp;#39;t even apply, a recruiter sought me out personally. I have the experience and qualifications. I&amp;#39;ve been in this business since I graduated in 2011, I have management experience, and a successful track record. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve already had multiple calls with hiring managers, and those have all been great, hence why I have advanced so far. However my next interview they said they are bringing in 2 outside guys from a data firm they contract to &amp;quot;gauge&amp;quot; my technical experience.&lt;/p&gt;

&lt;p&gt;This immediately brought in fear because I&amp;#39;ve been interviewing for many jobs over the past month and I&amp;#39;ve already had 3 interviews that I considered to be &amp;quot;air raid&amp;quot; interviews where it seemed like I was under attack from the panel rather than being interviewed by the panel.&lt;/p&gt;

&lt;p&gt;The first &amp;quot;air raid&amp;quot; interview I was facing a panel of 3 in a final interview setting and all of a sudden we get to a part where it was rapid fire trivia style questions. I felt like I was on the fast money round of Family Feud because I was only given 30 seconds to answer each question. The thing is it was pure Python trivia, a lot of questions over stuff I&amp;#39;ll automate anyway that I didn&amp;#39;t know off the top of my head and would have to reference my work. The guy leading the interview, the entire time during this, had a smug, skeptical look on his face like that blonde dude from Napoleon Dynamite who is always making fun of Napoelon. Afterward the 2 other interviewers were trying to apologize without apologizing as you could tell they werent a fan of that part of the interview. I did fine on everything else, but that was a train wreck and I got the rejection email the next week.&lt;/p&gt;

&lt;p&gt;Then about a month ago I had another panel style interview, going up against 3 guys who I felt were just trying to knock me down from the get go. Asking a lot of &amp;quot;booby trapped&amp;quot; questions that I was clever enough to &amp;quot;figure out&amp;quot; but there were a couple that got by me. Questions obviously designed to trip people up. It didn&amp;#39;t help that the microphones one of the interviewers had was horrible and I kept having to ask him to repeat questions, but these guys were very smug and hostile and played off of each other as if it were some kind of game to ruin the dreams of anyone applying. I got rejected by these people too, and actually got a customized rejection email saying that my skillset was below their standards. No shit, if they are judging my skillset based on that interview they won&amp;#39;t find anyone that meets their standards. Which is the case because I still get emails from LinkedIn telling me to apply for that job as they are still actively recruiting for it. Good luck!&lt;/p&gt;

&lt;p&gt;Finally I had a take home task that took me about 5 hours and I had to present it to a panel, and they were really unimpressed, not because of my conclusions/findings/work, but because it wasn&amp;#39;t &amp;quot;visually appealing&amp;quot; enough. They actually wanted me to present my findings in PowerPoint(I used R Markdown) with fancy infographics and such. I didn&amp;#39;t realize I was being hired for a graphic design job. I did what I was tasked to do and infographics were not in the task instructions. It&amp;#39;s not like my R Markdown graphs and charts were ugly or off, they told the story perfectly and accurately, but its clear they were looking for someone who wanted to be an artist as well. &lt;/p&gt;

&lt;p&gt;Anyway, I feel like these 3 experiences have prepared me for what I may be facing, and I suppose there is a chance that these 2 guys they are bringing in to &amp;quot;gauge&amp;quot; my technical skills won&amp;#39;t bombard me and will be totally cool, but I am not getting my hopes up. These two have already reached out to me and asked for my GitHub account, something I was not prepared for since I was never asked about it prior nor was it in the job description. I sent it over anyway but a lot of my code was written for a one man audience(me) and I wasn&amp;#39;t able to save all of my work from my past job as they blocked access to my work OneDrive as soon as I was laid off so I could only save my work samples that I had on my own machine. Therefore I don&amp;#39;t feel like my GitHub is truly putting my best foot forward. &lt;/p&gt;

&lt;p&gt;I had a good panel interview experience 3 weeks ago, I made it to the final 2 but was not selected, but it was still a great experience, and I let the recruiters/hiring managers know it. They didn&amp;#39;t try to pepper you with trivia or kaggle or leetcode, and instead just probed you about your conceptual understanding of concepts. &lt;/p&gt;

&lt;p&gt;When I was a hiring manager and was hiring people for my own teams, I always hired on potential, not pure ability. I didn&amp;#39;t hire them based on what they could do now, but what I felt like they could ultimately become. I feel like a lot of these interviews are structured around finding candidates with high floors, rather than high ceilings. &lt;/p&gt;

&lt;p&gt;Anyway, just kind of ranting as I mentally prepare for what could be another air raid interview. Anyone have any tips in case it gets to that point?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nhi11p,True,,dothingsright_,,170,True,all_ads,False,[],False,,/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/,515406,1621564466.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I've always pronounced it as ""eeee-pock"" which is how my Comp Sci professor who first taught me neural nets said it. But I hear people say ""epic"" or ""eh-pock"" all the time and it really irritates me for some reason.

How do you think it's supposed to be pronounced in a data science context?

Edit: I've learned from some commenters that the American pronunciation is supposed to be ""eh-puk"" (like epic) and the British pronunciation is supposed to be ""e-pock"". But I swear I hear some people sort of meet in the middle and use ""eh-pock"" as well.",t2_b2g0m82o,False,,0,False,"How do YOU pronounce ""epoch""?",[],r/datascience,False,6,discussion,0,,,False,t3_nhxqrn,False,dark,0.83,,public,16,0,{},,,False,[],,False,False,,{},Discussion,False,16,,False,False,self,1621625515.0,,[],{},,True,,1621646129.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve always pronounced it as &amp;quot;eeee-pock&amp;quot; which is how my Comp Sci professor who first taught me neural nets said it. But I hear people say &amp;quot;epic&amp;quot; or &amp;quot;eh-pock&amp;quot; all the time and it really irritates me for some reason.&lt;/p&gt;

&lt;p&gt;How do you think it&amp;#39;s supposed to be pronounced in a data science context?&lt;/p&gt;

&lt;p&gt;Edit: I&amp;#39;ve learned from some commenters that the American pronunciation is supposed to be &amp;quot;eh-puk&amp;quot; (like epic) and the British pronunciation is supposed to be &amp;quot;e-pock&amp;quot;. But I swear I hear some people sort of meet in the middle and use &amp;quot;eh-pock&amp;quot; as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhxqrn,True,,horsewithmanynames,,46,True,all_ads,False,[],False,,/r/datascience/comments/nhxqrn/how_do_you_pronounce_epoch/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhxqrn/how_do_you_pronounce_epoch/,515407,1621617329.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in the unique and fortunate situation of having the option of choosing between a data scientist (promotion in current company) or senior product analyst (offer from a new company) position for my next role.

Both would be significant upgrades for me, but the product analyst position at a new place will pay significantly more (I don't know by how much yet as I'm waiting for a formal offer from my current company). I have wanted to break into DS and get the title for a while, but I'm not sure if staying at my current company with lower pay is worth the title.

I know titles and actual work in our field really aren't well defined. I have some indication of what the DS work if I stayed would look like- some cool ML models, recommendation algorithms, testing, as well as more typical data analyst work with SQL and no modeling. My preference would be to do modeling and more DS long term, but I'm also kind of ready for a reset and new company. 

There's certainly some factors I'm leaving out,, but which opportunity would you jump at? A DS role with the title and maybe more interesting work, but lower pay and company frustrations - OR a senior product analyst role at a new, exciting company with significantly more pay and more typical product analytics work.

Maybe ""which would you pick"" is too subjective- but what considerations would you make?

Any advice welcome, thanks!",t2_ot5g6,False,,0,False,Data Scientist vs Senior Product Analyst,[],r/datascience,False,6,,0,,,False,t3_ni7cix,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,False,,[],{},,True,,1621674141.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the unique and fortunate situation of having the option of choosing between a data scientist (promotion in current company) or senior product analyst (offer from a new company) position for my next role.&lt;/p&gt;

&lt;p&gt;Both would be significant upgrades for me, but the product analyst position at a new place will pay significantly more (I don&amp;#39;t know by how much yet as I&amp;#39;m waiting for a formal offer from my current company). I have wanted to break into DS and get the title for a while, but I&amp;#39;m not sure if staying at my current company with lower pay is worth the title.&lt;/p&gt;

&lt;p&gt;I know titles and actual work in our field really aren&amp;#39;t well defined. I have some indication of what the DS work if I stayed would look like- some cool ML models, recommendation algorithms, testing, as well as more typical data analyst work with SQL and no modeling. My preference would be to do modeling and more DS long term, but I&amp;#39;m also kind of ready for a reset and new company. &lt;/p&gt;

&lt;p&gt;There&amp;#39;s certainly some factors I&amp;#39;m leaving out,, but which opportunity would you jump at? A DS role with the title and maybe more interesting work, but lower pay and company frustrations - OR a senior product analyst role at a new, exciting company with significantly more pay and more typical product analytics work.&lt;/p&gt;

&lt;p&gt;Maybe &amp;quot;which would you pick&amp;quot; is too subjective- but what considerations would you make?&lt;/p&gt;

&lt;p&gt;Any advice welcome, thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,ni7cix,True,,robo_capybara,,8,True,all_ads,False,[],False,,/r/datascience/comments/ni7cix/data_scientist_vs_senior_product_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni7cix/data_scientist_vs_senior_product_analyst/,515407,1621645341.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Confused as to when to use [StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer) vs StringIndexer+[OneHotEncoder](https://spark.apache.org/docs/latest/ml-features#onehotencoder).

The OneHotEncoder docs say

&gt;For string type input data, it is common to encode categorical features using StringIndexer first.

In what situations would I want to take the extra step of transforming StringIndex'ed output to one-hot encoded features? I can find a lot of resources on how to use which, but not in which cases the OneHotEncoder would be better.",t2_4rwmx54d,False,,0,False,"spark ml StringIndexer vs OneHotEncoder, when to use which?",[],r/datascience,False,6,discussion,0,,,False,t3_ni5ptp,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621668381.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Confused as to when to use &lt;a href=""https://spark.apache.org/docs/latest/ml-features#stringindexer""&gt;StringIndexer&lt;/a&gt; vs StringIndexer+&lt;a href=""https://spark.apache.org/docs/latest/ml-features#onehotencoder""&gt;OneHotEncoder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The OneHotEncoder docs say&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For string type input data, it is common to encode categorical features using StringIndexer first.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In what situations would I want to take the extra step of transforming StringIndex&amp;#39;ed output to one-hot encoded features? I can find a lot of resources on how to use which, but not in which cases the OneHotEncoder would be better.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni5ptp,True,,Anxious_Reporter,,3,True,all_ads,False,[],False,,/r/datascience/comments/ni5ptp/spark_ml_stringindexer_vs_onehotencoder_when_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni5ptp/spark_ml_stringindexer_vs_onehotencoder_when_to/,515407,1621639581.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_kkj7t,False,,0,False,The best data science newsletters that you subscribe for?,[],r/datascience,False,6,education,0,,,False,t3_ni0xcu,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1621654613.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni0xcu,True,,so_phrasing,,11,True,all_ads,False,[],False,,/r/datascience/comments/ni0xcu/the_best_data_science_newsletters_that_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni0xcu/the_best_data_science_newsletters_that_you/,515407,1621625813.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"[https://www.crosstab.io/articles/statistical-rethinking-review](https://www.crosstab.io/articles/statistical-rethinking-review)

I noticed a lot of folks recommending this book, so I followed the crowd and got a copy. It's oriented toward researchers in natural and social sciences, so I wrote up my thoughts about the book from the perspective of an industry data scientist",t2_arhctu9v,False,,0,False,"Review: Statistical Rethinking, by Richard McElreath",[],r/datascience,False,6,discussion,0,,,False,t3_nhv5w3,False,dark,0.87,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,False,,[],{},,True,,1621639528.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.crosstab.io/articles/statistical-rethinking-review""&gt;https://www.crosstab.io/articles/statistical-rethinking-review&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I noticed a lot of folks recommending this book, so I followed the crowd and got a copy. It&amp;#39;s oriented toward researchers in natural and social sciences, so I wrote up my thoughts about the book from the perspective of an industry data scientist&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhv5w3,True,,ctk_brian,,7,True,all_ads,False,[],False,,/r/datascience/comments/nhv5w3/review_statistical_rethinking_by_richard_mcelreath/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhv5w3/review_statistical_rethinking_by_richard_mcelreath/,515407,1621610728.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?auto=webp&amp;s=73e934a08f4dc1f41bf5a5c432f72216ca329ece', 'width': 508, 'height': 815}, 'resolutions': [{'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5624eb6861009cbd5a5357c36b161a292ac00b71', 'width': 108, 'height': 173}, {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e877d6591d7578affca157f982befa4165bf2f3', 'width': 216, 'height': 346}, {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50868d99e36a0c55ed989823a21a8ae6a3b2083d', 'width': 320, 'height': 513}], 'variants': {}, 'id': 'Y-GLK6h0320mqstot6PY-jNOxe5YOduEhhxJaHHh9Zs'}], 'enabled': False}",,,,,
,datascience,"I am working on a Data Science project at work and my manager gave me permission to use Python. However, he also said that my code should be clean and readable and I must explain it to my coworkers, since none of them have extensive experience with Python before. This is because I am an intern, so when I leave, my coworkers must be able to understand my code and maintain it. Because of this, do you think I should use Python? Or use something else like Excel to be safe cause everyone know Excel?",t2_4j10zx4j,False,,0,False,Should I use Python at work if my coworkers don't have experience with it?,[],r/datascience,False,6,tooling,0,,,False,t3_nhtcug,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Tooling,False,4,,False,False,self,False,,[],{},,True,,1621634790.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a Data Science project at work and my manager gave me permission to use Python. However, he also said that my code should be clean and readable and I must explain it to my coworkers, since none of them have extensive experience with Python before. This is because I am an intern, so when I leave, my coworkers must be able to understand my code and maintain it. Because of this, do you think I should use Python? Or use something else like Excel to be safe cause everyone know Excel?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhtcug,True,,---Imperator---,,7,True,all_ads,False,[],False,,/r/datascience/comments/nhtcug/should_i_use_python_at_work_if_my_coworkers_dont/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhtcug/should_i_use_python_at_work_if_my_coworkers_dont/,515407,1621605990.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I was recently reading some articles on the importance of ""interpertability"" when dealing with ""blackbox"" models. ""Blackbox"" models like neural networks are said to have a very low level of interpertability, because they don't allow the analyst to understand why the model is making a certain predictions for an individual observation.

On the other hand, models like decision trees and regression models are said to have much higher levels of interpertability. In a general sense, I can understand why models like decision trees are interpretable, because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation. 

If you look at a regression model, 

e.g. salary = 5.3 * height  + 2 * weight  - 15.8 * age 

A regression model can allow the analyst to understand how much each variable contributes to the prediction (e.g. in this example, age contributes more to the prediction by a factor of almost 8 times), and you can also find out how statistically significant each variable is (e.g. indivudal p-value of each regression coefficient). 

Is this what is meant by the ""interpertability of a regression model""?

Thanks",t2_xtuyc,False,,0,False,"how ""interpertable"" are regression models?",[],r/datascience,False,6,discussion,0,,,False,t3_ni2x2q,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621660025.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was recently reading some articles on the importance of &amp;quot;interpertability&amp;quot; when dealing with &amp;quot;blackbox&amp;quot; models. &amp;quot;Blackbox&amp;quot; models like neural networks are said to have a very low level of interpertability, because they don&amp;#39;t allow the analyst to understand why the model is making a certain predictions for an individual observation.&lt;/p&gt;

&lt;p&gt;On the other hand, models like decision trees and regression models are said to have much higher levels of interpertability. In a general sense, I can understand why models like decision trees are interpretable, because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation. &lt;/p&gt;

&lt;p&gt;If you look at a regression model, &lt;/p&gt;

&lt;p&gt;e.g. salary = 5.3 * height  + 2 * weight  - 15.8 * age &lt;/p&gt;

&lt;p&gt;A regression model can allow the analyst to understand how much each variable contributes to the prediction (e.g. in this example, age contributes more to the prediction by a factor of almost 8 times), and you can also find out how statistically significant each variable is (e.g. indivudal p-value of each regression coefficient). &lt;/p&gt;

&lt;p&gt;Is this what is meant by the &amp;quot;interpertability of a regression model&amp;quot;?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni2x2q,True,,ottawalanguages,,11,True,all_ads,False,[],False,,/r/datascience/comments/ni2x2q/how_interpertable_are_regression_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni2x2q/how_interpertable_are_regression_models/,515407,1621631225.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Example - at the company I work for, they had been trying to hire a analyst for quite some time. It was originally called ""technical analyst"", and the response was...lukewarm. 20-25 applicants, and some even withdrew their applications underway. 

Then HR renamed the job to ""Data Scientist"", included that in the tittle of the listing, and slapped on some buzzwords on the new tools we use.  

Result? Almost 300 applications. The shortlist included people with experience from big name tech and banking companies, prestigious schools, etc.",t2_klsal,False,,0,False,"It's crazy how effective it's to include ""Data Scientist"" in your job listing.",[],r/datascience,False,6,,0,,,False,t3_nguua9,False,dark,0.98,,public,491,0,{},,,False,[],,False,False,,{},Job Search,False,491,,False,False,self,False,,[],{},,True,,1621528881.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Example - at the company I work for, they had been trying to hire a analyst for quite some time. It was originally called &amp;quot;technical analyst&amp;quot;, and the response was...lukewarm. 20-25 applicants, and some even withdrew their applications underway. &lt;/p&gt;

&lt;p&gt;Then HR renamed the job to &amp;quot;Data Scientist&amp;quot;, included that in the tittle of the listing, and slapped on some buzzwords on the new tools we use.  &lt;/p&gt;

&lt;p&gt;Result? Almost 300 applications. The shortlist included people with experience from big name tech and banking companies, prestigious schools, etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nguua9,True,,trackerFF,,139,True,all_ads,False,[],False,,/r/datascience/comments/nguua9/its_crazy_how_effective_its_to_include_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nguua9/its_crazy_how_effective_its_to_include_data/,515407,1621500081.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_1xx5out6,False,,0,False,"Data Scientists, what are few things that you wish you knew before starting your data science journey?",[],r/datascience,False,6,discussion,0,,,False,t3_nh1as2,False,dark,0.94,,public,108,0,{},,,False,[],,False,False,,{},Discussion,False,108,,False,False,self,False,,[],{},,True,,1621548763.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh1as2,True,,freakNinja39,,103,True,all_ads,False,[],False,,/r/datascience/comments/nh1as2/data_scientists_what_are_few_things_that_you_wish/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh1as2/data_scientists_what_are_few_things_that_you_wish/,515407,1621519963.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Would love some advice/recommendations from Causal Inference Data Scientists - We are trying to look for tools/frameworks/platforms that can help boost the productivity of Data Scientists in a causal inferencing team. Right now, the Data Scientists are just doing their experimentation on AI Platform Notebooks, but would like to try to standardize their methodology, automate processes whenever possible and  track their experimentation.

I believe the current workflow is:

1. DS writes SQL query to pull in treatments, outcome variables data from Snowflake.
2. DS uses ECONML or DoWhy libraries to get the causal estimate, statistical significance, etc.
3. DS tracks experiments and different variables used on Mlflow

I'm sure this current workflow can be greatly approved and was wondering if there are some established industry best practices  that we can learn from with regards to causal inferencing and experimentation. Also, we would love to leverage any open source tooling you would recommend that would help in this domain",t2_53pxmg7h,False,,0,False,What are some tools/best practices that Causal Inferencing teams use for experimentation?,[],r/datascience,False,6,tooling,0,,,False,t3_nhfdr3,False,dark,0.94,,public,16,0,{},,,False,[],,False,False,,{},Tooling,False,16,,False,False,self,1621556151.0,,[],{},,True,,1621584633.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Would love some advice/recommendations from Causal Inference Data Scientists - We are trying to look for tools/frameworks/platforms that can help boost the productivity of Data Scientists in a causal inferencing team. Right now, the Data Scientists are just doing their experimentation on AI Platform Notebooks, but would like to try to standardize their methodology, automate processes whenever possible and  track their experimentation.&lt;/p&gt;

&lt;p&gt;I believe the current workflow is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;DS writes SQL query to pull in treatments, outcome variables data from Snowflake.&lt;/li&gt;
&lt;li&gt;DS uses ECONML or DoWhy libraries to get the causal estimate, statistical significance, etc.&lt;/li&gt;
&lt;li&gt;DS tracks experiments and different variables used on Mlflow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m sure this current workflow can be greatly approved and was wondering if there are some established industry best practices  that we can learn from with regards to causal inferencing and experimentation. Also, we would love to leverage any open source tooling you would recommend that would help in this domain&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhfdr3,True,,rirhun,,3,True,all_ads,False,[],False,,/r/datascience/comments/nhfdr3/what_are_some_toolsbest_practices_that_causal/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhfdr3/what_are_some_toolsbest_practices_that_causal/,515407,1621555833.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys, I have a scenario where a ML model needs to be deployed on client's server. There exists the possibility that the model might be reverse engineered and therefore there is a need to protect it.

Suggestions for achieving this?

Thanks!",t2_2mmql89p,False,,0,False,Securing Machine learning model,[],r/datascience,False,6,discussion,0,,,False,t3_nhp2ui,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621620960.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I have a scenario where a ML model needs to be deployed on client&amp;#39;s server. There exists the possibility that the model might be reverse engineered and therefore there is a need to protect it.&lt;/p&gt;

&lt;p&gt;Suggestions for achieving this?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhp2ui,True,,grid_world,,12,True,all_ads,False,[],False,,/r/datascience/comments/nhp2ui/securing_machine_learning_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhp2ui/securing_machine_learning_model/,515407,1621592160.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I would think it would be a bonus to have experience in various industries, but my encounters in applications and interviews makes me think otherwise",t2_5zbsxzuq,False,,0,False,"All else equal, would hiring managers rather have a candidate with all their experience in their industry or prefer someone with experience in multiple fields?",[],r/datascience,False,6,discussion,0,,,False,t3_nhbc7e,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1621573150.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would think it would be a bonus to have experience in various industries, but my encounters in applications and interviews makes me think otherwise&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhbc7e,True,,wsb146,,5,False,all_ads,False,[],False,,/r/datascience/comments/nhbc7e/all_else_equal_would_hiring_managers_rather_have/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhbc7e/all_else_equal_would_hiring_managers_rather_have/,515407,1621544350.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

I recently started my first job working as an entry level Data Scientist. I’ve been working at this company for roughly 3.5 months now and was put on a project where I am to extract phrases and classification codes from PDF documents in different languages (there is more to it than that - I’m just keeping it brief without disclosing too much).

I had relatively finished most of the algorithm that is able to extract and compile these phrases/codes - however, the dataset that I am using has all been entered manually by multiple different people who work at the company (~100+ people). This requires a lot of data cleaning to process duplicate phrases that are mapped to different codes, categories of codes, etc. Additionally, it appears that many people have formatted their inputs drastically differently. I am currently only doing this for the English language and then will have to do it for French, Spanish, and German in the coming weeks. Each dataset is initially 250,000 records where I can automate roughly 90% of the cleaning - the rest are all either really obscure cases or the classification of the duplicate phrases are too close to call causing me to have to closely examine and google them online to determine which one shouldn’t be there. 

I know all of this is all super vague - I am trying my best to explain what I can share (some things I can’t)

Back to my question - I have weekly meetings with management where some of them seem surprised when I tell them that I am still working on data cleaning (been working on it for 2 weeks now and will likely need more time than this as I haven’t even finished the English dataset). I would estimate that up to this point 70%-75% of the code I’ve written is for the sole purpose of data cleaning, preprocessing, and determining what belongs where (using fuzzy logic and embeddings). My question is how do I explain to them that the data cleaning process is most of the work a data scientist needs to do? Am I looking into this too much? Had I been given a perfectly clean dataset, I would be able to complete this in no time. Also, this is my first job out of college (bachelors degree in Data Science) and I definitely acknowledge the skill gap between me and the other members on my team who are Sr. Data Scientists. They are much more efficient than I am when it comes to things such as Deep Learning, the cloud, etc. 

Any advice is greatly appreciated



TL;DR My first job out of college. Been working at the company for 3.5 months as a data scientist. Management seems to be surprised that data cleaning is taking me so long (2 weeks and counting) to complete which makes me feel like I am not working efficiently enough. Does management have it backwards where they think building the ML models is more intense than the Data Cleaning portion?


Edit: Thank you all for the input and advice! I have a meeting with management later this week and I will definitely be using the suggestions and advice provided here

Edit 2: Wow!! I really can thank everyone enough for all the advice and feedback I received. You all have gave me some great guidance as to how I can navigate this issue. Thank you!

Edit 3: Grammar + Formatting",t2_m016cr,False,,0,False,How to explain to Management that Data Cleaning is a really important part of my job,[],r/datascience,False,6,career,0,,,False,t3_ngls7t,False,dark,0.99,,public,350,1,{},,,False,[],,False,False,,{},Career,False,350,,False,False,self,1621501617.0,,[],{},,True,,1621499248.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I recently started my first job working as an entry level Data Scientist. I’ve been working at this company for roughly 3.5 months now and was put on a project where I am to extract phrases and classification codes from PDF documents in different languages (there is more to it than that - I’m just keeping it brief without disclosing too much).&lt;/p&gt;

&lt;p&gt;I had relatively finished most of the algorithm that is able to extract and compile these phrases/codes - however, the dataset that I am using has all been entered manually by multiple different people who work at the company (~100+ people). This requires a lot of data cleaning to process duplicate phrases that are mapped to different codes, categories of codes, etc. Additionally, it appears that many people have formatted their inputs drastically differently. I am currently only doing this for the English language and then will have to do it for French, Spanish, and German in the coming weeks. Each dataset is initially 250,000 records where I can automate roughly 90% of the cleaning - the rest are all either really obscure cases or the classification of the duplicate phrases are too close to call causing me to have to closely examine and google them online to determine which one shouldn’t be there. &lt;/p&gt;

&lt;p&gt;I know all of this is all super vague - I am trying my best to explain what I can share (some things I can’t)&lt;/p&gt;

&lt;p&gt;Back to my question - I have weekly meetings with management where some of them seem surprised when I tell them that I am still working on data cleaning (been working on it for 2 weeks now and will likely need more time than this as I haven’t even finished the English dataset). I would estimate that up to this point 70%-75% of the code I’ve written is for the sole purpose of data cleaning, preprocessing, and determining what belongs where (using fuzzy logic and embeddings). My question is how do I explain to them that the data cleaning process is most of the work a data scientist needs to do? Am I looking into this too much? Had I been given a perfectly clean dataset, I would be able to complete this in no time. Also, this is my first job out of college (bachelors degree in Data Science) and I definitely acknowledge the skill gap between me and the other members on my team who are Sr. Data Scientists. They are much more efficient than I am when it comes to things such as Deep Learning, the cloud, etc. &lt;/p&gt;

&lt;p&gt;Any advice is greatly appreciated&lt;/p&gt;

&lt;p&gt;TL;DR My first job out of college. Been working at the company for 3.5 months as a data scientist. Management seems to be surprised that data cleaning is taking me so long (2 weeks and counting) to complete which makes me feel like I am not working efficiently enough. Does management have it backwards where they think building the ML models is more intense than the Data Cleaning portion?&lt;/p&gt;

&lt;p&gt;Edit: Thank you all for the input and advice! I have a meeting with management later this week and I will definitely be using the suggestions and advice provided here&lt;/p&gt;

&lt;p&gt;Edit 2: Wow!! I really can thank everyone enough for all the advice and feedback I received. You all have gave me some great guidance as to how I can navigate this issue. Thank you!&lt;/p&gt;

&lt;p&gt;Edit 3: Grammar + Formatting&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngls7t,True,,MAXnRUSSEL,,73,True,all_ads,False,[],False,,/r/datascience/comments/ngls7t/how_to_explain_to_management_that_data_cleaning/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngls7t/how_to_explain_to_management_that_data_cleaning/,515407,1621470448.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"

[View Poll](https://www.reddit.com/poll/nhzi43)",t2_8azmn3,False,,0,False,What is ONE single essential tool/program/skill that a new person absolutely must master when transitioning into a data science/analyst role?,[],r/datascience,False,6,discussion,0,,,False,t3_nhzi43,False,dark,0.18,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621650817.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nhzi43""&gt;View Poll&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhzi43,True,,TimboCA,,14,True,all_ads,False,[],False,,/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/,515407,1621622017.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,"{'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1621881217866, 'options': [{'text': 'R (or Python or similar code)', 'vote_count': 134, 'id': '8088652'}, {'text': 'Power BI (or Tableau)', 'vote_count': 5, 'id': '8088653'}, {'text': 'Excel', 'vote_count': 13, 'id': '8088654'}, {'text': 'SQL', 'vote_count': 59, 'id': '8088655'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 211, 'total_stake_amount': None}"
,datascience,"This drives me nuts. 

Unless you are looking for a relationship between time and your variable, graphing in time is useless. 

I so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. 

This isn't limited to the fresh faced grad who's just discovered R. But experienced technical experts in a field, typically engineering data in the fields I work (data logging etc.)

If you want to visually assess correlation between variables, plot them X vs Y, and then lets have a look.

* Where time is not a critical factor.",t2_aegwp2g1,False,,0,False,I'm fed up of seeing continuous data from tests in the time domain. It offers very little value.,[],r/datascience,False,6,discussion,0,,,False,t3_nhstoa,False,dark,0.31,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1621609528.0,,[],{},,True,,1621633317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This drives me nuts. &lt;/p&gt;

&lt;p&gt;Unless you are looking for a relationship between time and your variable, graphing in time is useless. &lt;/p&gt;

&lt;p&gt;I so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. &lt;/p&gt;

&lt;p&gt;This isn&amp;#39;t limited to the fresh faced grad who&amp;#39;s just discovered R. But experienced technical experts in a field, typically engineering data in the fields I work (data logging etc.)&lt;/p&gt;

&lt;p&gt;If you want to visually assess correlation between variables, plot them X vs Y, and then lets have a look.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Where time is not a critical factor.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhstoa,True,,fortuitous_monkey,,51,True,all_ads,False,[],False,,/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/,515407,1621604517.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," I am weighing the pros and cons of taking a contract position, which I haven’t done before. What are things I should consider?

All I know about contract work is: -they don’t take any taxes out -no benefits, no health insurance -less networking -not a part of team/culture -may or may not turn into a job -less stable

This is for a data analyst role. They are offering 40/hr. But my current annual salary is 75k with great health coverage.

What factors should I consider? What do you wish you knew before taking your first contract role? Why do people generally look down on contract offers, I don’t get it...?

Any help would be greatly appreciated",t2_2avj5jvp,False,,0,False,Pro/Cons of Contract vs FTE?,[],r/datascience,False,6,,0,,,False,t3_nh9tku,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1621569309.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am weighing the pros and cons of taking a contract position, which I haven’t done before. What are things I should consider?&lt;/p&gt;

&lt;p&gt;All I know about contract work is: -they don’t take any taxes out -no benefits, no health insurance -less networking -not a part of team/culture -may or may not turn into a job -less stable&lt;/p&gt;

&lt;p&gt;This is for a data analyst role. They are offering 40/hr. But my current annual salary is 75k with great health coverage.&lt;/p&gt;

&lt;p&gt;What factors should I consider? What do you wish you knew before taking your first contract role? Why do people generally look down on contract offers, I don’t get it...?&lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nh9tku,True,,jerodme,,10,True,all_ads,False,[],False,,/r/datascience/comments/nh9tku/procons_of_contract_vs_fte/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh9tku/procons_of_contract_vs_fte/,515407,1621540509.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I recently came across these topics - they look very interesting, but also very complicated. Has anyone ever dealt with them before? What kind of projects did you use them for?",t2_3tosvccj,False,,0,False,"Has anyone worked with ""structural equation modeling"" or ""statistical process control""?",[],r/datascience,False,6,discussion,0,,,False,t3_nh9g9y,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621568386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently came across these topics - they look very interesting, but also very complicated. Has anyone ever dealt with them before? What kind of projects did you use them for?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh9g9y,True,,jj4646,,8,True,all_ads,False,[],False,,/r/datascience/comments/nh9g9y/has_anyone_worked_with_structural_equation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh9g9y/has_anyone_worked_with_structural_equation/,515407,1621539586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Background info: I was a stats major in undergrad and now I'm getting my masters in cs from OMSCS (Georgia tech). I've been working at a large fortune 500 company for a year now as a data analyst/data scientist. 

My job consists mostly of building tables and metrics in SQL, and I recently started doing some python work involving solving the vehicle routing problem, so I've been introduced to a lot of machine learning models and libraries. I'm also familiar with cloud computing and use cloud technology in my day to day. 

The problem is that this team has absolutely no guidance, no one checks on my work and no one cares about my work, it's really assigned to me as a learning opportunity. The team doesn't really need a data scientist at all. Also, I'm the only data scientist on the team, everyone else is a software developer. I've learned a ton but the lack of guidance really really frustrates me and I think it puts me at a big disadvantage when it comes to learning. 

My question is: given my experience will I even be able to find another data science job or should I just stay at my current job?",t2_uf9472q,False,,0,False,Should I get a new job?,[],r/datascience,False,6,career,0,,,False,t3_ngzhno,False,dark,0.57,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1621544211.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Background info: I was a stats major in undergrad and now I&amp;#39;m getting my masters in cs from OMSCS (Georgia tech). I&amp;#39;ve been working at a large fortune 500 company for a year now as a data analyst/data scientist. &lt;/p&gt;

&lt;p&gt;My job consists mostly of building tables and metrics in SQL, and I recently started doing some python work involving solving the vehicle routing problem, so I&amp;#39;ve been introduced to a lot of machine learning models and libraries. I&amp;#39;m also familiar with cloud computing and use cloud technology in my day to day. &lt;/p&gt;

&lt;p&gt;The problem is that this team has absolutely no guidance, no one checks on my work and no one cares about my work, it&amp;#39;s really assigned to me as a learning opportunity. The team doesn&amp;#39;t really need a data scientist at all. Also, I&amp;#39;m the only data scientist on the team, everyone else is a software developer. I&amp;#39;ve learned a ton but the lack of guidance really really frustrates me and I think it puts me at a big disadvantage when it comes to learning. &lt;/p&gt;

&lt;p&gt;My question is: given my experience will I even be able to find another data science job or should I just stay at my current job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngzhno,True,,ithsefinque,,12,True,all_ads,False,[],False,,/r/datascience/comments/ngzhno/should_i_get_a_new_job/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngzhno/should_i_get_a_new_job/,515407,1621515411.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm beginning to learn data science and in most courses and programs I'm seeing so far, students are often asked to start with learning environments like Jupyter notebooks, Spyder or some sort of text editor like G Edit or something.   


I'm just curious as to how things happen in the real world. Do people still work in these environments? I use Jupyter notebooks and sometimes Spyder, but I understand that one can work in Terminal/Command Prompt as well. Although, I'm not sure I understand why - because editing code and stuff seems a lot easier otherwise. Just curious.",t2_9487g0zm,False,,0,False,Working environments in the real world.,[],r/datascience,False,6,discussion,0,,,False,t3_ng6gej,False,dark,0.98,,public,113,0,{},,,False,[],,False,False,,{},Discussion,False,113,,False,False,self,False,,[],{},,True,,1621461183.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m beginning to learn data science and in most courses and programs I&amp;#39;m seeing so far, students are often asked to start with learning environments like Jupyter notebooks, Spyder or some sort of text editor like G Edit or something.   &lt;/p&gt;

&lt;p&gt;I&amp;#39;m just curious as to how things happen in the real world. Do people still work in these environments? I use Jupyter notebooks and sometimes Spyder, but I understand that one can work in Terminal/Command Prompt as well. Although, I&amp;#39;m not sure I understand why - because editing code and stuff seems a lot easier otherwise. Just curious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng6gej,True,,Quaternion253,,58,True,all_ads,False,[],False,,/r/datascience/comments/ng6gej/working_environments_in_the_real_world/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng6gej/working_environments_in_the_real_world/,515407,1621432383.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I recently came across a newer technique called ""accumulated local effects"", that attempts to explain the effect of predictor variables on the response variable :  

https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html

Has anyone tried using this method on real data? Did you find it useful? Any stories/anecdotes/experiences/comments/reviews you would be willing to share regaeding this method?",t2_3f0i9m72,False,,0,False,Accumulated Local Effects,[],r/datascience,False,6,discussion,0,,,False,t3_nh19zq,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621548711.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently came across a newer technique called &amp;quot;accumulated local effects&amp;quot;, that attempts to explain the effect of predictor variables on the response variable :  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html""&gt;https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Has anyone tried using this method on real data? Did you find it useful? Any stories/anecdotes/experiences/comments/reviews you would be willing to share regaeding this method?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh19zq,True,,SQL_beginner,,1,True,all_ads,False,[],False,,/r/datascience/comments/nh19zq/accumulated_local_effects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh19zq/accumulated_local_effects/,515407,1621519911.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?auto=webp&amp;s=d357a478ca9eb7a2f140df123f7e0f456b4e6d28', 'width': 568, 'height': 352}, 'resolutions': [{'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0fab85cd22f4dba076917acb91b18e80547951d', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a320afc3c4baa211fc0a34bbc6a0cbbf746a317', 'width': 216, 'height': 133}, {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f8c471500038e92f0443135b411c99fd52cc70a', 'width': 320, 'height': 198}], 'variants': {}, 'id': 'CYFWfKV4HsIBu2g4-p4-alP-cSR5KE_NS9BlqhMKU5U'}], 'enabled': False}",,,,,
,datascience,"I'm in a situation where I need to ""productionise"" a large number of models written in various languages. We have a system set up for deploying Python models in Docker containers, accessible via API. 

Currently our approach is to attempt to wrap any non-Python models in Python code and deploy using our existing framework. For example, we can wrap an R model to resemble a Python one via a library such as RPy2. However, this isn't particularly elegant, and new wrappers have to be written for each new language (or even for particularly different models in the same language).

Another option I'd considered was having the models kept in their own language-specific scripts, which can be executed from inside Python app via calls to the command line. This seems a better approach since we're no longer dealing with wrappers, though I'm concerned that having to run import statements/load the model for each inference could cause high latency.

A third option I've been toying with (though haven't managed to figure out the details for) is to have the web app run in one container, and the model in a second, with some very minimal model serving code. I think this might allow us to sidestep the issue of loading dependencies, though it seems we'd have to write the model serving code in the model's language (the original problem).

I'm interested to hear your thoughts on this - has anyone else found an elegant solution to this?",t2_eetn0,False,,0,False,Language-agnostic deployment setup?,[],r/datascience,False,6,projects,0,,,False,t3_ngv3ts,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1621529877.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in a situation where I need to &amp;quot;productionise&amp;quot; a large number of models written in various languages. We have a system set up for deploying Python models in Docker containers, accessible via API. &lt;/p&gt;

&lt;p&gt;Currently our approach is to attempt to wrap any non-Python models in Python code and deploy using our existing framework. For example, we can wrap an R model to resemble a Python one via a library such as RPy2. However, this isn&amp;#39;t particularly elegant, and new wrappers have to be written for each new language (or even for particularly different models in the same language).&lt;/p&gt;

&lt;p&gt;Another option I&amp;#39;d considered was having the models kept in their own language-specific scripts, which can be executed from inside Python app via calls to the command line. This seems a better approach since we&amp;#39;re no longer dealing with wrappers, though I&amp;#39;m concerned that having to run import statements/load the model for each inference could cause high latency.&lt;/p&gt;

&lt;p&gt;A third option I&amp;#39;ve been toying with (though haven&amp;#39;t managed to figure out the details for) is to have the web app run in one container, and the model in a second, with some very minimal model serving code. I think this might allow us to sidestep the issue of loading dependencies, though it seems we&amp;#39;d have to write the model serving code in the model&amp;#39;s language (the original problem).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested to hear your thoughts on this - has anyone else found an elegant solution to this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngv3ts,True,,Coprosmo,,3,True,all_ads,False,[],False,,/r/datascience/comments/ngv3ts/languageagnostic_deployment_setup/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngv3ts/languageagnostic_deployment_setup/,515407,1621501077.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"When I studied DS for mastery, I noticed that people came from different backgrounds. Consequently, people were struggling in different classes according to what they studied before. For example, people that did statistics as bachelor mentioned that they had a lot of problems with the programming aspect of DS, but were much more comfortable with modeling. I had a lot of trouble with Bayesian statistics but was much more comfortable with programming (I was a Software engineer before).   


So out of curiosity, what was your experience like, what topic did you find difficult, and which ones did you find easy?",t2_j1u3fe,False,,0,False,What is the hardest topic that you encountered in Data Science when you studied it?,[],r/datascience,False,6,discussion,0,,,False,t3_ng951l,False,dark,0.8,,public,17,0,{},,,False,[],,False,False,,{},Discussion,False,17,,False,False,self,False,,[],{},,True,,1621467587.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I studied DS for mastery, I noticed that people came from different backgrounds. Consequently, people were struggling in different classes according to what they studied before. For example, people that did statistics as bachelor mentioned that they had a lot of problems with the programming aspect of DS, but were much more comfortable with modeling. I had a lot of trouble with Bayesian statistics but was much more comfortable with programming (I was a Software engineer before).   &lt;/p&gt;

&lt;p&gt;So out of curiosity, what was your experience like, what topic did you find difficult, and which ones did you find easy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng951l,True,,ngorovitch,,32,True,all_ads,False,[],False,,/r/datascience/comments/ng951l/what_is_the_hardest_topic_that_you_encountered_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng951l/what_is_the_hardest_topic_that_you_encountered_in/,515407,1621438787.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been contacted by an acquaintance to do some freelance work, and I'm unsure how to bill it: should I just set an hourly rate and charge according to time invested until completion? They are interested in the final product (not on the code), and since I would be using some personal libraries of mine, on the one hand I feel that being too fast is counterproductive with an hourly rate (= little money). However if I just set a flat price and there's unforeseen issues, I might have to work more, ending up with an effective hourly rate that is too low.  


How do you deal with this and what do you recommend?",t2_nqspn,False,,0,False,Question regarding freelancing,[],r/datascience,False,6,discussion,0,,,False,t3_ngg2su,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621484688.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been contacted by an acquaintance to do some freelance work, and I&amp;#39;m unsure how to bill it: should I just set an hourly rate and charge according to time invested until completion? They are interested in the final product (not on the code), and since I would be using some personal libraries of mine, on the one hand I feel that being too fast is counterproductive with an hourly rate (= little money). However if I just set a flat price and there&amp;#39;s unforeseen issues, I might have to work more, ending up with an effective hourly rate that is too low.  &lt;/p&gt;

&lt;p&gt;How do you deal with this and what do you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngg2su,True,,polidrupa,,2,True,all_ads,False,[],False,,/r/datascience/comments/ngg2su/question_regarding_freelancing/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngg2su/question_regarding_freelancing/,515407,1621455888.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have started to work for a small start-up company. We are only 2 data scientists. 

I am trying to understand consumer satisfaction by analyzing reviews. Sentiment Analysis and NER will be methods I will go for as initial step. 

My company doesn't have an NLP pipeline yet. 

I wonder which one is better: using Paid NLP Tools like Google Could NLP, IBM's Watson NLU or a self build NLP model?

I would be happy to hear what you think. Pros and cons.

Google's service seems a bit expensive. But, pricing is still confusing. For example, if I have 5 million reviews, how much am I expected to pay for Sentiment Analysis and NER services? (an estimate) 

Does Google charge me again whenever I run sentiment analysis? 

Is there any cheaper but still effective cloud computing NLP tool that you can suggest?

I would be happy to hear your insights!",t2_yio7w,False,,0,False,Paid NLP Tools vs Building Own Model,[],r/datascience,False,6,tooling,0,,,False,t3_ngcufb,False,dark,0.6,,public,1,1,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,1621456064.0,,[],{},,True,,1621476691.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have started to work for a small start-up company. We are only 2 data scientists. &lt;/p&gt;

&lt;p&gt;I am trying to understand consumer satisfaction by analyzing reviews. Sentiment Analysis and NER will be methods I will go for as initial step. &lt;/p&gt;

&lt;p&gt;My company doesn&amp;#39;t have an NLP pipeline yet. &lt;/p&gt;

&lt;p&gt;I wonder which one is better: using Paid NLP Tools like Google Could NLP, IBM&amp;#39;s Watson NLU or a self build NLP model?&lt;/p&gt;

&lt;p&gt;I would be happy to hear what you think. Pros and cons.&lt;/p&gt;

&lt;p&gt;Google&amp;#39;s service seems a bit expensive. But, pricing is still confusing. For example, if I have 5 million reviews, how much am I expected to pay for Sentiment Analysis and NER services? (an estimate) &lt;/p&gt;

&lt;p&gt;Does Google charge me again whenever I run sentiment analysis? &lt;/p&gt;

&lt;p&gt;Is there any cheaper but still effective cloud computing NLP tool that you can suggest?&lt;/p&gt;

&lt;p&gt;I would be happy to hear your insights!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngcufb,True,,geldersekifuzuli,,7,True,all_ads,False,[],False,,/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/,515407,1621447891.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"See title.

I'm interested in working with a large dataset of almost 100,000 faces for a research project, but I wish to remain within UK, EU and US  regulations for doing so.

What are, or where can I find, the regulations for such work?",t2_4mk1e,False,,0,False,Laws and regulations for working with large anonymised datasets of faces?,[],r/datascience,False,6,discussion,0,,,False,t3_ng287u,False,dark,0.6,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621449225.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;See title.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in working with a large dataset of almost 100,000 faces for a research project, but I wish to remain within UK, EU and US  regulations for doing so.&lt;/p&gt;

&lt;p&gt;What are, or where can I find, the regulations for such work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng287u,True,,Jimbobmij,,6,True,all_ads,False,[],False,,/r/datascience/comments/ng287u/laws_and_regulations_for_working_with_large/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng287u/laws_and_regulations_for_working_with_large/,515407,1621420425.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Due to restrictions from the software development team, I need to restrict the number of features (variables) for my model to within a maximum number, say 10 or less. I am wondering if anyone here has run into such a constraint and how you went about handling it (assuming the restriction above is non-negotiable; and trust me, we have tried negotiating with the software folks :)). I have tried a number of selection methods to go about doing this and selected the ""top"" 10 features, for instance, as sorted by the following:

1. Feature importance (Xgboost)
2. Boruta
3. SHAP

Some drop in performance was obviously expected relative to model with all features available but the drop in performance has been been much sharper than I am comfortable with (using either of the three methods listed above). Therefore, I am hoping folks here have experience with better ways to go about doing this. 

I would very much appreciate any feedback.",t2_fw5vcdv,False,,0,False,Selecting a limited number of features,[],r/datascience,False,6,discussion,0,,,False,t3_ng6lns,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621461537.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Due to restrictions from the software development team, I need to restrict the number of features (variables) for my model to within a maximum number, say 10 or less. I am wondering if anyone here has run into such a constraint and how you went about handling it (assuming the restriction above is non-negotiable; and trust me, we have tried negotiating with the software folks :)). I have tried a number of selection methods to go about doing this and selected the &amp;quot;top&amp;quot; 10 features, for instance, as sorted by the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feature importance (Xgboost)&lt;/li&gt;
&lt;li&gt;Boruta&lt;/li&gt;
&lt;li&gt;SHAP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some drop in performance was obviously expected relative to model with all features available but the drop in performance has been been much sharper than I am comfortable with (using either of the three methods listed above). Therefore, I am hoping folks here have experience with better ways to go about doing this. &lt;/p&gt;

&lt;p&gt;I would very much appreciate any feedback.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng6lns,True,,unnamedn00b,,27,True,all_ads,False,[],False,,/r/datascience/comments/ng6lns/selecting_a_limited_number_of_features/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng6lns/selecting_a_limited_number_of_features/,515407,1621432737.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This is a unique situation...

Let me start out by saying I am a “IT Support analyst intern” at my job, part time. What I do however is not all that complex, I use pivot tables and excel as forms to show company spending at several locations(I don’t recommend anything I simply show the bills in the best way I can, currently it’s a pivot table from the previous employee) 

My career goal is Data Science and starting out as a Data Analyst to get there. Perhaps getting a masters while being a Data Analyst. Currently, my higher ups told me if I can learn Python and how to somehow implement it in my job I can use it for resume building purposes, so I’m reading “Automate the Boring Stuff” since it has parts about Python with excel and PDFs.

Allow me to also note I am a CS major specializing in Data Science. This does have a class for Python with data science but I’d rather learn it sooner for experience purposes. This has nice a machine learning class too I won’t be able to take for another year. Of course SQL is in the database class next semester . 

My question is, what else should I be doing now to help get an actual data science internship sooner? Or data analyst if not, since that’s not my current job title. Would using Python with excel to show bill amounts count as a “Data analytic” experience? I would think not because it really doesn’t cover the broad strokes of the full job position “Data Scientist/Analyst” unless there’s a way I can visualize excel data I’m missing, apart from python. Is there any key skills I have to learn ASAP, even with a class coming up? Like SQL? And during this, what actual Data Science skills should I be looking at right now to aid in actually getting a possible data science internship? 

Is there any key skills I’m missing? Are there any good resources to learn these skills like Python(if not my current book), SQL, Spark, etc?",t2_55fytx,False,,0,False,Starting out as a Data Analyst to move into Data Science?,[],r/datascience,False,6,career,0,,,False,t3_nf8hxs,False,dark,0.94,,public,175,0,{},,,False,[],,False,False,,{},Career,False,175,,False,False,self,False,,[],{},,True,,1621366913.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a unique situation...&lt;/p&gt;

&lt;p&gt;Let me start out by saying I am a “IT Support analyst intern” at my job, part time. What I do however is not all that complex, I use pivot tables and excel as forms to show company spending at several locations(I don’t recommend anything I simply show the bills in the best way I can, currently it’s a pivot table from the previous employee) &lt;/p&gt;

&lt;p&gt;My career goal is Data Science and starting out as a Data Analyst to get there. Perhaps getting a masters while being a Data Analyst. Currently, my higher ups told me if I can learn Python and how to somehow implement it in my job I can use it for resume building purposes, so I’m reading “Automate the Boring Stuff” since it has parts about Python with excel and PDFs.&lt;/p&gt;

&lt;p&gt;Allow me to also note I am a CS major specializing in Data Science. This does have a class for Python with data science but I’d rather learn it sooner for experience purposes. This has nice a machine learning class too I won’t be able to take for another year. Of course SQL is in the database class next semester . &lt;/p&gt;

&lt;p&gt;My question is, what else should I be doing now to help get an actual data science internship sooner? Or data analyst if not, since that’s not my current job title. Would using Python with excel to show bill amounts count as a “Data analytic” experience? I would think not because it really doesn’t cover the broad strokes of the full job position “Data Scientist/Analyst” unless there’s a way I can visualize excel data I’m missing, apart from python. Is there any key skills I have to learn ASAP, even with a class coming up? Like SQL? And during this, what actual Data Science skills should I be looking at right now to aid in actually getting a possible data science internship? &lt;/p&gt;

&lt;p&gt;Is there any key skills I’m missing? Are there any good resources to learn these skills like Python(if not my current book), SQL, Spark, etc?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf8hxs,True,,ToothPickLegs,,106,True,all_ads,False,[],False,,/r/datascience/comments/nf8hxs/starting_out_as_a_data_analyst_to_move_into_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf8hxs/starting_out_as_a_data_analyst_to_move_into_data/,515407,1621338113.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"&gt;TL;DR: Neural Search is a new approach to retrieving information using neural networks. Traditional techniques to search typically meant writing rules to “understand” the data being searched and return the best results. But with neural search, developers don’t need to wrack their brains for these rules; The system learns the rules by itself and gets better as it goes along. Even developers who don’t know machine learning can quickly build a search engine using open-source frameworks such as [Jina](https://github.com/jina-ai/jina).

Table of contents

* What is Neural Search
* Evolution of search methods
* Rule-based Search vs Neural Search
* Applications of Neural Search
* Get started with Neural Search

## What is Neural Search?

There is a massive amount of data on the web; how can we effectively search through it for relevant information? And it’s not just the web where we need it: Our computers store terabytes of company and personal data that we need to work with; we need effective search to get our day-to-day job done. And what do I mean by effective search

* Can we go beyond just matching keywords?
* Can we search using natural language, just like we would write or speak?
* Can we make the search smart enough to forgive our minor mistakes?
* Can we search for things that aren’t an exact match but are “close enough”?

We can answer all those questions with one word: Yes. To understand how, we need to enter the world of Natural Language Processing. NLP is a field of computer science that deals with analyzing natural language data, like the conversations people have every day. NLP is the foundation of intelligent search, and we have seen three different approaches in this field as follows.

## Evolution of search methods

1. **Rules (1950–1990s)**Complex handwritten rules that emulate Natural Language Understanding.**Drawbacks:** Handwritten rules can only be made more accurate by increasing their complexity, which is a much more difficult task that becomes unmanageable over time.
2. **Statistics (1990s — 2010s)**Probabilistic decisions based on weights, machine learning and feature engineering.Creating and managing rules was solved with machine learning, where the system automatically learns rules by analysing large real-world texts.**Drawbacks:** These statistical methods require elaborate feature engineering.
3. **Neural Networks (Present)**Advanced machine learning methods such as deep neural networks and representation learning.Since 2015, statistical methods have been largely abandoned, and there has been a shift to [neural networks](https://en.wikipedia.org/wiki/Neural_network) in machine learning. Popular techniques using this method make it a more accurate and a scalable alternative. It involves

* Use of \`word embeddings\` to capture semantic properties of words
* Focus on end-to-end learning of higher-level tasks (e.g., question answering)

&amp;#x200B;

&gt;When you use Neural Networks to make your search smarter, we call this a **Neural Search System**. And as you will see, it addresses some of the critical shortcomings of other methods.

Note that the applications of Neural Search are not just limited to text. It goes well beyond what NLP covers. With neural search, we get additional capabilities to search images, audio, video, etc. Let’s look at a comparison of the extreme ends of search methods — “Rules” vs “Neural Networks”:

## Rules (Symbolic Search) vs Neural Networks (Neural Search)

While the Neural Search method has become more widespread since 2015, and should be the primary focus area of any new search system. However, we shouldn’t completely rule out Symbolic (rule-based) Search methods. In fact, using a combination of Neural Search and Symbolic Search may result in optimized results. Let’s look at some of the powerful applications of Neural Search

## Applications Of Neural Search

**Semantic search**

🔍 addidsa trosers (misspelled brand and category, still returns relevant results similar to query “adidas trousers”)

**Search between data types**

With Neural Search, you can use one kind of data to search another kind of data, for example using text to search for images, or audio to search for video.

**Search with multiple data types**

With Neural Search, you can build queries with multiple query data types e.g. search images with text+image

## Get started with Neural Search

For rule-based searches, **Apache Solr, Elasticsearch, and Lucene** are the de-facto solutions. On the other hand, Neural Search is relatively new domain, there aren’t so many off-the-shelf packages. Also training the neural network for such a system requires a lot of data. These challenges can be solved using [Jina](http://github.com/jina-ai/jina/), an open-source neural search framework. To get started with building your own Neural Search system using [Jina](http://github.com/jina-ai/jina/). 

&amp;#x200B;

**References/Notes:** 

Neural Search term is less academic form of the term **Neural Information Retrieval** which first appeared during a [research workshop in 2016](https://www.microsoft.com/en-us/research/event/neuir2016/). I also found it useful to learn about [how google search  works](https://www.youtube.com/watch?v=0eKVizvYSUQ).",t2_auwgbh53,False,,0,False,Neural Search - I did research on the topic and this is what I learned,[],r/datascience,False,6,education,0,,,False,t3_nfulh9,False,dark,0.81,,public,6,1,{},,,False,[],,False,False,,{},Education,False,6,,False,True,self,False,,[],{},,True,,1621423258.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;blockquote&gt;
&lt;p&gt;TL;DR: Neural Search is a new approach to retrieving information using neural networks. Traditional techniques to search typically meant writing rules to “understand” the data being searched and return the best results. But with neural search, developers don’t need to wrack their brains for these rules; The system learns the rules by itself and gets better as it goes along. Even developers who don’t know machine learning can quickly build a search engine using open-source frameworks such as &lt;a href=""https://github.com/jina-ai/jina""&gt;Jina&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Table of contents&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is Neural Search&lt;/li&gt;
&lt;li&gt;Evolution of search methods&lt;/li&gt;
&lt;li&gt;Rule-based Search vs Neural Search&lt;/li&gt;
&lt;li&gt;Applications of Neural Search&lt;/li&gt;
&lt;li&gt;Get started with Neural Search&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;What is Neural Search?&lt;/h2&gt;

&lt;p&gt;There is a massive amount of data on the web; how can we effectively search through it for relevant information? And it’s not just the web where we need it: Our computers store terabytes of company and personal data that we need to work with; we need effective search to get our day-to-day job done. And what do I mean by effective search&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can we go beyond just matching keywords?&lt;/li&gt;
&lt;li&gt;Can we search using natural language, just like we would write or speak?&lt;/li&gt;
&lt;li&gt;Can we make the search smart enough to forgive our minor mistakes?&lt;/li&gt;
&lt;li&gt;Can we search for things that aren’t an exact match but are “close enough”?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can answer all those questions with one word: Yes. To understand how, we need to enter the world of Natural Language Processing. NLP is a field of computer science that deals with analyzing natural language data, like the conversations people have every day. NLP is the foundation of intelligent search, and we have seen three different approaches in this field as follows.&lt;/p&gt;

&lt;h2&gt;Evolution of search methods&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Rules (1950–1990s)&lt;/strong&gt;Complex handwritten rules that emulate Natural Language Understanding.&lt;strong&gt;Drawbacks:&lt;/strong&gt; Handwritten rules can only be made more accurate by increasing their complexity, which is a much more difficult task that becomes unmanageable over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics (1990s — 2010s)&lt;/strong&gt;Probabilistic decisions based on weights, machine learning and feature engineering.Creating and managing rules was solved with machine learning, where the system automatically learns rules by analysing large real-world texts.&lt;strong&gt;Drawbacks:&lt;/strong&gt; These statistical methods require elaborate feature engineering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural Networks (Present)&lt;/strong&gt;Advanced machine learning methods such as deep neural networks and representation learning.Since 2015, statistical methods have been largely abandoned, and there has been a shift to &lt;a href=""https://en.wikipedia.org/wiki/Neural_network""&gt;neural networks&lt;/a&gt; in machine learning. Popular techniques using this method make it a more accurate and a scalable alternative. It involves&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Use of `word embeddings` to capture semantic properties of words&lt;/li&gt;
&lt;li&gt;Focus on end-to-end learning of higher-level tasks (e.g., question answering)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When you use Neural Networks to make your search smarter, we call this a &lt;strong&gt;Neural Search System&lt;/strong&gt;. And as you will see, it addresses some of the critical shortcomings of other methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that the applications of Neural Search are not just limited to text. It goes well beyond what NLP covers. With neural search, we get additional capabilities to search images, audio, video, etc. Let’s look at a comparison of the extreme ends of search methods — “Rules” vs “Neural Networks”:&lt;/p&gt;

&lt;h2&gt;Rules (Symbolic Search) vs Neural Networks (Neural Search)&lt;/h2&gt;

&lt;p&gt;While the Neural Search method has become more widespread since 2015, and should be the primary focus area of any new search system. However, we shouldn’t completely rule out Symbolic (rule-based) Search methods. In fact, using a combination of Neural Search and Symbolic Search may result in optimized results. Let’s look at some of the powerful applications of Neural Search&lt;/p&gt;

&lt;h2&gt;Applications Of Neural Search&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Semantic search&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;🔍 addidsa trosers (misspelled brand and category, still returns relevant results similar to query “adidas trousers”)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search between data types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Neural Search, you can use one kind of data to search another kind of data, for example using text to search for images, or audio to search for video.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search with multiple data types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Neural Search, you can build queries with multiple query data types e.g. search images with text+image&lt;/p&gt;

&lt;h2&gt;Get started with Neural Search&lt;/h2&gt;

&lt;p&gt;For rule-based searches, &lt;strong&gt;Apache Solr, Elasticsearch, and Lucene&lt;/strong&gt; are the de-facto solutions. On the other hand, Neural Search is relatively new domain, there aren’t so many off-the-shelf packages. Also training the neural network for such a system requires a lot of data. These challenges can be solved using &lt;a href=""http://github.com/jina-ai/jina/""&gt;Jina&lt;/a&gt;, an open-source neural search framework. To get started with building your own Neural Search system using &lt;a href=""http://github.com/jina-ai/jina/""&gt;Jina&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References/Notes:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Neural Search term is less academic form of the term &lt;strong&gt;Neural Information Retrieval&lt;/strong&gt; which first appeared during a &lt;a href=""https://www.microsoft.com/en-us/research/event/neuir2016/""&gt;research workshop in 2016&lt;/a&gt;. I also found it useful to learn about &lt;a href=""https://www.youtube.com/watch?v=0eKVizvYSUQ""&gt;how google search  works&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 250, 'id': 'award_a67d649d-5aa5-407e-a98b-32fd9e3a9696', 'penny_donate': None, 'award_sub_type': 'APPRECIATION', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Today I Learned', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfulh9,True,,opensourcecolumbus,,0,True,all_ads,False,[],False,,/r/datascience/comments/nfulh9/neural_search_i_did_research_on_the_topic_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfulh9/neural_search_i_did_research_on_the_topic_and/,515407,1621394458.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?auto=webp&amp;s=cd2e63dadee37edb122e0516c362b8ce99643a25', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82d184a49ef058c205b9d7612d08f4cf48443e57', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c6ef80d771bc40508e93c6f9bbe077ba29b6e74', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=570759531bc30f67662966eecdb8d9cd1d532eca', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85757a8a8a035a522f11ae1d746981f402a7b9db', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5404653f12829d0dda0c3c38825e6d54a9e56623', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7cf7fc15587f5a240b1ee84d9da7c702f9ad57df', 'width': 1080, 'height': 540}], 'variants': {}, 'id': '3qR6MNvriXkC-_X_6so4ExfyWsU16ABi90cpsuNLfUs'}], 'enabled': False}",,,,,
,datascience,"I love Jupyter Notebooks but never thought of them as a tool to put code into production.

So I was very surprised by this article [Beyond Interactive: Notebook Innovation at Netflix](https://netflixtechblog.com/notebook-innovation-591ee3221233) (found thanks to [u/yoursdata](https://www.reddit.com/user/yoursdata/)'s [recent post](https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/) introducing what it seems a very interesting [newsletter](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)).

This is a 2018 article, anyone can confirm whether this philosophy continues at Netflix? Any other companies out there doing this?",t2_c4rvshhm,False,,0,False,Does Netflix use Jupyter Notebooks in production?,[],r/datascience,False,6,tooling,0,,,False,t3_nf47se,False,dark,0.98,,public,139,0,{},,,False,[],,False,False,,{},Tooling,False,139,,False,False,self,1621323018.0,,[],{},,True,,1621351586.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I love Jupyter Notebooks but never thought of them as a tool to put code into production.&lt;/p&gt;

&lt;p&gt;So I was very surprised by this article &lt;a href=""https://netflixtechblog.com/notebook-innovation-591ee3221233""&gt;Beyond Interactive: Notebook Innovation at Netflix&lt;/a&gt; (found thanks to &lt;a href=""https://www.reddit.com/user/yoursdata/""&gt;u/yoursdata&lt;/a&gt;&amp;#39;s &lt;a href=""https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/""&gt;recent post&lt;/a&gt; introducing what it seems a very interesting &lt;a href=""https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1""&gt;newsletter&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This is a 2018 article, anyone can confirm whether this philosophy continues at Netflix? Any other companies out there doing this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf47se,True,,JB__Quix,,51,True,all_ads,False,[],False,,/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/,515407,1621322786.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?auto=webp&amp;s=241d8db42bc0ed254033e9a6c1b8f1c054344a01', 'width': 1200, 'height': 631}, 'resolutions': [{'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c73ed082ec17ae91d15d8fa4cc5f8319277e90bb', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de7bc04ff15c421e368256ee3b5b5d9f2ee82759', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=728c4f95a84ed33bfd5169f92a1e941b06155860', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2b163de365bda7d77a7b2e6d1eae01539a9f61b', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd7c6a2a309194329f4d43aa4ef409024ce85117', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=996a7abdd41b8852c170229bf02b67349bd1669e', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'U9tbXWRkvDO4t9f4Xeaz26on67WaEhmbVQuU5JRY3To'}], 'enabled': False}",,,,,
,datascience,"I am a self-taught data scientist who is working for a mining company. One thing I have always struggled with is to upskill in this field. If you are like me - who is not a beginner but have some years of experience, I am sure even you must have struggled with this.

Most of the youtube videos and blogs are focused on beginners and toy projects, which is not really helpful. I started reading companies engineering blogs and think this is the way to upskill after a certain level. I have also started curating these articles in a newsletter and will be publishing three links each week.

Links for this weeks are:-

1. [**A Five-Step Guide for Conducting Exploratory Data Analysis**](https://shopify.engineering/conducting-exploratory-data-analysis)
2. [**Beyond Interactive: Notebook Innovation at Netflix**](https://netflixtechblog.com/notebook-innovation-591ee3221233)
3. [**How machine learning powers Facebook’s News Feed ranking algorithm**](https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/)

If you are preparing for any system design interview, the third link can be helpful.

Link for my newsletter - [https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)

Will love to discuss it and any suggestion is welcome.

P.S:- If it breaks any community guidelines, let me know and I will delete this post.",t2_qv8q4b1,False,,0,False,Data Science in Practice,[],r/datascience,False,6,education,0,,,False,t3_neylas,False,dark,0.98,,public,348,3,{},,,False,[],,False,False,,{},Education,False,348,,False,False,self,False,,[],{},,True,,1621333010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a self-taught data scientist who is working for a mining company. One thing I have always struggled with is to upskill in this field. If you are like me - who is not a beginner but have some years of experience, I am sure even you must have struggled with this.&lt;/p&gt;

&lt;p&gt;Most of the youtube videos and blogs are focused on beginners and toy projects, which is not really helpful. I started reading companies engineering blogs and think this is the way to upskill after a certain level. I have also started curating these articles in a newsletter and will be publishing three links each week.&lt;/p&gt;

&lt;p&gt;Links for this weeks are:-&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=""https://shopify.engineering/conducting-exploratory-data-analysis""&gt;&lt;strong&gt;A Five-Step Guide for Conducting Exploratory Data Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://netflixtechblog.com/notebook-innovation-591ee3221233""&gt;&lt;strong&gt;Beyond Interactive: Notebook Innovation at Netflix&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/""&gt;&lt;strong&gt;How machine learning powers Facebook’s News Feed ranking algorithm&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are preparing for any system design interview, the third link can be helpful.&lt;/p&gt;

&lt;p&gt;Link for my newsletter - &lt;a href=""https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1""&gt;https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Will love to discuss it and any suggestion is welcome.&lt;/p&gt;

&lt;p&gt;P.S:- If it breaks any community guidelines, let me know and I will delete this post.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 400, 'id': 'award_84276b1e-cc8f-484f-a19c-be6c09adc1a5', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'An amazing showing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Bravo!', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=16&amp;height=16&amp;auto=webp&amp;s=647cccf78702582f30d23908180da092b135cffe', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=32&amp;height=32&amp;auto=webp&amp;s=4644ac0618ecdef010ae2368e2e58669953fd9a3', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca4efb2faa26429279f44ced2822f5e81ff37537', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=64&amp;height=64&amp;auto=webp&amp;s=3a307ad71aad031accfd47f1af82a2b1e09195cc', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=128&amp;height=128&amp;auto=webp&amp;s=fb9b2c432b1ddd85fd653ef3cc1a28e5edc40a1f', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neylas,True,,yoursdata,,48,True,all_ads,False,[],False,,/r/datascience/comments/neylas/data_science_in_practice/,all_ads,False,https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/,515407,1621304210.0,1,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?auto=webp&amp;s=913d3da85bb969ebe82241c03e6bb763b5e392a0', 'width': 1215, 'height': 510}, 'resolutions': [{'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=203500cfd14054ecebe791e8fe8412ae757dd1e4', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb91ca58f725bd794e098b08af5c42cfb8c0c8c7', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25bdc8313df5a3e39164acc34812f41b4669355c', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e51c17ec9349b0db56614ae0744a4783796b152b', 'width': 640, 'height': 268}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbbbba2887179ddb14bb428ec242fd33876bc064', 'width': 960, 'height': 402}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=633cd545b10fa99da6c61fd59bdb598452288409', 'width': 1080, 'height': 453}], 'variants': {}, 'id': '_BoKND9MoFf7PgT2dWpr5tp1bcI9Htozzg34wYezLQI'}], 'enabled': False}",,,,,
,datascience,"Hi all,

I work for a fairly small to mid-sized firm, and we're facing some growing challenges as a data team. Our leadership/management team currently respects teams that add value to the company, so our marketing team tends to receive significant recognition.

Other teams tend to take credit for the reports, models, and any other analysis we do. This lends to our team *apparently* not adding business value to the company and makes it more difficult to do salary raises, promotions, hiring, etc. Another issue is that another non-technical team is currently asking me to teach them how to use SQL to do their own data pulls. I want to be a team player, but this adds no value to my team except diminishing the workload they put on us.

How do I, as well as my team navigate through these office politics?",t2_f9vap,False,,0,False,How to make sure my team receives appropriate recognition?,[],r/datascience,False,6,career,0,,,False,t3_nfalsq,False,dark,0.88,,public,37,0,{},,,False,[],,False,False,,{},Career,False,37,,False,False,self,False,,[],{},,True,,1621372952.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I work for a fairly small to mid-sized firm, and we&amp;#39;re facing some growing challenges as a data team. Our leadership/management team currently respects teams that add value to the company, so our marketing team tends to receive significant recognition.&lt;/p&gt;

&lt;p&gt;Other teams tend to take credit for the reports, models, and any other analysis we do. This lends to our team &lt;em&gt;apparently&lt;/em&gt; not adding business value to the company and makes it more difficult to do salary raises, promotions, hiring, etc. Another issue is that another non-technical team is currently asking me to teach them how to use SQL to do their own data pulls. I want to be a team player, but this adds no value to my team except diminishing the workload they put on us.&lt;/p&gt;

&lt;p&gt;How do I, as well as my team navigate through these office politics?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfalsq,True,,snsdreborn,,18,True,all_ads,False,[],False,,/r/datascience/comments/nfalsq/how_to_make_sure_my_team_receives_appropriate/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfalsq/how_to_make_sure_my_team_receives_appropriate/,515407,1621344152.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,So I have a silly newbie question.  I am trying to work out pros  as cons for using these tools. We have a team of 20 people and some ( like 3 people) use Alteryx and the rest use python. I am trying to work out what the pros and cons would be for working a team. Does Alteryx make it easier or harder to work together.  What are the other pros and cons for a large team trying to work together vs in their own silos?,t2_deri9h9,False,,0,False,Alteryx vs Python for collaborating in a team,[],r/datascience,False,6,tooling,0,,,False,t3_nfgxdm,False,dark,0.73,,public,7,0,{},,,False,[],,False,False,,{},Tooling,False,7,,False,False,self,False,,[],{},,True,,1621388285.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have a silly newbie question.  I am trying to work out pros  as cons for using these tools. We have a team of 20 people and some ( like 3 people) use Alteryx and the rest use python. I am trying to work out what the pros and cons would be for working a team. Does Alteryx make it easier or harder to work together.  What are the other pros and cons for a large team trying to work together vs in their own silos?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfgxdm,True,,starbucksnamemike,,6,True,all_ads,False,[],False,,/r/datascience/comments/nfgxdm/alteryx_vs_python_for_collaborating_in_a_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfgxdm/alteryx_vs_python_for_collaborating_in_a_team/,515407,1621359485.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,Can you do freelance works in this field?,t2_6lgvt614,False,,0,False,Freelancing?,[],r/datascience,False,6,career,0,,,False,t3_nfa2e4,False,dark,0.74,,public,16,0,{},,,False,[],,False,False,,{},Career,False,16,,False,False,self,False,,[],{},,True,,1621371514.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can you do freelance works in this field?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfa2e4,True,,joacloz,,16,True,all_ads,False,[],False,,/r/datascience/comments/nfa2e4/freelancing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfa2e4/freelancing/,515407,1621342714.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Hey guys so I’m doing a personal project using Spotify data to recommend songs and also using kmeans to cluster songs for analysis/playlist. I use Pycharm and I have a pretty beefy cpu/gpu but sometimes it’ll take forever to run. I’m not sure if my IDE is just not optimized or is there anything else I can do to get faster runtimes when creating models etc. The dataset has 600k+ rows if that helps!,t2_4rukxh3o,False,,0,False,Personal project runtime,[],r/datascience,False,6,projects,0,,,False,t3_nfph2x,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1621408870.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys so I’m doing a personal project using Spotify data to recommend songs and also using kmeans to cluster songs for analysis/playlist. I use Pycharm and I have a pretty beefy cpu/gpu but sometimes it’ll take forever to run. I’m not sure if my IDE is just not optimized or is there anything else I can do to get faster runtimes when creating models etc. The dataset has 600k+ rows if that helps!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfph2x,True,,FreeDominosPizzaGuy,,10,True,all_ads,False,[],False,,/r/datascience/comments/nfph2x/personal_project_runtime/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfph2x/personal_project_runtime/,515407,1621380070.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi all!

I’m a Data Scientist working in Australia. I’m the sole DS in my company and all the other DS I know outside of there are at a similar level I am. I used to be in academia in a different field so I don’t have a more senior person to talk to.

All this is to ask if you folks know of any mentoring networks that I could join.",t2_zru6c3n,False,,0,False,Interested in mentoring networks,[],r/datascience,False,6,network,0,,,False,t3_nf4rnz,False,dark,0.81,,public,13,0,{},,,False,[],,False,False,,{},Networking,False,13,,False,False,self,False,,[],{},,True,,1621353728.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all!&lt;/p&gt;

&lt;p&gt;I’m a Data Scientist working in Australia. I’m the sole DS in my company and all the other DS I know outside of there are at a similar level I am. I used to be in academia in a different field so I don’t have a more senior person to talk to.&lt;/p&gt;

&lt;p&gt;All this is to ask if you folks know of any mentoring networks that I could join.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf4rnz,True,,ndurst,,12,True,all_ads,False,[],False,,/r/datascience/comments/nf4rnz/interested_in_mentoring_networks/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf4rnz/interested_in_mentoring_networks/,515407,1621324928.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"Most anomaly detection techniques/packages focus on anomaly detection within a single time series; ie, take some sort of steady-state average and alert if the data suddenly goes above or below some threshold. My problem is totally different, however.

I have a hardware device that performs the same operation repeatedly. Most of the time it succeeds, but sometimes it fails. I have sensors that measure position and angle (6DOF), and I have a large data set of each attempt, whether or not it was a success or failure, and the sensor data (and first and second derivatives) from a few seconds before the event to a few seconds after.

What I'm looking for is a technique or python package that can analyze all this time series data and, given label of Success or Failure, identify if there are any anomalies that typically lead to a failure. I've done quite a bit of Googling and Stack Overflowing, but keep coming up with ""typical"" anomaly detection packages. Maybe I'm using the wrong keywords or language here to describe what I'm looking for? Any suggestions or pointers in the right direction would be greatly appreciated!",t2_9bcqo,False,,0,False,Techniques for anomaly detection across different time series?,[],r/datascience,False,6,discussion,0,,,False,t3_nf2q8p,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,1621317721.0,,[],{},,True,,1621346308.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most anomaly detection techniques/packages focus on anomaly detection within a single time series; ie, take some sort of steady-state average and alert if the data suddenly goes above or below some threshold. My problem is totally different, however.&lt;/p&gt;

&lt;p&gt;I have a hardware device that performs the same operation repeatedly. Most of the time it succeeds, but sometimes it fails. I have sensors that measure position and angle (6DOF), and I have a large data set of each attempt, whether or not it was a success or failure, and the sensor data (and first and second derivatives) from a few seconds before the event to a few seconds after.&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m looking for is a technique or python package that can analyze all this time series data and, given label of Success or Failure, identify if there are any anomalies that typically lead to a failure. I&amp;#39;ve done quite a bit of Googling and Stack Overflowing, but keep coming up with &amp;quot;typical&amp;quot; anomaly detection packages. Maybe I&amp;#39;m using the wrong keywords or language here to describe what I&amp;#39;m looking for? Any suggestions or pointers in the right direction would be greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf2q8p,True,,Archa3opt3ryx,,10,True,all_ads,False,[],False,,/r/datascience/comments/nf2q8p/techniques_for_anomaly_detection_across_different/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf2q8p/techniques_for_anomaly_detection_across_different/,515407,1621317508.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"After reading through bunch of articles about the different roles I am still confused on specific jobs within the data science community. I really want to focus on the coding, model creation, model deployment and monitoring. Maybe where to ML models are already predefined but my job would be putting it into production and hooking everything up. 

What would this role be called? MLops? ML engineering? Dev ops? The sense that I get is no one seems to know what to call these guys or they are sort of interchangeable titles that might mean something totally different to each company you apply for. Also, would this include making data pipelines or is that a data engineer's job?",t2_sqqip,False,,0,False,Confused on different job titles/roles of data science,[],r/datascience,False,6,discussion,0,,,False,t3_nfezz6,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621383776.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After reading through bunch of articles about the different roles I am still confused on specific jobs within the data science community. I really want to focus on the coding, model creation, model deployment and monitoring. Maybe where to ML models are already predefined but my job would be putting it into production and hooking everything up. &lt;/p&gt;

&lt;p&gt;What would this role be called? MLops? ML engineering? Dev ops? The sense that I get is no one seems to know what to call these guys or they are sort of interchangeable titles that might mean something totally different to each company you apply for. Also, would this include making data pipelines or is that a data engineer&amp;#39;s job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfezz6,True,,AKing2713,,6,True,all_ads,False,[],False,,/r/datascience/comments/nfezz6/confused_on_different_job_titlesroles_of_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfezz6/confused_on_different_job_titlesroles_of_data/,515407,1621354976.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have been looking for a data science role in the financial sector for quite a while but its been tough with no experience. I have an interview for Mindshare which is a media agency, this is a more analytics position which gives me exposure to projects and clients etc so I thought it would be valuable. Would it being a media agency primarily limit me in any way or is the analytics experience good regardless?

&amp;#x200B;

[https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com](https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com)",t2_2gqjctj0,False,,0,False,Does the type of company you work for matter for career progression?,[],r/datascience,False,6,,0,,,False,t3_nf61wa,False,dark,0.56,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1621358483.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been looking for a data science role in the financial sector for quite a while but its been tough with no experience. I have an interview for Mindshare which is a media agency, this is a more analytics position which gives me exposure to projects and clients etc so I thought it would be valuable. Would it being a media agency primarily limit me in any way or is the analytics experience good regardless?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com""&gt;https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nf61wa,True,,randomperson3333,,7,True,all_ads,False,[],False,,/r/datascience/comments/nf61wa/does_the_type_of_company_you_work_for_matter_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf61wa/does_the_type_of_company_you_work_for_matter_for/,515407,1621329683.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?auto=webp&amp;s=44e61adf8464f54226d9cc8623e78c97ee548269', 'width': 1440, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9923c9936a78b633f20ee068505fadb00b319aaa', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbed24ac19bb47c27c8dc3f1856152d8f9ed8c34', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f435d04a1480908721cd140f4ea8b2d1fe293b08', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ffa47ba10e0cc61185f9dd080b2796135b79064', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=69aa28c9198d1bf7a119b589e62a051293f5369a', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4890a4bf5f95ee28853d7c145f61fa16628bb703', 'width': 1080, 'height': 675}], 'variants': {}, 'id': 'ehVvgccxokingeDnwFLbmjzBgbLv6nT_0WwfwDuxpcY'}], 'enabled': False}",,,,,
,datascience,Hello! I was just curious what everyone's on the job training and employer training program experiences have been like. I've really worked at some smaller startups where a senior data scientist would supervise and give advice. I recently got a job offer at a much bigger company so I'm curious about whether or not is normal for larger companies to have more organized training opportunities. Thank you!,t2_84fxp9sq,False,,0,False,On the job training and education,[],r/datascience,False,6,discussion,0,,,False,t3_nf24f6,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621344210.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I was just curious what everyone&amp;#39;s on the job training and employer training program experiences have been like. I&amp;#39;ve really worked at some smaller startups where a senior data scientist would supervise and give advice. I recently got a job offer at a much bigger company so I&amp;#39;m curious about whether or not is normal for larger companies to have more organized training opportunities. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf24f6,True,,trmntr308,,8,True,all_ads,False,[],False,,/r/datascience/comments/nf24f6/on_the_job_training_and_education/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf24f6/on_the_job_training_and_education/,515407,1621315410.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I am a full time data scientist but thinking about pivoting some of my skills on the side to get some extra income. Anyone have experience having a sperate company or offering certain services?,t2_6zqdq,False,,0,False,Anyone got a side hustle?,[],r/datascience,False,6,career,0,,,False,t3_nersx8,False,dark,0.78,,public,17,0,{},,,False,[],,False,False,,{},Career,False,17,,False,False,self,False,,[],{},,True,,1621314166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a full time data scientist but thinking about pivoting some of my skills on the side to get some extra income. Anyone have experience having a sperate company or offering certain services?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nersx8,True,,edisekeed,,16,True,all_ads,False,[],False,,/r/datascience/comments/nersx8/anyone_got_a_side_hustle/,all_ads,False,https://www.reddit.com/r/datascience/comments/nersx8/anyone_got_a_side_hustle/,515407,1621285366.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"For example I have a linear regression model ready. I dont get all the features at the start. for example, I get 6 out of 10 features. I need to see the target variable range I will get.

&amp;#x200B;

 Also  if I need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range.",t2_2kxvoy8x,False,,0,False,How to deploy a real time model which gets the variables(features) in phases?,[],r/datascience,False,6,discussion,0,,,False,t3_nf6hhq,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621360102.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example I have a linear regression model ready. I dont get all the features at the start. for example, I get 6 out of 10 features. I need to see the target variable range I will get.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Also  if I need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf6hhq,True,,spiritbear1,,7,True,all_ads,False,[],False,,/r/datascience/comments/nf6hhq/how_to_deploy_a_real_time_model_which_gets_the/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf6hhq/how_to_deploy_a_real_time_model_which_gets_the/,515407,1621331302.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi there! I'm an aspiring data scientist hoping to one day work at a music analytics/streaming company such as Spotify, Pandora, or Tidal. I would like to learn more about what data scientists do at such companies and how they analyse user data to eventually drive company decisions.

If there's anyone here working in a similar field (or even have similar interests), I would love to have a chat with you to learn more about what you do. 

If interested, please send me a message or comment on this post so that I can get in touch with you.

Thanks!",t2_3loxj2cz,False,,0,False,Anyone working in a music analytics/streaming company like Spotify?,[],r/datascience,False,6,network,0,,,False,t3_ne72ct,False,dark,0.99,,public,193,0,{},,,False,[],,False,False,,{},Networking,False,193,,False,False,self,False,,[],{},,True,,1621255927.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there! I&amp;#39;m an aspiring data scientist hoping to one day work at a music analytics/streaming company such as Spotify, Pandora, or Tidal. I would like to learn more about what data scientists do at such companies and how they analyse user data to eventually drive company decisions.&lt;/p&gt;

&lt;p&gt;If there&amp;#39;s anyone here working in a similar field (or even have similar interests), I would love to have a chat with you to learn more about what you do. &lt;/p&gt;

&lt;p&gt;If interested, please send me a message or comment on this post so that I can get in touch with you.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ne72ct,True,,hedgehogist,,42,True,all_ads,False,[],False,,/r/datascience/comments/ne72ct/anyone_working_in_a_music_analyticsstreaming/,all_ads,False,https://www.reddit.com/r/datascience/comments/ne72ct/anyone_working_in_a_music_analyticsstreaming/,515407,1621227127.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I am in the process of preparing a course on neural networks - it's a broad strokes walk through both basics and various different topics. I have decided that one week will be dedicated to transfer learning and multi task learning (together, since there are some interesting transfer learning approaches which leverage MTL). As part of the course, I want the students to solve a 'small' exercise where they improve some machine learning task by combining it with another one. 

However, it is really hard to come up with a good task like that :) regular fine tuning was easy enough - I decided to lean on the pytorch tutorial for that one. So far, I have not been able to find a similar 'toy example' that can both be run in google colab (I can not assume that my students have access to a GPU of their own) and where the benefit is tangible. My best bet so far (which is not working out yet) is to combine age and gender identification on a subset of the cropped version of the wiki face dataset ([https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)). I'm hoping that with a sufficiently small subset, I'll have a task which doesn't quite work on its own, but where real improvement can be seen when each image is analyzed in two different tasks.

 But, perhaps someone knows of a better problem?",t2_3moso,False,,0,False,anyone know of a good 'textbook example' for multi task learning?,[],r/datascience,False,6,discussion,0,,,False,t3_neqi90,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621311082.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am in the process of preparing a course on neural networks - it&amp;#39;s a broad strokes walk through both basics and various different topics. I have decided that one week will be dedicated to transfer learning and multi task learning (together, since there are some interesting transfer learning approaches which leverage MTL). As part of the course, I want the students to solve a &amp;#39;small&amp;#39; exercise where they improve some machine learning task by combining it with another one. &lt;/p&gt;

&lt;p&gt;However, it is really hard to come up with a good task like that :) regular fine tuning was easy enough - I decided to lean on the pytorch tutorial for that one. So far, I have not been able to find a similar &amp;#39;toy example&amp;#39; that can both be run in google colab (I can not assume that my students have access to a GPU of their own) and where the benefit is tangible. My best bet so far (which is not working out yet) is to combine age and gender identification on a subset of the cropped version of the wiki face dataset (&lt;a href=""https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/""&gt;https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/&lt;/a&gt;). I&amp;#39;m hoping that with a sufficiently small subset, I&amp;#39;ll have a task which doesn&amp;#39;t quite work on its own, but where real improvement can be seen when each image is analyzed in two different tasks.&lt;/p&gt;

&lt;p&gt;But, perhaps someone knows of a better problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neqi90,True,,miturian,,6,True,all_ads,False,[],False,,/r/datascience/comments/neqi90/anyone_know_of_a_good_textbook_example_for_multi/,all_ads,False,https://www.reddit.com/r/datascience/comments/neqi90/anyone_know_of_a_good_textbook_example_for_multi/,515407,1621282282.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?auto=webp&amp;s=a545e6cc58a39ec5b13aef5c521072f8d70a1ad4', 'width': 2400, 'height': 1800}, 'resolutions': [{'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af876d35ca7565cda28d1011c76a294b15d42fa5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7143cbff70d22d656d3e88bf6ec51051e495eff5', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec2236cd3de263eda0386841570f1a833468f641', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ee27475e8d6e100192807d8f64403fafcf8cabc', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=309cf9ad0fe864e26d92b791cc3facb6b846b8a9', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e8c0b6b254f170ef3a1c1215e8bbb41524bea7f', 'width': 1080, 'height': 810}], 'variants': {}, 'id': '9B_lz_-bwwQH4RzXVOgA3ziUA1HmzZs56DFeFdMjsHU'}], 'enabled': False}",,,,,
,datascience,"My current position is kind of a consultant style role where there isn't a specific product/project to work on and it's kind of whatever projects come through the door. My next position however is working with a specific domain Risk/Fraud. I'm curious if working on a specific area e.g. Fraud/Ads/Recommender Systems/etc ends up locking you into that area for the remainder of your career and future positions. Also, out of curiosity which domain has the greatest potential for future positions.",t2_1k6f27,False,,0,False,Thoughts on getting stuck with working on a specific domain?,[],r/datascience,False,6,career,0,,,False,t3_neqdpq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1621310785.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My current position is kind of a consultant style role where there isn&amp;#39;t a specific product/project to work on and it&amp;#39;s kind of whatever projects come through the door. My next position however is working with a specific domain Risk/Fraud. I&amp;#39;m curious if working on a specific area e.g. Fraud/Ads/Recommender Systems/etc ends up locking you into that area for the remainder of your career and future positions. Also, out of curiosity which domain has the greatest potential for future positions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neqdpq,True,,m908f,,3,True,all_ads,False,[],False,,/r/datascience/comments/neqdpq/thoughts_on_getting_stuck_with_working_on_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/neqdpq/thoughts_on_getting_stuck_with_working_on_a/,515407,1621281985.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have so far mostly worked in AdTech as a data scientist. Soon in about a month, I will be switching to electric vehicle fleet management which I have never worked with. I think the aspect which I mostly improved upon while working with AdTech was handling huge amount of data but the underlying models weren't so complex. It was almost always logistic regression. I did deal with some deep bayesian models.

Can anyone who has worked in similar field or related fields give me some pointers regarding the problems and the methodologies being used there? What I could gather so far was that the general problem is about vehicle routing and fleet management. What are the underlying methods being used there? What algorithms/models should I brush up on?",t2_3bdkl2w1,False,,0,False,Getting a head start when switching to a completely new domain,[],r/datascience,False,6,discussion,0,,,False,t3_necyqc,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1621277299.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have so far mostly worked in AdTech as a data scientist. Soon in about a month, I will be switching to electric vehicle fleet management which I have never worked with. I think the aspect which I mostly improved upon while working with AdTech was handling huge amount of data but the underlying models weren&amp;#39;t so complex. It was almost always logistic regression. I did deal with some deep bayesian models.&lt;/p&gt;

&lt;p&gt;Can anyone who has worked in similar field or related fields give me some pointers regarding the problems and the methodologies being used there? What I could gather so far was that the general problem is about vehicle routing and fleet management. What are the underlying methods being used there? What algorithms/models should I brush up on?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,necyqc,True,,proof_required,,11,True,all_ads,False,[],False,,/r/datascience/comments/necyqc/getting_a_head_start_when_switching_to_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/necyqc/getting_a_head_start_when_switching_to_a/,515407,1621248499.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What are the differences? Is one just in academia and one in industry or is it like a rectangles and squares kinda deal?,t2_44oxrfns,False,,0,False,Statistician vs data scientist?,[],r/datascience,False,6,meta,0,,,False,t3_ndpft6,False,dark,0.93,,public,159,1,{},,,False,[],,False,False,,{},Meta,False,159,,False,False,self,False,,[],{},,True,,1621203939.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are the differences? Is one just in academia and one in industry or is it like a rectangles and squares kinda deal?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndpft6,True,,medylan,,118,True,all_ads,False,[],False,,/r/datascience/comments/ndpft6/statistician_vs_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndpft6/statistician_vs_data_scientist/,515407,1621175139.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,
,datascience,"Based on my experience, role titles such as Data Engineer and Data Scientist mean very different things depending on the company.

I see three main cases:

1. **Normally Bigger (and Older) Companies**  
DEs do all MLOps: They build anything needed to gather and store data. They put DS models into production too.  
DSs train models, do exploratory analysis, create variables, validate hypothesis, but don't put any of that into production themselves.
2. **Normally Modern (Tech) Companies**  
DEs gather and store data.  
DSs don't just train but also deploy their models into production.
3. **Normally Smaller Companies**  
One profile does it all.

I work now for a startup whose product is a realtime end to end platform which eases the otherwise complicated MLOps stuff. I'm writing some docs that explain how realtime is done now VS my company proposal and I'm finding that defining how things are done now is not straightforward. So, do you agree with the three cases above? Would you add more? Where would you put Data Architects, ML Engineers, etc.?",t2_c4rvshhm,False,,0,False,Differences between DE and DS actual job along different companies,[],r/datascience,False,6,discussion,0,,,False,t3_necjee,False,dark,0.57,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1621278054.0,,[],{},,True,,1621275820.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Based on my experience, role titles such as Data Engineer and Data Scientist mean very different things depending on the company.&lt;/p&gt;

&lt;p&gt;I see three main cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normally Bigger (and Older) Companies&lt;/strong&gt;&lt;br/&gt;
DEs do all MLOps: They build anything needed to gather and store data. They put DS models into production too.&lt;br/&gt;
DSs train models, do exploratory analysis, create variables, validate hypothesis, but don&amp;#39;t put any of that into production themselves.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normally Modern (Tech) Companies&lt;/strong&gt;&lt;br/&gt;
DEs gather and store data.&lt;br/&gt;
DSs don&amp;#39;t just train but also deploy their models into production.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normally Smaller Companies&lt;/strong&gt;&lt;br/&gt;
One profile does it all.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I work now for a startup whose product is a realtime end to end platform which eases the otherwise complicated MLOps stuff. I&amp;#39;m writing some docs that explain how realtime is done now VS my company proposal and I&amp;#39;m finding that defining how things are done now is not straightforward. So, do you agree with the three cases above? Would you add more? Where would you put Data Architects, ML Engineers, etc.?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,necjee,True,,JB__Quix,,5,True,all_ads,False,[],False,,/r/datascience/comments/necjee/differences_between_de_and_ds_actual_job_along/,all_ads,False,https://www.reddit.com/r/datascience/comments/necjee/differences_between_de_and_ds_actual_job_along/,515407,1621247020.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am a self taught data professional, well versed in most data analytics/data engineering program, tools, math/statistics and software. At my current work place, I am constantly coming up with new analysis, tools, automation,  processes, troubleshooting and QA. I have earned the respect of my peers and managers, and I have excellent technical, teamwork, and communication skills. I am at par if not better overall than most of my colleagues whom have masters and PhDs. 

I personally feel as if I have a chip on my shoulder because of me being self taught. The grittiness needed to learn the material and find things efficiently on my own is something I value in myself.

It’s just really unfortunate that at face value, no matter what I do, there will be tons of people with advanced degrees in data, who get put ahead of me, and paid more than me, in the job search. I also feel like in the workplace, I also have to go the extra mile with new seniors I meet a lot of the times for them to take me seriously. I understand why it happens, but it really is just saddening and upsetting. 

Please tell me there are people in here that have felt and experienced the same thing as I have with this. If so, how have you learned to engage with this? 

Yes, I know one of the answers is to get a advanced degree.",t2_8x16rrzg,False,,0,False,Feeling overlooked and undervalued as a Self-Taught data professional when compared to advanced degree holders.,[],r/datascience,False,6,career,0,,,False,t3_nesj5b,False,dark,0.48,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1621315979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a self taught data professional, well versed in most data analytics/data engineering program, tools, math/statistics and software. At my current work place, I am constantly coming up with new analysis, tools, automation,  processes, troubleshooting and QA. I have earned the respect of my peers and managers, and I have excellent technical, teamwork, and communication skills. I am at par if not better overall than most of my colleagues whom have masters and PhDs. &lt;/p&gt;

&lt;p&gt;I personally feel as if I have a chip on my shoulder because of me being self taught. The grittiness needed to learn the material and find things efficiently on my own is something I value in myself.&lt;/p&gt;

&lt;p&gt;It’s just really unfortunate that at face value, no matter what I do, there will be tons of people with advanced degrees in data, who get put ahead of me, and paid more than me, in the job search. I also feel like in the workplace, I also have to go the extra mile with new seniors I meet a lot of the times for them to take me seriously. I understand why it happens, but it really is just saddening and upsetting. &lt;/p&gt;

&lt;p&gt;Please tell me there are people in here that have felt and experienced the same thing as I have with this. If so, how have you learned to engage with this? &lt;/p&gt;

&lt;p&gt;Yes, I know one of the answers is to get a advanced degree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nesj5b,True,,Justanotherguy2022,,38,True,all_ads,False,[],False,,/r/datascience/comments/nesj5b/feeling_overlooked_and_undervalued_as_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nesj5b/feeling_overlooked_and_undervalued_as_a/,515407,1621287179.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"[Source Code on Github](https://github.com/jina-ai/examples/tree/master/multimodal-search-pdf)

In this project I am using [Jina](https://github.com/jina-ai/jina) to search a repository of PDF files. The project allows a user to query the data by providing text, or an image, or both simultaneously.

**How to use it?**

Clone the project and run following commands

    # Install requirements
    pip install -r requirements.txt
    
    # Start the server
    python app.py -t query_restful
    
    # Query via REST API
    curl --request POST -d '{""top_k"": 10, ""mode"": ""search"",  ""data"": [""jina hello multimodal""]}' -H 'Content-Type: application/json' 'http://0.0.0.0:45670/api/search'

What's included in this example:

* Search text, image, PDF all in one Flow or in separate Flows
* Speed up indexing time with parallel Peas
* Use customized executors to better fit your needs
* Provide detailed docstrings for YAML files to help you understand the example  


Let me know your feedback and what would you use this project for. I'd love to help",t2_auwgbh53,False,,0,False,PDF search - Another project I built using Jina(AI Search framework),[],r/datascience,False,6,projects,0,,,False,t3_ne4tco,False,dark,0.75,,public,12,3,{},,,False,[],,False,False,,{},Projects,False,12,,False,True,self,False,,[],{},,True,,1621248594.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/jina-ai/examples/tree/master/multimodal-search-pdf""&gt;Source Code on Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this project I am using &lt;a href=""https://github.com/jina-ai/jina""&gt;Jina&lt;/a&gt; to search a repository of PDF files. The project allows a user to query the data by providing text, or an image, or both simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to use it?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Clone the project and run following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Install requirements
pip install -r requirements.txt

# Start the server
python app.py -t query_restful

# Query via REST API
curl --request POST -d &amp;#39;{&amp;quot;top_k&amp;quot;: 10, &amp;quot;mode&amp;quot;: &amp;quot;search&amp;quot;,  &amp;quot;data&amp;quot;: [&amp;quot;jina hello multimodal&amp;quot;]}&amp;#39; -H &amp;#39;Content-Type: application/json&amp;#39; &amp;#39;http://0.0.0.0:45670/api/search&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What&amp;#39;s included in this example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search text, image, PDF all in one Flow or in separate Flows&lt;/li&gt;
&lt;li&gt;Speed up indexing time with parallel Peas&lt;/li&gt;
&lt;li&gt;Use customized executors to better fit your needs&lt;/li&gt;
&lt;li&gt;Provide detailed docstrings for YAML files to help you understand the example&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let me know your feedback and what would you use this project for. I&amp;#39;d love to help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'award_2ae56630-cfe0-424e-b810-4945b9145358', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful (Pro)', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=57278b329d19fd1d345888bfff68a51528777538', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=db7b3f20402a8a6820a4ffebf35160d2557986e2', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=0100d8da8f4dae0dc81d430733aa622d752c268c', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=1029c080a179f45b6d83a51ed79dfd57997ae266', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=50e7f8a870f79df7bc38bedb8a12e01137df5a77', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ne4tco,True,,opensourcecolumbus,,2,True,all_ads,False,[],False,,/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/,all_ads,False,https://www.reddit.com/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/,515407,1621219794.0,3,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?auto=webp&amp;s=ce98c31438ca2d2be9fbbc00504d993b0e76200f', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e627429c5dcad41c7d9f8b18915a614615b12345', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4af1f92f7671efc51315e041b71df4d43421e66', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5086c155aa654acbcca2a87a6dc89b656bcda223', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=250ac075eb528bdb06ada9f8286f14581d9870a4', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d499c181a1d373dd210f26236d53ae577b8a1ab', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7c363d5b2a43f9e38123f8267966fe0148c47031', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'AfzM_QLKF3z7opsY74zTP-LhPancxGqB5QjRhBwqUiE'}], 'enabled': False}",,,,,
,datascience,"Hello!

I am personally loving work from home because it saves me a lot of time. But eventually everyone has to go back to the offices so I thought maybe it's a good idea to talk about the problems we face when working from physical office and how can we make the experience better.

Thanks!",t2_bv171ji2,False,,0,False,What is something you're absolutely NOT looking forward to once work from home ends and how do you plan to fix it?,[],r/datascience,False,6,discussion,0,,,False,t3_ndt83g,False,dark,0.94,,public,32,1,{},,,False,[],,False,False,,{},Discussion,False,32,,False,False,self,False,,[],{},,True,,1621214824.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am personally loving work from home because it saves me a lot of time. But eventually everyone has to go back to the offices so I thought maybe it&amp;#39;s a good idea to talk about the problems we face when working from physical office and how can we make the experience better.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndt83g,True,,quite--average,,49,True,all_ads,False,[],False,,/r/datascience/comments/ndt83g/what_is_something_youre_absolutely_not_looking/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndt83g/what_is_something_youre_absolutely_not_looking/,515407,1621186024.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Why bother mastering SQL when you can simply extract all of the data using a few basic SELECT commands and then do all of the data wrangling in pandas? 

Is there something important I’m missing by relying on pandas for data handling and manipulation?",t2_m7v8y,False,,0,False,SQL vs Pandas,[],r/datascience,False,6,discussion,0,,,False,t3_ndkwgm,False,dark,0.79,,public,95,1,{},,,False,[],,False,False,,{},Discussion,False,95,,False,False,self,False,,[],{},,True,,1621187390.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Why bother mastering SQL when you can simply extract all of the data using a few basic SELECT commands and then do all of the data wrangling in pandas? &lt;/p&gt;

&lt;p&gt;Is there something important I’m missing by relying on pandas for data handling and manipulation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndkwgm,True,,C_BearHill,,96,True,all_ads,False,[],False,,/r/datascience/comments/ndkwgm/sql_vs_pandas/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndkwgm/sql_vs_pandas/,515407,1621158590.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

It's been a year since I finished grad school and it feels like I need a refresher on the concepts. I started reading ISLR and I felt like I need to take notes so that next time I need to refresh on the concepts I can just go through the notes instead of reading the entire book.
The note taking is primarily because of potential future interviews.

Do you guys just do it old school by taking notes in a notebook or do it differently now?
Also, is there anything else I should do in order to prepare notes for interview prep?

Any other advice is welcome.

Thanks!",t2_bv171ji2,False,,0,False,How do you take notes while reading a statistics book?,[],r/datascience,False,6,discussion,0,,,False,t3_ndrch1,False,dark,0.88,,public,22,0,{},,,False,[],,False,False,,{},Discussion,False,22,,False,False,self,False,,[],{},,True,,1621209510.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;It&amp;#39;s been a year since I finished grad school and it feels like I need a refresher on the concepts. I started reading ISLR and I felt like I need to take notes so that next time I need to refresh on the concepts I can just go through the notes instead of reading the entire book.
The note taking is primarily because of potential future interviews.&lt;/p&gt;

&lt;p&gt;Do you guys just do it old school by taking notes in a notebook or do it differently now?
Also, is there anything else I should do in order to prepare notes for interview prep?&lt;/p&gt;

&lt;p&gt;Any other advice is welcome.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndrch1,True,,quite--average,,21,True,all_ads,False,[],False,,/r/datascience/comments/ndrch1/how_do_you_take_notes_while_reading_a_statistics/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndrch1/how_do_you_take_notes_while_reading_a_statistics/,515407,1621180710.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My team does not do much documentation. They tend to think it's not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it. 

We all tend to work on different models but often we need to understand how other models work and I think it's worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member.

I'm just wondering how other teams think about enforcing documentation. I don't think it's a waste of time and there will most likely be someone taking over someone's codebase eventually and it's painful to try to understand some functions when that person isn't around to ask questions to.",t2_zgml6,False,,0,False,Documenting code in data science...,[],r/datascience,False,6,discussion,0,,,False,t3_nd9io8,False,dark,0.99,,public,227,2,{},,,False,[],,False,False,,{},Discussion,False,227,,False,False,self,False,,[],{'gid_1': 1},,True,,1621144630.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My team does not do much documentation. They tend to think it&amp;#39;s not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it. &lt;/p&gt;

&lt;p&gt;We all tend to work on different models but often we need to understand how other models work and I think it&amp;#39;s worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m just wondering how other teams think about enforcing documentation. I don&amp;#39;t think it&amp;#39;s a waste of time and there will most likely be someone taking over someone&amp;#39;s codebase eventually and it&amp;#39;s painful to try to understand some functions when that person isn&amp;#39;t around to ask questions to.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd9io8,True,,svartgeit,,53,True,all_ads,False,[],False,,/r/datascience/comments/nd9io8/documenting_code_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd9io8/documenting_code_in_data_science/,515407,1621115830.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 16 May 2021 - 23 May 2021,[],r/datascience,False,6,,0,,,False,t3_ndmuat,False,dark,0.91,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1621195230.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,ndmuat,True,,datascience-bot,,208,False,all_ads,False,[],False,dark,/r/datascience/comments/ndmuat/weekly_entering_transitioning_thread_16_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndmuat/weekly_entering_transitioning_thread_16_may_2021/,515407,1621166430.0,0,,False,,,,,,,,
,datascience,,t2_gafbm9o,False,,0,False,What do you dislike most about the data science industry?,[],r/datascience,False,6,discussion,0,,,False,t3_ndhg4h,False,dark,0.88,,public,23,0,{},,,False,[],,False,False,,{},Discussion,False,23,,False,False,default,False,,[],{},,False,,1621172182.0,text,6,,,text,self.TheAnalystEconomy,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndhg4h,True,,Kobedoggg,,27,True,all_ads,False,[],False,,/r/datascience/comments/ndhg4h/what_do_you_dislike_most_about_the_data_science/,all_ads,False,/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/,515407,1621143382.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'TheAnalystEconomy', 'selftext': '', 'author_fullname': 't2_gafbm9o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you hate most about the legacy data analysis/insights industry?', 'link_flair_richtext': [{'e': 'text', 't': 'General discussion'}], 'subreddit_name_prefixed': 'r/TheAnalystEconomy', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n67zzg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'General discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1620338603.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.TheAnalystEconomy', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dcd9e614-ad4a-11eb-9428-0e37c2f586d7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_4bo841', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#dadada', 'id': 'n67zzg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Kobedoggg', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/', 'subreddit_subscribers': 1661, 'created_utc': 1620309803.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/,t3_n67zzg,
,datascience,"Hey everyone, I first posted this on r/analytics but realized it doesn't fit very well there. I need to scrape some elections data from a website. It has JavaScript and around a 1000 individual pages that all have the same format and variables, stored in a table. I'm new to scraping but have read a bit today and it seems like Python is my best bet. I was wondering if this is the type of thing I should use a full crawler on, like Scrapy. The URLs for the pages i need all have this format:

https://elections.amo.on.ca/web/en/municipal/XXXXX 

Where XXXXX seems to be an ID code made up of digits for each page.  I don't know which 5 digit codes actually correspond to the pages I need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.

Should I just get started with learning Scrapy in Python, or do you think there is a better tool for this task?",t2_16c56a,False,,0,False,need advice on the best web scraping tool/approach for this job,[],r/datascience,False,6,projects,0,,,False,t3_ndobxv,False,dark,0.62,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1621200464.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I first posted this on &lt;a href=""/r/analytics""&gt;r/analytics&lt;/a&gt; but realized it doesn&amp;#39;t fit very well there. I need to scrape some elections data from a website. It has JavaScript and around a 1000 individual pages that all have the same format and variables, stored in a table. I&amp;#39;m new to scraping but have read a bit today and it seems like Python is my best bet. I was wondering if this is the type of thing I should use a full crawler on, like Scrapy. The URLs for the pages i need all have this format:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://elections.amo.on.ca/web/en/municipal/XXXXX""&gt;https://elections.amo.on.ca/web/en/municipal/XXXXX&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Where XXXXX seems to be an ID code made up of digits for each page.  I don&amp;#39;t know which 5 digit codes actually correspond to the pages I need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.&lt;/p&gt;

&lt;p&gt;Should I just get started with learning Scrapy in Python, or do you think there is a better tool for this task?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndobxv,True,,ReedCube,,14,True,all_ads,False,[],False,,/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/,515407,1621171664.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi, i just started to learn about market basket analysis with FP-growth.  


what is the amount of data do i need to create an accurate association rule? is there a minimum? is it pure arbitrary?   


thanks for the help.",t2_4qf1547i,False,,0,False,Market Basket Analysis Help,[],r/datascience,False,6,discussion,0,,,False,t3_nddpj8,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621158154.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, i just started to learn about market basket analysis with FP-growth.  &lt;/p&gt;

&lt;p&gt;what is the amount of data do i need to create an accurate association rule? is there a minimum? is it pure arbitrary?   &lt;/p&gt;

&lt;p&gt;thanks for the help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nddpj8,True,,MorBid23,,5,True,all_ads,False,[],False,,/r/datascience/comments/nddpj8/market_basket_analysis_help/,all_ads,False,https://www.reddit.com/r/datascience/comments/nddpj8/market_basket_analysis_help/,515407,1621129354.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The idea of creating confidence intervals in regression models is quite straightforward. 

For example :
https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png

But do confidence intervals carry over to classification models?

1) For example, in this picture here (confidence interval for an ROC curve corresponding to a classification model ) https://i.stack.imgur.com/Y7KSNm.png - can the lower limit of the confidence interval from this ROC curve be used to gauge how well this model might generalize to new data?

2) For a classification model, can we make a confidence interval for the prediction of an individual observation? For instance, I wrote some R code (you can directly copy/paste the code) for a particular example where a random forest (i.e. classification model) is used to predict whether if an observation will be ""approved"" or ""rejected"" (see here for the code: https://shrib.com/#Madelyn_NMjYE8) .

Thus, for each observation, the classification model predicts the probability that this observation will be ""approved"" or ""rejected"". Whichever probability is higher, the model selects that outcome for the given observation.

Also, the higher one of these probabilities are, this means the classification model is more confident with its prediction (e.g. for two observations, the ratio of approved:rejected 0.9:0.1 vs 0.6:0.4 . The model is more confident about the first prediction)

Thus, is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions?

Thanks",t2_xtuyc,False,,0,False,Confidence Intervals for Classification Models,[],r/datascience,False,6,discussion,0,,,False,t3_nd81cs,False,dark,0.75,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1621140233.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The idea of creating confidence intervals in regression models is quite straightforward. &lt;/p&gt;

&lt;p&gt;For example :
&lt;a href=""https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png""&gt;https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But do confidence intervals carry over to classification models?&lt;/p&gt;

&lt;p&gt;1) For example, in this picture here (confidence interval for an ROC curve corresponding to a classification model ) &lt;a href=""https://i.stack.imgur.com/Y7KSNm.png""&gt;https://i.stack.imgur.com/Y7KSNm.png&lt;/a&gt; - can the lower limit of the confidence interval from this ROC curve be used to gauge how well this model might generalize to new data?&lt;/p&gt;

&lt;p&gt;2) For a classification model, can we make a confidence interval for the prediction of an individual observation? For instance, I wrote some R code (you can directly copy/paste the code) for a particular example where a random forest (i.e. classification model) is used to predict whether if an observation will be &amp;quot;approved&amp;quot; or &amp;quot;rejected&amp;quot; (see here for the code: &lt;a href=""https://shrib.com/#Madelyn_NMjYE8""&gt;https://shrib.com/#Madelyn_NMjYE8&lt;/a&gt;) .&lt;/p&gt;

&lt;p&gt;Thus, for each observation, the classification model predicts the probability that this observation will be &amp;quot;approved&amp;quot; or &amp;quot;rejected&amp;quot;. Whichever probability is higher, the model selects that outcome for the given observation.&lt;/p&gt;

&lt;p&gt;Also, the higher one of these probabilities are, this means the classification model is more confident with its prediction (e.g. for two observations, the ratio of approved:rejected 0.9:0.1 vs 0.6:0.4 . The model is more confident about the first prediction)&lt;/p&gt;

&lt;p&gt;Thus, is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd81cs,True,,ottawalanguages,,1,True,all_ads,False,[],False,,/r/datascience/comments/nd81cs/confidence_intervals_for_classification_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd81cs/confidence_intervals_for_classification_models/,515407,1621111433.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?auto=webp&amp;s=8e860f1a1c8a6af4e65dcc625f963436be4e78f8', 'width': 850, 'height': 564}, 'resolutions': [{'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef9391d4cebbf98101bbb4097b78f458b80e813e', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f56792912fb59217da945f2fd0bf48bec989605', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=efefc3710f9d6e8c578ad72f2427a444746cd037', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76b95ca1ed0286bd1a4e74a6b5082f46016c54fe', 'width': 640, 'height': 424}], 'variants': {}, 'id': 'oGCJVM-H1RJsGRRY38Dnb6bSYoQ07ajwhpfO3pjwiWs'}], 'enabled': False}",,,,,
,datascience,"I recently moved from IT Governance Audit, in which we have codex like COBIT 5 to follow. I wonder if there is something like that for data pipelining?",t2_17d3z1,False,,0,False,What are some industry standard codex for data pipelining?,[],r/datascience,False,6,education,0,,,False,t3_ndcl23,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1621154250.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently moved from IT Governance Audit, in which we have codex like COBIT 5 to follow. I wonder if there is something like that for data pipelining?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndcl23,True,,votekonan,,0,True,all_ads,False,[],False,,/r/datascience/comments/ndcl23/what_are_some_industry_standard_codex_for_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndcl23/what_are_some_industry_standard_codex_for_data/,515407,1621125450.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Let me first say that I am not a data scientist, or even a college degree holder. I'm just someone who thought of something but has no idea how to do it - which I am going to ask how to do here.

Pretty much I run a Minecraft server, and want to show player activity over the course of the last 6 months that I have had it. Every 50 or so seconds the console log outputs the following line:

\[ {Day of Week}, {Numerical Day}. {Month} {Year} {Hour}:{Minute}:{Second} UTC INFO\] There are {# of players} of a max of {Max # of players} players online: {Player names seperated by ,}

&amp;#x200B;

For example:

\[Fri, 14. May 2021 05:15:34 UTC INFO\] There are 0 of a max of 15 players online:

\[Fri, 14. May 2021 14:51:30 UTC INFO\] There are 1 of a max of 15 players online: Player1

\[Fri, 14. May 2021 03:40:22 UTC INFO\] There are 3 of a max of 15 players online: Player1, Player2, Player3

&amp;#x200B;

Is there a program out there that I can use to look at those lines of text, and convert them into a graph that can show for how many minutes or something of the sort in a single day a player was on for each day over the last 6 months all the while ignore every other line? And, if possible, how I can make the program do that?

Any and all help is appreciated!",t2_4de1e9bv,False,,0,False,Silly Question,[],r/datascience,False,6,discussion,0,,,False,t3_ncfzss,False,dark,0.93,,public,192,1,{},,,False,[],,False,False,,{},Discussion,False,192,,False,False,self,False,,[],{},,True,,1621047110.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let me first say that I am not a data scientist, or even a college degree holder. I&amp;#39;m just someone who thought of something but has no idea how to do it - which I am going to ask how to do here.&lt;/p&gt;

&lt;p&gt;Pretty much I run a Minecraft server, and want to show player activity over the course of the last 6 months that I have had it. Every 50 or so seconds the console log outputs the following line:&lt;/p&gt;

&lt;p&gt;[ {Day of Week}, {Numerical Day}. {Month} {Year} {Hour}:{Minute}:{Second} UTC INFO] There are {# of players} of a max of {Max # of players} players online: {Player names seperated by ,}&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 05:15:34 UTC INFO] There are 0 of a max of 15 players online:&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 14:51:30 UTC INFO] There are 1 of a max of 15 players online: Player1&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 03:40:22 UTC INFO] There are 3 of a max of 15 players online: Player1, Player2, Player3&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there a program out there that I can use to look at those lines of text, and convert them into a graph that can show for how many minutes or something of the sort in a single day a player was on for each day over the last 6 months all the while ignore every other line? And, if possible, how I can make the program do that?&lt;/p&gt;

&lt;p&gt;Any and all help is appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncfzss,True,,_coop007,,68,True,all_ads,False,[],False,,/r/datascience/comments/ncfzss/silly_question/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncfzss/silly_question/,515407,1621018310.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello,

I finally came to realize that I use almost every libraries as pure blackboxes and that it is a problem (I thought before it was not as soon as you can give the management a correct result) if I want to improve myself professionally speaking.

I have zero background in mathematics so learning through a university syllabus is very very hard and I don't think I'll be able to finish that. Would you have any resources that is a bit interactive and noob-friendly ? I enjoyed learning Python and R through small own made project (that's how I ended with my current data clerk position) but I don't see how I could do that for stats.

Thanks,",t2_k15r8mh,False,,0,False,Any good resources for learning statistics applied to datascience from scratch ?,[],r/datascience,False,6,education,0,,,False,t3_nd9bi3,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1621144028.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I finally came to realize that I use almost every libraries as pure blackboxes and that it is a problem (I thought before it was not as soon as you can give the management a correct result) if I want to improve myself professionally speaking.&lt;/p&gt;

&lt;p&gt;I have zero background in mathematics so learning through a university syllabus is very very hard and I don&amp;#39;t think I&amp;#39;ll be able to finish that. Would you have any resources that is a bit interactive and noob-friendly ? I enjoyed learning Python and R through small own made project (that&amp;#39;s how I ended with my current data clerk position) but I don&amp;#39;t see how I could do that for stats.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd9bi3,True,,Dyolekythos,,9,True,all_ads,False,[],False,,/r/datascience/comments/nd9bi3/any_good_resources_for_learning_statistics/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd9bi3/any_good_resources_for_learning_statistics/,515407,1621115228.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,Best answer becomes a meme :-),t2_8pzf0806,False,,0,False,Tell us you’re a data scientist without telling us you’re a data scientist.,[],r/datascience,False,6,fun,0,,,False,t3_ncqjfj,False,dark,0.66,,public,12,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,12,,False,False,self,False,,[],{},,True,,1621080916.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Best answer becomes a meme :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncqjfj,True,,datasci-live,,71,True,all_ads,False,[],False,,/r/datascience/comments/ncqjfj/tell_us_youre_a_data_scientist_without_telling_us/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncqjfj/tell_us_youre_a_data_scientist_without_telling_us/,515407,1621052116.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_3xeoyrv0,False,,0,False,In your experience what were the questions the interviewer asked that made you realise you shouldn't work at this company (or under the interviewer)? (Or do you have any questions to ask that help you decide whether the company/interviewer is good),[],r/datascience,False,6,,0,,,False,t3_ncfk0c,False,dark,0.91,,public,71,0,{},,,False,[],,False,False,,{},Job Search,False,71,,False,False,self,False,,[],{},,True,,1621045946.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,ncfk0c,True,,m_o_n_t_e,,80,True,all_ads,False,[],False,,/r/datascience/comments/ncfk0c/in_your_experience_what_were_the_questions_the/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncfk0c/in_your_experience_what_were_the_questions_the/,515407,1621017146.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi All,

Are there any data scientists here working in manufacturing (food and beverage ideally)?

I'm part of a new startup data science teams and we're looking for good use cases

Thanks all from Dublin, Ireland",t2_1vl22t3k,False,,0,False,Data Science Projects in Manufacturing,[],r/datascience,False,6,projects,0,,,False,t3_nckuwn,False,dark,0.88,,public,28,0,{},,,False,[],,False,False,,{},Projects,False,28,,False,False,self,False,,[],{},,True,,1621060493.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;Are there any data scientists here working in manufacturing (food and beverage ideally)?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m part of a new startup data science teams and we&amp;#39;re looking for good use cases&lt;/p&gt;

&lt;p&gt;Thanks all from Dublin, Ireland&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nckuwn,True,,EoinJFleming,,10,True,all_ads,False,[],False,,/r/datascience/comments/nckuwn/data_science_projects_in_manufacturing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nckuwn/data_science_projects_in_manufacturing/,515407,1621031693.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Let's say I've built a model using past bank data that predicts which customers will apply for an auto loan with us.  I now have a potential email list with our current customer data that I want to check which specific members who are most likely to apply for a loan and a predicted agg count of how many will apply for a loan, what would be the best way to do so? Can I have it read a CSV file or connect to a SQL table to do this?  I've read a little bit about FLASK, but I haven't seen anything like I'm wanting done in flask. Not looking for a step-by-step or for someone to do the work for me, more of a ""yes/no this can be done"" and maybe a link that might assist me. Thank you",t2_38c47u6w,False,,0,False,Best Way to Deploy a Model into Production,[],r/datascience,False,6,projects,0,,,False,t3_ncgqh7,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Projects,False,7,,False,False,self,False,,[],{},,True,,1621049057.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I&amp;#39;ve built a model using past bank data that predicts which customers will apply for an auto loan with us.  I now have a potential email list with our current customer data that I want to check which specific members who are most likely to apply for a loan and a predicted agg count of how many will apply for a loan, what would be the best way to do so? Can I have it read a CSV file or connect to a SQL table to do this?  I&amp;#39;ve read a little bit about FLASK, but I haven&amp;#39;t seen anything like I&amp;#39;m wanting done in flask. Not looking for a step-by-step or for someone to do the work for me, more of a &amp;quot;yes/no this can be done&amp;quot; and maybe a link that might assist me. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncgqh7,True,,beauconstrictor,,5,True,all_ads,False,[],False,,/r/datascience/comments/ncgqh7/best_way_to_deploy_a_model_into_production/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncgqh7/best_way_to_deploy_a_model_into_production/,515407,1621020257.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,,t2_4bix301o,False,,0,False,What are some examples of data science application in Israel Iron Dome technology?,[],r/datascience,False,6,discussion,0,,,False,t3_nd3na2,False,dark,0.16,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621127582.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd3na2,True,,levenshteinn,,7,True,all_ads,False,[],False,,/r/datascience/comments/nd3na2/what_are_some_examples_of_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd3na2/what_are_some_examples_of_data_science/,515407,1621098782.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"\-Also referring to - [https://www.reddit.com/r/datascience/comments/h8bz3b/anyone\_working\_in\_conservation\_wildlife/](https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/)

Anyone working in nature conservation (wildlife, deforestation, ..) AND also working on the field would like to elaborate how much is beneficial to have a background in data-science ?   


Do you use DS frequently, or manage team making use of DS ?  
How much is valuable for engaging in outdoor activities and negotiate opportunities to go on the field for part of the job ?  
What kind of activities do you do ?

\-   

Possibly, I'd like to hear about opportunities of fieldwork in the EU - and possibly, if only volunteering pops up, at least volunteering activities offering full cover of costs and relocation.  


My situation is that I'd like to gain a relevant background for PhD making use of tech (e.g. ML) and in perspective aim for jobs where I can live outdoor part of the time. In order to apply for a PhD, I need to get a MSc first (at least in the EU), and due to my background it may be easier to get a MSc in a tech domain rather than a knowledge-related domain (e.g. biology / natural science / ethology, .., that would require prior related academic background in these disciplines).  


I'd like to hear from other experiences to think about appropriate expectations and tips to consider - in perspective, I don't want to stay 130% of my time in front of a computer, while I find very motivating the possibility to match direct observations and outdoor activities with the part for processing.   


In perspectives, I think tech will open up for new opportunities also in this field, and again I wonder how a tech background would be considered and leveraged VS knowledge domain backgrounds or direct on-field experience.",t2_698l9,False,,0,False,On-the field jobs with DataScience - wildlife / nature conservation ?,[],r/datascience,False,6,career,0,,,False,t3_nc36cc,False,dark,0.79,,public,13,0,{},,,False,[],,False,False,,{},Career,False,13,,False,False,self,False,,[],{},,True,,1621007688.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;-Also referring to - &lt;a href=""https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/""&gt;https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Anyone working in nature conservation (wildlife, deforestation, ..) AND also working on the field would like to elaborate how much is beneficial to have a background in data-science ?   &lt;/p&gt;

&lt;p&gt;Do you use DS frequently, or manage team making use of DS ?&lt;br/&gt;
How much is valuable for engaging in outdoor activities and negotiate opportunities to go on the field for part of the job ?&lt;br/&gt;
What kind of activities do you do ?&lt;/p&gt;

&lt;p&gt;-   &lt;/p&gt;

&lt;p&gt;Possibly, I&amp;#39;d like to hear about opportunities of fieldwork in the EU - and possibly, if only volunteering pops up, at least volunteering activities offering full cover of costs and relocation.  &lt;/p&gt;

&lt;p&gt;My situation is that I&amp;#39;d like to gain a relevant background for PhD making use of tech (e.g. ML) and in perspective aim for jobs where I can live outdoor part of the time. In order to apply for a PhD, I need to get a MSc first (at least in the EU), and due to my background it may be easier to get a MSc in a tech domain rather than a knowledge-related domain (e.g. biology / natural science / ethology, .., that would require prior related academic background in these disciplines).  &lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to hear from other experiences to think about appropriate expectations and tips to consider - in perspective, I don&amp;#39;t want to stay 130% of my time in front of a computer, while I find very motivating the possibility to match direct observations and outdoor activities with the part for processing.   &lt;/p&gt;

&lt;p&gt;In perspectives, I think tech will open up for new opportunities also in this field, and again I wonder how a tech background would be considered and leveraged VS knowledge domain backgrounds or direct on-field experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nc36cc,True,,gg4u,,10,True,all_ads,False,[],False,,/r/datascience/comments/nc36cc/onthe_field_jobs_with_datascience_wildlife_nature/,all_ads,False,https://www.reddit.com/r/datascience/comments/nc36cc/onthe_field_jobs_with_datascience_wildlife_nature/,515407,1620978888.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I know that we are all working remotely to an extent now. But, does anyone here have experience with working on a team which is fully remote?

I am in talk with a recruiter for an exciting position in a fully remote company (well funded startup and recruiter promises a good work-life balance). I have had colleagues in the same location in all the places I worked before- I very much enjoyed the social aspect of office. So being in a fully remote team is something new for me and I am being a bit cautious.",t2_awahof9r,False,,0,False,Anyone has experience on working with ‘Fully Remote Team’ ?,[],r/datascience,False,6,career,0,,,False,t3_nbo4ik,False,dark,0.96,,public,115,1,{},,,False,[],,False,False,,{},Career,False,115,,False,False,self,False,,[],{},,True,,1620960585.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that we are all working remotely to an extent now. But, does anyone here have experience with working on a team which is fully remote?&lt;/p&gt;

&lt;p&gt;I am in talk with a recruiter for an exciting position in a fully remote company (well funded startup and recruiter promises a good work-life balance). I have had colleagues in the same location in all the places I worked before- I very much enjoyed the social aspect of office. So being in a fully remote team is something new for me and I am being a bit cautious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbo4ik,True,,st_pallella,,69,True,all_ads,False,[],False,,/r/datascience/comments/nbo4ik/anyone_has_experience_on_working_with_fully/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbo4ik/anyone_has_experience_on_working_with_fully/,515407,1620931785.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"A client produces 6-12 spreadsheets each quarter.

They have an excel guru put together a big document that visualizes the data. The document contains things like bar charts and descriptive statistics.

I would like to prepare a dashboard application  that is hosted online or could be shared as a standalone application.

The application should be able to accept the data sets as input (ideally, with a drag-and-drop graphical interface), COMBINE the data sets behind the scenes, and produce the necessary data viz.

Goal is to automate the data viz process, and the mechanics should be straightforward since the data sets have the same structure from year to year.

CHALLENGES:

• The data sets are company sensitive. I cannot host a web application on any old web server. I need some contractual guarantee the data isn’t being spied on. My understanding is that most companies have freedom to intercept online info. Maybe then I don’t host the application online at all? An .html file could work.

• Client has restrictions on what kind of software they can install. I may be able to install PowerBI or Tableau on my system, but client may not be able.

I am aware that Tableau can easily visualize data from multiple sources. I.e. it would be simple for ME to organize the spreadsheets, visualize key metrics, and host it online. What I would like is a freestanding application where the CLIENT drags and drops the spreadsheets and an application spits out some visuals.

Thoughts?

Is this too advanced for PowerBI/Tableau? Do I need Shiny/Dash?",t2_8a6ts,False,,0,False,"Can dashboard software (Tableau, PowerBI) help with this business case?",[],r/datascience,False,6,tooling,0,,,False,t3_ncbvv2,False,dark,0.38,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1621036417.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A client produces 6-12 spreadsheets each quarter.&lt;/p&gt;

&lt;p&gt;They have an excel guru put together a big document that visualizes the data. The document contains things like bar charts and descriptive statistics.&lt;/p&gt;

&lt;p&gt;I would like to prepare a dashboard application  that is hosted online or could be shared as a standalone application.&lt;/p&gt;

&lt;p&gt;The application should be able to accept the data sets as input (ideally, with a drag-and-drop graphical interface), COMBINE the data sets behind the scenes, and produce the necessary data viz.&lt;/p&gt;

&lt;p&gt;Goal is to automate the data viz process, and the mechanics should be straightforward since the data sets have the same structure from year to year.&lt;/p&gt;

&lt;p&gt;CHALLENGES:&lt;/p&gt;

&lt;p&gt;• The data sets are company sensitive. I cannot host a web application on any old web server. I need some contractual guarantee the data isn’t being spied on. My understanding is that most companies have freedom to intercept online info. Maybe then I don’t host the application online at all? An .html file could work.&lt;/p&gt;

&lt;p&gt;• Client has restrictions on what kind of software they can install. I may be able to install PowerBI or Tableau on my system, but client may not be able.&lt;/p&gt;

&lt;p&gt;I am aware that Tableau can easily visualize data from multiple sources. I.e. it would be simple for ME to organize the spreadsheets, visualize key metrics, and host it online. What I would like is a freestanding application where the CLIENT drags and drops the spreadsheets and an application spits out some visuals.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;

&lt;p&gt;Is this too advanced for PowerBI/Tableau? Do I need Shiny/Dash?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncbvv2,True,,BrisklyBrusque,,16,True,all_ads,False,[],False,,/r/datascience/comments/ncbvv2/can_dashboard_software_tableau_powerbi_help_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncbvv2/can_dashboard_software_tableau_powerbi_help_with/,515407,1621007617.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_1umdosna,False,,0,False,How does differencing leads to stationarity in time series ?,[],r/datascience,False,6,discussion,0,,,False,t3_nc6jwu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,default,False,,[],{},,False,,1621021190.0,text,6,,,text,self.statistics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nc6jwu,True,,venkarafa,,6,False,all_ads,False,[],False,,/r/datascience/comments/nc6jwu/how_does_differencing_leads_to_stationarity_in/,all_ads,False,/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/,515407,1620992390.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'statistics', 'selftext': 'When we talk about Stationarity of a Time Series, the properties we attribute  are constant mean, constant variance, constant autocorrelation structure and no seasonality. \n\nI came across several articles and books which says differencing makes a series stationary.\n\nThe illustration normally begins with taking an AR(1) model as example\n\n  Yt = ɸYt-1 + ɛt   \n\nAlso it is assumed that the above equation has a unit root i.e., |ɸ| = 1,  \n\nThen they difference the series and represent it as below\n\n   **Δ**Yt = Yt - Yt-1 = ɛt \n\nThey then claim that the error (ɛt) in AR process is White noise. \n\nWhite noise by definition has constant mean, finite and Constant variance and no correlation structure.\n\nThey then go on to take expected value of the error term and say  E(ɛt) = 0, \n\nThus implying that mean is constant.\n\nOk so far we have ticked one of the check boxes to prove it is stationary. The the other check box i.e. to prove constant variance remains unchecked.\n\nThe next part is what is very unclear and often left unproved\n\nThey say the variance of the error term is  Var(ɛt) = σ2 \n\nThen without proving that variance is constant they say differencing has lead to constant mean and variance.\n\nMy question is how can one prove that differencing leads to constant variance ? would be grateful for any explanation.', 'author_fullname': 't2_1umdosna', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Question] How differencing a time series leads to stationarity ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/statistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nc3co8', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1621008435.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.statistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When we talk about Stationarity of a Time Series, the properties we attribute  are constant mean, constant variance, constant autocorrelation structure and no seasonality. &lt;/p&gt;\n\n&lt;p&gt;I came across several articles and books which says differencing makes a series stationary.&lt;/p&gt;\n\n&lt;p&gt;The illustration normally begins with taking an AR(1) model as example&lt;/p&gt;\n\n&lt;p&gt;Yt = ɸYt-1 + ɛt   &lt;/p&gt;\n\n&lt;p&gt;Also it is assumed that the above equation has a unit root i.e., |ɸ| = 1,  &lt;/p&gt;\n\n&lt;p&gt;Then they difference the series and represent it as below&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Δ&lt;/strong&gt;Yt = Yt - Yt-1 = ɛt &lt;/p&gt;\n\n&lt;p&gt;They then claim that the error (ɛt) in AR process is White noise. &lt;/p&gt;\n\n&lt;p&gt;White noise by definition has constant mean, finite and Constant variance and no correlation structure.&lt;/p&gt;\n\n&lt;p&gt;They then go on to take expected value of the error term and say  E(ɛt) = 0, &lt;/p&gt;\n\n&lt;p&gt;Thus implying that mean is constant.&lt;/p&gt;\n\n&lt;p&gt;Ok so far we have ticked one of the check boxes to prove it is stationary. The the other check box i.e. to prove constant variance remains unchecked.&lt;/p&gt;\n\n&lt;p&gt;The next part is what is very unclear and often left unproved&lt;/p&gt;\n\n&lt;p&gt;They say the variance of the error term is  Var(ɛt) = σ2 &lt;/p&gt;\n\n&lt;p&gt;Then without proving that variance is constant they say differencing has lead to constant mean and variance.&lt;/p&gt;\n\n&lt;p&gt;My question is how can one prove that differencing leads to constant variance ? would be grateful for any explanation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nc3co8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'venkarafa', 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/', 'subreddit_subscribers': 310859, 'created_utc': 1620979635.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/,t3_nc3co8,
,datascience,"I have an interview for a Data Analyst position coming up soon and have several questions.   


In the job responsibilities, it mentioned typical responsibilities for a Data Analyst except for 'Predictive Analytics'. Isn't this a Data Scientist's responsibility? Since it would require knowledge a typical Data Analyst would not know. Would it be fair to mention in the interview? The salary range was 60-70k. Assuming I can convince them it's a data scientist position and switch the title, would I be able to negotiate a salary above the range? 

&amp;#x200B;

Thanks in advance!",t2_5t13ti,False,,0,False,Data Scientist responsibilities in a Data Analyst job description?,[],r/datascience,False,6,,0,,,False,t3_nbneik,False,dark,0.87,,public,18,0,{},,,False,[],,False,False,,{},Job Search,False,18,,False,False,self,False,,[],{},,True,,1620958801.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an interview for a Data Analyst position coming up soon and have several questions.   &lt;/p&gt;

&lt;p&gt;In the job responsibilities, it mentioned typical responsibilities for a Data Analyst except for &amp;#39;Predictive Analytics&amp;#39;. Isn&amp;#39;t this a Data Scientist&amp;#39;s responsibility? Since it would require knowledge a typical Data Analyst would not know. Would it be fair to mention in the interview? The salary range was 60-70k. Assuming I can convince them it&amp;#39;s a data scientist position and switch the title, would I be able to negotiate a salary above the range? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nbneik,True,,HongFu_Magic,,22,True,all_ads,False,[],False,,/r/datascience/comments/nbneik/data_scientist_responsibilities_in_a_data_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbneik/data_scientist_responsibilities_in_a_data_analyst/,515407,1620930001.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Many of my Data Science Candidates and Coaching Client's face Imposter syndrome, I compiled some resources on what is Imposter Syndrome, How to recognize and combat it. [Here is a link to the full article with YouTube videos.](https://www.rexrecruiting.com/staffing-recruitment-blogs/imposter-syndrome-what-is-imposter-syndrome-what-can-you-do-about-imposter-syndrome/)

# IMPOSTER SYNDROME

&gt;“It seems like whenever I have a problem and I go to StackExchange, I almost always get a response like  
&gt;  
&gt;“Well obviously you have to pass your indexed features into a Regix 3D optimizer before regressing every i-th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met.”  
&gt;  
&gt;It’s like **where are these guys learning this stuff?” -** [Link](https://www.reddit.com/r/datascience/comments/cnvc3e/does_anyone_else_get_intimidated_by_how_much_you/)

## CHARACTERISTICS OF IMPOSTER SYNDROME

Some of the common signs of imposter syndrome include ([reference](https://so06.tci-thaijo.org/index.php/IJBS/article/view/521/pdf)):

* Self-doubt
* An inability to realistically assess your competence and skills
* Attributing your success to external factors
* Berating your performance
* Fear that you won’t live up to expectations
* Overachieving
* Sabotaging your own success
* Setting incredibly challenging goals and feeling disappointed when you fall short

## WHAT IS IMPOSTER SYNDROME?

[YouTube Video - The Imposter Syndrome](https://youtu.be/eqhUHyVpAwE)

Imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud. It disproportionately affects high-achieving people, who find it difficult to accept their accomplishments. Many Data Scientists question whether they are deserving of accolades, their job, recognition, or the like.

* You do not have enough time to learn something you want to learn.
* You look around and see that there are other people that know that thing you don’t have time to learn.
* You feel incompetent.

Why do so many Data Scientists have it?

Data Science is an extremely broad field of study. There are core competencies required to have a successful career in data science, but there is also a lot of industry specific and technical knowledge that is ever changing.  
Data Science is a career which has many job options, all of which require a high level of expertise and knowledge. If the broad, seemingly confused data science job postings show us anything, it is that many companies do not really understand what a data scientist is, how they compare to a data engineer or software engineer, and how to train or support them within an organization. To add to this, the labor market for data scientists in predominantly new graduated or early career professionals.

When challenge is high, and expectations are unknown it encourages people to fall into high arousal, anxiety, and worry. You can see this from psychologist’s [Mihaly Csikszentmihalyi](https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi) flow model.

These feelings are compounded by a lack of support, feedback, and mentorship provided within a company. This is not generally intentional but a product of small data science departments, business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes.

## HOW CAN DATA SCIENTISTS DEAL WITH IMPOSTER SYNDROME?

[According to the American Psychology Association](https://www.apa.org/gradpsych/2013/11/fraud), If you recognize yourself in the description of the impostor phenomenon, take heart. There are ways to overcome the belief that you don’t measure up.

In a nutshell, there are three ideas that you need to get in your head in order to get over imposter syndrome:

* You are a generally competent person.
* There are always going to be people that know more about a certain area of data science than you and that’s ok and expected. Even more importantly: you’re not the smartest person in the planet, so if you look hard enough, you’re going to find people that are better than you at everything you do and that’s ok.
* You have a finite amount of time to learn things, and your goal shouldn’t be to learn the most, but to learn the things that maximize your specific goals – generally, this is going to be career advancement, but for some it may be something else.

When the Imposter Syndrome feeling comes up:

1. Remind yourself that you are a competent person – if you weren’t, you wouldn’t have gotten to the position you are in right now, whether that’s graduating from college or leading a data science team (yes, even DS team leaders catch the ‘drome from time to time).
2. Remind yourself that when you look for people who know more than you about a specific area, you are guaranteed to find them – that’s just how it works. People choose to specialize in certain areas, and if you only focus on that area of expertise, you are going to feel inadequate. But even more importantly, recognize that if you run into someone who is better than you at literally everything you do, that doesn’t diminish your value – it just means you have run into someone that is pretty special\*
3. Get back to prioritizing what to learn. Do you *need* to learn that or do you just *want* to learn it to feel better about yourself? If the latter, learn to let it go, and focus on the things you need to learn – and save the things you want to learn for when you have the time, which will come.

[u/dfphd – PhD | Head of Data Science &amp; Ecommerce](https://www.reddit.com/r/datascience/comments/m71ijk/imposter_syndrome_and_prioritizing_what_to_learn/)  


[Youtube - What is Imposter Syndrome and How can you  combat it?](https://youtu.be/ZQUxL4Jm1Lo)

### TALK TO YOUR MENTORS.

“The thing that made so much of a difference was supportive, encouraging supervision”.

Many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational. Though many will often struggle with these feelings, you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals.

### RECOGNIZE YOUR EXPERTISE.

Don’t just look to those who are more experienced, more popular, or more successful for help. Tutoring or working with younger students, for instance, can help you realize how far you’ve come and how much knowledge you have to impart. This can be a great way for a Data Scientist to give back to the industry as well as set a more realistic benchmark of your perceived value.

### REMEMBER WHAT YOU DO WELL.

Psychologists Suzanne Imes, PhD, and Pauline Rose Clance, PhD, in the 1970s, impostor phenomenon occurs among high achievers who are unable to internalize and accept their success.

Imes encourages her clients to make a realistic assessment of their abilities. “Most high achievers are pretty smart people, and many really smart people wish they were geniuses. But most of us aren’t,” she says. “We have areas where we’re quite smart and areas where we’re not so smart.” She suggests writing down the things you’re truly good at, and the areas that might need work. That can help you recognize where you’re doing well, and where there’s legitimate room for improvement.

## REALIZE NO ONE IS PERFECT.

Clance urges people with impostor feelings to stop focusing on perfection. “Do a task ‘well enough,'” she says. It’s also important to take time to appreciate the fruits of your hard work. “Develop and implement rewards for success — learn to celebrate,” she adds.

### CHANGE YOUR THINKING.

&gt;“let the challenge excite you rather than overwhelm you.”

People with impostor feelings must reframe the way they think about their achievements, says Imes. She helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle. That has best done incrementally, she says. For instance, rather than spending 10 hours on an assignment, you might cut yourself off at eight. Or you may let a friend read a draft that you haven’t yet perfectly polished. “Superstitions need to be changed very gradually because they are so strong,” she says.

Avoid all or nothing thinking. Just like a standard distribution, most Data Scientists fall within the center. If you find yourself comparing to outliers, then you are going to continue to feel like a fraud, which will in return stifle your career in data science.  


[YouTube - How you can use imposter syndrome to your benefit - Mike Cannon-Brookes](https://www.youtube.com/watch?v=ZkwqZfvbdFw&amp;ab_channel=TED)

### TALK TO SOMEONE WHO CAN HELP.

For many people with impostor feelings, individual therapy can be extremely helpful. A psychologist or other therapist can give you tools to help you break the cycle of impostor thinking.

The impostor phenomenon is still an experience that tends to fly under the radar. Often the people affected by impostor feelings don’t realize they could be living some other way. They don’t have any idea it’s possible not to feel so anxious and fearful all the time.",t2_16sq47,False,,0,False,In the spirit of Mental Health Month - Imposter Syndrome,[],r/datascience,False,6,discussion,0,,,False,t3_nauc4w,False,dark,0.98,,public,451,10,{},,,False,[],,False,False,,{},Discussion,False,451,,False,False,self,False,,[],{'gid_1': 3},,True,,1620869373.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of my Data Science Candidates and Coaching Client&amp;#39;s face Imposter syndrome, I compiled some resources on what is Imposter Syndrome, How to recognize and combat it. &lt;a href=""https://www.rexrecruiting.com/staffing-recruitment-blogs/imposter-syndrome-what-is-imposter-syndrome-what-can-you-do-about-imposter-syndrome/""&gt;Here is a link to the full article with YouTube videos.&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;IMPOSTER SYNDROME&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“It seems like whenever I have a problem and I go to StackExchange, I almost always get a response like  &lt;/p&gt;

&lt;p&gt;“Well obviously you have to pass your indexed features into a Regix 3D optimizer before regressing every i-th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met.”  &lt;/p&gt;

&lt;p&gt;It’s like &lt;strong&gt;where are these guys learning this stuff?” -&lt;/strong&gt; &lt;a href=""https://www.reddit.com/r/datascience/comments/cnvc3e/does_anyone_else_get_intimidated_by_how_much_you/""&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;CHARACTERISTICS OF IMPOSTER SYNDROME&lt;/h2&gt;

&lt;p&gt;Some of the common signs of imposter syndrome include (&lt;a href=""https://so06.tci-thaijo.org/index.php/IJBS/article/view/521/pdf""&gt;reference&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Self-doubt&lt;/li&gt;
&lt;li&gt;An inability to realistically assess your competence and skills&lt;/li&gt;
&lt;li&gt;Attributing your success to external factors&lt;/li&gt;
&lt;li&gt;Berating your performance&lt;/li&gt;
&lt;li&gt;Fear that you won’t live up to expectations&lt;/li&gt;
&lt;li&gt;Overachieving&lt;/li&gt;
&lt;li&gt;Sabotaging your own success&lt;/li&gt;
&lt;li&gt;Setting incredibly challenging goals and feeling disappointed when you fall short&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;WHAT IS IMPOSTER SYNDROME?&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/eqhUHyVpAwE""&gt;YouTube Video - The Imposter Syndrome&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud. It disproportionately affects high-achieving people, who find it difficult to accept their accomplishments. Many Data Scientists question whether they are deserving of accolades, their job, recognition, or the like.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You do not have enough time to learn something you want to learn.&lt;/li&gt;
&lt;li&gt;You look around and see that there are other people that know that thing you don’t have time to learn.&lt;/li&gt;
&lt;li&gt;You feel incompetent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why do so many Data Scientists have it?&lt;/p&gt;

&lt;p&gt;Data Science is an extremely broad field of study. There are core competencies required to have a successful career in data science, but there is also a lot of industry specific and technical knowledge that is ever changing.&lt;br/&gt;
Data Science is a career which has many job options, all of which require a high level of expertise and knowledge. If the broad, seemingly confused data science job postings show us anything, it is that many companies do not really understand what a data scientist is, how they compare to a data engineer or software engineer, and how to train or support them within an organization. To add to this, the labor market for data scientists in predominantly new graduated or early career professionals.&lt;/p&gt;

&lt;p&gt;When challenge is high, and expectations are unknown it encourages people to fall into high arousal, anxiety, and worry. You can see this from psychologist’s &lt;a href=""https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi""&gt;Mihaly Csikszentmihalyi&lt;/a&gt; flow model.&lt;/p&gt;

&lt;p&gt;These feelings are compounded by a lack of support, feedback, and mentorship provided within a company. This is not generally intentional but a product of small data science departments, business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes.&lt;/p&gt;

&lt;h2&gt;HOW CAN DATA SCIENTISTS DEAL WITH IMPOSTER SYNDROME?&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://www.apa.org/gradpsych/2013/11/fraud""&gt;According to the American Psychology Association&lt;/a&gt;, If you recognize yourself in the description of the impostor phenomenon, take heart. There are ways to overcome the belief that you don’t measure up.&lt;/p&gt;

&lt;p&gt;In a nutshell, there are three ideas that you need to get in your head in order to get over imposter syndrome:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are a generally competent person.&lt;/li&gt;
&lt;li&gt;There are always going to be people that know more about a certain area of data science than you and that’s ok and expected. Even more importantly: you’re not the smartest person in the planet, so if you look hard enough, you’re going to find people that are better than you at everything you do and that’s ok.&lt;/li&gt;
&lt;li&gt;You have a finite amount of time to learn things, and your goal shouldn’t be to learn the most, but to learn the things that maximize your specific goals – generally, this is going to be career advancement, but for some it may be something else.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the Imposter Syndrome feeling comes up:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remind yourself that you are a competent person – if you weren’t, you wouldn’t have gotten to the position you are in right now, whether that’s graduating from college or leading a data science team (yes, even DS team leaders catch the ‘drome from time to time).&lt;/li&gt;
&lt;li&gt;Remind yourself that when you look for people who know more than you about a specific area, you are guaranteed to find them – that’s just how it works. People choose to specialize in certain areas, and if you only focus on that area of expertise, you are going to feel inadequate. But even more importantly, recognize that if you run into someone who is better than you at literally everything you do, that doesn’t diminish your value – it just means you have run into someone that is pretty special*&lt;/li&gt;
&lt;li&gt;Get back to prioritizing what to learn. Do you &lt;em&gt;need&lt;/em&gt; to learn that or do you just &lt;em&gt;want&lt;/em&gt; to learn it to feel better about yourself? If the latter, learn to let it go, and focus on the things you need to learn – and save the things you want to learn for when you have the time, which will come.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/r/datascience/comments/m71ijk/imposter_syndrome_and_prioritizing_what_to_learn/""&gt;u/dfphd – PhD | Head of Data Science &amp;amp; Ecommerce&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/ZQUxL4Jm1Lo""&gt;Youtube - What is Imposter Syndrome and How can you  combat it?&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;TALK TO YOUR MENTORS.&lt;/h3&gt;

&lt;p&gt;“The thing that made so much of a difference was supportive, encouraging supervision”.&lt;/p&gt;

&lt;p&gt;Many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational. Though many will often struggle with these feelings, you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals.&lt;/p&gt;

&lt;h3&gt;RECOGNIZE YOUR EXPERTISE.&lt;/h3&gt;

&lt;p&gt;Don’t just look to those who are more experienced, more popular, or more successful for help. Tutoring or working with younger students, for instance, can help you realize how far you’ve come and how much knowledge you have to impart. This can be a great way for a Data Scientist to give back to the industry as well as set a more realistic benchmark of your perceived value.&lt;/p&gt;

&lt;h3&gt;REMEMBER WHAT YOU DO WELL.&lt;/h3&gt;

&lt;p&gt;Psychologists Suzanne Imes, PhD, and Pauline Rose Clance, PhD, in the 1970s, impostor phenomenon occurs among high achievers who are unable to internalize and accept their success.&lt;/p&gt;

&lt;p&gt;Imes encourages her clients to make a realistic assessment of their abilities. “Most high achievers are pretty smart people, and many really smart people wish they were geniuses. But most of us aren’t,” she says. “We have areas where we’re quite smart and areas where we’re not so smart.” She suggests writing down the things you’re truly good at, and the areas that might need work. That can help you recognize where you’re doing well, and where there’s legitimate room for improvement.&lt;/p&gt;

&lt;h2&gt;REALIZE NO ONE IS PERFECT.&lt;/h2&gt;

&lt;p&gt;Clance urges people with impostor feelings to stop focusing on perfection. “Do a task ‘well enough,&amp;#39;” she says. It’s also important to take time to appreciate the fruits of your hard work. “Develop and implement rewards for success — learn to celebrate,” she adds.&lt;/p&gt;

&lt;h3&gt;CHANGE YOUR THINKING.&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;“let the challenge excite you rather than overwhelm you.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;People with impostor feelings must reframe the way they think about their achievements, says Imes. She helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle. That has best done incrementally, she says. For instance, rather than spending 10 hours on an assignment, you might cut yourself off at eight. Or you may let a friend read a draft that you haven’t yet perfectly polished. “Superstitions need to be changed very gradually because they are so strong,” she says.&lt;/p&gt;

&lt;p&gt;Avoid all or nothing thinking. Just like a standard distribution, most Data Scientists fall within the center. If you find yourself comparing to outliers, then you are going to continue to feel like a fraud, which will in return stifle your career in data science.  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=ZkwqZfvbdFw&amp;amp;ab_channel=TED""&gt;YouTube - How you can use imposter syndrome to your benefit - Mike Cannon-Brookes&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;TALK TO SOMEONE WHO CAN HELP.&lt;/h3&gt;

&lt;p&gt;For many people with impostor feelings, individual therapy can be extremely helpful. A psychologist or other therapist can give you tools to help you break the cycle of impostor thinking.&lt;/p&gt;

&lt;p&gt;The impostor phenomenon is still an experience that tends to fly under the radar. Often the people affected by impostor feelings don’t realize they could be living some other way. They don’t have any idea it’s possible not to feel so anxious and fearful all the time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_beccaae0-d745-44f9-bc5c-3c9f8117699b', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""No matter how hard I try, I'm too shy to confess my love!"", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'I Shy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=16&amp;height=16&amp;auto=webp&amp;s=ef2899c48784a11826c15c0d93e44adf63f49b39', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=32&amp;height=32&amp;auto=webp&amp;s=d5ac1c72de38a183f491506658c8939116242abc', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=48&amp;height=48&amp;auto=webp&amp;s=2b7a39097b30d524faffd98bcef5df43c9d22011', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=64&amp;height=64&amp;auto=webp&amp;s=02dc06d251a3d96f9ef4c1561c05d9008df2920b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=128&amp;height=128&amp;auto=webp&amp;s=f4591ce312385592dde56dc6077c55cf2393f8fd', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nauc4w,True,,RexRecruiting,,45,True,all_ads,False,[],False,,/r/datascience/comments/nauc4w/in_the_spirit_of_mental_health_month_imposter/,all_ads,False,https://www.reddit.com/r/datascience/comments/nauc4w/in_the_spirit_of_mental_health_month_imposter/,515407,1620840573.0,2,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?auto=webp&amp;s=b32e93c31050224cd1bb52e1c8e97daf191dd240', 'width': 720, 'height': 460}, 'resolutions': [{'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74951330ef876d7f8133da781c52e5b501269866', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c197ccc83b6567ba2f97961175055d1838f9776', 'width': 216, 'height': 138}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=221b1b517ab5da9a1910251a94835e82d41209e5', 'width': 320, 'height': 204}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=770d0dd84d645ad7b16c9ddb766feb3e3a975bff', 'width': 640, 'height': 408}], 'variants': {}, 'id': 'WZY2EB3c7gM2-edTy62h9KaszfzuHM0wlYfe11DJ_qs'}], 'enabled': False}",,,,,
,datascience,"I'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I'm going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.",t2_2dwso7l3,False,,0,False,Sentiment Analysis Recommendations on Review data,[],r/datascience,False,6,discussion,0,,,False,t3_nbj29s,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1620947960.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I&amp;#39;m going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbj29s,True,,Jaypal17,,10,True,all_ads,False,[],False,,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,515407,1620919160.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," I've got a relatively big dataset (8 GB) and pandas crashes when trying to load it into a dataframe. I've tried modin and pyspark, all with no luck. Are there any Python packages that can work with big data? Currently the data is stored in SQL.

I'm running this on a company VM which has 16GB RAM I believe.",t2_8g8aebq1,False,,0,False,Are there any Python packages that can work with big data?,[],r/datascience,False,6,projects,0,,,False,t3_nbjbv6,False,dark,0.57,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,1620937256.0,,[],{},,True,,1620948644.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got a relatively big dataset (8 GB) and pandas crashes when trying to load it into a dataframe. I&amp;#39;ve tried modin and pyspark, all with no luck. Are there any Python packages that can work with big data? Currently the data is stored in SQL.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m running this on a company VM which has 16GB RAM I believe.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbjbv6,True,,karthur4,,27,True,all_ads,False,[],False,,/r/datascience/comments/nbjbv6/are_there_any_python_packages_that_can_work_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbjbv6/are_there_any_python_packages_that_can_work_with/,515407,1620919844.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Do you guys have any idea? Sklearn doenst have a built in library to do that

edit: i mean inhomogeneous INTERNAL densities",t2_15fsjo,False,,0,False,How can i create a dataset featuring clusters with inhomogeneous densities with python?,[],r/datascience,False,6,discussion,0,,,False,t3_nbk99e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1620951046.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you guys have any idea? Sklearn doenst have a built in library to do that&lt;/p&gt;

&lt;p&gt;edit: i mean inhomogeneous INTERNAL densities&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbk99e,True,,gng3quionbve4,,5,True,all_ads,False,[],False,,/r/datascience/comments/nbk99e/how_can_i_create_a_dataset_featuring_clusters/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbk99e/how_can_i_create_a_dataset_featuring_clusters/,515407,1620922246.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello,

I'm new and working on a logistic regression model. I have two variables that are (what I would consider) abnormally large at 10.2 and 30.7. I read online that it's usually a bad sign if this is the case and that this may signal multicollinearity. However, the VIF Factor on these is 1.2 (for both) under the 5.0 that I've read signals multicollinearity. Is it OK to leave these in? Should I remove them? Thanks.",t2_38c47u6w,False,,0,False,Is it OK to have a coefficient greater than 1?,[],r/datascience,False,6,projects,0,,,False,t3_nbirwr,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1620947214.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new and working on a logistic regression model. I have two variables that are (what I would consider) abnormally large at 10.2 and 30.7. I read online that it&amp;#39;s usually a bad sign if this is the case and that this may signal multicollinearity. However, the VIF Factor on these is 1.2 (for both) under the 5.0 that I&amp;#39;ve read signals multicollinearity. Is it OK to leave these in? Should I remove them? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbirwr,True,,beauconstrictor,,5,True,all_ads,False,[],False,,/r/datascience/comments/nbirwr/is_it_ok_to_have_a_coefficient_greater_than_1/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbirwr/is_it_ok_to_have_a_coefficient_greater_than_1/,515407,1620918414.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hello all,

**From 2019 till today (May 2021), I've submitted roughly 550+ applications for various tech roles in California:**

* Software engineering internships
* Data Science / Data analytics internships
* Machine learning / Deep Learning / Computer visions internships
* Entry-level DS / SWE positions (non-internships)

My background: 3.0/4.0 GPA recent graduate with a bachelor's in physics (a top 10 public school) and a background in data science. 

***Disclaimer***: I have never interviewed with a FAANG company so this certainly is not a guide to landing your dream FAANG job. 

As a non-CS major with a mediocre academic standing, **I hope some of you guys will be able to relate to or learn from my intern searching experience!**

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Starting off: The resume (for beginners)**

In my 2nd year of university, I started taking interest in data science and signed up for some online courses. After 3 months, I quickly realized that my resume was lacking in work experience, skills, and just an overall description of my career objectives. Moreover, my degree in physics was worrying: how can I compete with undergraduate students studying CS or Data science?

The solution: **Open source data projects**.

This is key to kicking off your data science resume-building journey and in my opinion, the most efficient way to learn data science. Learn a programming language for data science: *R or Python*, and stick with it for at least a year. You should start with:

1) Basic programming syntax, coding logic, data structures

2) Data cleaning and wrangling

3) Data Visualization

4) Basics of statistical analysis

5) Linear Algebra 

6) Regression modeling (*more advanced, but a great way to start learning Machine Learning*)

I found DataQuest to be very digestible and enjoyable in the early stages of learning data science. There are also tons of other free resources that you can use to practice and learn your basic data wrangling skills (Kaggle, UC Irvine's Repository, [https://www.analyticsvidhya.com/](https://www.analyticsvidhya.com/)).

In a few months, try to formulate a project idea that can be solved with data science. I wanted to analyze/visualize time-series data for a company's drone flights and luckily, they were kind enough to send me some data files to work with. Using python to do some cleaning and visualizations, I discovered that drones were able to fly further in a specific air temperature range. An engineering team could potentially use this information to implement a cooling system in their drones to maintain internal temperature and improve flight efficiency!

Even if your project doesn't feel ""insightful"" or ""creative"", try to stick with it for the sake of learning! Even if you don't make an astonishing finding, you will develop very strong DS fundamentals through these data exploration processes. 

**TLDR: If you feel that your resume is lacking experience, work on data science projects. The more unique and personal, the better. These will become great talking points in your interviews.**

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Building your experience: Networking &amp; Research (for those lacking experience)**

By 2019 (3rd-year undergrad), my resume contained 2 simple EDA projects (Exploratory Data Analysis) and I felt ready to start applying to internships. For the next 365 days, I applied to 207 positions. During this time, I received 6 interviews, and 1 offer for an unpaid data science position (remote). 

With a **3.0% hear back rate**, I felt very discouraged. The one offer I got was hardly an internship. This percentage told me that my resume and skills were probably not competitive enough, which drives me to my next point: **take advantage of your academic resources while you are in school.**

I did not attend any career fairs while I was in university (shame on me) but to compensate for this, I reached out to several professors in my department, offering to do research work for them for FREE...up to 20 hours a week. Thanks to a colleague, I landed a data analyst researcher role for my university in the summer of 2020. During this time, I picked up some shell scripting, real-time programming, and data modeling skills. 

I know unpaid research is not as flashy or alluring as a Google internship, but trust me, anything that adds 2-6 months of data science experience to your resume is HUGE.

Regardless, I wished I had focused on networking more when I was still in university. Would you rather spent 6 months applying to 200 positions or immediately land several interviews in a matter of weeks due to connections? Yep, I would have preferred the latter. 

**TDLR: Reach out to professors, colleagues, or even cold email recruiters. Be resourceful.**

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**General Interview/Job-searching Tips** 

* If you did poorly on an interview or got turned down, **learn from it.** You should quickly discover what you are lacking after several rounds:
   * Is my coding logic poor or rusty? Practice more Leetcode while talking out loud.
   * Is my skill set not desirable enough? Research valuable DS skills for the current year.
   * What sort of skills are desired in this specific industry? Tailor your resume accordingly.
   * Stuttering or awkwardness? Record yourself or practice behavioral questions with a friend.
* Know your script: your resume, projects, experience, strong points, weak points.
   * While I do have a google doc containing scripts for what I should say, I never actually read from it. An interview script should be more of a rough blueprint for rehearsing and practicing.
* Try to talk about something personal or outside of the job. Don't force it, but if can laugh with your interviewer + connect on a more personal level, you will leave a strong impression.
* Tailor your resume to what you are applying to:
   * If you are targeting computer vision roles, orient your projects around that
   * If you are focused on machine learning roles, your resume should reflect expertise in modeling and training/prediction techniques
* An interview is mutual. After 20 rejections, I know it is easy to place immense pressure on yourself to land that dream internship. Remind yourself that you are interviewing the company as well. Embracing this mindset will turn your interviews into meaningful conversations, which is what you want.
* Ask good questions. If you feel that the bulk of your interview was not excellent, try finishing strong by asking great questions. Some examples:
   * What are some of the biggest challenges you've faced in your role?
   * A favorite project that you've worked on?
   * Where do you hope to see your data science department in X-Y years?
   * How is your company utilizing machine learning techniques to improve sales/customer retention?
* Do some brief research on LinkedIn before you meet with your interviewers:
   * Might find something in common or help you come up with good questions
   * It's also a good idea to read into the company, their products, how they generate their $$$
* You will likely hear back from companies in LA/Irvine/OC more often than ones in the bay area. 
   * 63% of my hear backs (interviews) were from companies located in southern California. 

**\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_**

**My job hunt statistics**

\[***Rough breakdown\]***

* **50% of applied jobs were related to data internships (DS, DA, ML, Deep learning, Data Eng)**
* **25% of applied jobs were related to software internships (front-end, back-end, etc)**
* **15% of applied jobs were related to full-time entry-level positions in Data Analytics / DS**
* **10% of applied jobs were related to marketing analyst, business analyst, or other non-tech analyst positions**

&amp;#8203;

    Year	    2019    2020    2021    Total
    Apps	     207      57     257      516
    Interviews    6	       0       5       11
    Offers	      1	       0       2	3

My three offers consisted of 1 remote, unpaid, part-time data science internship, 1 digital marketing analyst position, and 1 paid full-time data science internship (currently pursuing). 

**TDLR: DON'T GIVE UP. This field is competitive but through sheer numbers, extraordinary connections, interview skills, or luck, you will find a suitable internship. I believe in you!**

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Resources**

* My favorite job searching platforms:
   * LinkedIn
   * Indeed
   * Glassdoor
* My favorite data science resources:
   * Kaggle
   * UCI repo
   * Tech with Tim (Youtube channel)
   * 3Blue1Brown (Youtube channel)
   * Amazon's datasets
   * [https://www.analyticsvidhya.com/](https://www.analyticsvidhya.com/)
   * Cracking the coding interview (a painful book to grind through, but it is very helpful)
   * O'Reilly's Hands-on Machine Learning with Sk-Learn, Keras, and TensorFlow (2nd Ed.)
      * My favorite book!!!

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Reach out to me**

In 12 months, I hope to be enrolled in a graduate program for data science. I am no expert in data science, and I still have a lot to learn. Nonetheless, the past 2 years have taught me a lot about the data science job market. If you any questions or a strong desire to talk about data science, please feel free to reach out to me. I am happy to share my project source code, resume, and additional tips upon request.",t2_ktt7wn2,False,,0,False,Some beginner-friendly tips on landing data science internships from a recent college grad,[],r/datascience,False,6,,0,,,False,t3_nb2fdc,False,dark,0.82,,public,14,1,{},,,True,[],,False,False,,{},Job Search,False,14,,False,False,self,False,,[],{},,True,,1620890440.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;From 2019 till today (May 2021), I&amp;#39;ve submitted roughly 550+ applications for various tech roles in California:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Software engineering internships&lt;/li&gt;
&lt;li&gt;Data Science / Data analytics internships&lt;/li&gt;
&lt;li&gt;Machine learning / Deep Learning / Computer visions internships&lt;/li&gt;
&lt;li&gt;Entry-level DS / SWE positions (non-internships)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My background: 3.0/4.0 GPA recent graduate with a bachelor&amp;#39;s in physics (a top 10 public school) and a background in data science. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/strong&gt;: I have never interviewed with a FAANG company so this certainly is not a guide to landing your dream FAANG job. &lt;/p&gt;

&lt;p&gt;As a non-CS major with a mediocre academic standing, &lt;strong&gt;I hope some of you guys will be able to relate to or learn from my intern searching experience!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;_________________________________________________________________________________________________________&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Starting off: The resume (for beginners)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In my 2nd year of university, I started taking interest in data science and signed up for some online courses. After 3 months, I quickly realized that my resume was lacking in work experience, skills, and just an overall description of my career objectives. Moreover, my degree in physics was worrying: how can I compete with undergraduate students studying CS or Data science?&lt;/p&gt;

&lt;p&gt;The solution: &lt;strong&gt;Open source data projects&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is key to kicking off your data science resume-building journey and in my opinion, the most efficient way to learn data science. Learn a programming language for data science: &lt;em&gt;R or Python&lt;/em&gt;, and stick with it for at least a year. You should start with:&lt;/p&gt;

&lt;p&gt;1) Basic programming syntax, coding logic, data structures&lt;/p&gt;

&lt;p&gt;2) Data cleaning and wrangling&lt;/p&gt;

&lt;p&gt;3) Data Visualization&lt;/p&gt;

&lt;p&gt;4) Basics of statistical analysis&lt;/p&gt;

&lt;p&gt;5) Linear Algebra &lt;/p&gt;

&lt;p&gt;6) Regression modeling (&lt;em&gt;more advanced, but a great way to start learning Machine Learning&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;I found DataQuest to be very digestible and enjoyable in the early stages of learning data science. There are also tons of other free resources that you can use to practice and learn your basic data wrangling skills (Kaggle, UC Irvine&amp;#39;s Repository, &lt;a href=""https://www.analyticsvidhya.com/""&gt;https://www.analyticsvidhya.com/&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In a few months, try to formulate a project idea that can be solved with data science. I wanted to analyze/visualize time-series data for a company&amp;#39;s drone flights and luckily, they were kind enough to send me some data files to work with. Using python to do some cleaning and visualizations, I discovered that drones were able to fly further in a specific air temperature range. An engineering team could potentially use this information to implement a cooling system in their drones to maintain internal temperature and improve flight efficiency!&lt;/p&gt;

&lt;p&gt;Even if your project doesn&amp;#39;t feel &amp;quot;insightful&amp;quot; or &amp;quot;creative&amp;quot;, try to stick with it for the sake of learning! Even if you don&amp;#39;t make an astonishing finding, you will develop very strong DS fundamentals through these data exploration processes. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TLDR: If you feel that your resume is lacking experience, work on data science projects. The more unique and personal, the better. These will become great talking points in your interviews.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;___________________________________________________________________________________________________________&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building your experience: Networking &amp;amp; Research (for those lacking experience)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By 2019 (3rd-year undergrad), my resume contained 2 simple EDA projects (Exploratory Data Analysis) and I felt ready to start applying to internships. For the next 365 days, I applied to 207 positions. During this time, I received 6 interviews, and 1 offer for an unpaid data science position (remote). &lt;/p&gt;

&lt;p&gt;With a &lt;strong&gt;3.0% hear back rate&lt;/strong&gt;, I felt very discouraged. The one offer I got was hardly an internship. This percentage told me that my resume and skills were probably not competitive enough, which drives me to my next point: &lt;strong&gt;take advantage of your academic resources while you are in school.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I did not attend any career fairs while I was in university (shame on me) but to compensate for this, I reached out to several professors in my department, offering to do research work for them for FREE...up to 20 hours a week. Thanks to a colleague, I landed a data analyst researcher role for my university in the summer of 2020. During this time, I picked up some shell scripting, real-time programming, and data modeling skills. &lt;/p&gt;

&lt;p&gt;I know unpaid research is not as flashy or alluring as a Google internship, but trust me, anything that adds 2-6 months of data science experience to your resume is HUGE.&lt;/p&gt;

&lt;p&gt;Regardless, I wished I had focused on networking more when I was still in university. Would you rather spent 6 months applying to 200 positions or immediately land several interviews in a matter of weeks due to connections? Yep, I would have preferred the latter. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TDLR: Reach out to professors, colleagues, or even cold email recruiters. Be resourceful.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;___________________________________________________________________________________________________________&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;General Interview/Job-searching Tips&lt;/strong&gt; &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you did poorly on an interview or got turned down, &lt;strong&gt;learn from it.&lt;/strong&gt; You should quickly discover what you are lacking after several rounds:

&lt;ul&gt;
&lt;li&gt;Is my coding logic poor or rusty? Practice more Leetcode while talking out loud.&lt;/li&gt;
&lt;li&gt;Is my skill set not desirable enough? Research valuable DS skills for the current year.&lt;/li&gt;
&lt;li&gt;What sort of skills are desired in this specific industry? Tailor your resume accordingly.&lt;/li&gt;
&lt;li&gt;Stuttering or awkwardness? Record yourself or practice behavioral questions with a friend.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Know your script: your resume, projects, experience, strong points, weak points.

&lt;ul&gt;
&lt;li&gt;While I do have a google doc containing scripts for what I should say, I never actually read from it. An interview script should be more of a rough blueprint for rehearsing and practicing.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Try to talk about something personal or outside of the job. Don&amp;#39;t force it, but if can laugh with your interviewer + connect on a more personal level, you will leave a strong impression.&lt;/li&gt;
&lt;li&gt;Tailor your resume to what you are applying to:

&lt;ul&gt;
&lt;li&gt;If you are targeting computer vision roles, orient your projects around that&lt;/li&gt;
&lt;li&gt;If you are focused on machine learning roles, your resume should reflect expertise in modeling and training/prediction techniques&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;An interview is mutual. After 20 rejections, I know it is easy to place immense pressure on yourself to land that dream internship. Remind yourself that you are interviewing the company as well. Embracing this mindset will turn your interviews into meaningful conversations, which is what you want.&lt;/li&gt;
&lt;li&gt;Ask good questions. If you feel that the bulk of your interview was not excellent, try finishing strong by asking great questions. Some examples:

&lt;ul&gt;
&lt;li&gt;What are some of the biggest challenges you&amp;#39;ve faced in your role?&lt;/li&gt;
&lt;li&gt;A favorite project that you&amp;#39;ve worked on?&lt;/li&gt;
&lt;li&gt;Where do you hope to see your data science department in X-Y years?&lt;/li&gt;
&lt;li&gt;How is your company utilizing machine learning techniques to improve sales/customer retention?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Do some brief research on LinkedIn before you meet with your interviewers:

&lt;ul&gt;
&lt;li&gt;Might find something in common or help you come up with good questions&lt;/li&gt;
&lt;li&gt;It&amp;#39;s also a good idea to read into the company, their products, how they generate their $$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;You will likely hear back from companies in LA/Irvine/OC more often than ones in the bay area. 

&lt;ul&gt;
&lt;li&gt;63% of my hear backs (interviews) were from companies located in southern California. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;___________________________________________________________________________________________________________________&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My job hunt statistics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;&lt;em&gt;Rough breakdown]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;50% of applied jobs were related to data internships (DS, DA, ML, Deep learning, Data Eng)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;25% of applied jobs were related to software internships (front-end, back-end, etc)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;15% of applied jobs were related to full-time entry-level positions in Data Analytics / DS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10% of applied jobs were related to marketing analyst, business analyst, or other non-tech analyst positions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#8203;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Year        2019    2020    2021    Total
Apps         207      57     257      516
Interviews    6        0       5       11
Offers        1        0       2    3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My three offers consisted of 1 remote, unpaid, part-time data science internship, 1 digital marketing analyst position, and 1 paid full-time data science internship (currently pursuing). &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TDLR: DON&amp;#39;T GIVE UP. This field is competitive but through sheer numbers, extraordinary connections, interview skills, or luck, you will find a suitable internship. I believe in you!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;__________________________________________________________________________________________________________&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;My favorite job searching platforms:

&lt;ul&gt;
&lt;li&gt;LinkedIn&lt;/li&gt;
&lt;li&gt;Indeed&lt;/li&gt;
&lt;li&gt;Glassdoor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;My favorite data science resources:

&lt;ul&gt;
&lt;li&gt;Kaggle&lt;/li&gt;
&lt;li&gt;UCI repo&lt;/li&gt;
&lt;li&gt;Tech with Tim (Youtube channel)&lt;/li&gt;
&lt;li&gt;3Blue1Brown (Youtube channel)&lt;/li&gt;
&lt;li&gt;Amazon&amp;#39;s datasets&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.analyticsvidhya.com/""&gt;https://www.analyticsvidhya.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cracking the coding interview (a painful book to grind through, but it is very helpful)&lt;/li&gt;
&lt;li&gt;O&amp;#39;Reilly&amp;#39;s Hands-on Machine Learning with Sk-Learn, Keras, and TensorFlow (2nd Ed.)

&lt;ul&gt;
&lt;li&gt;My favorite book!!!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;____________________________________________________________________________________________________________&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reach out to me&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In 12 months, I hope to be enrolled in a graduate program for data science. I am no expert in data science, and I still have a lot to learn. Nonetheless, the past 2 years have taught me a lot about the data science job market. If you any questions or a strong desire to talk about data science, please feel free to reach out to me. I am happy to share my project source code, resume, and additional tips upon request.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nb2fdc,True,,LeviMeep,,0,True,all_ads,False,[],False,,/r/datascience/comments/nb2fdc/some_beginnerfriendly_tips_on_landing_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nb2fdc/some_beginnerfriendly_tips_on_landing_data/,515407,1620861640.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,Is there a data science discord? Would be nice to have one :),t2_omc58,False,,0,False,R/Datascience Discord,[],r/datascience,False,6,network,0,,,False,t3_nb8m31,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Networking,False,3,,False,False,self,False,,[],{},,True,,1620909783.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a data science discord? Would be nice to have one :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nb8m31,True,,Takafraka,,5,True,all_ads,False,[],False,,/r/datascience/comments/nb8m31/rdatascience_discord/,all_ads,False,https://www.reddit.com/r/datascience/comments/nb8m31/rdatascience_discord/,515407,1620880983.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I don’t want to say much more than that but I was at an online conference a few days ago, and in the chat I said I was totally stoked to be thinking about data analytics all day, and they blocked me from the chat because they thought I was sarcastic.

I just love me some data is all.  So I reposted saying that I meant it, and then the organiser thanked me.

I am just too hard core. Data data data.  Nom nom nom.",t2_8z0in2jy,False,,0,False,I got blocked from an online data conference for loving data too much,[],r/datascience,False,6,fun,0,,,False,t3_nbspk6,False,dark,0.29,,public,0,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,0,,False,False,self,False,,[],{},,True,,1620972309.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I don’t want to say much more than that but I was at an online conference a few days ago, and in the chat I said I was totally stoked to be thinking about data analytics all day, and they blocked me from the chat because they thought I was sarcastic.&lt;/p&gt;

&lt;p&gt;I just love me some data is all.  So I reposted saying that I meant it, and then the organiser thanked me.&lt;/p&gt;

&lt;p&gt;I am just too hard core. Data data data.  Nom nom nom.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbspk6,True,,Wu_Fan,,8,True,all_ads,False,[],False,,/r/datascience/comments/nbspk6/i_got_blocked_from_an_online_data_conference_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbspk6/i_got_blocked_from_an_online_data_conference_for/,515407,1620943509.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,"What is it with data scientists and being snobs about Excel? It's a great tool if one wants to get the texture of a data, build simple one-off things, or even prototype logic for a workflow. Maybe I'm weird, but I like having stats and using that to flip around data in simple pivots, then digging into the crosstabs.

I understand we like our cutting edge and bespoke tooling, but Excel is as if not more effective to walk a client through parts of their data in a familiar environment. Since I've been in the field I had the idea in the back of my mind that it is a difference between people that tend to think in terms of functions vs tables. 

I come from microbiology/mycology research where every record was painstakingly recorded in a table, so I probably put more value on dissecting the data much more. As I moved from that career track in 2013 to where I am now, that focus was on increasingly larger ('big') data and greater distance from the data. This may have just been the different between capturing hundreds of observations in the lab by hand to inferring risk from millions of user permissions.

Or, is it customary to dunk on Excel now?",t2_4an3hfer,False,,0,False,Excel Hate,[],r/datascience,False,6,tooling,0,,,False,t3_nagk6a,False,dark,0.84,,public,108,0,{},,,False,[],,False,False,,{},Tooling,False,108,,False,False,self,False,,[],{},,True,,1620823211.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is it with data scientists and being snobs about Excel? It&amp;#39;s a great tool if one wants to get the texture of a data, build simple one-off things, or even prototype logic for a workflow. Maybe I&amp;#39;m weird, but I like having stats and using that to flip around data in simple pivots, then digging into the crosstabs.&lt;/p&gt;

&lt;p&gt;I understand we like our cutting edge and bespoke tooling, but Excel is as if not more effective to walk a client through parts of their data in a familiar environment. Since I&amp;#39;ve been in the field I had the idea in the back of my mind that it is a difference between people that tend to think in terms of functions vs tables. &lt;/p&gt;

&lt;p&gt;I come from microbiology/mycology research where every record was painstakingly recorded in a table, so I probably put more value on dissecting the data much more. As I moved from that career track in 2013 to where I am now, that focus was on increasingly larger (&amp;#39;big&amp;#39;) data and greater distance from the data. This may have just been the different between capturing hundreds of observations in the lab by hand to inferring risk from millions of user permissions.&lt;/p&gt;

&lt;p&gt;Or, is it customary to dunk on Excel now?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nagk6a,True,,urban_citrus,,180,True,all_ads,False,[],False,,/r/datascience/comments/nagk6a/excel_hate/,all_ads,False,https://www.reddit.com/r/datascience/comments/nagk6a/excel_hate/,515407,1620794411.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi!

I was just wondering if I was on the low side of number of hours people work a day. I talked to a friend who works at Amazon and they said that they do 8 hours of work. By work I mean when you're sitting on your desk and doing stuff. Not including the meetings, although I understand meetings are also part of work. 
I realized I do maybe 4 hours of actual work, rest is just thinking about some stuff for work, lunch, break etc.
It's hard to imagine how can someone just sit and do 8 hours. Won't they be burnt out?

How many hours do you put in?

Thanks!",t2_bv171ji2,False,,0,False,"How many hours of actual ""work"" do you do everyday?",[],r/datascience,False,6,discussion,0,,,False,t3_na5kg7,False,dark,0.97,,public,372,0,{},,,False,[],,False,False,,{},Discussion,False,372,,False,False,self,False,,[],{},,True,,1620790561.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I was just wondering if I was on the low side of number of hours people work a day. I talked to a friend who works at Amazon and they said that they do 8 hours of work. By work I mean when you&amp;#39;re sitting on your desk and doing stuff. Not including the meetings, although I understand meetings are also part of work. 
I realized I do maybe 4 hours of actual work, rest is just thinking about some stuff for work, lunch, break etc.
It&amp;#39;s hard to imagine how can someone just sit and do 8 hours. Won&amp;#39;t they be burnt out?&lt;/p&gt;

&lt;p&gt;How many hours do you put in?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,na5kg7,True,,quite--average,,169,True,all_ads,False,[],False,,/r/datascience/comments/na5kg7/how_many_hours_of_actual_work_do_you_do_everyday/,all_ads,False,https://www.reddit.com/r/datascience/comments/na5kg7/how_many_hours_of_actual_work_do_you_do_everyday/,515407,1620761761.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all. I'm a data analyst on a non-technical team that collects and utilizes a lot of data. Because it's a non-technical team, there is essentially no data infrastructure set up. Specifically, there is no data storage or management software being used. We simply access our data directly in the file system. The scope of our work has grown a lot recently and this way of doing things is quickly becoming unmanageable, especially with respect to logging, versioning, and being able to find and reproduce old analyses. 

So I'm wondering if it's time we use some actual software to help us with this. I've been reading about data lakes (our data is whole files, can't be put into a relational database/data warehouse). I'm wondering if a data lake might help us. However, I've read that data lakes don't have any inherent structure to them, and this is what allows them to store any type of data. 

So what's the benefit of using data lake software/services over just organizing our file system storage? If I use a data lake, will I still end up with the same question of ""how do I organize all this?"" Is there another type of software that I should be looking at that's more attuned to my needs?

Keep in mind this is a non technical team, so I will need to train any newcomer on whatever I use and can't assume they will have DS/CS knowledge (but they will likely be comfortable with simple scripting in Python/R/bash/etc).

Thank you for any help!",t2_ldo0h,False,,0,False,"What's a good ""first step"" data management software for a team that currently has no data infrastructure?",[],r/datascience,False,6,discussion,0,,,False,t3_naxegy,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1620877162.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I&amp;#39;m a data analyst on a non-technical team that collects and utilizes a lot of data. Because it&amp;#39;s a non-technical team, there is essentially no data infrastructure set up. Specifically, there is no data storage or management software being used. We simply access our data directly in the file system. The scope of our work has grown a lot recently and this way of doing things is quickly becoming unmanageable, especially with respect to logging, versioning, and being able to find and reproduce old analyses. &lt;/p&gt;

&lt;p&gt;So I&amp;#39;m wondering if it&amp;#39;s time we use some actual software to help us with this. I&amp;#39;ve been reading about data lakes (our data is whole files, can&amp;#39;t be put into a relational database/data warehouse). I&amp;#39;m wondering if a data lake might help us. However, I&amp;#39;ve read that data lakes don&amp;#39;t have any inherent structure to them, and this is what allows them to store any type of data. &lt;/p&gt;

&lt;p&gt;So what&amp;#39;s the benefit of using data lake software/services over just organizing our file system storage? If I use a data lake, will I still end up with the same question of &amp;quot;how do I organize all this?&amp;quot; Is there another type of software that I should be looking at that&amp;#39;s more attuned to my needs?&lt;/p&gt;

&lt;p&gt;Keep in mind this is a non technical team, so I will need to train any newcomer on whatever I use and can&amp;#39;t assume they will have DS/CS knowledge (but they will likely be comfortable with simple scripting in Python/R/bash/etc).&lt;/p&gt;

&lt;p&gt;Thank you for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naxegy,True,,marathonjohnathon,,8,True,all_ads,False,[],False,,/r/datascience/comments/naxegy/whats_a_good_first_step_data_management_software/,all_ads,False,https://www.reddit.com/r/datascience/comments/naxegy/whats_a_good_first_step_data_management_software/,515407,1620848362.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi there! We've just added a new dataset to Gourdian, this one courtesy of Google's Project Sunroof. This dataset essentially describes the rooftop solar potential for different regions, based on Google's analysis of Google Maps data to find rooftops where solar would work, and aggregate those into region-wide statistics.

It comes in a couple of aggregation flavors - by census tract ( https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_censustract#summary ), where the region name is the census tract id, and by postal code ( https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_postal_code#summary ), where the name is the postal code. Each also contains latitude/longitude bounding boxes and averages, so that you can download based on that, and you should be able to do custom larger aggregations using those, if you'd like.

This dataset seems like it'd be interesting to cross reference with things like weather, and perhaps electricity prices, to find the best places for people to invest in rooftop solar. If you have any other ideas of what it'd be good to combine with, let us know, and we can try to prioritize ingesting those!",t2_3peod,False,,0,False,Gourdian Free Dataset Download: Project Sunroof - Solar Electricity Generation Potential by Census Tract/Postal Code,[],r/datascience,False,6,tooling,0,,,False,t3_nb6f0j,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1620902173.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there! We&amp;#39;ve just added a new dataset to Gourdian, this one courtesy of Google&amp;#39;s Project Sunroof. This dataset essentially describes the rooftop solar potential for different regions, based on Google&amp;#39;s analysis of Google Maps data to find rooftops where solar would work, and aggregate those into region-wide statistics.&lt;/p&gt;

&lt;p&gt;It comes in a couple of aggregation flavors - by census tract ( &lt;a href=""https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_censustract#summary""&gt;https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_censustract#summary&lt;/a&gt; ), where the region name is the census tract id, and by postal code ( &lt;a href=""https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_postal_code#summary""&gt;https://gourdian.net/g/eric/sunroof_solar.solar_potential_by_postal_code#summary&lt;/a&gt; ), where the name is the postal code. Each also contains latitude/longitude bounding boxes and averages, so that you can download based on that, and you should be able to do custom larger aggregations using those, if you&amp;#39;d like.&lt;/p&gt;

&lt;p&gt;This dataset seems like it&amp;#39;d be interesting to cross reference with things like weather, and perhaps electricity prices, to find the best places for people to invest in rooftop solar. If you have any other ideas of what it&amp;#39;d be good to combine with, let us know, and we can try to prioritize ingesting those!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nb6f0j,True,,prestodigitarium,,4,True,all_ads,False,[],False,,/r/datascience/comments/nb6f0j/gourdian_free_dataset_download_project_sunroof/,all_ads,False,https://www.reddit.com/r/datascience/comments/nb6f0j/gourdian_free_dataset_download_project_sunroof/,515407,1620873373.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I don't know how squarly the argument fall into data science or experimental design, I have worked in 2  R&amp;D departments of industrial machinery dedicated to PVD and CVD (physical vapor deposition, chemical vapor deposition).

How do you convince people at work that d optimal design is not a scam when the end game is formulating a model through multilinear regression?

Edit: i explained both the high level goal and the advantages but the supervisors don't seem convinced I wanted to ask whether or not someone applied the technique in the workplace",t2_nztnj,False,,0,False,D-Optimal design in the workplace,[],r/datascience,False,6,projects,0,,,False,t3_naujhs,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,1620852359.0,,[],{},,True,,1620869894.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I don&amp;#39;t know how squarly the argument fall into data science or experimental design, I have worked in 2  R&amp;amp;D departments of industrial machinery dedicated to PVD and CVD (physical vapor deposition, chemical vapor deposition).&lt;/p&gt;

&lt;p&gt;How do you convince people at work that d optimal design is not a scam when the end game is formulating a model through multilinear regression?&lt;/p&gt;

&lt;p&gt;Edit: i explained both the high level goal and the advantages but the supervisors don&amp;#39;t seem convinced I wanted to ask whether or not someone applied the technique in the workplace&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naujhs,True,,Sojir,,8,True,all_ads,False,[],False,,/r/datascience/comments/naujhs/doptimal_design_in_the_workplace/,all_ads,False,https://www.reddit.com/r/datascience/comments/naujhs/doptimal_design_in_the_workplace/,515407,1620841094.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi,

I'm a self-taught data analyst currently working as an office clerk. It's not *that* bad since I'm still a huge beginner and the job leaves me enough freedom to keep learning.

The main problem is that I must end with the Office package that my company uses. I'd love making some dash app (be it only to train my python skills) but since I can't find a way to export them in our Holy Sharepoint, I just end up making boring Excel dashboards.

Do you have any idea that could work ? I can't make my own online website since our data must stay protected.

Thanks,",t2_k15r8mh,False,,0,False,Plotly app to Sharepoint ?,[],r/datascience,False,6,education,0,,,False,t3_naqhuu,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1620859759.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a self-taught data analyst currently working as an office clerk. It&amp;#39;s not &lt;em&gt;that&lt;/em&gt; bad since I&amp;#39;m still a huge beginner and the job leaves me enough freedom to keep learning.&lt;/p&gt;

&lt;p&gt;The main problem is that I must end with the Office package that my company uses. I&amp;#39;d love making some dash app (be it only to train my python skills) but since I can&amp;#39;t find a way to export them in our Holy Sharepoint, I just end up making boring Excel dashboards.&lt;/p&gt;

&lt;p&gt;Do you have any idea that could work ? I can&amp;#39;t make my own online website since our data must stay protected.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naqhuu,True,,Dyolekythos,,3,True,all_ads,False,[],False,,/r/datascience/comments/naqhuu/plotly_app_to_sharepoint/,all_ads,False,https://www.reddit.com/r/datascience/comments/naqhuu/plotly_app_to_sharepoint/,515407,1620830959.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I might be getting an automation/data science python job soon, and it's full time and I'm still in school full time. To me this job would mostly be worth it if the experience here would be sought after by future SWE roles that i apply at when i graduate, is this the case? I love automation with python but am not personally a huge pandas/numpy guy myself.",t2_x1l7p,False,,0,False,Are data science skills transferrable to regular SWE roles?,[],r/datascience,False,6,career,0,,,False,t3_naibge,False,dark,0.84,,public,13,0,{},,,False,[],,False,False,,{},Career,False,13,,False,False,self,False,,[],{},,True,,1620830408.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I might be getting an automation/data science python job soon, and it&amp;#39;s full time and I&amp;#39;m still in school full time. To me this job would mostly be worth it if the experience here would be sought after by future SWE roles that i apply at when i graduate, is this the case? I love automation with python but am not personally a huge pandas/numpy guy myself.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naibge,True,,jonnycross10,,12,True,all_ads,False,[],False,,/r/datascience/comments/naibge/are_data_science_skills_transferrable_to_regular/,all_ads,False,https://www.reddit.com/r/datascience/comments/naibge/are_data_science_skills_transferrable_to_regular/,515407,1620801608.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Edit: The title says, ""Training"" because I'm an idiot. Reddit doesn't let you edit titles, but I meant to say, ""Testing"".


I've got a Python Keras Sequential model that I would like to use Early Stopping on as soon as the test mse gets out of hand (to prevent overfitting), but I'm not seeing any way to feed Keras the test data and tell it to calculate a metric off that.",t2_s1q046f,False,,0,False,How to use training data in Python Keras Sequential metric?,[],r/datascience,False,6,tooling,0,,,False,t3_nb3mbb,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,True,self,1620875220.0,,[],{},,True,,1620893840.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: The title says, &amp;quot;Training&amp;quot; because I&amp;#39;m an idiot. Reddit doesn&amp;#39;t let you edit titles, but I meant to say, &amp;quot;Testing&amp;quot;.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve got a Python Keras Sequential model that I would like to use Early Stopping on as soon as the test mse gets out of hand (to prevent overfitting), but I&amp;#39;m not seeing any way to feed Keras the test data and tell it to calculate a metric off that.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nb3mbb,True,,muh_reddit_accout,,6,True,all_ads,False,[],False,,/r/datascience/comments/nb3mbb/how_to_use_training_data_in_python_keras/,all_ads,False,https://www.reddit.com/r/datascience/comments/nb3mbb/how_to_use_training_data_in_python_keras/,515407,1620865040.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey all

I have a question about EDA . So I've been working on this project, ""The Movie recommendation system"".  My dataset is a pretty standard one ([https://www.kaggle.com/rounakbanik/the-movies-dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset)). Now I want to understand how much depth should I go into to while performing EDA. Because after a certain point, the conclusions from EDA no longer make the ML model better. However, I can go on making conclusions from the dataset, finding relations between all combinations of features. When do I stop?",t2_43mjijer,False,,0,False,EDA query,[],r/datascience,False,6,discussion,0,,,False,t3_nao1d8,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620853063.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all&lt;/p&gt;

&lt;p&gt;I have a question about EDA . So I&amp;#39;ve been working on this project, &amp;quot;The Movie recommendation system&amp;quot;.  My dataset is a pretty standard one (&lt;a href=""https://www.kaggle.com/rounakbanik/the-movies-dataset""&gt;https://www.kaggle.com/rounakbanik/the-movies-dataset&lt;/a&gt;). Now I want to understand how much depth should I go into to while performing EDA. Because after a certain point, the conclusions from EDA no longer make the ML model better. However, I can go on making conclusions from the dataset, finding relations between all combinations of features. When do I stop?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nao1d8,True,,YOU_TUBE_PERSON,,12,True,all_ads,False,[],False,,/r/datascience/comments/nao1d8/eda_query/,all_ads,False,https://www.reddit.com/r/datascience/comments/nao1d8/eda_query/,515407,1620824263.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?auto=webp&amp;s=2de476a84d95a745e12524d818fe10d6d4e98189', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57a9223e48e6fb19437c9b3137726d4fcd0b37a4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fba1c695e45a1593bb97e6b961286f492a939e74', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b43c3b8f17a6b2e78bf5b4c4d42b8f9c34659e88', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=45d1d76949cf870137249c31618bde2945d3b386', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b95da1b3b6928bf27d52f447b2800f15f693624c', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/Zhv_chf4xNawn91Ebh6rHgUb6_1P5a4mKFcQ8XsYkCQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8d0d1cdf18b138fde9a62929f9237d83f96568b', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': '_LRlay9lu1SzOLMgUzoEMxCQJwTXrarRsWZlKtSCqhE'}], 'enabled': False}",,,,,
,datascience,"I've been in the data analytics industry for over 15 years now.  And I have been thinking about starting a company that teaches data science.  Did research and found a lot of competition between Coursera, Udemy, Universities and Boot Camps.  I'd be targeting people who are considering a career change or professionals that need to retrain/upskill.

I really have a passion for training/teaching others and I have credibility from my experience working in the field. I was thinking about how to differentiate myself: personalized training, teaching adjacent skills such as project management for data professionals, and coaching in the job search process.

Do you think it's worth pursuing or is the field just too saturated?",t2_6fhgonyi,False,,0,False,Data Science Training Company - Worth it?,[],r/datascience,False,6,career,0,,,False,t3_naqcd2,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,True,self,1620831386.0,,[],{},,True,,1620859361.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been in the data analytics industry for over 15 years now.  And I have been thinking about starting a company that teaches data science.  Did research and found a lot of competition between Coursera, Udemy, Universities and Boot Camps.  I&amp;#39;d be targeting people who are considering a career change or professionals that need to retrain/upskill.&lt;/p&gt;

&lt;p&gt;I really have a passion for training/teaching others and I have credibility from my experience working in the field. I was thinking about how to differentiate myself: personalized training, teaching adjacent skills such as project management for data professionals, and coaching in the job search process.&lt;/p&gt;

&lt;p&gt;Do you think it&amp;#39;s worth pursuing or is the field just too saturated?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naqcd2,True,,AgnosticPrankster,,12,True,all_ads,False,[],False,,/r/datascience/comments/naqcd2/data_science_training_company_worth_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/naqcd2/data_science_training_company_worth_it/,515407,1620830561.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I’m looking to see if there is a data science ""category"" that focuses on physical phenomena (i.e. physics, math, chemistry etc) instead of data science methods used when analyzing human behavior (like voting tendencies or purchasing habits). I know that both areas can use the same machine learning models, but I'm curious to know if the physical sciences tend to benefit from a certain ""category"" of data science methods. 

Am I right in assuming that our approach to data science in the physical sciences can be different than our approach to data science in the ""softer"" sciences like psychology and sociology? If so, I would greatly appreciate your thoughts and any potential references to already existing literature that relates to the topic. 

Thanks in advance!",t2_3bz664hc,False,,0,False,Is there a sub-discipline of Data Science that focuses on the physical sciences?,[],r/datascience,False,6,discussion,0,,,False,t3_n9wqtb,False,dark,0.91,,public,85,0,{},,,False,[],,False,False,,{},Discussion,False,85,,False,False,self,False,,[],{},,True,,1620767688.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m looking to see if there is a data science &amp;quot;category&amp;quot; that focuses on physical phenomena (i.e. physics, math, chemistry etc) instead of data science methods used when analyzing human behavior (like voting tendencies or purchasing habits). I know that both areas can use the same machine learning models, but I&amp;#39;m curious to know if the physical sciences tend to benefit from a certain &amp;quot;category&amp;quot; of data science methods. &lt;/p&gt;

&lt;p&gt;Am I right in assuming that our approach to data science in the physical sciences can be different than our approach to data science in the &amp;quot;softer&amp;quot; sciences like psychology and sociology? If so, I would greatly appreciate your thoughts and any potential references to already existing literature that relates to the topic. &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n9wqtb,True,,conteph,,72,True,all_ads,False,[],False,,/r/datascience/comments/n9wqtb/is_there_a_subdiscipline_of_data_science_that/,all_ads,False,https://www.reddit.com/r/datascience/comments/n9wqtb/is_there_a_subdiscipline_of_data_science_that/,515407,1620738888.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,There is not much available on internet about data logging. Any resources or explanations from anyone which can sum it up ?,t2_6fqhdn2z,False,,0,False,Data logging.,[],r/datascience,False,6,discussion,0,,,False,t3_najtbp,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620837070.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There is not much available on internet about data logging. Any resources or explanations from anyone which can sum it up ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,najtbp,True,,Berserk_l_,,5,True,all_ads,False,[],False,,/r/datascience/comments/najtbp/data_logging/,all_ads,False,https://www.reddit.com/r/datascience/comments/najtbp/data_logging/,515407,1620808270.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Apologies if this isn't the correct sub. I believe I've seen similar posts on here before so I figured it'd probably be fine.

Title says it all. Thus far, I've been focusing my search on data science, and to be honest, data analytics positions. For those of you within data engineering, do you feel that the entry level market is significantly less saturated? Or about the same?

Would really like to hear from people who have experience in that field (rather than guesses from those who've exclusively had experience with DS).",t2_3l96vk8r,False,,0,False,Is the market for data engineering significantly better than data science (specifically for entry level)?,[],r/datascience,False,6,,0,,,False,t3_na4ytl,False,dark,0.9,,public,15,0,{},,,False,[],,False,False,,{},Job Search,False,15,,False,False,self,False,,[],{},,True,,1620789100.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Apologies if this isn&amp;#39;t the correct sub. I believe I&amp;#39;ve seen similar posts on here before so I figured it&amp;#39;d probably be fine.&lt;/p&gt;

&lt;p&gt;Title says it all. Thus far, I&amp;#39;ve been focusing my search on data science, and to be honest, data analytics positions. For those of you within data engineering, do you feel that the entry level market is significantly less saturated? Or about the same?&lt;/p&gt;

&lt;p&gt;Would really like to hear from people who have experience in that field (rather than guesses from those who&amp;#39;ve exclusively had experience with DS).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,na4ytl,True,,nemesiswithatophat,,13,True,all_ads,False,[],False,,/r/datascience/comments/na4ytl/is_the_market_for_data_engineering_significantly/,all_ads,False,https://www.reddit.com/r/datascience/comments/na4ytl/is_the_market_for_data_engineering_significantly/,515407,1620760300.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Sorry I’m new and can hardly explain what I’m looking for - basically what I want to know is, what are the best network visualizaton tools/software? I’m trying to build a data visualization that represents networks kind of like bicycle tires and spokes. Id be interested in any software that lets me make an interactive diagram of different networks represented in an interesting way, and showing how networks connect to one another.",t2_17491n,False,,0,False,What are the best data visualization tools?,[],r/datascience,False,6,tooling,0,,,False,t3_nah26c,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1620825186.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry I’m new and can hardly explain what I’m looking for - basically what I want to know is, what are the best network visualizaton tools/software? I’m trying to build a data visualization that represents networks kind of like bicycle tires and spokes. Id be interested in any software that lets me make an interactive diagram of different networks represented in an interesting way, and showing how networks connect to one another.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nah26c,True,,elementalemmental,,6,True,all_ads,False,[],False,,/r/datascience/comments/nah26c/what_are_the_best_data_visualization_tools/,all_ads,False,https://www.reddit.com/r/datascience/comments/nah26c/what_are_the_best_data_visualization_tools/,515407,1620796386.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_35uqulkz,False,,0,False,How likely is it to work a fully remote job in Data Science?,[],r/datascience,False,6,discussion,0,,,False,t3_na8gjw,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1620797859.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,na8gjw,True,,magneticmaxx,,20,True,all_ads,False,[],False,,/r/datascience/comments/na8gjw/how_likely_is_it_to_work_a_fully_remote_job_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/na8gjw/how_likely_is_it_to_work_a_fully_remote_job_in/,515407,1620769059.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Tried lunchclub, horrible experience - didn't have a single conversation of 10+ scheduled chats. Considering reaching out to folks on LinkedIn, but would love to know if there's a smarter way to do this, like some active discord channels. 

P.S. with all the reform talks going on, would be worth opening a discord server or slack channel for the sub as well.",t2_ot46v,False,,0,False,Coffee chats - what's your preferred way of requesting networking connects? LinkedIn / Email / lunchclub?,[],r/datascience,False,6,network,0,,,False,t3_nacqar,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Networking,False,2,,False,False,self,False,,[],{},,True,,1620810321.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Tried lunchclub, horrible experience - didn&amp;#39;t have a single conversation of 10+ scheduled chats. Considering reaching out to folks on LinkedIn, but would love to know if there&amp;#39;s a smarter way to do this, like some active discord channels. &lt;/p&gt;

&lt;p&gt;P.S. with all the reform talks going on, would be worth opening a discord server or slack channel for the sub as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nacqar,True,,mild_animal,,2,True,all_ads,False,[],False,,/r/datascience/comments/nacqar/coffee_chats_whats_your_preferred_way_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/nacqar/coffee_chats_whats_your_preferred_way_of/,515407,1620781521.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"Hi,

I am looking for pointers on how/where to start the use case of ranking associates. For example, there is a class of 10 students who gave exams for last 3 years and we have the marks in those tests. Now can I use the historical marks for each subject and predict the future marks and rank the associates accordingly. Also can I give suggestions to students on where to improve to get a better rank/marks ?",t2_3ep37bn,False,,0,False,Model for Ranking people,[],r/datascience,False,6,projects,0,,,False,t3_naf8ra,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1620818406.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am looking for pointers on how/where to start the use case of ranking associates. For example, there is a class of 10 students who gave exams for last 3 years and we have the marks in those tests. Now can I use the historical marks for each subject and predict the future marks and rank the associates accordingly. Also can I give suggestions to students on where to improve to get a better rank/marks ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,naf8ra,True,,chanu4dincha,,3,True,all_ads,False,[],False,,/r/datascience/comments/naf8ra/model_for_ranking_people/,all_ads,False,https://www.reddit.com/r/datascience/comments/naf8ra/model_for_ranking_people/,515407,1620789606.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I always see amazing glassdoor ratings for small (&lt;500 employee) companies and wonder if this is genuine. The general story seems to be that work-life balance is a lot better at large companies like fortune 500s, but glassdoor seems to disagree. Is this a coincidence with the companies I'm looking at, or this there something I'm missing? Is it just that start-ups expect no work-life balance so they give it 5\*?

Also, any tips on finding a job with work-life balance? (currently an overworked data analyst)",t2_kefis4v,False,,0,False,Glassdoor reviews &amp; work-life-balance,[],r/datascience,False,6,,0,,,False,t3_na7z1g,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1620796590.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I always see amazing glassdoor ratings for small (&amp;lt;500 employee) companies and wonder if this is genuine. The general story seems to be that work-life balance is a lot better at large companies like fortune 500s, but glassdoor seems to disagree. Is this a coincidence with the companies I&amp;#39;m looking at, or this there something I&amp;#39;m missing? Is it just that start-ups expect no work-life balance so they give it 5*?&lt;/p&gt;

&lt;p&gt;Also, any tips on finding a job with work-life balance? (currently an overworked data analyst)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,na7z1g,True,,GoinRoundTheClock,,4,True,all_ads,False,[],False,,/r/datascience/comments/na7z1g/glassdoor_reviews_worklifebalance/,all_ads,False,https://www.reddit.com/r/datascience/comments/na7z1g/glassdoor_reviews_worklifebalance/,515407,1620767790.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"The data science interview process is something that we have seen evolve over the last 5-10 years, taking on several shapes and hitting specific fads along the way. Back when DS got popular, the process was a lot like every other interview process - questions about your resume, some questions about technical topics to make sure that you knew what a person in that role should know, etc.

Then came the ""well, Google asks people these weird, seemingly nonsensical questions and it helps them *understand how you think!"".* So that became the big trend - how many ping pong balls can you fit into this room, how many pizzas are sold in Manhattan every day, etc.

Then came the behavioralists. Everything can be figured out by asking questions of the format ""tell me about a time when..."".

Then came leetcode (which is still alive).

Then came the FAANG ""product interview"", which has now bred literal online courses in how to pass the product interview.

I hit the breaking point of frustration a week ago when I engaged with a recruiter at one of these companies and I was sent a link to several medium articles to prepare for the interview, including one with a line so tone-deaf (not to be coming from the author of the article, but to be coming from the recruiter) that it left me speechless:

&gt;As I describe my own experience, I can’t help thinking of a **common misconception** I often hear: it’s not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. 

I'll stop here for a second, beacause I know I'm going to get flooded hate. I agree  - you can 100% acquire enough knowledge about a topic to pass ""know"" enough to pass a screening. However, there is always a gap between knowing something on paper and in practice - and in fact, that is *exactly* the gap that you're trying to quantify during an interview process.  

And this is the core of my issue with interview processes of this kind: if the interview process is one that a person can prepare for, then what you are evaluating people on isn't their ability to the job - you're just evaluating them on their ability to prepare for your interview process. And no matter how strong you think the interview process is as a proxy for that person's ability to do the actual job, the more efficiently someone can prepare for the interview, the weaker that proxy becomes.

To give an analogy - I could probably get an average 12 year old to pass a calculus test without them ever actually understanding calculus if someone told me in advance what were the 20 most likely questions to be asked. If I know the test is going to require taking the derivative of 10 functions, and I knew what were the 20 most common functions, I can probably get someone to get 6 out of 10 questions right and pass with a C-. 

It's actually one of the things that instructors in math courses always try (and it's not easy) to accomplish - giving questions that are not foreign enough to completely trip up a student, while simultaneously different enough to not be solvable through sheer memorization. 

As others have mentioned in the past, part of what is challenging about designing interview processes is controlling for the fact that most people are bad at interviewing. The more scripted, structured, rigid the interview process is, the easier it is to ensure that interviewers can execute the process correctly (and unbiasedly).

The problem - the trade-off - is that in doing so you are potentially developing a really bad process. That is, you may be sacrificing accuracy for precision. 

Is there a magical answer? Probably not. The answer is probably to invest more time and resources in ensuring that interviewers can be equal parts unpredictable in the nature of their questions and predictable in how they execute and evaluate said questions. 

But I think it is very much needed to start talking about how this process is likely broken - and that the quality of hires that these companies are making is much more driven by their brand, compensation, and ability to attract high quality hires than it is by filtering out the best ones out of their candidate pool.",t2_3epw0pud,False,,0,False,"Rant: If your company's interview process can be ""practiced"" for, it's probably not a very good one",[],r/datascience,False,6,,0,,,False,t3_n9aj13,False,dark,0.94,,public,373,0,{},,,False,[],,False,False,,{},,False,373,,False,False,self,False,seniorflair,[],{},,True,,1620695819.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The data science interview process is something that we have seen evolve over the last 5-10 years, taking on several shapes and hitting specific fads along the way. Back when DS got popular, the process was a lot like every other interview process - questions about your resume, some questions about technical topics to make sure that you knew what a person in that role should know, etc.&lt;/p&gt;

&lt;p&gt;Then came the &amp;quot;well, Google asks people these weird, seemingly nonsensical questions and it helps them &lt;em&gt;understand how you think!&amp;quot;.&lt;/em&gt; So that became the big trend - how many ping pong balls can you fit into this room, how many pizzas are sold in Manhattan every day, etc.&lt;/p&gt;

&lt;p&gt;Then came the behavioralists. Everything can be figured out by asking questions of the format &amp;quot;tell me about a time when...&amp;quot;.&lt;/p&gt;

&lt;p&gt;Then came leetcode (which is still alive).&lt;/p&gt;

&lt;p&gt;Then came the FAANG &amp;quot;product interview&amp;quot;, which has now bred literal online courses in how to pass the product interview.&lt;/p&gt;

&lt;p&gt;I hit the breaking point of frustration a week ago when I engaged with a recruiter at one of these companies and I was sent a link to several medium articles to prepare for the interview, including one with a line so tone-deaf (not to be coming from the author of the article, but to be coming from the recruiter) that it left me speechless:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As I describe my own experience, I can’t help thinking of a &lt;strong&gt;common misconception&lt;/strong&gt; I often hear: it’s not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;#39;ll stop here for a second, beacause I know I&amp;#39;m going to get flooded hate. I agree  - you can 100% acquire enough knowledge about a topic to pass &amp;quot;know&amp;quot; enough to pass a screening. However, there is always a gap between knowing something on paper and in practice - and in fact, that is &lt;em&gt;exactly&lt;/em&gt; the gap that you&amp;#39;re trying to quantify during an interview process.  &lt;/p&gt;

&lt;p&gt;And this is the core of my issue with interview processes of this kind: if the interview process is one that a person can prepare for, then what you are evaluating people on isn&amp;#39;t their ability to the job - you&amp;#39;re just evaluating them on their ability to prepare for your interview process. And no matter how strong you think the interview process is as a proxy for that person&amp;#39;s ability to do the actual job, the more efficiently someone can prepare for the interview, the weaker that proxy becomes.&lt;/p&gt;

&lt;p&gt;To give an analogy - I could probably get an average 12 year old to pass a calculus test without them ever actually understanding calculus if someone told me in advance what were the 20 most likely questions to be asked. If I know the test is going to require taking the derivative of 10 functions, and I knew what were the 20 most common functions, I can probably get someone to get 6 out of 10 questions right and pass with a C-. &lt;/p&gt;

&lt;p&gt;It&amp;#39;s actually one of the things that instructors in math courses always try (and it&amp;#39;s not easy) to accomplish - giving questions that are not foreign enough to completely trip up a student, while simultaneously different enough to not be solvable through sheer memorization. &lt;/p&gt;

&lt;p&gt;As others have mentioned in the past, part of what is challenging about designing interview processes is controlling for the fact that most people are bad at interviewing. The more scripted, structured, rigid the interview process is, the easier it is to ensure that interviewers can execute the process correctly (and unbiasedly).&lt;/p&gt;

&lt;p&gt;The problem - the trade-off - is that in doing so you are potentially developing a really bad process. That is, you may be sacrificing accuracy for precision. &lt;/p&gt;

&lt;p&gt;Is there a magical answer? Probably not. The answer is probably to invest more time and resources in ensuring that interviewers can be equal parts unpredictable in the nature of their questions and predictable in how they execute and evaluate said questions. &lt;/p&gt;

&lt;p&gt;But I think it is very much needed to start talking about how this process is likely broken - and that the quality of hires that these companies are making is much more driven by their brand, compensation, and ability to attract high quality hires than it is by filtering out the best ones out of their candidate pool.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,PhD | Head of Data Science | Ecommerce,[],False,,,,t5_2sptq,,,,n9aj13,True,,dfphd,,97,True,all_ads,False,[],False,dark,/r/datascience/comments/n9aj13/rant_if_your_companys_interview_process_can_be/,all_ads,False,https://www.reddit.com/r/datascience/comments/n9aj13/rant_if_your_companys_interview_process_can_be/,515407,1620667019.0,0,,False,,,,,,,,
,datascience,"Our team works on various models and prediction problems. Sometimes we get bogged down in discussion about better approaches and want to test new ideas. In each team members specific individual scripts they have test and evaluation data which makes true side by side comparisons trickier than otherwise.

My question is, is there a service, open source server or even a paid tool where we could send our predictions to be tested against a single universal out of sample test set for model evaluation? 

The only example I can think of is Kaggle. Years ago I attempted to join a competition and you would submit a csv of predictions on some test data and your score would show on a leader board.

Are there any tools, servers, libraries or platforms out there that work in a similar fashion that would allow our team to compare and compete with each other on some prediction tasks?",t2_80foqbw9,False,,0,False,Seeking recommendations for a model evaluation tool if one exists?,[],r/datascience,False,6,tooling,0,,,False,t3_na88bo,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1620797266.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Our team works on various models and prediction problems. Sometimes we get bogged down in discussion about better approaches and want to test new ideas. In each team members specific individual scripts they have test and evaluation data which makes true side by side comparisons trickier than otherwise.&lt;/p&gt;

&lt;p&gt;My question is, is there a service, open source server or even a paid tool where we could send our predictions to be tested against a single universal out of sample test set for model evaluation? &lt;/p&gt;

&lt;p&gt;The only example I can think of is Kaggle. Years ago I attempted to join a competition and you would submit a csv of predictions on some test data and your score would show on a leader board.&lt;/p&gt;

&lt;p&gt;Are there any tools, servers, libraries or platforms out there that work in a similar fashion that would allow our team to compare and compete with each other on some prediction tasks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,na88bo,True,,okjnbvgfertyuiojhg,,2,True,all_ads,False,[],False,,/r/datascience/comments/na88bo/seeking_recommendations_for_a_model_evaluation/,all_ads,False,https://www.reddit.com/r/datascience/comments/na88bo/seeking_recommendations_for_a_model_evaluation/,515407,1620768466.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I'm a ""data scientist"" that does data engineering.  I get data science interviews from my job title alone.  Does anyone else think data science is too broad of a field to ever feel prepared for the interview.  For example, I feel data science jobs can be broken down into the following types of roles:





1) The typical data scientist: This is what we typically how we imagine a data scientist.  The role involves a bit of data exploration, ML model building, presentations to management, etc.




2) The deep learning data scientist: This is kind of like the previous example, but with a greater emphasis on deep learning over traditional ML.  The role is more likely to ask for a PhD.  This role looks at more interesting problems in my opinion, such as computer vision and NLP.





3) The data engineering data scientist: This is like my current role.  I work on ETL pipelines and bring new data to data scientists in the previous categories for ML model building.  Because of my job title, I might be asked to do some data analysis work.  I work a lot with python, SQL, and AWS.





4) Software Engineer (Data Science): This data scientist is in reality a software engineer attached to a data science team.  This is not as common, but definitely exists.



5) The data analyst with a data scientist job title: With this type of data scientist, there is less python and ML, and more SQL, Excel, and presentations.  Hiring managers typically look at non-technical skills over technical skills.






Those are all the roles I can think of, and I am sure I am missing some.  But assuming you fit one of the categories, it's pretty hard to prepare for all other data science interviews.  Some roles only leetcode you, others might ask SQL questions, others might ask math/stats trivia, others might give you a take home presentation to prepare.",t2_2k1plfwa,False,,0,False,Is data science too broad to ever feel prepared for an interview?,[],r/datascience,False,6,,0,,,False,t3_n959rr,False,dark,0.97,,public,423,5,{},,,False,[],,False,False,,{},Job Search,False,423,,False,False,self,False,,[],{},,True,,1620684452.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a &amp;quot;data scientist&amp;quot; that does data engineering.  I get data science interviews from my job title alone.  Does anyone else think data science is too broad of a field to ever feel prepared for the interview.  For example, I feel data science jobs can be broken down into the following types of roles:&lt;/p&gt;

&lt;p&gt;1) The typical data scientist: This is what we typically how we imagine a data scientist.  The role involves a bit of data exploration, ML model building, presentations to management, etc.&lt;/p&gt;

&lt;p&gt;2) The deep learning data scientist: This is kind of like the previous example, but with a greater emphasis on deep learning over traditional ML.  The role is more likely to ask for a PhD.  This role looks at more interesting problems in my opinion, such as computer vision and NLP.&lt;/p&gt;

&lt;p&gt;3) The data engineering data scientist: This is like my current role.  I work on ETL pipelines and bring new data to data scientists in the previous categories for ML model building.  Because of my job title, I might be asked to do some data analysis work.  I work a lot with python, SQL, and AWS.&lt;/p&gt;

&lt;p&gt;4) Software Engineer (Data Science): This data scientist is in reality a software engineer attached to a data science team.  This is not as common, but definitely exists.&lt;/p&gt;

&lt;p&gt;5) The data analyst with a data scientist job title: With this type of data scientist, there is less python and ML, and more SQL, Excel, and presentations.  Hiring managers typically look at non-technical skills over technical skills.&lt;/p&gt;

&lt;p&gt;Those are all the roles I can think of, and I am sure I am missing some.  But assuming you fit one of the categories, it&amp;#39;s pretty hard to prepare for all other data science interviews.  Some roles only leetcode you, others might ask SQL questions, others might ask math/stats trivia, others might give you a take home presentation to prepare.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n959rr,True,,memcpy94,,69,True,all_ads,False,[],False,,/r/datascience/comments/n959rr/is_data_science_too_broad_to_ever_feel_prepared/,all_ads,False,https://www.reddit.com/r/datascience/comments/n959rr/is_data_science_too_broad_to_ever_feel_prepared/,515407,1620655652.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_92lh73i2,False,,0,False,"Has anyone ever applied an unsupervised learning method or reinforcement learning method to define ‘roles’ at the company? (Think permissions, AD Groups, and on-boarding)",[],r/datascience,False,6,discussion,0,,,False,t3_na081v,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1620777279.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,na081v,True,,TheNukedWhale,,6,True,all_ads,False,[],False,,/r/datascience/comments/na081v/has_anyone_ever_applied_an_unsupervised_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/na081v/has_anyone_ever_applied_an_unsupervised_learning/,515407,1620748479.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Is there a reddit for data science that isnt focused on career questions? I'm on r/statistics but I didnt know if there was a similar home for DS specifically.

EDIT: and would anyone else be interested in seeing this sub be more than a flood of career entering and movement threads? If that's what is wanted great, but I've always thought that most of the content in this sub could be contained in a single pinned thread.",t2_jfue2,False,,0,False,Non career focused data science subreddit.,[],r/datascience,False,6,discussion,0,,,False,t3_n942nw,False,dark,0.95,,public,38,0,{},,,False,[],,False,False,,{},Discussion,False,38,,False,False,self,1620652430.0,,[],{},,True,,1620681022.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a reddit for data science that isnt focused on career questions? I&amp;#39;m on &lt;a href=""/r/statistics""&gt;r/statistics&lt;/a&gt; but I didnt know if there was a similar home for DS specifically.&lt;/p&gt;

&lt;p&gt;EDIT: and would anyone else be interested in seeing this sub be more than a flood of career entering and movement threads? If that&amp;#39;s what is wanted great, but I&amp;#39;ve always thought that most of the content in this sub could be contained in a single pinned thread.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,MS | Data Scientist,[],False,,,,t5_2sptq,,,,n942nw,True,,skeerp,,41,True,all_ads,False,[],False,dark,/r/datascience/comments/n942nw/non_career_focused_data_science_subreddit/,all_ads,False,https://www.reddit.com/r/datascience/comments/n942nw/non_career_focused_data_science_subreddit/,515407,1620652222.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," Packt will be publishing **“Data Science Projects”**

As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on [Amazon.com](https://amazon.com/).

Here is the table of contents of the book:

*1* *Data Exploration and Cleaning*

*2* *Introduction to Scikit-Learn and Model Evaluation*

*3* *Details of Logistic Regression and Feature Exploration*

*4* *The Bias Variance Trade-off*

*5* *Decision* *Trees and* *Random* *Forests*

*6* *Gradient Boosting, SHAP values (SHapley* *Additive* *exPlanations), and dealing with missing data*

*7* *Financial Analysis and Delivery to Client*

Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. 

Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 15th of May 2021.",t2_bxq52s4y,False,,0,False,Opportunity to Read and Review New Book published by Packt,[],r/datascience,False,6,education,0,,,False,t3_n9r0wi,False,dark,1.0,,public,1,0,{},,,True,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1620745322.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Packt will be publishing &lt;strong&gt;“Data Science Projects”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on &lt;a href=""https://amazon.com/""&gt;Amazon.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the table of contents of the book:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1&lt;/em&gt; &lt;em&gt;Data Exploration and Cleaning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2&lt;/em&gt; &lt;em&gt;Introduction to Scikit-Learn and Model Evaluation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3&lt;/em&gt; &lt;em&gt;Details of Logistic Regression and Feature Exploration&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4&lt;/em&gt; &lt;em&gt;The Bias Variance Trade-off&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;5&lt;/em&gt; &lt;em&gt;Decision&lt;/em&gt; &lt;em&gt;Trees and&lt;/em&gt; &lt;em&gt;Random&lt;/em&gt; &lt;em&gt;Forests&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;6&lt;/em&gt; &lt;em&gt;Gradient Boosting, SHAP values (SHapley&lt;/em&gt; &lt;em&gt;Additive&lt;/em&gt; &lt;em&gt;exPlanations), and dealing with missing data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;7&lt;/em&gt; &lt;em&gt;Financial Analysis and Delivery to Client&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. &lt;/p&gt;

&lt;p&gt;Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 15th of May 2021.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n9r0wi,True,,kunal_packtpub,,0,True,all_ads,False,[],False,,/r/datascience/comments/n9r0wi/opportunity_to_read_and_review_new_book_published/,all_ads,False,https://www.reddit.com/r/datascience/comments/n9r0wi/opportunity_to_read_and_review_new_book_published/,515407,1620716522.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I work at a relatively small, publicly listed bank (corporate hq) as a data analyst intern and both physical and cyber security standards are high. 

My job, and most people’s job in my department (finance) entails importing data into excel or modifying data from excel. 

As you may know, excel doesn’t really play nice with all kinds of data. For example, we have excel functions calling a cell from a different sheet. If the new data to be imported is minutely different, it may invalidate the function. Many times, this results in hours or troubleshooting (manually parsing through cells and rematching them). Furthermore, excel isn’t really robust enough to efficiently do linear algebra so any automation involves manually going through cells and writing functions.

Additionally, everyday that I’ve come in so far, the first half of the workday involves my colleagues complaining about how the software used to manipulate data results in errors. On my first day, they left me a text book about said software and I read it in full. It’s essentially visual, simplified R. 

I come from a stem background and I think it would be evident to anyone from my background that the current data storage system is highly inefficient and attempts at improving efficiency (data manipulation software) is negligible at best. 

The problem is, to access anything, we go through a portal. The portal has applications we can use. You can’t access the shell. There are no language interpreters or text editors. Even if I could access the shell, it would either be cmd or powershell. USBs aren’t allowed. And data cannot be stored locally.

IT has shutdown any propositions I’ve made about adding tools. The cybersecurity administration is extremely hesitant along with my boss.

From an accessibility stand point, I do understand. I’m probably the only person in that entire building who knows how to code outside of a query if statement. If we were to make a project, I’d be the only one who could understand it. And if I left, they’d have to hire someone with those skills and completely change this hypothetical project. So that’s not really what I’m asking for. What I want is more tools to do the job they hired me (personally) for. Without those tools, I’m just every other employee. I don’t really have a purpose. 

I feel like because I’m young, they don’t really take me seriously. But it just seems so clearly evident that there is no way they can continue to grow like this. It’s just too much data to parse through with the existing process. Unless they just hire more and more but at some point, the overhead will become too high (which is what is currently happening; paying sums for software that doesn’t work, hiring more to parse negate data issues)

Ironically, one of the company commandments (for a lack of a better word) relates to cutting costs anywhere possible.

So my question is, has anyone under similar circumstances successfully passed a proposition for more resources in a similar context? How did you do it?

tl,dr: How do I ask for more resources in the context of a highly secure network.",t2_8vsh5,False,,0,False,How do I ask for more resources in a secure environment,[],r/datascience,False,6,career,0,,,False,t3_n9izej,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1620717390.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work at a relatively small, publicly listed bank (corporate hq) as a data analyst intern and both physical and cyber security standards are high. &lt;/p&gt;

&lt;p&gt;My job, and most people’s job in my department (finance) entails importing data into excel or modifying data from excel. &lt;/p&gt;

&lt;p&gt;As you may know, excel doesn’t really play nice with all kinds of data. For example, we have excel functions calling a cell from a different sheet. If the new data to be imported is minutely different, it may invalidate the function. Many times, this results in hours or troubleshooting (manually parsing through cells and rematching them). Furthermore, excel isn’t really robust enough to efficiently do linear algebra so any automation involves manually going through cells and writing functions.&lt;/p&gt;

&lt;p&gt;Additionally, everyday that I’ve come in so far, the first half of the workday involves my colleagues complaining about how the software used to manipulate data results in errors. On my first day, they left me a text book about said software and I read it in full. It’s essentially visual, simplified R. &lt;/p&gt;

&lt;p&gt;I come from a stem background and I think it would be evident to anyone from my background that the current data storage system is highly inefficient and attempts at improving efficiency (data manipulation software) is negligible at best. &lt;/p&gt;

&lt;p&gt;The problem is, to access anything, we go through a portal. The portal has applications we can use. You can’t access the shell. There are no language interpreters or text editors. Even if I could access the shell, it would either be cmd or powershell. USBs aren’t allowed. And data cannot be stored locally.&lt;/p&gt;

&lt;p&gt;IT has shutdown any propositions I’ve made about adding tools. The cybersecurity administration is extremely hesitant along with my boss.&lt;/p&gt;

&lt;p&gt;From an accessibility stand point, I do understand. I’m probably the only person in that entire building who knows how to code outside of a query if statement. If we were to make a project, I’d be the only one who could understand it. And if I left, they’d have to hire someone with those skills and completely change this hypothetical project. So that’s not really what I’m asking for. What I want is more tools to do the job they hired me (personally) for. Without those tools, I’m just every other employee. I don’t really have a purpose. &lt;/p&gt;

&lt;p&gt;I feel like because I’m young, they don’t really take me seriously. But it just seems so clearly evident that there is no way they can continue to grow like this. It’s just too much data to parse through with the existing process. Unless they just hire more and more but at some point, the overhead will become too high (which is what is currently happening; paying sums for software that doesn’t work, hiring more to parse negate data issues)&lt;/p&gt;

&lt;p&gt;Ironically, one of the company commandments (for a lack of a better word) relates to cutting costs anywhere possible.&lt;/p&gt;

&lt;p&gt;So my question is, has anyone under similar circumstances successfully passed a proposition for more resources in a similar context? How did you do it?&lt;/p&gt;

&lt;p&gt;tl,dr: How do I ask for more resources in the context of a highly secure network.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n9izej,True,,xhitcramp,,15,True,all_ads,False,[],False,,/r/datascience/comments/n9izej/how_do_i_ask_for_more_resources_in_a_secure/,all_ads,False,https://www.reddit.com/r/datascience/comments/n9izej/how_do_i_ask_for_more_resources_in_a_secure/,515407,1620688590.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"If you read posts/comments from this sub, people are quite assertive that their path is the main path. They say get a masters/PhD in “ “ and that’s the only way in. They say don’t get a masters in analytics/data science but encourage the analytics @ Georgia Tech. Worry about your own goals and domains that you want to get into and focus YOUR path for that. There’s data scientist from a million different backgrounds and educational levels.",t2_1vbildz3,False,,0,False,How to become a data scientist? There’s a million ways.,[],r/datascience,False,6,discussion,0,,,False,t3_n8tqfe,False,dark,0.79,,public,53,0,{},,,False,[],,False,False,,{},Discussion,False,53,,False,False,self,False,,[],{},,True,,1620642854.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you read posts/comments from this sub, people are quite assertive that their path is the main path. They say get a masters/PhD in “ “ and that’s the only way in. They say don’t get a masters in analytics/data science but encourage the analytics @ Georgia Tech. Worry about your own goals and domains that you want to get into and focus YOUR path for that. There’s data scientist from a million different backgrounds and educational levels.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n8tqfe,True,,drewm8080,,36,True,all_ads,False,[],False,,/r/datascience/comments/n8tqfe/how_to_become_a_data_scientist_theres_a_million/,all_ads,False,https://www.reddit.com/r/datascience/comments/n8tqfe/how_to_become_a_data_scientist_theres_a_million/,515407,1620614054.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The courses at university teach me how to understand and build a model. However, we do not learn what to do with the model once it's done. Like how to put it into production for a company. I would like to understand this aspect a bit more.

As I understand it, simple models can be saved and stored on a cloud server and accessed (through API) by the end application to make predictions based on new data. Is this realistic?

How do you deploy models in your work environment?",t2_9bzaw11v,False,,0,False,So you trained a model. Now what?,[],r/datascience,False,6,discussion,0,,,False,t3_n8ezvx,False,dark,0.97,,public,347,1,{},,,False,[],,False,False,,{},Discussion,False,347,,False,False,self,False,,[],{},,True,,1620598304.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The courses at university teach me how to understand and build a model. However, we do not learn what to do with the model once it&amp;#39;s done. Like how to put it into production for a company. I would like to understand this aspect a bit more.&lt;/p&gt;

&lt;p&gt;As I understand it, simple models can be saved and stored on a cloud server and accessed (through API) by the end application to make predictions based on new data. Is this realistic?&lt;/p&gt;

&lt;p&gt;How do you deploy models in your work environment?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n8ezvx,True,,von_Olivenbaum,,71,True,all_ads,False,[],False,,/r/datascience/comments/n8ezvx/so_you_trained_a_model_now_what/,all_ads,False,https://www.reddit.com/r/datascience/comments/n8ezvx/so_you_trained_a_model_now_what/,515407,1620569504.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I'm thinking about exploring DS in the freelance world.  I have started working full time in January, but the position is not very challenging or time consuming and I need more projects.  I'm wondering if anyone out there has any experience as a freelancer, consultant, or part-time and what your tips are to be successful? 

Thanks!",t2_4vyk0,False,,0,False,Any DS freelancers out there?,[],r/datascience,False,6,discussion,0,,,False,t3_n97h4x,False,dark,0.6,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620689008.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m thinking about exploring DS in the freelance world.  I have started working full time in January, but the position is not very challenging or time consuming and I need more projects.  I&amp;#39;m wondering if anyone out there has any experience as a freelancer, consultant, or part-time and what your tips are to be successful? &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n97h4x,True,,ayaPapaya,,5,True,all_ads,False,[],False,,/r/datascience/comments/n97h4x/any_ds_freelancers_out_there/,all_ads,False,https://www.reddit.com/r/datascience/comments/n97h4x/any_ds_freelancers_out_there/,515407,1620660208.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

Apologies in advance if I'm breaking any rules or if this is more suited to the weekly thread. 

I've been working as a Data Analyst for a healthcare company for the past year. A lot of my work surrounds creating queries to track certain metrics then building dashboards to create insights using visualization tools. 

I do like the field, but unsure of where to move next. I do enjoy the coding aspect of my position (Lots of SQL, don't like excel as much), as well as hacking away at a problem and figuring out how to fix certain issues with the code we have. 

However, I hate coming up with insights and solutions. I don't mind creating the dashboards, but I don't like the proactive and analytical work that comes with it. ""Oh that's a good find, maybe we should look into this next"". I enjoy more when there's a problem to fix, then I fix that problem. 

Does data engineering fit more into the interests I've mentioned above? I imagine a role as a Data scientist would be more similar to what I'm currently doing.

Thanks in advance.",t2_ihm0e,False,,0,False,Career path options as a current Data Analyst,[],r/datascience,False,6,career,0,,,False,t3_n7vzf6,False,dark,0.97,,public,252,3,{},,,False,[],,False,False,,{},Career,False,252,,False,False,self,False,,[],{'gid_1': 2},,True,,1620529185.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Apologies in advance if I&amp;#39;m breaking any rules or if this is more suited to the weekly thread. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been working as a Data Analyst for a healthcare company for the past year. A lot of my work surrounds creating queries to track certain metrics then building dashboards to create insights using visualization tools. &lt;/p&gt;

&lt;p&gt;I do like the field, but unsure of where to move next. I do enjoy the coding aspect of my position (Lots of SQL, don&amp;#39;t like excel as much), as well as hacking away at a problem and figuring out how to fix certain issues with the code we have. &lt;/p&gt;

&lt;p&gt;However, I hate coming up with insights and solutions. I don&amp;#39;t mind creating the dashboards, but I don&amp;#39;t like the proactive and analytical work that comes with it. &amp;quot;Oh that&amp;#39;s a good find, maybe we should look into this next&amp;quot;. I enjoy more when there&amp;#39;s a problem to fix, then I fix that problem. &lt;/p&gt;

&lt;p&gt;Does data engineering fit more into the interests I&amp;#39;ve mentioned above? I imagine a role as a Data scientist would be more similar to what I&amp;#39;m currently doing.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n7vzf6,True,,CodeRed1234,,94,True,all_ads,False,[],False,,/r/datascience/comments/n7vzf6/career_path_options_as_a_current_data_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/n7vzf6/career_path_options_as_a_current_data_analyst/,515407,1620500385.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 09 May 2021 - 16 May 2021,[],r/datascience,False,6,,0,,,False,t3_n8ct8y,False,dark,0.91,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,False,,[],{},,True,,1620590430.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,n8ct8y,True,,datascience-bot,,150,False,all_ads,False,[],False,dark,/r/datascience/comments/n8ct8y/weekly_entering_transitioning_thread_09_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/n8ct8y/weekly_entering_transitioning_thread_09_may_2021/,515407,1620561630.0,0,,False,,,,,,,,
,datascience,"Reporting and Monitoring.

There are  two important types of tools in BI one is for reporting &amp; one is for real time monitoring.

For reporting - Power BI, Tableau, Qlik Sense, Looker Etc.

For real time monitoring i believe there a few categories based in what i have read in various articles.

[Cliff.ai](https://Cliff.ai) \- for real-time monitoring of business metrics.

[montecarlodata.com](https://montecarlodata.com) \- for real time monitoring of data.

What others tools for reporting &amp; real time monitoring tools you use or know about????",t2_6fqhdn2z,False,,0,False,Most Important Reporting &amp; Monitoring Tools for your business.,[],r/datascience,False,6,discussion,0,,,False,t3_n8f0v3,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1620598393.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reporting and Monitoring.&lt;/p&gt;

&lt;p&gt;There are  two important types of tools in BI one is for reporting &amp;amp; one is for real time monitoring.&lt;/p&gt;

&lt;p&gt;For reporting - Power BI, Tableau, Qlik Sense, Looker Etc.&lt;/p&gt;

&lt;p&gt;For real time monitoring i believe there a few categories based in what i have read in various articles.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://Cliff.ai""&gt;Cliff.ai&lt;/a&gt; - for real-time monitoring of business metrics.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://montecarlodata.com""&gt;montecarlodata.com&lt;/a&gt; - for real time monitoring of data.&lt;/p&gt;

&lt;p&gt;What others tools for reporting &amp;amp; real time monitoring tools you use or know about????&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n8f0v3,True,,Berserk_l_,,13,True,all_ads,False,[],False,,/r/datascience/comments/n8f0v3/most_important_reporting_monitoring_tools_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/n8f0v3/most_important_reporting_monitoring_tools_for/,515407,1620569593.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?auto=webp&amp;s=1de8f8ea8b0d18641b8b03ead96d1448e34f8bb6', 'width': 1806, 'height': 936}, 'resolutions': [{'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=661107b0fe1db9eabc42a03bfc7eec35ce4e046f', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53c58eb8b6e587031de765d6bf12132858818203', 'width': 216, 'height': 111}, {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4682622996b5c89d968b24413e73e158ddc619aa', 'width': 320, 'height': 165}, {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=139880fd6910080be353925fc9bfa71ad6ea1e8d', 'width': 640, 'height': 331}, {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=280dc23e1a32f9764c5e0bbb4738ced57114c54b', 'width': 960, 'height': 497}, {'url': 'https://external-preview.redd.it/feb_55dkF0ZKBDNplf_VQx50vJA9Ix4l42G8L3sY2XY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=384732c5d6fe0d92d642e373e2d72793ae619ef6', 'width': 1080, 'height': 559}], 'variants': {}, 'id': 'qFwZmD1oU9calYfPPo-bGa8oI31ry80VoO5ROmKlQRc'}], 'enabled': False}",,,,,
,datascience,"Hey folks,

I'm in the process of building a fashion aggregation website as a portfolio piece. To obtain the data for this site, I have multiple scrapers for many of the largest fashion retailers in North America. 

To describe the data - one product can have many variants. e.g, 'Summer Dress' that comes in 3 colours and 8 sizes.  As such this product would have 24 variant products, the generic being the 'parent' product. Each variant may have different pricing, availability, sizing, colour etc.

My question concerns table construction - my scrapers output every variant of every product with their associated data, which can be in CSV or pandas DataFrame format. Can any of you tell me how I would go about populating two database tables from this data:

1. Table with the parent product, with foreign keys to each of its variants

2. All variant products of a parent product, linked to that product such that on my eventual website, they can all be found through searching the foreign keys of the parent product. 

I hope that makes sense, in a bit of a jam here. I'm not sure what to Google - even if you can't provide an answer, any points on even what to research would be of great help. 

TLDR: How do I populate 2 database tables with one CSV file, one table for the generic product, and one for all of its variant's fields?",t2_f8nzs1v,False,,0,False,Populating E-commerce Tables with Products and Variants,[],r/datascience,False,6,projects,0,,,False,t3_n8ddv3,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1620592580.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey folks,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m in the process of building a fashion aggregation website as a portfolio piece. To obtain the data for this site, I have multiple scrapers for many of the largest fashion retailers in North America. &lt;/p&gt;

&lt;p&gt;To describe the data - one product can have many variants. e.g, &amp;#39;Summer Dress&amp;#39; that comes in 3 colours and 8 sizes.  As such this product would have 24 variant products, the generic being the &amp;#39;parent&amp;#39; product. Each variant may have different pricing, availability, sizing, colour etc.&lt;/p&gt;

&lt;p&gt;My question concerns table construction - my scrapers output every variant of every product with their associated data, which can be in CSV or pandas DataFrame format. Can any of you tell me how I would go about populating two database tables from this data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Table with the parent product, with foreign keys to each of its variants&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All variant products of a parent product, linked to that product such that on my eventual website, they can all be found through searching the foreign keys of the parent product. &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope that makes sense, in a bit of a jam here. I&amp;#39;m not sure what to Google - even if you can&amp;#39;t provide an answer, any points on even what to research would be of great help. &lt;/p&gt;

&lt;p&gt;TLDR: How do I populate 2 database tables with one CSV file, one table for the generic product, and one for all of its variant&amp;#39;s fields?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n8ddv3,True,,Theendangeredmoose,,8,True,all_ads,False,[],False,,/r/datascience/comments/n8ddv3/populating_ecommerce_tables_with_products_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/n8ddv3/populating_ecommerce_tables_with_products_and/,515407,1620563780.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Background: 

I was hired as a Data Analyst a couple of months ago. It's a startup with a small data science team. The team is really endearing and I absolutely adore them. 

That sounds all good. I love the work I do as it's writing code and I love writing code. 

But I'm not doing any data analyst work. I write ETL processes, come up with formulas to do certain calculations, fix historical data because they were calculated wrong, i.e. fix formulas that are close enough but there are better ways to calculate them and reprocess the entire data, and writing SQL queries to see things I wanna see. The closest to what we can call 'Analysis' I do is to verify if the ETL Process I wrote shows/updates the graphs and numbers in the platform and if they're correct. 

So what exactly am I?

Edit: grammar",t2_78e26vzo,False,,0,False,Not sure where in the spectrum I fall...,[],r/datascience,False,6,discussion,0,,,False,t3_n86zp8,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,1620537153.0,,[],{},,True,,1620565190.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Background: &lt;/p&gt;

&lt;p&gt;I was hired as a Data Analyst a couple of months ago. It&amp;#39;s a startup with a small data science team. The team is really endearing and I absolutely adore them. &lt;/p&gt;

&lt;p&gt;That sounds all good. I love the work I do as it&amp;#39;s writing code and I love writing code. &lt;/p&gt;

&lt;p&gt;But I&amp;#39;m not doing any data analyst work. I write ETL processes, come up with formulas to do certain calculations, fix historical data because they were calculated wrong, i.e. fix formulas that are close enough but there are better ways to calculate them and reprocess the entire data, and writing SQL queries to see things I wanna see. The closest to what we can call &amp;#39;Analysis&amp;#39; I do is to verify if the ETL Process I wrote shows/updates the graphs and numbers in the platform and if they&amp;#39;re correct. &lt;/p&gt;

&lt;p&gt;So what exactly am I?&lt;/p&gt;

&lt;p&gt;Edit: grammar&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n86zp8,True,,FoolForWool,,12,True,all_ads,False,[],False,,/r/datascience/comments/n86zp8/not_sure_where_in_the_spectrum_i_fall/,all_ads,False,https://www.reddit.com/r/datascience/comments/n86zp8/not_sure_where_in_the_spectrum_i_fall/,515407,1620536390.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"At my current job I've been using Bayesian Structural Time Series models to (hopefully) deal with COVID related outliers more gracefully in forecasting. Curious if anyone here has found a way to incorporate Bayesian approaches into their work. If so, what applications are you using them for?",t2_7v2b47me,False,,0,False,Where are my Bayesians?,[],r/datascience,False,6,discussion,0,,,False,t3_n7x3fe,False,dark,0.88,,public,20,0,{},,,False,[],,False,False,,{},Discussion,False,20,,False,False,self,False,,[],{},,True,,1620532483.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;At my current job I&amp;#39;ve been using Bayesian Structural Time Series models to (hopefully) deal with COVID related outliers more gracefully in forecasting. Curious if anyone here has found a way to incorporate Bayesian approaches into their work. If so, what applications are you using them for?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n7x3fe,True,,redneckhippynerd,,21,True,all_ads,False,[],False,,/r/datascience/comments/n7x3fe/where_are_my_bayesians/,all_ads,False,https://www.reddit.com/r/datascience/comments/n7x3fe/where_are_my_bayesians/,515407,1620503683.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So the situation is basically I want to break down parsed data from a game called Counter Strike and try to figure out what locations on a map are important to winning a round. The features I was thinking of collecting would be

the tick: 128 = 1 second so would probably normalize this as the maximum amount a round can last is 1 minute 55 seconds + 40 seconds (usually lower amount than this).
10 vectors of [91x1] which would be one hot encoded to break down roughly 90 in game locations that I have labelled. A player can only be in one of those locations at a time. A player can also be labelled dead or off the map so was thinking of indicator for that.
10 [1x1] vectors to indicate whether a player is CT or T
Training label would be 0 or 1 to indicate either T or CT side win (two teams of 5v5 face off each round to a best of 30 conclusion for anyone unfamiliar with the rules).
So all in all the ending goal of mine would be to try and figure out how important those locations. Is my idea for the feature set adequate? Or do you see certain aspects of it that would be an issue.

Also if the feature set is fine what sort of algorithms or approaches would be best to go by, I just have not flexed these ML muscles in a decent amount so would like the advice.",t2_y5sdh,False,,0,False,Need advice about building out a model in terms of algorithm picking,[],r/datascience,False,6,discussion,0,,,False,t3_n7nuuf,False,dark,0.91,,public,66,0,{},,,False,[],,False,False,,{},Discussion,False,66,,False,False,self,False,,[],{},,True,,1620504410.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So the situation is basically I want to break down parsed data from a game called Counter Strike and try to figure out what locations on a map are important to winning a round. The features I was thinking of collecting would be&lt;/p&gt;

&lt;p&gt;the tick: 128 = 1 second so would probably normalize this as the maximum amount a round can last is 1 minute 55 seconds + 40 seconds (usually lower amount than this).
10 vectors of [91x1] which would be one hot encoded to break down roughly 90 in game locations that I have labelled. A player can only be in one of those locations at a time. A player can also be labelled dead or off the map so was thinking of indicator for that.
10 [1x1] vectors to indicate whether a player is CT or T
Training label would be 0 or 1 to indicate either T or CT side win (two teams of 5v5 face off each round to a best of 30 conclusion for anyone unfamiliar with the rules).
So all in all the ending goal of mine would be to try and figure out how important those locations. Is my idea for the feature set adequate? Or do you see certain aspects of it that would be an issue.&lt;/p&gt;

&lt;p&gt;Also if the feature set is fine what sort of algorithms or approaches would be best to go by, I just have not flexed these ML muscles in a decent amount so would like the advice.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n7nuuf,True,,nkashyap14,,28,True,all_ads,False,[],False,,/r/datascience/comments/n7nuuf/need_advice_about_building_out_a_model_in_terms/,all_ads,False,https://www.reddit.com/r/datascience/comments/n7nuuf/need_advice_about_building_out_a_model_in_terms/,515407,1620475610.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"For training statistical/machine learning models in R, are there any advantages of using the ""caret"" package compared to the ""mlr"" package? Do you guys have any preferences? For instance - I noticed that caret does not easily allow you to perform grid search on the ""max node size"" hyperparameter in the random forest model. 

Does anyone have any advice?

Thanks",t2_o4xj9,False,,0,False,Mlr vs caret,[],r/datascience,False,6,discussion,0,,,False,t3_n84g3e,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620556252.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For training statistical/machine learning models in R, are there any advantages of using the &amp;quot;caret&amp;quot; package compared to the &amp;quot;mlr&amp;quot; package? Do you guys have any preferences? For instance - I noticed that caret does not easily allow you to perform grid search on the &amp;quot;max node size&amp;quot; hyperparameter in the random forest model. &lt;/p&gt;

&lt;p&gt;Does anyone have any advice?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n84g3e,True,,blueest,,5,True,all_ads,False,[],False,,/r/datascience/comments/n84g3e/mlr_vs_caret/,all_ads,False,https://www.reddit.com/r/datascience/comments/n84g3e/mlr_vs_caret/,515407,1620527452.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone. Does anybody knows how XGBoost treats missing values in the Y dataset when a model is trained? I know it handles well missing values in the independent variables but I have not found what happens if there are some in the Y dataset. I am doing several regression models and it seems to work fine but dont quite understand yet if the NaN rows are excluded automatically or if the training of the model is affected in some way. There are just a few NaNs by the way (less than 5%)

Thanks a lot",t2_qpce5xk,False,,0,False,Missing values in XGBoost,[],r/datascience,False,6,discussion,0,,,False,t3_n81n29,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620546610.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone. Does anybody knows how XGBoost treats missing values in the Y dataset when a model is trained? I know it handles well missing values in the independent variables but I have not found what happens if there are some in the Y dataset. I am doing several regression models and it seems to work fine but dont quite understand yet if the NaN rows are excluded automatically or if the training of the model is affected in some way. There are just a few NaNs by the way (less than 5%)&lt;/p&gt;

&lt;p&gt;Thanks a lot&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n81n29,True,,ezt93,,4,True,all_ads,False,[],False,,/r/datascience/comments/n81n29/missing_values_in_xgboost/,all_ads,False,https://www.reddit.com/r/datascience/comments/n81n29/missing_values_in_xgboost/,515407,1620517810.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello!

I'm working on my first DS project where our goal is prediction.
Sometimes I find myself having a hard time understanding a concept related to business therefore ending up asking questions from the person who gave me this project. Different time zone doesn't help and all the conversation is through email, making it harder to communicate. 
I don't set up meeting since I think I shouldn't waste someone's time and understand what I can through email only. Usually, here's how it goes, I send my EDA results, then the guy will reply with some more eda stuff I need to do. So sometimes I don't understand that requirement and end up asking a few confused questions related to the requirements.

For people who supervise, is there anything I should change or do you encourage someone like me a fresher, beginner in this field, asking questions?
Is it possible that I'll end up irritating the other person?

For what it's worth, I'm not officially a ""Data Scientist"", just been working on a project because of my interest and having to study Statistics in my Masters.

Thanks!",t2_bv171ji2,False,,0,False,"On your first DS project, is it normal to ask a lot of questions or have a hard time understanding some of the business process aspects?",[],r/datascience,False,6,discussion,0,,,False,t3_n73pgf,False,dark,0.91,,public,219,1,{},,,False,[],,False,False,,{},Discussion,False,219,,False,False,self,False,,[],{},,True,,1620436649.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on my first DS project where our goal is prediction.
Sometimes I find myself having a hard time understanding a concept related to business therefore ending up asking questions from the person who gave me this project. Different time zone doesn&amp;#39;t help and all the conversation is through email, making it harder to communicate. 
I don&amp;#39;t set up meeting since I think I shouldn&amp;#39;t waste someone&amp;#39;s time and understand what I can through email only. Usually, here&amp;#39;s how it goes, I send my EDA results, then the guy will reply with some more eda stuff I need to do. So sometimes I don&amp;#39;t understand that requirement and end up asking a few confused questions related to the requirements.&lt;/p&gt;

&lt;p&gt;For people who supervise, is there anything I should change or do you encourage someone like me a fresher, beginner in this field, asking questions?
Is it possible that I&amp;#39;ll end up irritating the other person?&lt;/p&gt;

&lt;p&gt;For what it&amp;#39;s worth, I&amp;#39;m not officially a &amp;quot;Data Scientist&amp;quot;, just been working on a project because of my interest and having to study Statistics in my Masters.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n73pgf,True,,quite--average,,68,True,all_ads,False,[],False,,/r/datascience/comments/n73pgf/on_your_first_ds_project_is_it_normal_to_ask_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/n73pgf/on_your_first_ds_project_is_it_normal_to_ask_a/,515407,1620407849.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If so please comment on their uses, I occasionally use R and it feels weird that lm() is the only builtin ML function and yet I have so far found no real uses for (aside from perhaps simple forecasting and constrained optimization of a hidden function).

I've heard that Andrew Ng seems to find them useful enough that it is almost always the first thing he tries to solve a problem (probably for exploratory purposes). Can anyone comment on that?",t2_qrw52,False,,0,False,Are linear models still useful at all?,[],r/datascience,False,6,discussion,0,,,False,t3_n7akrr,False,dark,0.7,,public,12,0,{},,,False,[],,False,False,,{},Discussion,False,12,,False,False,self,False,,[],{},,True,,1620454702.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If so please comment on their uses, I occasionally use R and it feels weird that lm() is the only builtin ML function and yet I have so far found no real uses for (aside from perhaps simple forecasting and constrained optimization of a hidden function).&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve heard that Andrew Ng seems to find them useful enough that it is almost always the first thing he tries to solve a problem (probably for exploratory purposes). Can anyone comment on that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n7akrr,True,,Udon_noodles,,95,True,all_ads,False,[],False,,/r/datascience/comments/n7akrr/are_linear_models_still_useful_at_all/,all_ads,False,https://www.reddit.com/r/datascience/comments/n7akrr/are_linear_models_still_useful_at_all/,515407,1620425902.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I got canned from my first job in the industry. Joined a tech startup where devs ran the entire show and did wtf they wanted, not the management. I wasn't the extrovert personality the ex-consultant management seemed to want, client work didn't come in. They nit picked on small stuff in my 3mo review like not responding to slack messages immediately on a Sunday and canned me a week before Christmas. Seemingly nothing really to do with the work I did. Didn't even get to go past my desk to get my stuff.

I now work for one of their clients but 1.5 years on I struggle to let it go of the shame that I got fired from a job.",t2_rlr56,False,,0,False,Anyone ever get fired?,[],r/datascience,False,6,career,0,,,False,t3_n6fgjw,False,dark,0.96,,public,450,0,{},,,False,[],,False,False,,{},Career,False,450,,False,False,self,1620330611.0,,[],{},,True,,1620358254.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got canned from my first job in the industry. Joined a tech startup where devs ran the entire show and did wtf they wanted, not the management. I wasn&amp;#39;t the extrovert personality the ex-consultant management seemed to want, client work didn&amp;#39;t come in. They nit picked on small stuff in my 3mo review like not responding to slack messages immediately on a Sunday and canned me a week before Christmas. Seemingly nothing really to do with the work I did. Didn&amp;#39;t even get to go past my desk to get my stuff.&lt;/p&gt;

&lt;p&gt;I now work for one of their clients but 1.5 years on I struggle to let it go of the shame that I got fired from a job.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6fgjw,True,,speedisntfree,,132,True,all_ads,False,[],False,,/r/datascience/comments/n6fgjw/anyone_ever_get_fired/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6fgjw/anyone_ever_get_fired/,515407,1620329454.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I've been working as a new data scientist for 3 months and noticed some people could just jump straight in and type away on jupyter notebook without referring to anything, while I have to maintain a ""cheatsheet"" of syntax

Need to bin categories? Refer to my cheatsheet
Need to plot a few bar charts with annotations? Refer to cheatsheet
Need to do a random forest? Err, refer to cheatsheet

Usually, there's nothing wrong with referring to notes or getting some help from Google, of course. But as a professional data scientist, it gets really embarrassing when I'm having a discussion with my colleagues on a project I'm working on, and they're like ""can you create these new features and let me see how their plots look like real quick"", and I'm like...let me refer to my notes on how to do that",t2_amc46yj2,False,,0,False,How do you guys remember syntaxes?,[],r/datascience,False,6,discussion,0,,,False,t3_n74ex9,False,dark,0.72,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1620438503.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been working as a new data scientist for 3 months and noticed some people could just jump straight in and type away on jupyter notebook without referring to anything, while I have to maintain a &amp;quot;cheatsheet&amp;quot; of syntax&lt;/p&gt;

&lt;p&gt;Need to bin categories? Refer to my cheatsheet
Need to plot a few bar charts with annotations? Refer to cheatsheet
Need to do a random forest? Err, refer to cheatsheet&lt;/p&gt;

&lt;p&gt;Usually, there&amp;#39;s nothing wrong with referring to notes or getting some help from Google, of course. But as a professional data scientist, it gets really embarrassing when I&amp;#39;m having a discussion with my colleagues on a project I&amp;#39;m working on, and they&amp;#39;re like &amp;quot;can you create these new features and let me see how their plots look like real quick&amp;quot;, and I&amp;#39;m like...let me refer to my notes on how to do that&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n74ex9,True,,wanderingcatto,,13,True,all_ads,False,[],False,,/r/datascience/comments/n74ex9/how_do_you_guys_remember_syntaxes/,all_ads,False,https://www.reddit.com/r/datascience/comments/n74ex9/how_do_you_guys_remember_syntaxes/,515407,1620409703.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Today we have loads of companies who either have their data foundation run in the cloud or hybrid solutions where some business processes are in the cloud and some on-premise. 

I have been a data engineer building and maintaining an on premise datawarehouse for quite a long time now but I am very intrigued by learning more about cloud datawarehousing and cloud solutions in general as I think its a very practical and promising technology for the future. I have been getting my certificates and want to start working with cloud technologies now as well.

My questions to you guys, do you think that cloud solutions will be the future, or will on premise data systems be there for a while?
And how alike is working as a data engineer in on premise vs cloud solutions?

Please share your thoughts!",t2_183f7a,False,,0,False,Data engineering: what is the future and what to focus on?,[],r/datascience,False,6,career,0,,,False,t3_n74138,False,dark,0.81,,public,6,0,{},,,False,[],,False,False,,{},Career,False,6,,False,False,self,False,,[],{},,True,,1620437492.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Today we have loads of companies who either have their data foundation run in the cloud or hybrid solutions where some business processes are in the cloud and some on-premise. &lt;/p&gt;

&lt;p&gt;I have been a data engineer building and maintaining an on premise datawarehouse for quite a long time now but I am very intrigued by learning more about cloud datawarehousing and cloud solutions in general as I think its a very practical and promising technology for the future. I have been getting my certificates and want to start working with cloud technologies now as well.&lt;/p&gt;

&lt;p&gt;My questions to you guys, do you think that cloud solutions will be the future, or will on premise data systems be there for a while?
And how alike is working as a data engineer in on premise vs cloud solutions?&lt;/p&gt;

&lt;p&gt;Please share your thoughts!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n74138,True,,faalschildpad,,10,True,all_ads,False,[],False,,/r/datascience/comments/n74138/data_engineering_what_is_the_future_and_what_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/n74138/data_engineering_what_is_the_future_and_what_to/,515407,1620408692.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Has anyone ever worked on a machine learning model for ""queues""? Suppose there is a bakery: the bakery has has ""n"" people working, ""m"" people in line""  and ""q"" orders that they are currently working on. The bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer's order is ready and how long will the next customer have to wait before they can place an order. 

Has anyone ever come across a machine learning model which can predict waiting and processing times? I have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit, as well as trying different m/m/k combinations... but has anyone ever come across an instance where machine learning algorithms (e.g. random forest, neural networks) are used to predict waiting times? 

I saw something like this: https://arxiv.org/abs/2002.10788 

But there was no python or R code for this paper. Can anyone recommend some source (blog, github, website, book, YouTube lectures etc) which show and provide computer code for analyzing queues using machine learning models?

Thanks",t2_3f0i9m72,False,,0,False,"has anyone ever worked on a machine learning model for ""queues""?",[],r/datascience,False,6,discussion,0,,,False,t3_n78g0y,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1620448928.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone ever worked on a machine learning model for &amp;quot;queues&amp;quot;? Suppose there is a bakery: the bakery has has &amp;quot;n&amp;quot; people working, &amp;quot;m&amp;quot; people in line&amp;quot;  and &amp;quot;q&amp;quot; orders that they are currently working on. The bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer&amp;#39;s order is ready and how long will the next customer have to wait before they can place an order. &lt;/p&gt;

&lt;p&gt;Has anyone ever come across a machine learning model which can predict waiting and processing times? I have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit, as well as trying different m/m/k combinations... but has anyone ever come across an instance where machine learning algorithms (e.g. random forest, neural networks) are used to predict waiting times? &lt;/p&gt;

&lt;p&gt;I saw something like this: &lt;a href=""https://arxiv.org/abs/2002.10788""&gt;https://arxiv.org/abs/2002.10788&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;But there was no python or R code for this paper. Can anyone recommend some source (blog, github, website, book, YouTube lectures etc) which show and provide computer code for analyzing queues using machine learning models?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n78g0y,True,,SQL_beginner,,5,True,all_ads,False,[],False,,/r/datascience/comments/n78g0y/has_anyone_ever_worked_on_a_machine_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/n78g0y/has_anyone_ever_worked_on_a_machine_learning/,515407,1620420128.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello DS, 

I work as the only data analyst of a small subscription-based streaming company.  Our app is developed by an external company and we have no access to the raw data. 

We receive monthly csv files showing usage per customer. We also receive daily updated csv file for subscription data (start and end date per user). This is also the only data I have access to. 

Using these through Web API, I've built Power BI dashboards and run some analyses on python. 

For usage data: at the start of every month I run 2 .py scripts that aggregate the new month with the existing data and save as csv file in the Data Analytics sharepoint drive. Then I use this data in Power BI or in further analyses in python. 

For subscription: Power BI connects directly to the web to retrieve this data. For analyses, I have a small function in python that retrieves the most recent data from the web.  

For my scripts and data, I have a folder on our Data Analytics Teams group, where I save all my .py and .pbix files. 

I thought this would work okay for the little data we have, and it did for the last year. But now..

* I'm still using a lot of time to run simple analysis on python because I changed some definitions (e.g. which users count as converted), 
* I'm 100% dependent on Power BI because I have no other way of sharing analysis. Even analysis that would be much easier to run on python has to be on Power BI because that's where I have all my definitions (aka. feature engineering, categorizing users etc.), and it's the easiest way to share analysis with collegues. 
* I want to push to get access to the raw data, but I don't know how I would implement that in my current way of working

I feel like there must be a better way: more automatized, more structured, cleaner way of working, but I have no idea how.

I've been asking my boss to hire a data engineer consultant temporarily to advise us on this, but he hasn't prioritised it yet. 

Do you have any advice? What would you do in this situation?",t2_il31r,False,,0,False,Scalable infrastructure for lonely data analysts with no access to raw data,[],r/datascience,False,6,career,0,,,False,t3_n6wfog,False,dark,0.7,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1620416159.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello DS, &lt;/p&gt;

&lt;p&gt;I work as the only data analyst of a small subscription-based streaming company.  Our app is developed by an external company and we have no access to the raw data. &lt;/p&gt;

&lt;p&gt;We receive monthly csv files showing usage per customer. We also receive daily updated csv file for subscription data (start and end date per user). This is also the only data I have access to. &lt;/p&gt;

&lt;p&gt;Using these through Web API, I&amp;#39;ve built Power BI dashboards and run some analyses on python. &lt;/p&gt;

&lt;p&gt;For usage data: at the start of every month I run 2 .py scripts that aggregate the new month with the existing data and save as csv file in the Data Analytics sharepoint drive. Then I use this data in Power BI or in further analyses in python. &lt;/p&gt;

&lt;p&gt;For subscription: Power BI connects directly to the web to retrieve this data. For analyses, I have a small function in python that retrieves the most recent data from the web.  &lt;/p&gt;

&lt;p&gt;For my scripts and data, I have a folder on our Data Analytics Teams group, where I save all my .py and .pbix files. &lt;/p&gt;

&lt;p&gt;I thought this would work okay for the little data we have, and it did for the last year. But now..&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I&amp;#39;m still using a lot of time to run simple analysis on python because I changed some definitions (e.g. which users count as converted), &lt;/li&gt;
&lt;li&gt;I&amp;#39;m 100% dependent on Power BI because I have no other way of sharing analysis. Even analysis that would be much easier to run on python has to be on Power BI because that&amp;#39;s where I have all my definitions (aka. feature engineering, categorizing users etc.), and it&amp;#39;s the easiest way to share analysis with collegues. &lt;/li&gt;
&lt;li&gt;I want to push to get access to the raw data, but I don&amp;#39;t know how I would implement that in my current way of working&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I feel like there must be a better way: more automatized, more structured, cleaner way of working, but I have no idea how.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been asking my boss to hire a data engineer consultant temporarily to advise us on this, but he hasn&amp;#39;t prioritised it yet. &lt;/p&gt;

&lt;p&gt;Do you have any advice? What would you do in this situation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6wfog,True,,giraffeteapot_,,4,True,all_ads,False,[],False,,/r/datascience/comments/n6wfog/scalable_infrastructure_for_lonely_data_analysts/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6wfog/scalable_infrastructure_for_lonely_data_analysts/,515407,1620387359.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have a music recommendation survey, and I have developed a music recommendation algorithm (A), and a simplified *control* algorithm (B). 

The users are given a list of songs to rate (1-6) based on how much they like them. My question is, should each user either listen to songs from the same system exclusively (so AAAAA or BBBBB), or should the recommendations from both systems be interlaced (ABABABA randomly) in order for me to be able to compare the average ratings for each system? 

The first approach means that some users will never hear my recommendations, and given that I do not expect a huge amount of respondents that is a bit of a concern for me. Also, I expect a large chunk of the respondents to be from family and friends, thus I worry that some of the responses will be biased by that fact (so if the user gets the fake system, they would still rate it relatively highly since they are being nice).

What should I do ?",t2_r6jti,False,,0,False,Best way to evaluate two competing recommendation systems,[],r/datascience,False,6,projects,0,,,False,t3_n6w7oq,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},Projects,False,5,,False,False,self,False,,[],{},,True,,1620415341.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a music recommendation survey, and I have developed a music recommendation algorithm (A), and a simplified &lt;em&gt;control&lt;/em&gt; algorithm (B). &lt;/p&gt;

&lt;p&gt;The users are given a list of songs to rate (1-6) based on how much they like them. My question is, should each user either listen to songs from the same system exclusively (so AAAAA or BBBBB), or should the recommendations from both systems be interlaced (ABABABA randomly) in order for me to be able to compare the average ratings for each system? &lt;/p&gt;

&lt;p&gt;The first approach means that some users will never hear my recommendations, and given that I do not expect a huge amount of respondents that is a bit of a concern for me. Also, I expect a large chunk of the respondents to be from family and friends, thus I worry that some of the responses will be biased by that fact (so if the user gets the fake system, they would still rate it relatively highly since they are being nice).&lt;/p&gt;

&lt;p&gt;What should I do ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6w7oq,True,,thunderbirdsetup,,9,True,all_ads,False,[],False,,/r/datascience/comments/n6w7oq/best_way_to_evaluate_two_competing_recommendation/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6w7oq/best_way_to_evaluate_two_competing_recommendation/,515407,1620386541.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm currently employed as a full-time Data Scientist but functionally I'm more of a machine learning engineer. ML is fun, ""hot"" and all that, but I'm more interested in taking a higher level approach to inferring insights from data (via ML or otherwise). Also, despite my title, I haven't ever built a data base or used cloud computing. It's been mostly local computations, csv files, pandas, etc. So I'd love to patch up those holes so I can do more properly data science work.

Given that, are there any somewhat well known/respected boot camps or MOOCs out there that would help me patch those holes? I have a BS in Physics so something more mathy wouldn't be a huge issue. So far I've been looking at DataCamp and Springboard but I'd love some of your thoughts!

&amp;#x200B;

Update: I read all your comments and I really appreciated the feedback! It seems like the best move is to hit some books and maybe take some one-off courses in specific libraries or technologies that would be useful. 

Next steps include: identify a solid statistics course/textbook (preferably something with rigor as I like thinking from the ground up), a one-off course in SQL, a one-off course in AWS/Azure, and a one-off course on Hadoop or Spark. I think that'll cover all my bases.",t2_wjgdp,False,,0,False,"Respected data science bootcamps focusing on cloud, databases, big data frameworks, and statistics?",[],r/datascience,False,6,education,0,,,False,t3_n6c4vh,False,dark,0.93,,public,98,0,{},,,False,[],,False,False,,{},Education,False,98,,False,False,self,1620347485.0,,[],{},,True,,1620349564.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently employed as a full-time Data Scientist but functionally I&amp;#39;m more of a machine learning engineer. ML is fun, &amp;quot;hot&amp;quot; and all that, but I&amp;#39;m more interested in taking a higher level approach to inferring insights from data (via ML or otherwise). Also, despite my title, I haven&amp;#39;t ever built a data base or used cloud computing. It&amp;#39;s been mostly local computations, csv files, pandas, etc. So I&amp;#39;d love to patch up those holes so I can do more properly data science work.&lt;/p&gt;

&lt;p&gt;Given that, are there any somewhat well known/respected boot camps or MOOCs out there that would help me patch those holes? I have a BS in Physics so something more mathy wouldn&amp;#39;t be a huge issue. So far I&amp;#39;ve been looking at DataCamp and Springboard but I&amp;#39;d love some of your thoughts!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Update: I read all your comments and I really appreciated the feedback! It seems like the best move is to hit some books and maybe take some one-off courses in specific libraries or technologies that would be useful. &lt;/p&gt;

&lt;p&gt;Next steps include: identify a solid statistics course/textbook (preferably something with rigor as I like thinking from the ground up), a one-off course in SQL, a one-off course in AWS/Azure, and a one-off course on Hadoop or Spark. I think that&amp;#39;ll cover all my bases.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6c4vh,True,,arimill,,40,True,all_ads,False,[],False,,/r/datascience/comments/n6c4vh/respected_data_science_bootcamps_focusing_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6c4vh/respected_data_science_bootcamps_focusing_on/,515407,1620320764.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,Just wondering if I can ditch my ergonomic keyboard and use my gaming RGB keyboard for work if the typing is not as intensive.,t2_7kwotrw9,False,,0,False,Do data scientists type as much on their keyboard as software engineers?,[],r/datascience,False,6,discussion,0,,,False,t3_n73pb4,False,dark,0.13,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1620436639.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just wondering if I can ditch my ergonomic keyboard and use my gaming RGB keyboard for work if the typing is not as intensive.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n73pb4,True,,quickmoz2,,5,True,all_ads,False,[],False,,/r/datascience/comments/n73pb4/do_data_scientists_type_as_much_on_their_keyboard/,all_ads,False,https://www.reddit.com/r/datascience/comments/n73pb4/do_data_scientists_type_as_much_on_their_keyboard/,515407,1620407839.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I find that Jupyter is a little buggy occasionally and also along with all my other stuff open in chrome i'd rather have a specific window. 

Main tasks day to day - condition based based analysis, 
Data collection and cleaning
Graphing 
Time/frequency/ angle domain analysis
Data sets vary from  1gb to 50gb, rarely more. 

What do you guys use?


Edit: Update. 

VS Code is the one. I like it a lot, though I did have multiple issues with numpy, due to conda environment. Already moved over my current project did some housekeeping and all looks great. 

Also switched to miniconda which is much better again.",t2_aegwp2g1,False,,0,False,Alternatives to Jupyter (Pyhton)?,[],r/datascience,False,6,discussion,0,,,False,t3_n68og6,False,dark,0.83,,public,23,0,{},,,False,[],,False,False,,{},Discussion,False,23,,False,False,self,1620458316.0,,[],{},,True,,1620340522.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I find that Jupyter is a little buggy occasionally and also along with all my other stuff open in chrome i&amp;#39;d rather have a specific window. &lt;/p&gt;

&lt;p&gt;Main tasks day to day - condition based based analysis, 
Data collection and cleaning
Graphing 
Time/frequency/ angle domain analysis
Data sets vary from  1gb to 50gb, rarely more. &lt;/p&gt;

&lt;p&gt;What do you guys use?&lt;/p&gt;

&lt;p&gt;Edit: Update. &lt;/p&gt;

&lt;p&gt;VS Code is the one. I like it a lot, though I did have multiple issues with numpy, due to conda environment. Already moved over my current project did some housekeeping and all looks great. &lt;/p&gt;

&lt;p&gt;Also switched to miniconda which is much better again.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n68og6,True,,fortuitous_monkey,,51,True,all_ads,False,[],False,,/r/datascience/comments/n68og6/alternatives_to_jupyter_pyhton/,all_ads,False,https://www.reddit.com/r/datascience/comments/n68og6/alternatives_to_jupyter_pyhton/,515407,1620311722.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm considering taking an Algorithms course next semester. I doubt the content will add much to my knowledge considering the other courses I've taken, but it would surely be useful for interviews. I'm curious when the entry-level coding tests (stuff like dynamic programming, longest increasing subsequence, etc.) I'm used to will start to disappear in interviews.

This is assuming someone with significant experience won't be bugged with these type of interview questions, could be wrong.",t2_8l0cb,False,,0,False,At what experience level do coding tests (take-home and onsite) go away?,[],r/datascience,False,6,,0,,,False,t3_n64nxf,False,dark,0.85,,public,22,0,{},,,False,[],,False,False,,{},Job Search,False,22,,False,False,self,False,,[],{},,True,,1620327654.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m considering taking an Algorithms course next semester. I doubt the content will add much to my knowledge considering the other courses I&amp;#39;ve taken, but it would surely be useful for interviews. I&amp;#39;m curious when the entry-level coding tests (stuff like dynamic programming, longest increasing subsequence, etc.) I&amp;#39;m used to will start to disappear in interviews.&lt;/p&gt;

&lt;p&gt;This is assuming someone with significant experience won&amp;#39;t be bugged with these type of interview questions, could be wrong.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n64nxf,True,,japooki,,24,True,all_ads,False,[],False,,/r/datascience/comments/n64nxf/at_what_experience_level_do_coding_tests_takehome/,all_ads,False,https://www.reddit.com/r/datascience/comments/n64nxf/at_what_experience_level_do_coding_tests_takehome/,515407,1620298854.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi!

I'm 26 and work as a BI developer/ Data Analyst at a fortune 500 company. My job pays well and I live comfortably. But sometimes I crave a change, a change of company, a change of tools I use at the current job. Using outdated technology right now is kinda the only reason I want to switch.
Then I think if I switch job, it might be a better paying job but could be bad for my work life balance. Right now my work life balance is super, my manager is absolutely fantastic, knows his boundaries, doesn't check my performance in terms of how many hours I'm sitting on my desk. I can stop working at 4, 4.30 or 5, I won't be asked any questions. I can work till 6 and I don't have to put effort in showing that. My hobbies are in check.

To the seniors of this sub or people of my age, what do you value the most in a job?

Thanks!",t2_bv171ji2,False,,0,False,How important was/is work life balance in your mid 20's and what did you do to maintain or destroy it?,[],r/datascience,False,6,discussion,0,,,False,t3_n5kuyz,False,dark,0.97,,public,412,3,{},,,False,[],,False,False,,{},Discussion,False,412,,False,False,self,False,,[],{},,True,,1620262951.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m 26 and work as a BI developer/ Data Analyst at a fortune 500 company. My job pays well and I live comfortably. But sometimes I crave a change, a change of company, a change of tools I use at the current job. Using outdated technology right now is kinda the only reason I want to switch.
Then I think if I switch job, it might be a better paying job but could be bad for my work life balance. Right now my work life balance is super, my manager is absolutely fantastic, knows his boundaries, doesn&amp;#39;t check my performance in terms of how many hours I&amp;#39;m sitting on my desk. I can stop working at 4, 4.30 or 5, I won&amp;#39;t be asked any questions. I can work till 6 and I don&amp;#39;t have to put effort in showing that. My hobbies are in check.&lt;/p&gt;

&lt;p&gt;To the seniors of this sub or people of my age, what do you value the most in a job?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n5kuyz,True,,quite--average,,128,True,all_ads,False,[],False,,/r/datascience/comments/n5kuyz/how_important_wasis_work_life_balance_in_your_mid/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5kuyz/how_important_wasis_work_life_balance_in_your_mid/,515407,1620234151.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,Checked google but its only showing adtech vs martech. Would be great to know what the difference is between marketing data analytics and martech!,t2_4smgznuz,False,,0,False,Whats the difference between marketing data analytics and martech?,[],r/datascience,False,6,career,0,,,False,t3_n691ci,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1620341458.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Checked google but its only showing adtech vs martech. Would be great to know what the difference is between marketing data analytics and martech!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n691ci,True,,Flewizzle,,8,True,all_ads,False,[],False,,/r/datascience/comments/n691ci/whats_the_difference_between_marketing_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/n691ci/whats_the_difference_between_marketing_data/,515407,1620312658.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Packt will be publishing **“Data Science Projects”**

As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on Amazon.

Here is the table of contents of the book:

*1* *Data Exploration and Cleaning*

*2* *Introduction to Scikit-Learn and Model Evaluation*

*3* *Details of Logistic Regression and Feature Exploration*

*4* *The Bias Variance Trade-off*

*5* *Decision* *Trees and* *Random* *Forests*

*6* *Gradient Boosting, SHAP values (SHapley* *Additive* *exPlanations), and dealing with missing data*

*7* *Financial Analysis and Delivery to Client*

Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. 

Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 10th of May 2021.",t2_bxq52s4y,False,,0,False,Opportunity to Read and Review New Book published by Packt,[],r/datascience,False,6,education,0,,,False,t3_n6atu0,False,dark,1.0,,public,1,0,{},,,True,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1620346150.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Packt will be publishing &lt;strong&gt;“Data Science Projects”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on Amazon.&lt;/p&gt;

&lt;p&gt;Here is the table of contents of the book:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1&lt;/em&gt; &lt;em&gt;Data Exploration and Cleaning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2&lt;/em&gt; &lt;em&gt;Introduction to Scikit-Learn and Model Evaluation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3&lt;/em&gt; &lt;em&gt;Details of Logistic Regression and Feature Exploration&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4&lt;/em&gt; &lt;em&gt;The Bias Variance Trade-off&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;5&lt;/em&gt; &lt;em&gt;Decision&lt;/em&gt; &lt;em&gt;Trees and&lt;/em&gt; &lt;em&gt;Random&lt;/em&gt; &lt;em&gt;Forests&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;6&lt;/em&gt; &lt;em&gt;Gradient Boosting, SHAP values (SHapley&lt;/em&gt; &lt;em&gt;Additive&lt;/em&gt; &lt;em&gt;exPlanations), and dealing with missing data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;7&lt;/em&gt; &lt;em&gt;Financial Analysis and Delivery to Client&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. &lt;/p&gt;

&lt;p&gt;Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 10th of May 2021.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6atu0,True,,kunal_packtpub,,0,True,all_ads,False,[],False,,/r/datascience/comments/n6atu0/opportunity_to_read_and_review_new_book_published/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6atu0/opportunity_to_read_and_review_new_book_published/,515407,1620317350.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Packt will be publishing **“Data Science Projects”**

As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on Amazon.

Here is the table of contents of the book:

*1* *Data Exploration and Cleaning*

*2* *Introduction to Scikit-Learn and Model Evaluation*

*3* *Details of Logistic Regression and Feature Exploration*

*4* *The Bias Variance Trade-off*

*5* *Decision* *Trees and* *Random* *Forests*

*6* *Gradient Boosting, SHAP values (SHapley* *Additive* *exPlanations), and dealing with missing data*

*7* *Financial Analysis and Delivery to Client*

Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. 

Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 10th of May 2021.",t2_bxq52s4y,False,,0,False,Opportunity to Read and Review New Book published by Packt,[],r/datascience,False,6,education,0,,,False,t3_n6asfn,False,dark,1.0,,public,1,0,{},,,True,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1620346054.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Packt will be publishing &lt;strong&gt;“Data Science Projects”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As part of this activity, we will be sending a free digital copy of the book to you and seek your unbiased feedback about the book on Amazon.&lt;/p&gt;

&lt;p&gt;Here is the table of contents of the book:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1&lt;/em&gt; &lt;em&gt;Data Exploration and Cleaning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2&lt;/em&gt; &lt;em&gt;Introduction to Scikit-Learn and Model Evaluation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3&lt;/em&gt; &lt;em&gt;Details of Logistic Regression and Feature Exploration&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4&lt;/em&gt; &lt;em&gt;The Bias Variance Trade-off&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;5&lt;/em&gt; &lt;em&gt;Decision&lt;/em&gt; &lt;em&gt;Trees and&lt;/em&gt; &lt;em&gt;Random&lt;/em&gt; &lt;em&gt;Forests&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;6&lt;/em&gt; &lt;em&gt;Gradient Boosting, SHAP values (SHapley&lt;/em&gt; &lt;em&gt;Additive&lt;/em&gt; &lt;em&gt;exPlanations), and dealing with missing data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;7&lt;/em&gt; &lt;em&gt;Financial Analysis and Delivery to Client&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here we are offering you an opportunity to be a reviewer for our newly launched book. You will be entitled to get a free copy of the book if you are willing to become a reviewer. You can take your time to read the book and provide your unbiased review on our book’s Amazon page. &lt;/p&gt;

&lt;p&gt;Let me know whether anyone would be interested in this opportunity. If yes, kindly post in your comments on or before the 10th of May 2021.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6asfn,True,,kunal_packtpub,,0,True,all_ads,False,[],False,,/r/datascience/comments/n6asfn/opportunity_to_read_and_review_new_book_published/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6asfn/opportunity_to_read_and_review_new_book_published/,515407,1620317254.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I built a sweet tool and showcased it and everyone loved it. Then I got the “that looks easier than you made it seem” comments, like yeah because I had to break everything else to figure all of this out. Maybe it’s just me.",t2_g9nl2yr,False,,0,False,DAE get frustrated after finally finishing something cool and everyone takes for granted how hard it was?,[],r/datascience,False,6,discussion,0,,,False,t3_n5sci0,False,dark,0.96,,public,23,0,{},,,False,[],,False,False,,{},Discussion,False,23,,False,False,self,False,,[],{},,True,,1620282265.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I built a sweet tool and showcased it and everyone loved it. Then I got the “that looks easier than you made it seem” comments, like yeah because I had to break everything else to figure all of this out. Maybe it’s just me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n5sci0,True,,NotSodiumFree,,13,True,all_ads,False,[],False,,/r/datascience/comments/n5sci0/dae_get_frustrated_after_finally_finishing/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5sci0/dae_get_frustrated_after_finally_finishing/,515407,1620253465.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_803tnh3u,False,,0,False,What is your official title currently?,[],r/datascience,False,6,discussion,0,,,False,t3_n5uj0z,False,dark,0.81,,public,14,0,{},,,False,[],,False,False,,{},Discussion,False,14,,False,False,self,False,,[],{},,True,,1620288614.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n5uj0z,True,,Striking_Exchange659,,72,True,all_ads,False,[],False,,/r/datascience/comments/n5uj0z/what_is_your_official_title_currently/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5uj0z/what_is_your_official_title_currently/,515407,1620259814.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"It seems theres a massive influx of recent PhD grads in various fields but especially stem that couldn't cut it in academia or research and claim to be experts in data science but dont necessarily have any qualifications or background in analytics 

PhD in stem? Congrats you're a data scientist. Dont worry you dont have to know anything as long as you read blogs on data science central and took an intro stats course you can bullshit your way through the job and nobody will question you because you have a PhD right?

Wonder what implications of this fad will be long term for job market",t2_803tnh3u,False,,0,False,"Is data science turning into a ""catch all"" title for recent Ph.D grads?",[],r/datascience,False,6,discussion,0,,,False,t3_n6dx6f,False,dark,0.41,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1620325697.0,,[],{},,True,,1620354224.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It seems theres a massive influx of recent PhD grads in various fields but especially stem that couldn&amp;#39;t cut it in academia or research and claim to be experts in data science but dont necessarily have any qualifications or background in analytics &lt;/p&gt;

&lt;p&gt;PhD in stem? Congrats you&amp;#39;re a data scientist. Dont worry you dont have to know anything as long as you read blogs on data science central and took an intro stats course you can bullshit your way through the job and nobody will question you because you have a PhD right?&lt;/p&gt;

&lt;p&gt;Wonder what implications of this fad will be long term for job market&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n6dx6f,True,,Striking_Exchange659,,32,True,all_ads,False,[],False,,/r/datascience/comments/n6dx6f/is_data_science_turning_into_a_catch_all_title/,all_ads,False,https://www.reddit.com/r/datascience/comments/n6dx6f/is_data_science_turning_into_a_catch_all_title/,515407,1620325424.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"When I work on sql queries I usually start with a base query and edit as I go depending on the requirements, sometimes running the query and debugging. However, after being in a sql interview it seems like this approach doesn’t work the best for interviews as I frequently go down the wrong path before correcting myself, and especially not in situations where I can’t actually run intermediate queries. 

What’s a good approach for working out a sql query in a structured way?",t2_17ji8fcu,False,,0,False,How to be more structured in formulating queries for SQL interviews?,[],r/datascience,False,6,,0,,,False,t3_n53afv,False,dark,0.98,,public,189,0,{},,,False,[],,False,False,,{},Job Search,False,189,,False,False,self,False,,[],{},,True,,1620205014.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I work on sql queries I usually start with a base query and edit as I go depending on the requirements, sometimes running the query and debugging. However, after being in a sql interview it seems like this approach doesn’t work the best for interviews as I frequently go down the wrong path before correcting myself, and especially not in situations where I can’t actually run intermediate queries. &lt;/p&gt;

&lt;p&gt;What’s a good approach for working out a sql query in a structured way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n53afv,True,,suggestabledata,,64,True,all_ads,False,[],False,,/r/datascience/comments/n53afv/how_to_be_more_structured_in_formulating_queries/,all_ads,False,https://www.reddit.com/r/datascience/comments/n53afv/how_to_be_more_structured_in_formulating_queries/,515407,1620176214.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi all! I apologize for the forthcoming long post, but I’d appreciate any insights regarding negotiating/walking away from my recent offer.

I recently got my first data scientist offer after spending the last few months actively interviewing for various analyst and data scientist positions.

For some background, I’m in the midwest area of the US. I have 2+ YOE in data analytics, and I’m currently a lead data analyst managing a team focused on building internal NLP, forecasting, and other machine learning models, along with various reporting/dashboarding responsibilities using R, Python, Tableau, etc. I also have a B.S. in Stats and I’m working on my M.S.

I’ve been excited about this position as it is very involved with building and deploying customer-facing machine learning models. I was even more excited to get an offer, but I’m now feeling fairly disappointed after receiving an offer of 65k.

In my interviews with other companies and from my own research, I’ve never even discussed/seen a salary this low for a data scientist position. It seems to match what I’ve seen and been offered for other analyst roles, and I know this isn’t a case where the position is just named data “scientist” while actually being more of an analyst role.

Am I being lowballed, or does this offer make sense for someone starting their first “real” data scientist position? 

I’m typically interviewing 3+ times a week, so I don’t want to undersell myself, but since this is my first offer I’m worried I might be missing out on a good opportunity. Am I being greedy expecting more than 65k regarding my experience, or what range should I be expecting?

TL;DR First DS offer of 65k. 2+ YOE and current lead analyst. Was I lowballed, or am I over-evaluating myself regarding typical data scientist compensation?

EDIT: Thank you all for the great comments and advice! You’ve helped ease my mind, and I’ve followed your suggestions to try and negotiate, but they don’t seem interested in budging. I’ll take what I’ve learned from you all on to my next offer! Thanks!",t2_13jkwanu,False,,0,False,Lowball offer or am I being greedy?,[],r/datascience,False,6,,0,,,False,t3_n5fme5,False,dark,0.88,,public,12,0,{},,,False,[],,False,False,,{},Job Search,False,12,,False,False,self,1620242223.0,,[],{},,True,,1620249213.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all! I apologize for the forthcoming long post, but I’d appreciate any insights regarding negotiating/walking away from my recent offer.&lt;/p&gt;

&lt;p&gt;I recently got my first data scientist offer after spending the last few months actively interviewing for various analyst and data scientist positions.&lt;/p&gt;

&lt;p&gt;For some background, I’m in the midwest area of the US. I have 2+ YOE in data analytics, and I’m currently a lead data analyst managing a team focused on building internal NLP, forecasting, and other machine learning models, along with various reporting/dashboarding responsibilities using R, Python, Tableau, etc. I also have a B.S. in Stats and I’m working on my M.S.&lt;/p&gt;

&lt;p&gt;I’ve been excited about this position as it is very involved with building and deploying customer-facing machine learning models. I was even more excited to get an offer, but I’m now feeling fairly disappointed after receiving an offer of 65k.&lt;/p&gt;

&lt;p&gt;In my interviews with other companies and from my own research, I’ve never even discussed/seen a salary this low for a data scientist position. It seems to match what I’ve seen and been offered for other analyst roles, and I know this isn’t a case where the position is just named data “scientist” while actually being more of an analyst role.&lt;/p&gt;

&lt;p&gt;Am I being lowballed, or does this offer make sense for someone starting their first “real” data scientist position? &lt;/p&gt;

&lt;p&gt;I’m typically interviewing 3+ times a week, so I don’t want to undersell myself, but since this is my first offer I’m worried I might be missing out on a good opportunity. Am I being greedy expecting more than 65k regarding my experience, or what range should I be expecting?&lt;/p&gt;

&lt;p&gt;TL;DR First DS offer of 65k. 2+ YOE and current lead analyst. Was I lowballed, or am I over-evaluating myself regarding typical data scientist compensation?&lt;/p&gt;

&lt;p&gt;EDIT: Thank you all for the great comments and advice! You’ve helped ease my mind, and I’ve followed your suggestions to try and negotiate, but they don’t seem interested in budging. I’ll take what I’ve learned from you all on to my next offer! Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n5fme5,True,,SlalomMcLalom,,29,True,all_ads,False,[],False,,/r/datascience/comments/n5fme5/lowball_offer_or_am_i_being_greedy/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5fme5/lowball_offer_or_am_i_being_greedy/,515407,1620220413.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I use my own home-brewed feature engineering package in R, but haven't had time to translate it to Python. I learned from a mentor a while ago about this strategy for encoding ordinal variables which is useful for certain coefficient based regression models. The idea is similar to one-hot encoding, but each level 'adds' on to the effect of the prior lower level. So, let's say you have the following data:

|Customer|Salary Score|
|:-|:-|
|A|1|
|B|2|
|C|3|
|D|4|

You only have the 'score' of the salary, which is essentially a rank. Needs to be encoded somehow to use in a NN. What I personally call 'ordinal encoding' in my personal R package would transform this to:

|Customer|SS\_1|SS\_2|SS\_3|SS\_4|
|:-|:-|:-|:-|:-|
|A|1|0|0|0|
|B|1|1|0|0|
|C|1|1|1|0|
|D|1|1|1|1|

Does anyone know of an implementation of this in Python?",t2_i8ujh,False,,0,False,Anyone know of this type of ordinal encoding in Python?,[],r/datascience,False,6,discussion,0,,,False,t3_n5gcwe,False,dark,0.81,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,False,,[],{},,True,,1620251352.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I use my own home-brewed feature engineering package in R, but haven&amp;#39;t had time to translate it to Python. I learned from a mentor a while ago about this strategy for encoding ordinal variables which is useful for certain coefficient based regression models. The idea is similar to one-hot encoding, but each level &amp;#39;adds&amp;#39; on to the effect of the prior lower level. So, let&amp;#39;s say you have the following data:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;Customer&lt;/th&gt;
&lt;th align=""left""&gt;Salary Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;A&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;B&lt;/td&gt;
&lt;td align=""left""&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;C&lt;/td&gt;
&lt;td align=""left""&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;D&lt;/td&gt;
&lt;td align=""left""&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;You only have the &amp;#39;score&amp;#39; of the salary, which is essentially a rank. Needs to be encoded somehow to use in a NN. What I personally call &amp;#39;ordinal encoding&amp;#39; in my personal R package would transform this to:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;Customer&lt;/th&gt;
&lt;th align=""left""&gt;SS_1&lt;/th&gt;
&lt;th align=""left""&gt;SS_2&lt;/th&gt;
&lt;th align=""left""&gt;SS_3&lt;/th&gt;
&lt;th align=""left""&gt;SS_4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;A&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;B&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;C&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;D&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Does anyone know of an implementation of this in Python?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n5gcwe,True,,suspicious_gardener,,23,True,all_ads,False,[],False,,/r/datascience/comments/n5gcwe/anyone_know_of_this_type_of_ordinal_encoding_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5gcwe/anyone_know_of_this_type_of_ordinal_encoding_in/,515407,1620222552.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have to run some computationally intensive jobs and I'm not sure what the best practices are.

Right now we manually run ec2 instances in the morning, but I'm looking to automate this.

We've looked into apache airflow, simple scripts, aws lambdas, kubernetes cron, and rundeck. Can you run jobs with spark?

Rundeck seems the most promising, what do you guys think?",t2_9plkn,False,,0,False,What do you use to run jobs on a schedule?,[],r/datascience,False,6,tooling,0,,,False,t3_n52xsf,False,dark,0.86,,public,16,0,{},,,False,[],,False,False,,{},Tooling,False,16,,False,False,self,False,,[],{},,True,,1620203977.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have to run some computationally intensive jobs and I&amp;#39;m not sure what the best practices are.&lt;/p&gt;

&lt;p&gt;Right now we manually run ec2 instances in the morning, but I&amp;#39;m looking to automate this.&lt;/p&gt;

&lt;p&gt;We&amp;#39;ve looked into apache airflow, simple scripts, aws lambdas, kubernetes cron, and rundeck. Can you run jobs with spark?&lt;/p&gt;

&lt;p&gt;Rundeck seems the most promising, what do you guys think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n52xsf,True,,cMan_,,17,True,all_ads,False,[],False,,/r/datascience/comments/n52xsf/what_do_you_use_to_run_jobs_on_a_schedule/,all_ads,False,https://www.reddit.com/r/datascience/comments/n52xsf/what_do_you_use_to_run_jobs_on_a_schedule/,515407,1620175177.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I have two machine learning models trained on the same task using the same data; call them A and B. I have calculated the QWK, MSE, and r for both models using their predictions on the test set. I want to use these metrics to determine how much better (or worse) A is than B.

How should I go about this? My supervisor has led me to believe I can just subtract `QWK_B` from `QWK_A`, and that the resulting QWK delta will represent the gain (or loss) of A over B, but is that correct? And what about MSE or r? I’m pretty sure literally subtracting the metrics in those cases is not valid, in which case I don’t know how to compare them.

Any advice appreciated, TIA!",t2_x66s9,False,,0,False,"How to compare metrics? (QWK, MSE, r)",[],r/datascience,False,6,discussion,0,,,False,t3_n5fltp,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1620249166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two machine learning models trained on the same task using the same data; call them A and B. I have calculated the QWK, MSE, and r for both models using their predictions on the test set. I want to use these metrics to determine how much better (or worse) A is than B.&lt;/p&gt;

&lt;p&gt;How should I go about this? My supervisor has led me to believe I can just subtract &lt;code&gt;QWK_B&lt;/code&gt; from &lt;code&gt;QWK_A&lt;/code&gt;, and that the resulting QWK delta will represent the gain (or loss) of A over B, but is that correct? And what about MSE or r? I’m pretty sure literally subtracting the metrics in those cases is not valid, in which case I don’t know how to compare them.&lt;/p&gt;

&lt;p&gt;Any advice appreciated, TIA!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n5fltp,True,,synthphreak,,1,True,all_ads,False,[],False,,/r/datascience/comments/n5fltp/how_to_compare_metrics_qwk_mse_r/,all_ads,False,https://www.reddit.com/r/datascience/comments/n5fltp/how_to_compare_metrics_qwk_mse_r/,515407,1620220366.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi! 

I recently joined Blind app to get some insights on an interview I had at one of the FAANGs. 
I came across a lot of comments like ""Amazon is giving you $110k, are working as a janitor there?"". I understand many of those people are just doing a playful banter but there were serious posts like ""I'm depressed because I earn only 320k/year and my friends earn way more than me"".
My question is, are those people totally oblivious of the average salary or the fact that not everyone earns 6 figure salary?
I have just started working in the tech industry so I don't have that much experience, so I wanted to ask some of the senior people here that is a salary such 300-400K really possible in the tech industry and if it is what percentage of people do actually reach that level in life?

Thanks!",t2_bv171ji2,False,,0,False,"To the senior people of this sub, how much reality is there on the app, ""Blind""?",[],r/datascience,False,6,discussion,0,,,False,t3_n4to0o,False,dark,0.84,,public,19,0,{},,,False,[],,False,False,,{},Discussion,False,19,,False,False,self,False,,[],{},,True,,1620177030.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! &lt;/p&gt;

&lt;p&gt;I recently joined Blind app to get some insights on an interview I had at one of the FAANGs. 
I came across a lot of comments like &amp;quot;Amazon is giving you $110k, are working as a janitor there?&amp;quot;. I understand many of those people are just doing a playful banter but there were serious posts like &amp;quot;I&amp;#39;m depressed because I earn only 320k/year and my friends earn way more than me&amp;quot;.
My question is, are those people totally oblivious of the average salary or the fact that not everyone earns 6 figure salary?
I have just started working in the tech industry so I don&amp;#39;t have that much experience, so I wanted to ask some of the senior people here that is a salary such 300-400K really possible in the tech industry and if it is what percentage of people do actually reach that level in life?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n4to0o,True,,quite--average,,32,True,all_ads,False,[],False,,/r/datascience/comments/n4to0o/to_the_senior_people_of_this_sub_how_much_reality/,all_ads,False,https://www.reddit.com/r/datascience/comments/n4to0o/to_the_senior_people_of_this_sub_how_much_reality/,515407,1620148230.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I applied for a Data Science job recently thinking I was ready. I've been a Data Analyst for a few years now.  I had an phone interview over the phone and the job is way, WAY over my head. I don't know Python much, just basic syntax and slight use of Pandas, yet they want me to be able to code there. I meet the SQL requirements for sure, however.

The recruiter over the phone did not sound confident when I told her I did not expect Python to be used so heavily per the job description. I also told her I am aware of a models that would be used in the role like linear regression, k mean clustering, etc, but I have never actually coded them. Still, she wanted me to progress to an in-person interview next week.

I really don't think I can do this job. Some of the things she mentioned I have never even heard of. I'd love to work for this company in the future, but I'd like to bow out of the interview gracefully for now. Tips?",t2_age9ju1i,False,,0,False,How do I gracefully exit an interview I am not qualified for?,[],r/datascience,False,6,,0,,,False,t3_n4xodw,False,dark,0.57,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,False,,[],{},,True,,1620189724.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I applied for a Data Science job recently thinking I was ready. I&amp;#39;ve been a Data Analyst for a few years now.  I had an phone interview over the phone and the job is way, WAY over my head. I don&amp;#39;t know Python much, just basic syntax and slight use of Pandas, yet they want me to be able to code there. I meet the SQL requirements for sure, however.&lt;/p&gt;

&lt;p&gt;The recruiter over the phone did not sound confident when I told her I did not expect Python to be used so heavily per the job description. I also told her I am aware of a models that would be used in the role like linear regression, k mean clustering, etc, but I have never actually coded them. Still, she wanted me to progress to an in-person interview next week.&lt;/p&gt;

&lt;p&gt;I really don&amp;#39;t think I can do this job. Some of the things she mentioned I have never even heard of. I&amp;#39;d love to work for this company in the future, but I&amp;#39;d like to bow out of the interview gracefully for now. Tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n4xodw,True,,showsomeskin1337,,22,True,all_ads,False,[],False,,/r/datascience/comments/n4xodw/how_do_i_gracefully_exit_an_interview_i_am_not/,all_ads,False,https://www.reddit.com/r/datascience/comments/n4xodw/how_do_i_gracefully_exit_an_interview_i_am_not/,515407,1620160924.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"**DISCLAIMER**: This is completely free and not sponsored in any way. I really just enjoy helping students get started and potentially transition into Data Science

As the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host **another** Data Science Q&amp;A this Thursday at 5:30 PM PST. This time I'll have **Krishna Rao** join me. Susan is an Applied Scientist at **Amazon** and is responsible for building state-of-the-art advertising recommendation systems! Krishna has had a slightly unconventional path to get to this point. His background is in Civil Engineering and he was first a Data Science consultant before joining Amazon. I'm looking forward to having him share his journey and the tips he picked up along the way.

The last session was an absolute blast with over 250 people who attended from all over the world. I hope you see you all there!

Register Here:

[https://disney.zoom.us/webinar/register/WN\_RF0xeFZZTWqi8l7ZAN4KOg](https://disney.zoom.us/webinar/register/WN_RF0xeFZZTWqi8l7ZAN4KOg)

Verification:

My photo: [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)

My LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (feel free to connect)

Krishna’s LinkedIn: [https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/](https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/)",t2_hgs0n,False,,0,False,I'm a Senior Data Scientist at Disney and I'm hosting another Data Science Q&amp;A session this Thursday @ 5:30 PM PST. I'll be joined by an Applied Scientist at Amazon!,[],r/datascience,False,6,network,0,,,False,t3_n3v93k,False,dark,0.94,,public,705,5,{},,,False,[],,False,False,,{},Networking,False,705,,False,False,self,1620048029.0,,[],{'gid_1': 1},,True,,1620075201.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;: This is completely free and not sponsored in any way. I really just enjoy helping students get started and potentially transition into Data Science&lt;/p&gt;

&lt;p&gt;As the title mentions, I&amp;#39;m a Senior Data Scientist at Disney and I&amp;#39;m going to host &lt;strong&gt;another&lt;/strong&gt; Data Science Q&amp;amp;A this Thursday at 5:30 PM PST. This time I&amp;#39;ll have &lt;strong&gt;Krishna Rao&lt;/strong&gt; join me. Susan is an Applied Scientist at &lt;strong&gt;Amazon&lt;/strong&gt; and is responsible for building state-of-the-art advertising recommendation systems! Krishna has had a slightly unconventional path to get to this point. His background is in Civil Engineering and he was first a Data Science consultant before joining Amazon. I&amp;#39;m looking forward to having him share his journey and the tips he picked up along the way.&lt;/p&gt;

&lt;p&gt;The last session was an absolute blast with over 250 people who attended from all over the world. I hope you see you all there!&lt;/p&gt;

&lt;p&gt;Register Here:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://disney.zoom.us/webinar/register/WN_RF0xeFZZTWqi8l7ZAN4KOg""&gt;https://disney.zoom.us/webinar/register/WN_RF0xeFZZTWqi8l7ZAN4KOg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Verification:&lt;/p&gt;

&lt;p&gt;My photo: &lt;a href=""https://imgur.com/a/Wg3DMLV""&gt;https://imgur.com/a/Wg3DMLV&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My LinkedIn: &lt;a href=""https://www.linkedin.com/in/madhavthaker/""&gt;https://www.linkedin.com/in/madhavthaker/&lt;/a&gt; (feel free to connect)&lt;/p&gt;

&lt;p&gt;Krishna’s LinkedIn: &lt;a href=""https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/""&gt;https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3v93k,True,,madzthakz,,51,True,all_ads,False,[],False,,/r/datascience/comments/n3v93k/im_a_senior_data_scientist_at_disney_and_im/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3v93k/im_a_senior_data_scientist_at_disney_and_im/,515407,1620046401.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?auto=webp&amp;s=97393a977e0e80f4a8f50828b461061143579f7b', 'width': 1125, 'height': 2000}, 'resolutions': [{'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2206c692018db7ec9c35968034ecd8eb7b278995', 'width': 108, 'height': 192}, {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=130280ab79784bf8f11813d16529cd57572f2645', 'width': 216, 'height': 384}, {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb5477f17da5ace98771381e79fcf63907e10214', 'width': 320, 'height': 568}, {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e0acb00dd9dcebb38c34c0ac14b7657ccbc76bd', 'width': 640, 'height': 1137}, {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1d6ecbaa238132454f3b2ec316be7a70d14718e', 'width': 960, 'height': 1706}, {'url': 'https://external-preview.redd.it/5fye2tXKh28Ut8vuG6F9-F0K7V4RQTu-QjWZI16al-w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c5d177761023df20a5c21a59002acbdb81385c57', 'width': 1080, 'height': 1920}], 'variants': {}, 'id': 'i2D16ro_ykx1fH7JYPnsIFBkdjW1nVxfvvDNpnqiY4U'}], 'enabled': False}",,,,,
,datascience," 

Hi all,

I am the CMO of an ecommerce company and have a Shopify store. I have been working towards improving the way I go about presenting in our weekly advertising meetings. In the past I've listed out campaigns, the performance of key metrics, sales attributed from each platforms and then discussed budget based on the performance.

Recently, I have been trying to correlate our advertising campaigns with the percentage of sales related to specific advertising objectives. For instance, if we're spending 50% of our ad dollars promoting wedding products, what percentage of our products sold are wedding related?

This has been helpful but I want to take it a few steps further. I'd like to chart product sales week over week but I am struggling to figure out the best way to go about it. We have a handful of products which each have different designs / customization options. More specifically if one of our wedding products sold 30 times last week and only 12 times this week. I would like it to say the Product Name, the units sold, and then the percentage change from the previous week. The goal will be to use this data to monitor how certain products are selling and then make changes or not based on the data. For instance if a product is not selling and we think it could be due to visibility, we could test highlighting it on the homepage or other landing pages and see week over week if it preforms better. Does that make sense?

Does anyone have a better way that they track and analyze sales and marketing dollars?

*\* I also posted this in* [r/analytics](https://www.reddit.com/r/analytics/) *and someone suggested using ecommerce tracking in google analytics. I am just confused how to go about setting this up*",t2_1lyw61xf,False,,0,False,Product Sales + Marketing Data Reporting,[],r/datascience,False,6,discussion,0,,,False,t3_n4rmfw,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1620172155.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I am the CMO of an ecommerce company and have a Shopify store. I have been working towards improving the way I go about presenting in our weekly advertising meetings. In the past I&amp;#39;ve listed out campaigns, the performance of key metrics, sales attributed from each platforms and then discussed budget based on the performance.&lt;/p&gt;

&lt;p&gt;Recently, I have been trying to correlate our advertising campaigns with the percentage of sales related to specific advertising objectives. For instance, if we&amp;#39;re spending 50% of our ad dollars promoting wedding products, what percentage of our products sold are wedding related?&lt;/p&gt;

&lt;p&gt;This has been helpful but I want to take it a few steps further. I&amp;#39;d like to chart product sales week over week but I am struggling to figure out the best way to go about it. We have a handful of products which each have different designs / customization options. More specifically if one of our wedding products sold 30 times last week and only 12 times this week. I would like it to say the Product Name, the units sold, and then the percentage change from the previous week. The goal will be to use this data to monitor how certain products are selling and then make changes or not based on the data. For instance if a product is not selling and we think it could be due to visibility, we could test highlighting it on the homepage or other landing pages and see week over week if it preforms better. Does that make sense?&lt;/p&gt;

&lt;p&gt;Does anyone have a better way that they track and analyze sales and marketing dollars?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;\&lt;/em&gt; I also posted this in* &lt;a href=""https://www.reddit.com/r/analytics/""&gt;r/analytics&lt;/a&gt; &lt;em&gt;and someone suggested using ecommerce tracking in google analytics. I am just confused how to go about setting this up&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n4rmfw,True,,OwnStep,,0,True,all_ads,False,[],False,,/r/datascience/comments/n4rmfw/product_sales_marketing_data_reporting/,all_ads,False,https://www.reddit.com/r/datascience/comments/n4rmfw/product_sales_marketing_data_reporting/,515407,1620143355.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,True,,,,
,datascience,"I am ***happy*** to announce that today, I am quitting my current position of 3 years - I built my current team of DEs and DSs from the ground up, but I'm always the last priority in terms of pay and advancement, and on top of it, non-technical colleagues get all the credit for my work, so while they advance, I've been stuck for quite a while. Our leadership team is non-technical and yet, they dictate our development cycles (causing 36 hour work *days*), and to top it off, we have a terrible IT department that has built a horribly configured stack.  

Every day can be a nightmare for some of us due to various factors. What's your story? Why did you take the job in the first place and when did it become too much?  Did the experience lead you to land your dream job or was the whole transition a total disaster?",t2_4ciy3,False,,0,False,I Quit! Stories,[],r/datascience,False,6,career,0,,,False,t3_n3up4g,False,dark,0.95,,public,81,1,{},,,False,[],,False,False,,{},Career,False,81,,False,False,self,False,,[],{},,True,,1620073592.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am &lt;strong&gt;&lt;em&gt;happy&lt;/em&gt;&lt;/strong&gt; to announce that today, I am quitting my current position of 3 years - I built my current team of DEs and DSs from the ground up, but I&amp;#39;m always the last priority in terms of pay and advancement, and on top of it, non-technical colleagues get all the credit for my work, so while they advance, I&amp;#39;ve been stuck for quite a while. Our leadership team is non-technical and yet, they dictate our development cycles (causing 36 hour work &lt;em&gt;days&lt;/em&gt;), and to top it off, we have a terrible IT department that has built a horribly configured stack.  &lt;/p&gt;

&lt;p&gt;Every day can be a nightmare for some of us due to various factors. What&amp;#39;s your story? Why did you take the job in the first place and when did it become too much?  Did the experience lead you to land your dream job or was the whole transition a total disaster?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3up4g,True,,tmotytmoty,,30,True,all_ads,False,[],False,,/r/datascience/comments/n3up4g/i_quit_stories/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3up4g/i_quit_stories/,515407,1620044792.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I think some of us are either solo'ing a lot of DS efforts in our current roles/orgs. , or work with teams that may be inexperienced and rely on you as a ""subject matter expert"" for data science (analyzing / modelling structured data).

That's my situation at least....so how do I know how I'm doing?

* Comparing to kaggle is fine, but honestly most notebooks on kaggle are quite poor quality (and even the better ones are more CS focused than deep thorough analysis)
* My team is happy with my work, but that isn't saying much (they don't know better)

Appreciate some insight here, suggestions, etc.

Thanks!",t2_7m1zlf41,False,,0,False,"Working in a bubble, how do I know how I'm *actually* doing?",[],r/datascience,False,6,discussion,0,,,False,t3_n44nzl,False,dark,0.89,,public,14,0,{},,,False,[],,False,False,,{},Discussion,False,14,,False,False,self,False,,[],{},,True,,1620098097.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I think some of us are either solo&amp;#39;ing a lot of DS efforts in our current roles/orgs. , or work with teams that may be inexperienced and rely on you as a &amp;quot;subject matter expert&amp;quot; for data science (analyzing / modelling structured data).&lt;/p&gt;

&lt;p&gt;That&amp;#39;s my situation at least....so how do I know how I&amp;#39;m doing?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comparing to kaggle is fine, but honestly most notebooks on kaggle are quite poor quality (and even the better ones are more CS focused than deep thorough analysis)&lt;/li&gt;
&lt;li&gt;My team is happy with my work, but that isn&amp;#39;t saying much (they don&amp;#39;t know better)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Appreciate some insight here, suggestions, etc.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n44nzl,True,,Limp-Ad-7289,,10,True,all_ads,False,[],False,,/r/datascience/comments/n44nzl/working_in_a_bubble_how_do_i_know_how_im_actually/,all_ads,False,https://www.reddit.com/r/datascience/comments/n44nzl/working_in_a_bubble_how_do_i_know_how_im_actually/,515407,1620069297.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Python is really an easy programming language and  a lot of people claim to know. So I am thinking about if it's a good investment of time and efforts to learn Python, or I should try Software Engineering with JVM. C, or some other CS field.",t2_5t56uq7x,False,,0,False,Would it be a better career choice for me to learn Python (with frameworks) for a Data Science job or the field is already overcrowded with so many Python programmers?,[],r/datascience,False,6,career,0,,,False,t3_n4qh8b,False,dark,0.18,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1620169292.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Python is really an easy programming language and  a lot of people claim to know. So I am thinking about if it&amp;#39;s a good investment of time and efforts to learn Python, or I should try Software Engineering with JVM. C, or some other CS field.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n4qh8b,True,,Born-Comment3359,,8,True,all_ads,False,[],False,,/r/datascience/comments/n4qh8b/would_it_be_a_better_career_choice_for_me_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/n4qh8b/would_it_be_a_better_career_choice_for_me_to/,515407,1620140492.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",t2_xtuyc,False,,0,False,Inevitable Manual Work Required in Data Science Projects,[],r/datascience,False,6,discussion,0,,,False,t3_n4i0e9,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1620140063.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? &lt;/p&gt;

&lt;p&gt;For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let&amp;#39;s say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let&amp;#39;s call this &amp;quot;class 1&amp;quot;) or a non-serious condition (let&amp;#39;s call this &amp;quot;class 0&amp;quot;). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. &lt;/p&gt;

&lt;p&gt;The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as &amp;quot;class 1&amp;quot; or &amp;quot;class 0&amp;quot;. For example, for &amp;quot;class 0&amp;quot; : one of the doctors could clearly write at the end of a report &amp;quot;all medical tests were conducted and the results and were all negative&amp;quot;, and another doctor could end the report by saying &amp;quot;the patient should seriously consider changing their lifestyle and eat healthier food. benign.&amp;quot; . &lt;/p&gt;

&lt;p&gt;In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a &amp;quot;serious condition&amp;quot; or a &amp;quot;non-serious condition&amp;quot;? I was thinking of using something like &amp;quot;sentiment analysis&amp;quot; to capture the &amp;quot;mood&amp;quot; of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is &amp;quot;dark&amp;quot; (serious condition) or &amp;quot;light&amp;quot; (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?&lt;/p&gt;

&lt;p&gt;In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a &amp;quot;serious&amp;quot; or a &amp;quot;non-serious&amp;quot; condition?&lt;/p&gt;

&lt;p&gt;PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n4i0e9,True,,ottawalanguages,,4,True,all_ads,False,[],False,,/r/datascience/comments/n4i0e9/inevitable_manual_work_required_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/n4i0e9/inevitable_manual_work_required_in_data_science/,515407,1620111263.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have been working for a startup where we are a team of 5-6 data scientists. We regularly have to make decisions regarding our infrastructure requirements and the tools that we would use for our analysis/model building.

We have a powerful EC2 server where we run our computation and data-heavy analysis using Jupyter notebooks and python scripts. 

However, this probably would not scale and we already have situations where the RAM runs out and we run code overnight to get some results.

Is there an obvious solution on the modern cloud ecosystem that would preclude the need to have a server and allow us to use compute power and RAM as and when we need it? Preferably within the AWS ecosystem.",t2_tetcu,False,,0,False,The modern way to run notebooks on the cloud,[],r/datascience,False,6,tooling,0,,,False,t3_n3wdn8,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},Tooling,False,7,,False,False,self,False,,[],{},,True,,1620078170.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working for a startup where we are a team of 5-6 data scientists. We regularly have to make decisions regarding our infrastructure requirements and the tools that we would use for our analysis/model building.&lt;/p&gt;

&lt;p&gt;We have a powerful EC2 server where we run our computation and data-heavy analysis using Jupyter notebooks and python scripts. &lt;/p&gt;

&lt;p&gt;However, this probably would not scale and we already have situations where the RAM runs out and we run code overnight to get some results.&lt;/p&gt;

&lt;p&gt;Is there an obvious solution on the modern cloud ecosystem that would preclude the need to have a server and allow us to use compute power and RAM as and when we need it? Preferably within the AWS ecosystem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3wdn8,True,,dhruvnigam93,,18,True,all_ads,False,[],False,,/r/datascience/comments/n3wdn8/the_modern_way_to_run_notebooks_on_the_cloud/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3wdn8/the_modern_way_to_run_notebooks_on_the_cloud/,515407,1620049370.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Or any organization that isn't obviously evil, I guess. Jokes aside, I would really appreciate if you could share what role you play in your org and if it's fulfilling work. This is the setting I'm determined to work in, and I need an idea of what roles are given to people with data science skills. TIA",t2_4zfjv7m1,False,,0,False,Anyone here working for a non-profit?,[],r/datascience,False,6,career,0,,,False,t3_n3n4hb,False,dark,1.0,,public,23,0,{},,,False,[],,False,False,,{},Career,False,23,,False,False,self,False,,[],{},,True,,1620042197.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Or any organization that isn&amp;#39;t obviously evil, I guess. Jokes aside, I would really appreciate if you could share what role you play in your org and if it&amp;#39;s fulfilling work. This is the setting I&amp;#39;m determined to work in, and I need an idea of what roles are given to people with data science skills. TIA&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3n4hb,True,,MunchkinCatto,,14,True,all_ads,False,[],False,,/r/datascience/comments/n3n4hb/anyone_here_working_for_a_nonprofit/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3n4hb/anyone_here_working_for_a_nonprofit/,515407,1620013397.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'd like to know how often do companies look for players from another country, if any of you were able to achieve this, considering that earning in dollar would be incredibly benefitial for me, even on a JR analyst position.",t2_8g8wy,False,,0,False,"In DS field is it possible to get a remote job, working from another country?",[],r/datascience,False,6,discussion,0,,,False,t3_n3ew7z,False,dark,0.88,,public,73,0,{},,,False,[],,False,False,,{},Discussion,False,73,,False,False,self,False,,[],{},,True,,1620015268.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;d like to know how often do companies look for players from another country, if any of you were able to achieve this, considering that earning in dollar would be incredibly benefitial for me, even on a JR analyst position.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3ew7z,True,,ruzanovsky,,49,True,all_ads,False,[],False,,/r/datascience/comments/n3ew7z/in_ds_field_is_it_possible_to_get_a_remote_job/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3ew7z/in_ds_field_is_it_possible_to_get_a_remote_job/,515407,1619986468.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone,A number of volunteers (product, software, management backgrounds) are trying to build a model to predict how Oxygen demand will shape. When supplies improve, they wish to be able to help agencies distribute to the correct places, logistics issue as you might imagine. There is a model they are trying to build but too many unknown variables/uncertainties.

Do we have folks in **data science** who can help? Even if you can add a little it helps a lot. If you know folks with relevant **forecasting/demand prediction** experience it could be immensely valuable. We can share whatever data points we have. But most people in this sub probably already know what this entails.

Thank you so much,


**Update**:

I am so sorry that I am adding this  late. Here is what I can say the intro document: https://docs.google.com/document/d/1HQDeIlTzl3UrtntNncQVunZNQRU6DXLV4T2UA76Np_w/edit#

Here are the issues on GH for what is becoming the large volunteer alliance, one DB: https://github.com/gantir/covid19alliance-masterdata/issues

All volunteer discussions take place in Slack: https://slack.raksha.life, main channel for this is #coronasafe-demand-prediction

I will respond to each of you through DM.",t2_d4cx4,False,,0,False,Covid-19 India related ask,[],r/datascience,False,6,projects,0,,,False,t3_n3ba0g,False,dark,0.93,,public,103,0,{},,,False,[],,False,False,,{},Projects,False,103,,False,False,self,1620023987.0,,[],{},,True,,1620005122.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,A number of volunteers (product, software, management backgrounds) are trying to build a model to predict how Oxygen demand will shape. When supplies improve, they wish to be able to help agencies distribute to the correct places, logistics issue as you might imagine. There is a model they are trying to build but too many unknown variables/uncertainties.&lt;/p&gt;

&lt;p&gt;Do we have folks in &lt;strong&gt;data science&lt;/strong&gt; who can help? Even if you can add a little it helps a lot. If you know folks with relevant &lt;strong&gt;forecasting/demand prediction&lt;/strong&gt; experience it could be immensely valuable. We can share whatever data points we have. But most people in this sub probably already know what this entails.&lt;/p&gt;

&lt;p&gt;Thank you so much,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;I am so sorry that I am adding this  late. Here is what I can say the intro document: &lt;a href=""https://docs.google.com/document/d/1HQDeIlTzl3UrtntNncQVunZNQRU6DXLV4T2UA76Np_w/edit#""&gt;https://docs.google.com/document/d/1HQDeIlTzl3UrtntNncQVunZNQRU6DXLV4T2UA76Np_w/edit#&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are the issues on GH for what is becoming the large volunteer alliance, one DB: &lt;a href=""https://github.com/gantir/covid19alliance-masterdata/issues""&gt;https://github.com/gantir/covid19alliance-masterdata/issues&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All volunteer discussions take place in Slack: &lt;a href=""https://slack.raksha.life""&gt;https://slack.raksha.life&lt;/a&gt;, main channel for this is #coronasafe-demand-prediction&lt;/p&gt;

&lt;p&gt;I will respond to each of you through DM.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3ba0g,True,,sumitdatta,,43,True,all_ads,False,[],False,,/r/datascience/comments/n3ba0g/covid19_india_related_ask/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3ba0g/covid19_india_related_ask/,515407,1619976322.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?auto=webp&amp;s=29ac6679dc20a4dc3b5af2852bb840c70f8198b1', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e483577c7755a6c44c69ab6b5477336b353f52b8', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=090d5c2ffe3b7518fdea6c4fd4e0e77e360a14cb', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=37ace997abef485275492cbad1489e30159dbdbf', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed9fdd08be26016a4d4ffa8603ef236221cbcc72', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=249e33fc941813d48654b2bfaeed44e47f43808a', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/XG2LLxmeo3zPPLVvyKgMDt9kPhKDNFm0peVoqrf3unA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cf54f89bd63e570aa9c405932cc5a92f3573dd9', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'vtnC622qRAlgSXk-f9YoWrLT4dJmMoBmckjbRqckQbc'}], 'enabled': False}",,,,,
,datascience,"Lead Product Manager here who recently joined a new company. I have about ten years of experience but this is the first time I’m working on a heavy data science product with a junior data science team 😔

I am new to the org. and on my second day the lead of the data science team put in his two weeks but didn’t stick around to provide institutional knowledge or any time for me to understand what’s been going on. The data science is a small team of 4 and they are very very junior.

With that said, I’m really trying to get them involved in our daily meetings so they can understand the vision and product we are building so ultimately be a individual contributor and ultimately 

- point out where data science work is needed. The problem is they don’t say a word...

It’s been 8 weeks and the data science really does not give any input at all - not even on our designs which is super important and When asked to give feedback they stall and take it off line but I really think they just don’t have proper leadership. 

I just can’t get them to give any Input so I am not able to understand Backend implications and such. I imagine at a certain point the data science team will have to build “something” to power BE efforts for example, but if they don’t tell me that up front when discussing designs or requirements ...I can’t plan or gather solid data science requirements which is nuts. 

What can I do to get more data science input to fuel our mission. 80 percent of requirements have a data science implication from what I see and I need the data science team to step up more. What can I do to make sure we have proper data science requirements?

How can I set up a successful data science strategy?",t2_9m6dxgcx,False,,0,False,I’m a PM a and need data science advice.,[],r/datascience,False,6,projects,0,,,False,t3_n3ca4c,False,dark,0.88,,public,26,0,{},,,False,[],,False,False,,{},Projects,False,26,,False,False,self,False,,[],{},,True,,1620008039.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lead Product Manager here who recently joined a new company. I have about ten years of experience but this is the first time I’m working on a heavy data science product with a junior data science team 😔&lt;/p&gt;

&lt;p&gt;I am new to the org. and on my second day the lead of the data science team put in his two weeks but didn’t stick around to provide institutional knowledge or any time for me to understand what’s been going on. The data science is a small team of 4 and they are very very junior.&lt;/p&gt;

&lt;p&gt;With that said, I’m really trying to get them involved in our daily meetings so they can understand the vision and product we are building so ultimately be a individual contributor and ultimately &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;point out where data science work is needed. The problem is they don’t say a word...&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s been 8 weeks and the data science really does not give any input at all - not even on our designs which is super important and When asked to give feedback they stall and take it off line but I really think they just don’t have proper leadership. &lt;/p&gt;

&lt;p&gt;I just can’t get them to give any Input so I am not able to understand Backend implications and such. I imagine at a certain point the data science team will have to build “something” to power BE efforts for example, but if they don’t tell me that up front when discussing designs or requirements ...I can’t plan or gather solid data science requirements which is nuts. &lt;/p&gt;

&lt;p&gt;What can I do to get more data science input to fuel our mission. 80 percent of requirements have a data science implication from what I see and I need the data science team to step up more. What can I do to make sure we have proper data science requirements?&lt;/p&gt;

&lt;p&gt;How can I set up a successful data science strategy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3ca4c,True,,gullygang1,,36,True,all_ads,False,[],False,,/r/datascience/comments/n3ca4c/im_a_pm_a_and_need_data_science_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3ca4c/im_a_pm_a_and_need_data_science_advice/,515407,1619979239.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Question in title. 

Context: I’m looking to organize the text datasets that we have at where I work. Right now it’s a bit all over the place depending on the data source: BigQuery, PostgreSQL (there are reasons why we have two separate dbs but I won’t get into that), Google Sheets, csvs, etc. Besides of labeled datasets, we also have many dictionaries that we use to build simple rules. 

The motivation is, each project usually has their own datasets but there have been a lot of cases where we can leverage other project’s datasets. Currently the only way is to ask the PIC of the project. It doesn’t seem to be very scalable now especially that we’re now having more NLP projects and more people working on them.

Right now, as a first step, I’m thinking to just maintain a list of queries for the ones in db and keep all the docs/csvs in one folder. Although ideally I’d love to have these datasets easily accessible much like TensorFlow/HuggingFace datasets (not sure if we have the bandwidth to build a GUI to explore the datasets though). 

Have you faced a similar problem? I would like to hear any suggestions or your experience if any, as well as the challenges you encountered. Thanks a lot!",t2_7erhw469,False,,0,False,How do you organize your text datasets for NLP projects?,[],r/datascience,False,6,discussion,0,,,False,t3_n3mhga,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1620039900.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Question in title. &lt;/p&gt;

&lt;p&gt;Context: I’m looking to organize the text datasets that we have at where I work. Right now it’s a bit all over the place depending on the data source: BigQuery, PostgreSQL (there are reasons why we have two separate dbs but I won’t get into that), Google Sheets, csvs, etc. Besides of labeled datasets, we also have many dictionaries that we use to build simple rules. &lt;/p&gt;

&lt;p&gt;The motivation is, each project usually has their own datasets but there have been a lot of cases where we can leverage other project’s datasets. Currently the only way is to ask the PIC of the project. It doesn’t seem to be very scalable now especially that we’re now having more NLP projects and more people working on them.&lt;/p&gt;

&lt;p&gt;Right now, as a first step, I’m thinking to just maintain a list of queries for the ones in db and keep all the docs/csvs in one folder. Although ideally I’d love to have these datasets easily accessible much like TensorFlow/HuggingFace datasets (not sure if we have the bandwidth to build a GUI to explore the datasets though). &lt;/p&gt;

&lt;p&gt;Have you faced a similar problem? I would like to hear any suggestions or your experience if any, as well as the challenges you encountered. Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3mhga,True,,Glittering_Coyote307,,1,True,all_ads,False,[],False,,/r/datascience/comments/n3mhga/how_do_you_organize_your_text_datasets_for_nlp/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3mhga/how_do_you_organize_your_text_datasets_for_nlp/,515407,1620011100.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This transaction could be anything depending on a website like in my case(financial website) it could range from opening a bank account/ linking an account,etc etc.
Also the problem with using common metrics bounce rate,exit rate is that it completely disregard the intention of the visitors, maybe the person is just there for explorative purpose and if he doesn't perform an action that not necessarily means it's a friction point given the intention",t2_1j8uucki,False,,0,False,How can someone measure/quantify friction faced by visitors on a website while performing a transaction/action,[],r/datascience,False,6,discussion,0,,,False,t3_n3339v,False,dark,0.96,,public,103,0,{},,,False,[],,False,False,,{},Discussion,False,103,,False,False,self,False,,[],{},,True,,1619977568.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This transaction could be anything depending on a website like in my case(financial website) it could range from opening a bank account/ linking an account,etc etc.
Also the problem with using common metrics bounce rate,exit rate is that it completely disregard the intention of the visitors, maybe the person is just there for explorative purpose and if he doesn&amp;#39;t perform an action that not necessarily means it&amp;#39;s a friction point given the intention&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3339v,True,,fAceHigh,,58,True,all_ads,False,[],False,,/r/datascience/comments/n3339v/how_can_someone_measurequantify_friction_faced_by/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3339v/how_can_someone_measurequantify_friction_faced_by/,515407,1619948768.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I usually just use the site to talk about stuff I do on my free-time, but subs like unpopularopinion have me thinking that it would fun to tinker around and compile some dataframes. Noticed some examples on dataisbeautiful and datart.

If you have used Reddit itself for making models, what models did you make and how did you go about data cleansing?",t2_anh4i,False,,0,False,Anyone play around with subreddit data?,[],r/datascience,False,6,fun,0,,,False,t3_n363uv,False,dark,0.94,,public,27,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,27,,False,False,self,False,,[],{},,True,,1619989855.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I usually just use the site to talk about stuff I do on my free-time, but subs like unpopularopinion have me thinking that it would fun to tinker around and compile some dataframes. Noticed some examples on dataisbeautiful and datart.&lt;/p&gt;

&lt;p&gt;If you have used Reddit itself for making models, what models did you make and how did you go about data cleansing?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n363uv,True,,rogerthealien17,,13,True,all_ads,False,[],False,,/r/datascience/comments/n363uv/anyone_play_around_with_subreddit_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/n363uv/anyone_play_around_with_subreddit_data/,515407,1619961055.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,"Hi All,

I am analyzing our companies transaction data &amp; visits data , we have lot of user actions before making a transaction . I am trying to segment the user base on the activity signals they are generating ,but was confused on how to merge multiple data signals distribution.

For example :

I have user performing actions such as 

 View an Item

List an item

Buy an Item 

Since all of these activities have their own distributions , I was thinking to normalize each of them for a user last 12 months activity ,  but was not sure if it would make sense to sum up these individual normalized signals in some way so that I have just one score against which I can benchmark users current activity.

Like if I see that the user has X value (combined score across all actions) in last 12 months , and this month he has X- 12% , I can estimate that he is decreasing so on so forth",t2_6ys5mu5,False,,0,False,Normalizing &amp; Merging two different data signals,[],r/datascience,False,6,discussion,0,,,False,t3_n3btyo,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1620006749.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;I am analyzing our companies transaction data &amp;amp; visits data , we have lot of user actions before making a transaction . I am trying to segment the user base on the activity signals they are generating ,but was confused on how to merge multiple data signals distribution.&lt;/p&gt;

&lt;p&gt;For example :&lt;/p&gt;

&lt;p&gt;I have user performing actions such as &lt;/p&gt;

&lt;p&gt;View an Item&lt;/p&gt;

&lt;p&gt;List an item&lt;/p&gt;

&lt;p&gt;Buy an Item &lt;/p&gt;

&lt;p&gt;Since all of these activities have their own distributions , I was thinking to normalize each of them for a user last 12 months activity ,  but was not sure if it would make sense to sum up these individual normalized signals in some way so that I have just one score against which I can benchmark users current activity.&lt;/p&gt;

&lt;p&gt;Like if I see that the user has X value (combined score across all actions) in last 12 months , and this month he has X- 12% , I can estimate that he is decreasing so on so forth&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n3btyo,True,,user19911506,,5,True,all_ads,False,[],False,,/r/datascience/comments/n3btyo/normalizing_merging_two_different_data_signals/,all_ads,False,https://www.reddit.com/r/datascience/comments/n3btyo/normalizing_merging_two_different_data_signals/,515407,1619977949.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 02 May 2021 - 09 May 2021,[],r/datascience,False,6,,0,,,False,t3_n34zv0,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1619985630.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,n34zv0,True,,datascience-bot,,143,False,all_ads,False,[],False,dark,/r/datascience/comments/n34zv0/weekly_entering_transitioning_thread_02_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/n34zv0/weekly_entering_transitioning_thread_02_may_2021/,515407,1619956830.0,0,,False,,,,,,,,
,datascience,"Hi All,

I have to set up a GitHub repo for an upcoming project and was researching some data science templates to follow. I came across cookie cutter and this template by drivendata: [https://github.com/drivendata/cookiecutter-data-science](https://github.com/drivendata/cookiecutter-data-science)

&amp;#x200B;

It looks pretty comprehensive, but I feel I might not need a lot of it, like my data would be pulled straight from a db and not from a dump and neither it would be stored somewhere so the need for data folder is not there.

I would be developing a modelling pipeline and would not be saving the serialised model files, so no need of model folder as well. I think you guys know where I am going here.

So, I just felt like I will get to know what the community is following?

Thanks.",t2_14zvnbe7,False,,0,False,What GitHub template do you guys follow?,[],r/datascience,False,6,discussion,0,,,False,t3_n2gqwb,False,dark,0.97,,public,194,0,{},,,False,[],,False,False,,{},Discussion,False,194,,False,False,self,False,,[],{},,True,,1619898282.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;I have to set up a GitHub repo for an upcoming project and was researching some data science templates to follow. I came across cookie cutter and this template by drivendata: &lt;a href=""https://github.com/drivendata/cookiecutter-data-science""&gt;https://github.com/drivendata/cookiecutter-data-science&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;It looks pretty comprehensive, but I feel I might not need a lot of it, like my data would be pulled straight from a db and not from a dump and neither it would be stored somewhere so the need for data folder is not there.&lt;/p&gt;

&lt;p&gt;I would be developing a modelling pipeline and would not be saving the serialised model files, so no need of model folder as well. I think you guys know where I am going here.&lt;/p&gt;

&lt;p&gt;So, I just felt like I will get to know what the community is following?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n2gqwb,True,,pm_me_tap_ins,,32,True,all_ads,False,[],False,,/r/datascience/comments/n2gqwb/what_github_template_do_you_guys_follow/,all_ads,False,https://www.reddit.com/r/datascience/comments/n2gqwb/what_github_template_do_you_guys_follow/,515407,1619869482.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?auto=webp&amp;s=4ebb327356e30070c1f56be99ad78f9ca059f467', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9193f72bb3a941fbde33441a026742b3da18263a', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b5bd038397b9d464d360abbed9d3e733e0f7f1d', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22e03f95ffe9854025172950b5143f1914958830', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a1a3a8a05ae8f482d317eee2c4d310b79a98f30', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbc95b9577cda1ec47383a19662c8fd5be4af6d3', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/I_8Vf2E3UlWNW-xhpUyvcRYFhtvEvj-voOlAn89SEQQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=661d38ef77304d1313eda435744ebcc047eb598f', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'pmHgbNz2kCPjr7MwfKunWSMWfUFVRMXy26gd5WR8x9s'}], 'enabled': False}",,,,,
,datascience,"I'm on track to graduate with a MS in Statistics in a few months time. I have no prior working experience (only software and data engineering internships) and my bachelors was in engineering (Industrial Engineering with Computer Science minor).

I tried applying to data science jobs but not much luck so far. Most of my interview callbacks are from engineering positions. I honestly do not mind being an engineer. However, does it mean that I would be wasting my MS?

Ps I currently have an offer as an engineer in Cyber Security. Just would like to know the general feedback for a graduate with no work experience. Thanks for reading.",t2_7amwafxa,False,,0,False,Is it a waste to not go into data science after getting my MS?,[],r/datascience,False,6,,0,,,False,t3_n2je24,False,dark,0.81,,public,21,1,{},,,False,[],,False,False,,{},Job Search,False,21,,False,False,self,False,,[],{},,True,,1619908024.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m on track to graduate with a MS in Statistics in a few months time. I have no prior working experience (only software and data engineering internships) and my bachelors was in engineering (Industrial Engineering with Computer Science minor).&lt;/p&gt;

&lt;p&gt;I tried applying to data science jobs but not much luck so far. Most of my interview callbacks are from engineering positions. I honestly do not mind being an engineer. However, does it mean that I would be wasting my MS?&lt;/p&gt;

&lt;p&gt;Ps I currently have an offer as an engineer in Cyber Security. Just would like to know the general feedback for a graduate with no work experience. Thanks for reading.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n2je24,True,,Bright_Log5644,,19,True,all_ads,False,[],False,,/r/datascience/comments/n2je24/is_it_a_waste_to_not_go_into_data_science_after/,all_ads,False,https://www.reddit.com/r/datascience/comments/n2je24/is_it_a_waste_to_not_go_into_data_science_after/,515407,1619879224.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I’ve been in my first data science opportunity for almost a year now and I’m starting to question if I made a mistake entering this field. 

My job is all politics. I’m pulled every which way. I’m constantly interrupted whenever I try to share any ideas. My work is often tossed out. And if I have a good idea, it’s ignored until someone else presents the same idea, then everyone loves it. I’m constantly asked by non-technical people to do things that are incorrect, and when I try to speak up, I’m ignored and my manager doesn’t defend me either. I was promised technical work but I’m stuck working out of excel and PowerPoint while I desperately try to maintain my coding and modeling skills outside of work. 

I’m a woman of color working in a conservative field. I’m exhausted. Is this normal? Do I need to find another field? Are there companies/ types of companies that you recommend I look into that aren’t like this? This isn’t what I thought data science would be.

EDIT: Thank you for the responses everyone! I’ve reached out to some of you privately and will try to respond to everyone else. Based on the comments and some of the suggestions (which were helpful, but already tried), I think it’s time to plan an exit strategy. Being in this environment has led to burnout and mental/physical health is more important than a job. 

To those of you suggesting this as an opportunity to develop soft skills or work on my excel/ppt skills, that’s actually exactly how I pitched it to myself when I first started this role and realized it wouldn’t be as technical as I’d like. But being in an environment like this has actually been detrimental to my soft skills. I’ve lost all confidence in my ability to speak in front of others. And my deck designs are constantly tossed out even after spending hours trying to make them as nice as possible. To anyone else reading this that is experiencing this, you deserve better. You do not have to put up with this in the name of resilience. At a certain point, you are just ramming yourself into a wall over and over again. Others in my organization were getting to work on data science work, so it wasn’t a bait and switch for everyone. Just some of us (coincidentally, all women). 

I’m not going to leave DS yet. I worked too hard to develop these skills to just let them go to waste. But I think an industry change is due.",t2_2kku5j71,False,,0,False,Disillusioned with the field of data science,[],r/datascience,False,6,career,0,,,False,t3_n1wnp6,False,dark,0.91,,public,474,0,{},,,False,[],,False,False,,{},Career,False,474,,False,False,self,1619886309.0,,[],{},,True,,1619826401.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’ve been in my first data science opportunity for almost a year now and I’m starting to question if I made a mistake entering this field. &lt;/p&gt;

&lt;p&gt;My job is all politics. I’m pulled every which way. I’m constantly interrupted whenever I try to share any ideas. My work is often tossed out. And if I have a good idea, it’s ignored until someone else presents the same idea, then everyone loves it. I’m constantly asked by non-technical people to do things that are incorrect, and when I try to speak up, I’m ignored and my manager doesn’t defend me either. I was promised technical work but I’m stuck working out of excel and PowerPoint while I desperately try to maintain my coding and modeling skills outside of work. &lt;/p&gt;

&lt;p&gt;I’m a woman of color working in a conservative field. I’m exhausted. Is this normal? Do I need to find another field? Are there companies/ types of companies that you recommend I look into that aren’t like this? This isn’t what I thought data science would be.&lt;/p&gt;

&lt;p&gt;EDIT: Thank you for the responses everyone! I’ve reached out to some of you privately and will try to respond to everyone else. Based on the comments and some of the suggestions (which were helpful, but already tried), I think it’s time to plan an exit strategy. Being in this environment has led to burnout and mental/physical health is more important than a job. &lt;/p&gt;

&lt;p&gt;To those of you suggesting this as an opportunity to develop soft skills or work on my excel/ppt skills, that’s actually exactly how I pitched it to myself when I first started this role and realized it wouldn’t be as technical as I’d like. But being in an environment like this has actually been detrimental to my soft skills. I’ve lost all confidence in my ability to speak in front of others. And my deck designs are constantly tossed out even after spending hours trying to make them as nice as possible. To anyone else reading this that is experiencing this, you deserve better. You do not have to put up with this in the name of resilience. At a certain point, you are just ramming yourself into a wall over and over again. Others in my organization were getting to work on data science work, so it wasn’t a bait and switch for everyone. Just some of us (coincidentally, all women). &lt;/p&gt;

&lt;p&gt;I’m not going to leave DS yet. I worked too hard to develop these skills to just let them go to waste. But I think an industry change is due.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n1wnp6,True,,Passacagalia,,181,True,all_ads,False,[],False,,/r/datascience/comments/n1wnp6/disillusioned_with_the_field_of_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/n1wnp6/disillusioned_with_the_field_of_data_science/,515407,1619797601.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello all I’m currently a rising junior, majoring in statistics. My goal is to become a “data scientist”. Prior to being a statistics Major I was a data analytics major, and took two courses of software development, one of which focused on data structures and algorithms. 

I switched majors because I wanted to learn more of the math, and I just genuinely liked statistics and wasn’t all into the systems design courses later on in the major.

Long story short, my naive young self a few months ago kept thinking that as a stats major I could get away with working with machine learning algorithms and big data with a masters degree in statistics, and that I would never face data structures and algorithms again. After reading this sub I came across several people who mentioned it’s important and I should learn it because I need a software side and they ask in interviews.

I am now spending my summer trying to get good at them for interviews, but man... am I struggling. I’m sitting here learning linkedlists, being asked to implement a function for like getting the position of an element or deleting an element and my mind goes blank. I don’t know where to start. Not to mention I can’t even do leetcode easy. I’m just really frustrated because I can wrangle data, make dashboards, know when to use specific models and have an understanding of statistical learning, but it is this leetcode data structures and algorithms that is holding me back. 

I don’t know how to learn this stuff other than mindlessly memorize leetcode problems. Any suggestions?",t2_5w4i5kd1,False,,0,False,I’m struggling with data structures and algorithms,[],r/datascience,False,6,discussion,0,,,False,t3_n22njp,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1619843280.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all I’m currently a rising junior, majoring in statistics. My goal is to become a “data scientist”. Prior to being a statistics Major I was a data analytics major, and took two courses of software development, one of which focused on data structures and algorithms. &lt;/p&gt;

&lt;p&gt;I switched majors because I wanted to learn more of the math, and I just genuinely liked statistics and wasn’t all into the systems design courses later on in the major.&lt;/p&gt;

&lt;p&gt;Long story short, my naive young self a few months ago kept thinking that as a stats major I could get away with working with machine learning algorithms and big data with a masters degree in statistics, and that I would never face data structures and algorithms again. After reading this sub I came across several people who mentioned it’s important and I should learn it because I need a software side and they ask in interviews.&lt;/p&gt;

&lt;p&gt;I am now spending my summer trying to get good at them for interviews, but man... am I struggling. I’m sitting here learning linkedlists, being asked to implement a function for like getting the position of an element or deleting an element and my mind goes blank. I don’t know where to start. Not to mention I can’t even do leetcode easy. I’m just really frustrated because I can wrangle data, make dashboards, know when to use specific models and have an understanding of statistical learning, but it is this leetcode data structures and algorithms that is holding me back. &lt;/p&gt;

&lt;p&gt;I don’t know how to learn this stuff other than mindlessly memorize leetcode problems. Any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n22njp,True,,veeeerain,,11,True,all_ads,False,[],False,,/r/datascience/comments/n22njp/im_struggling_with_data_structures_and_algorithms/,all_ads,False,https://www.reddit.com/r/datascience/comments/n22njp/im_struggling_with_data_structures_and_algorithms/,515407,1619814480.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If you have billions of rows of data sitting on a cluster, and you need to develop a model that will then be used to make billions of predictions. And it's intractable to develop your model (i.e., run experiments, tune hyperparameters, compare models) on all data, because it's too expensive. How would you approach developing a model? 

I'm curious if people have principled ways of approaching these kinds of settings. Mine would be:

* Take a stratified random sample of the data. The stratification should respect the distribution of target labels, and any features you consider important. The sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross-validation rather than simple train-test splits.
* Once you've gone through iterations of feature engineering/model comparison/analysis, and identified your best/candidate model. Re-train on a larger dataset to plot the learning curve and see how important additional data is.
* Depending on how much your model benefits from additional data, re-train on as large a dataset as possible and use that model in production. 

In terms of tools/libraries, I'd imagine the stratified sampling would be done using Spark. The model development with libraries like scikit-learn and PyTorch. 

I realize there are ML frameworks, such as SparkML, that allow you train models on Spark, but I feel like they aren't nearly fleshed out enough to support the iterative workflow described above. However, since these libraries are a lot more efficient, you could train on a lot more data. Thoughts on the tradeoff between more iterations on smaller data vs. fewer iterations on more data?

And for inferencing (making predictions), I assume there are ways to deploy scikit-learn/PyTorch models in a Spark environment.",t2_3khic7tq,False,,0,False,What's your approach to developing/iterating on models when you have enormous amount of data?,[],r/datascience,False,6,discussion,0,,,False,t3_n240bf,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1619847274.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you have billions of rows of data sitting on a cluster, and you need to develop a model that will then be used to make billions of predictions. And it&amp;#39;s intractable to develop your model (i.e., run experiments, tune hyperparameters, compare models) on all data, because it&amp;#39;s too expensive. How would you approach developing a model? &lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious if people have principled ways of approaching these kinds of settings. Mine would be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take a stratified random sample of the data. The stratification should respect the distribution of target labels, and any features you consider important. The sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross-validation rather than simple train-test splits.&lt;/li&gt;
&lt;li&gt;Once you&amp;#39;ve gone through iterations of feature engineering/model comparison/analysis, and identified your best/candidate model. Re-train on a larger dataset to plot the learning curve and see how important additional data is.&lt;/li&gt;
&lt;li&gt;Depending on how much your model benefits from additional data, re-train on as large a dataset as possible and use that model in production. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In terms of tools/libraries, I&amp;#39;d imagine the stratified sampling would be done using Spark. The model development with libraries like scikit-learn and PyTorch. &lt;/p&gt;

&lt;p&gt;I realize there are ML frameworks, such as SparkML, that allow you train models on Spark, but I feel like they aren&amp;#39;t nearly fleshed out enough to support the iterative workflow described above. However, since these libraries are a lot more efficient, you could train on a lot more data. Thoughts on the tradeoff between more iterations on smaller data vs. fewer iterations on more data?&lt;/p&gt;

&lt;p&gt;And for inferencing (making predictions), I assume there are ways to deploy scikit-learn/PyTorch models in a Spark environment.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n240bf,True,,EazyStrides,,4,True,all_ads,False,[],False,,/r/datascience/comments/n240bf/whats_your_approach_to_developingiterating_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/n240bf/whats_your_approach_to_developingiterating_on/,515407,1619818474.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So I’m looking at this job and they mention wanting tableau or powerBI (Prefer powerBI) so I feel like that’s the big one. How long would it take to pick up enough power BI to feel comfortable listing it in on a resume? Tableau took all of a few hours to pick up the basics and I’ve heard powerBI is even easier
Edit - Should probably mention I’m a DS with 6 years xp",t2_8548zxhj,False,,0,False,How hard is powerBI to learn coming from plotly / dash?,[],r/datascience,False,6,career,0,,,False,t3_n20bu5,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1619836567.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I’m looking at this job and they mention wanting tableau or powerBI (Prefer powerBI) so I feel like that’s the big one. How long would it take to pick up enough power BI to feel comfortable listing it in on a resume? Tableau took all of a few hours to pick up the basics and I’ve heard powerBI is even easier
Edit - Should probably mention I’m a DS with 6 years xp&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n20bu5,True,,Evening_Top,,7,True,all_ads,False,[],False,,/r/datascience/comments/n20bu5/how_hard_is_powerbi_to_learn_coming_from_plotly/,all_ads,False,https://www.reddit.com/r/datascience/comments/n20bu5/how_hard_is_powerbi_to_learn_coming_from_plotly/,515407,1619807767.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"This might be the wrong sub, so please direct me to the correct one if it is, but: 

Is there software that can create a model for research findings for a disease? Preferably free/cheap.

Wishlist:

- Organized per subtopic (immune system / metabolome / genetic etc).  
- You can mark if something was found normal/abnormal with color, with more than 1 option per thing because some research is contradicting (researcher A found thing X was higher than normal but researcher B found it was lower), with a link to the pubmed article, and maybe a field for a pop-up with notes when you click on it 
- Click a button and only show the abnormal results (fade out the rest)
- searchable?

Something like this maybe

https://imgur.com/mytOuJW

where you can create a giant visual mindmap web of research results and zoom in on the different areas. 

What do medical researchers use for this anyway? Is there some kind of central model/database per disease or is everyone starting from scratch with their own research area?",t2_n1eqz,False,,0,False,Is there software that can build a 'model' of disease research findings with what's normal/abnormal?,[],r/datascience,False,6,tooling,0,,,False,t3_n2ftwx,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1619894314.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This might be the wrong sub, so please direct me to the correct one if it is, but: &lt;/p&gt;

&lt;p&gt;Is there software that can create a model for research findings for a disease? Preferably free/cheap.&lt;/p&gt;

&lt;p&gt;Wishlist:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Organized per subtopic (immune system / metabolome / genetic etc).&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;You can mark if something was found normal/abnormal with color, with more than 1 option per thing because some research is contradicting (researcher A found thing X was higher than normal but researcher B found it was lower), with a link to the pubmed article, and maybe a field for a pop-up with notes when you click on it &lt;/li&gt;
&lt;li&gt;Click a button and only show the abnormal results (fade out the rest)&lt;/li&gt;
&lt;li&gt;searchable?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Something like this maybe&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://imgur.com/mytOuJW""&gt;https://imgur.com/mytOuJW&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;where you can create a giant visual mindmap web of research results and zoom in on the different areas. &lt;/p&gt;

&lt;p&gt;What do medical researchers use for this anyway? Is there some kind of central model/database per disease or is everyone starting from scratch with their own research area?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n2ftwx,True,,kwbwrites,,3,True,all_ads,False,[],False,,/r/datascience/comments/n2ftwx/is_there_software_that_can_build_a_model_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/n2ftwx/is_there_software_that_can_build_a_model_of/,515407,1619865514.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uBOTVkTPG0Cl7-xpl5re1XoBjMv_TAfGEi_wvCiYmJ8.jpg?auto=webp&amp;s=235d76c869adf6149bbe647c8fbc968707aea063', 'width': 600, 'height': 315}, 'resolutions': [{'url': 'https://external-preview.redd.it/uBOTVkTPG0Cl7-xpl5re1XoBjMv_TAfGEi_wvCiYmJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3031b41b0b1799b4183b34fcdb6c314873126147', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/uBOTVkTPG0Cl7-xpl5re1XoBjMv_TAfGEi_wvCiYmJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfc82fd4e3b92db138ac5862080fcbb14e1dd766', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/uBOTVkTPG0Cl7-xpl5re1XoBjMv_TAfGEi_wvCiYmJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90deca270cca226902684ba5817d33cb0274fd09', 'width': 320, 'height': 168}], 'variants': {}, 'id': 'IkbkBVR0tfzz5FqL-spkIILeJTKoZBbOXlicHccuHo4'}], 'enabled': False}",,,,,
,datascience,"I had an interview for a DS position last week and the interviewer asked me a couple of questions that threw me for a loop. First, he asked me to compare a *rectilinear decision tree* to a random forest, and second, he asked me about *scale-free distributions*. My guess is that he wanted me to speak of the advantages of using an ensemble of trees (along with randomly selected features and bagging) over a single tree. As for the distribution question, no clue and I can't find anything on Google.

&amp;#x200B;

Any thoughts?",t2_7v2b47me,False,,0,False,"Am I an idiot, or is there some lingo I wasn't taught in school?",[],r/datascience,False,6,,0,,,False,t3_n1xvpz,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1619829804.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had an interview for a DS position last week and the interviewer asked me a couple of questions that threw me for a loop. First, he asked me to compare a &lt;em&gt;rectilinear decision tree&lt;/em&gt; to a random forest, and second, he asked me about &lt;em&gt;scale-free distributions&lt;/em&gt;. My guess is that he wanted me to speak of the advantages of using an ensemble of trees (along with randomly selected features and bagging) over a single tree. As for the distribution question, no clue and I can&amp;#39;t find anything on Google.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n1xvpz,True,,redneckhippynerd,,11,True,all_ads,False,[],False,,/r/datascience/comments/n1xvpz/am_i_an_idiot_or_is_there_some_lingo_i_wasnt/,all_ads,False,https://www.reddit.com/r/datascience/comments/n1xvpz/am_i_an_idiot_or_is_there_some_lingo_i_wasnt/,515407,1619801004.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Anyone else has to pedantically update jira tickets for every task (subtask) with story point estimates?

I'm having trouble communicating the fact that even i don't know how much time an analysis for a client might take. It's not a developer journey where i know where the insight/function i have to write pre hand. i don't know where the insight might lie. I can't give you an estimate that's accurate to the hour. Or am i being naive here? ",t2_lng4s,False,,0,False,"JIRA roadmap, backlog?",[],r/datascience,False,6,discussion,0,,,False,t3_n1wwyo,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1619827149.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anyone else has to pedantically update jira tickets for every task (subtask) with story point estimates?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m having trouble communicating the fact that even i don&amp;#39;t know how much time an analysis for a client might take. It&amp;#39;s not a developer journey where i know where the insight/function i have to write pre hand. i don&amp;#39;t know where the insight might lie. I can&amp;#39;t give you an estimate that&amp;#39;s accurate to the hour. Or am i being naive here? &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n1wwyo,True,,kunaguerooo123,,4,True,all_ads,False,[],False,,/r/datascience/comments/n1wwyo/jira_roadmap_backlog/,all_ads,False,https://www.reddit.com/r/datascience/comments/n1wwyo/jira_roadmap_backlog/,515407,1619798349.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Context: I used to love working with technology. When I was younger I did computer science at school, worked at Apple at 17 &amp; had work experience at Toshiba Research Europe. Everything was going great until I got my GCSE grades back and realised my coursework was terrible. It wasn’t my fault but rather the teacher had taught us the complete wrong thing to do and only 1 person managed to pass. He was fired but when it came to A Levels I didn’t end up picking computer science. As much as I wanted to, I was anxiety riddled as a teenager and I didn’t believe in myself to do it. I ended up going to university, dropping out because of severe depression &amp; going into bookkeeping. Then lockdown happened. I had so much free time that I ended up doing programming for fun &amp; I got Reddit to try and find fixes to syntax errors when I’m programming but Reddit recommended me this subreddit &amp; data is beautiful and I would check it everyday just because I found it interesting &amp; it was the perfect blend between number crunching and technology - leading me to learn Python &amp; get better with excel.

Fast forward to a few days ago and I manage to get an interview with an amazing employer to work as a Junior Data Analyst. I was really worried because I didn’t know who or what the competition was but I did my best &amp; I mentioned that I followed these pages on Reddit. Turns out they only interviewed one other person and I had the edge as I used Reddit &amp; taught myself in my spare time showing huge enthusiasm! Thank you to everyone on this page you are all legends!!!!!!!! ❤️❤️❤️




TLDR; I fucked up computer science when I was a teen even though I loved it so much. Taught myself over lockdown and got a job partly because I read these subreddits in my spare time",t2_omc58,False,,0,False,Thank you r/datascience &amp; r/dataisbeautiful - you guys helped me get my dream job! ❤️,[],r/datascience,False,6,,0,,,False,t3_n10o03,False,dark,0.98,,public,786,8,{},,,False,[],,False,False,,{},Job Search,False,786,,False,False,self,False,,[],{},,True,,1619717153.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Context: I used to love working with technology. When I was younger I did computer science at school, worked at Apple at 17 &amp;amp; had work experience at Toshiba Research Europe. Everything was going great until I got my GCSE grades back and realised my coursework was terrible. It wasn’t my fault but rather the teacher had taught us the complete wrong thing to do and only 1 person managed to pass. He was fired but when it came to A Levels I didn’t end up picking computer science. As much as I wanted to, I was anxiety riddled as a teenager and I didn’t believe in myself to do it. I ended up going to university, dropping out because of severe depression &amp;amp; going into bookkeeping. Then lockdown happened. I had so much free time that I ended up doing programming for fun &amp;amp; I got Reddit to try and find fixes to syntax errors when I’m programming but Reddit recommended me this subreddit &amp;amp; data is beautiful and I would check it everyday just because I found it interesting &amp;amp; it was the perfect blend between number crunching and technology - leading me to learn Python &amp;amp; get better with excel.&lt;/p&gt;

&lt;p&gt;Fast forward to a few days ago and I manage to get an interview with an amazing employer to work as a Junior Data Analyst. I was really worried because I didn’t know who or what the competition was but I did my best &amp;amp; I mentioned that I followed these pages on Reddit. Turns out they only interviewed one other person and I had the edge as I used Reddit &amp;amp; taught myself in my spare time showing huge enthusiasm! Thank you to everyone on this page you are all legends!!!!!!!! ❤️❤️❤️&lt;/p&gt;

&lt;p&gt;TLDR; I fucked up computer science when I was a teen even though I loved it so much. Taught myself over lockdown and got a job partly because I read these subreddits in my spare time&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,n10o03,True,,Takafraka,,78,True,all_ads,False,[],False,,/r/datascience/comments/n10o03/thank_you_rdatascience_rdataisbeautiful_you_guys/,all_ads,False,https://www.reddit.com/r/datascience/comments/n10o03/thank_you_rdatascience_rdataisbeautiful_you_guys/,515407,1619688353.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I know this might sound like a silly question: but when people use the term ""deep"" neural networks - is there a minimum number of layers/neurons required for a neural network to be called ""deep""?",t2_o4xj9,False,,0,False,"[D] how many layers/neurons are required for a neural network to be considered as ""deep""?",[],r/datascience,False,6,discussion,0,,,False,t3_n21tiw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1619840858.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know this might sound like a silly question: but when people use the term &amp;quot;deep&amp;quot; neural networks - is there a minimum number of layers/neurons required for a neural network to be called &amp;quot;deep&amp;quot;?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n21tiw,True,,blueest,,2,True,all_ads,False,[],False,,/r/datascience/comments/n21tiw/d_how_many_layersneurons_are_required_for_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/n21tiw/d_how_many_layersneurons_are_required_for_a/,515407,1619812058.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_81vkr2em,False,,0,False,9 Data Science Programming Languages to Know,[],r/datascience,False,6,discussion,0,93.0,,False,t3_n2bflc,False,dark,0.19,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,True,https://b.thumbs.redditmedia.com/8GKEsCbhnDlRtSSDTcm3DpdISgMZ4ANY3Q0AT71zhGU.jpg,False,,[],{},,False,,1619873251.0,text,6,,,text,tectalk.co,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n2bflc,True,,sportifynews,,0,True,all_ads,False,[],False,,/r/datascience/comments/n2bflc/9_data_science_programming_languages_to_know/,all_ads,False,https://www.tectalk.co/9-data-science-programming-languages-to-know/,515407,1619844451.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?auto=webp&amp;s=113c778dd01e374730450af74dbced4714a8105c', 'width': 1880, 'height': 1254}, 'resolutions': [{'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07f04515801a9688e446611cef4c787ebb6f40c8', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1916cc39ca0d356ed7990d3fbdea31509bd21b68', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e8a7868fc40d08bb9d3a3f681ae2f877e2b4939', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77774eba65fb81cbce810eccdbb880ceffcb1c77', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512d053cddf3b642d4bd4c74701b6d663c685088', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/8i378gKspJTKjtxWT99FcAoabbtUqvTcJ8QuCg_uiTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52c74fa655ecce3eee6f77f113f5636d29c3b243', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'nTixKopHUJspjUw48o1t4glPHhPCwYPo0svaJtBFnIM'}], 'enabled': False}",,,https://www.tectalk.co/9-data-science-programming-languages-to-know/,,
,datascience,"As part of a few courses I've been creating tutorials on the class forums, which are hosted using using a platfrom called [Ed](https://edstem.org/us/). Ed has the ability to write in LaTeX natively using a built in WYSIWYG editor, or just writing out the LaTeX code between $dollar signs$. It doesn't seem to support markdown, but I would be fine with markdown support.

I'd like to begin transferring my content to my own site, and was wondering if anyone has any recommendations.

So far I've seen some excellent blogs hosted on Ghost and using Jekyll and am considering those two. I'm not interested in Medium due to it's pay model and requirement that people either subscribe or use incognito mode to view it. Wordpress is another option, but I've used it before and it's just a little too bloated.

Any thoughts? Anyone run their own site and can recommend any platform? Many thanks in advance!",t2_66ju5,False,,0,False,Platform to create a data science blog,[],r/datascience,False,6,education,0,,,False,t3_n1c3gr,False,dark,0.77,,public,9,0,{},,,False,[],,False,False,,{},Education,False,9,,False,False,self,False,,[],{},,True,,1619753251.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As part of a few courses I&amp;#39;ve been creating tutorials on the class forums, which are hosted using using a platfrom called &lt;a href=""https://edstem.org/us/""&gt;Ed&lt;/a&gt;. Ed has the ability to write in LaTeX natively using a built in WYSIWYG editor, or just writing out the LaTeX code between $dollar signs$. It doesn&amp;#39;t seem to support markdown, but I would be fine with markdown support.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to begin transferring my content to my own site, and was wondering if anyone has any recommendations.&lt;/p&gt;

&lt;p&gt;So far I&amp;#39;ve seen some excellent blogs hosted on Ghost and using Jekyll and am considering those two. I&amp;#39;m not interested in Medium due to it&amp;#39;s pay model and requirement that people either subscribe or use incognito mode to view it. Wordpress is another option, but I&amp;#39;ve used it before and it&amp;#39;s just a little too bloated.&lt;/p&gt;

&lt;p&gt;Any thoughts? Anyone run their own site and can recommend any platform? Many thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n1c3gr,True,,kelkulus,,9,True,all_ads,False,[],False,,/r/datascience/comments/n1c3gr/platform_to_create_a_data_science_blog/,all_ads,False,https://www.reddit.com/r/datascience/comments/n1c3gr/platform_to_create_a_data_science_blog/,515407,1619724451.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I got lucky enough to stumble in to an analyst role at my job and have recently been handed a huge archive of documents that have been collecting 'dust' for the last couple of years. I have been tasked with ""Seeing if there is anything worth finding"" in this beast because apparently someone up the food chain recently read a McKinsey article on strategic analysis.  ¯\_༼ ಥ ‿ ಥ ༽_/¯

Up until now I have been lucky enough to only mess with curated data and, on my worst days, a folder of Excel docs full of simple transactional data.  
This dataset is altogether terrifying. Each files contains a single sheet but is structured almost like a comic book; by which I mean whoever put the intial 'template' together was clearly never intending it to be parsed by anything other than a human. (Varying field names, merged cells, no ACTUAL tables, imported pictures,  clip art, check boxes, and other odd bits and bobs that I don't understand existing in Excel). 

I prostrate myself before you actual data scientists with a simple query; where the hell do I start? Do I try to programatically convert them to CSV? JSON? Is this legit ML territory that I have no business touching? I am at such a loss that even suggested search terms for me to start researching what to do next would be a huge help.",t2_6lc18,False,,0,False,Any advice on how best to parse ~1TB of Excel files with horrific formatting?,[],r/datascience,False,6,tooling,0,,,False,t3_n0wlj5,False,dark,0.99,,public,80,0,{},,,False,[],,False,False,,{},Tooling,False,80,,False,False,self,False,,[],{},,True,,1619698585.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got lucky enough to stumble in to an analyst role at my job and have recently been handed a huge archive of documents that have been collecting &amp;#39;dust&amp;#39; for the last couple of years. I have been tasked with &amp;quot;Seeing if there is anything worth finding&amp;quot; in this beast because apparently someone up the food chain recently read a McKinsey article on strategic analysis.  ¯_༼ ಥ ‿ ಥ ༽_/¯&lt;/p&gt;

&lt;p&gt;Up until now I have been lucky enough to only mess with curated data and, on my worst days, a folder of Excel docs full of simple transactional data.&lt;br/&gt;
This dataset is altogether terrifying. Each files contains a single sheet but is structured almost like a comic book; by which I mean whoever put the intial &amp;#39;template&amp;#39; together was clearly never intending it to be parsed by anything other than a human. (Varying field names, merged cells, no ACTUAL tables, imported pictures,  clip art, check boxes, and other odd bits and bobs that I don&amp;#39;t understand existing in Excel). &lt;/p&gt;

&lt;p&gt;I prostrate myself before you actual data scientists with a simple query; where the hell do I start? Do I try to programatically convert them to CSV? JSON? Is this legit ML territory that I have no business touching? I am at such a loss that even suggested search terms for me to start researching what to do next would be a huge help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0wlj5,True,,KlavierKatze,,40,True,all_ads,False,[],False,,/r/datascience/comments/n0wlj5/any_advice_on_how_best_to_parse_1tb_of_excel/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0wlj5/any_advice_on_how_best_to_parse_1tb_of_excel/,515407,1619669785.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I’m curious which big concepts from Calculus do Data Scientists  need to know and use regularly? 

Do you use any of these concepts often? Things like integrals? Polar Coordinates? Single variable calculus or multi variable calculus? Fundamental theorem of line integrals? 

What concepts from calculus is used heavily in data science work? 

Thanks for your time!",t2_r0aihsp,False,,0,False,Which topics from Calculus do you use regularly in your work?,[],r/datascience,False,6,education,0,,,False,t3_n15d9y,False,dark,1.0,,public,8,1,{},,,False,[],,False,False,,{},Education,False,8,,False,False,self,False,,[],{'gid_1': 1},,True,,1619734772.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m curious which big concepts from Calculus do Data Scientists  need to know and use regularly? &lt;/p&gt;

&lt;p&gt;Do you use any of these concepts often? Things like integrals? Polar Coordinates? Single variable calculus or multi variable calculus? Fundamental theorem of line integrals? &lt;/p&gt;

&lt;p&gt;What concepts from calculus is used heavily in data science work? &lt;/p&gt;

&lt;p&gt;Thanks for your time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n15d9y,True,,BlueskyPrime,,21,True,all_ads,False,[],False,,/r/datascience/comments/n15d9y/which_topics_from_calculus_do_you_use_regularly/,all_ads,False,https://www.reddit.com/r/datascience/comments/n15d9y/which_topics_from_calculus_do_you_use_regularly/,515407,1619705972.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"The two most common cases I've seen NaN values appear in datasets are either because the data was simply not collected and/or is just missing for no meaningful reason, OR that a ""response"" is not applicable to a feature due to the nature of that specific data point. 

As an example:

|ID|HAS ARTHRITIS|ARTHRITIS LIMITS ABILITY TO WORK|
|:-|:-|:-|
|1|yes|yes|
|2|yes|no|
|3|yes|NaN|
|4|no|NaN|

In the table above the values are missing in rows 3 and 4 for different reasons.  Seemingly the value in row 3 Is missing because most likely the data was not collected. However, in row 4, the missing value is due to the feature not being relevant (I.e. we do not need to ask if a patient's arthritis limits their ability to work if they do not have arthritis).  

It would thus seem that in order to make the most accurate model we should not treat these two cases the same.

What are some methods for dealing with these types of situations?",t2_1aa3rxlb,False,,0,False,How to encoding NaN values with meaning.,[],r/datascience,False,6,discussion,0,,,False,t3_n17etb,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1619740464.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The two most common cases I&amp;#39;ve seen NaN values appear in datasets are either because the data was simply not collected and/or is just missing for no meaningful reason, OR that a &amp;quot;response&amp;quot; is not applicable to a feature due to the nature of that specific data point. &lt;/p&gt;

&lt;p&gt;As an example:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;ID&lt;/th&gt;
&lt;th align=""left""&gt;HAS ARTHRITIS&lt;/th&gt;
&lt;th align=""left""&gt;ARTHRITIS LIMITS ABILITY TO WORK&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;1&lt;/td&gt;
&lt;td align=""left""&gt;yes&lt;/td&gt;
&lt;td align=""left""&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;2&lt;/td&gt;
&lt;td align=""left""&gt;yes&lt;/td&gt;
&lt;td align=""left""&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;3&lt;/td&gt;
&lt;td align=""left""&gt;yes&lt;/td&gt;
&lt;td align=""left""&gt;NaN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;4&lt;/td&gt;
&lt;td align=""left""&gt;no&lt;/td&gt;
&lt;td align=""left""&gt;NaN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;In the table above the values are missing in rows 3 and 4 for different reasons.  Seemingly the value in row 3 Is missing because most likely the data was not collected. However, in row 4, the missing value is due to the feature not being relevant (I.e. we do not need to ask if a patient&amp;#39;s arthritis limits their ability to work if they do not have arthritis).  &lt;/p&gt;

&lt;p&gt;It would thus seem that in order to make the most accurate model we should not treat these two cases the same.&lt;/p&gt;

&lt;p&gt;What are some methods for dealing with these types of situations?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n17etb,True,,neslef,,7,True,all_ads,False,[],False,,/r/datascience/comments/n17etb/how_to_encoding_nan_values_with_meaning/,all_ads,False,https://www.reddit.com/r/datascience/comments/n17etb/how_to_encoding_nan_values_with_meaning/,515407,1619711664.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey folks. I’ve been working as a sales data analyst for the past two years. Just kind of worked my way into this role, despite the fact that my education is in the soft sciences (as in some light STATA use).

The problem is that now I’m the CRM admin. And we’re about to go from a company of 500 people to 5000 people, thanks to a dozen or more acquisitions coming up. I look at our infrastructure and all I see is a looming train wreck. Most of these companies keep their sales data on excel files in a shared drive. Some have Salesforce (my current company does not).

My goal is to best prepare myself and my company for these small, but numerous data integrations. I want to keep things organized and have a clear input/output system for all data sources. And then of course I need a way to easily access all of these to compile company-wide KPI reports. Reading the wiki, I think this counts as “data science” and I think I should start with learning R. Anything else to add?",t2_54ngnjlf,False,,0,False,Little Analyst in a Big Data Pond,[],r/datascience,False,6,career,0,,,False,t3_n0oz21,False,dark,0.98,,public,113,0,{},,,False,[],,False,False,,{},Career,False,113,,False,False,self,False,,[],{},,True,,1619673219.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey folks. I’ve been working as a sales data analyst for the past two years. Just kind of worked my way into this role, despite the fact that my education is in the soft sciences (as in some light STATA use).&lt;/p&gt;

&lt;p&gt;The problem is that now I’m the CRM admin. And we’re about to go from a company of 500 people to 5000 people, thanks to a dozen or more acquisitions coming up. I look at our infrastructure and all I see is a looming train wreck. Most of these companies keep their sales data on excel files in a shared drive. Some have Salesforce (my current company does not).&lt;/p&gt;

&lt;p&gt;My goal is to best prepare myself and my company for these small, but numerous data integrations. I want to keep things organized and have a clear input/output system for all data sources. And then of course I need a way to easily access all of these to compile company-wide KPI reports. Reading the wiki, I think this counts as “data science” and I think I should start with learning R. Anything else to add?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0oz21,True,,A_Terrible_Texan,,24,True,all_ads,False,[],False,,/r/datascience/comments/n0oz21/little_analyst_in_a_big_data_pond/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0oz21/little_analyst_in_a_big_data_pond/,515407,1619644419.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm modeling with Random Forest algorithm medium size data set, but I didn't get any difference using CV roc\_auc\_score. I don't know if I'm just spending time doing CV, then could someone explain which cases it's good to do it for rfc.",t2_84kj28iv,False,,0,False,In which cases Random Forest Algorithm requires Cross-Validation?,[],r/datascience,False,6,discussion,0,,,False,t3_n1ampx,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1619749276.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m modeling with Random Forest algorithm medium size data set, but I didn&amp;#39;t get any difference using CV roc_auc_score. I don&amp;#39;t know if I&amp;#39;m just spending time doing CV, then could someone explain which cases it&amp;#39;s good to do it for rfc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n1ampx,True,,RoPhysis,,2,True,all_ads,False,[],False,,/r/datascience/comments/n1ampx/in_which_cases_random_forest_algorithm_requires/,all_ads,False,https://www.reddit.com/r/datascience/comments/n1ampx/in_which_cases_random_forest_algorithm_requires/,515407,1619720476.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Suppose you want to make a model that predicts if a student will drop out of university- you have historical information about many students and whether they dropped out or not. You also have access to the ZIP code (postal code) where they lived.

1) Is it common to actually the zip code as an input variable (probably not, since there are too many categories)? Or, maybe use the first 3 numbers of the zip code as an input variable?

2) I was always told to avoid using a predictor variable that has too many categories. Is there a mathematical reason behind this? From a mathematical standpoint: if your data has 1000 rows and one of your predictor has 450 categories - mathematically speaking, why might this harm your statistical model? I can understand it intuitively - having too many categories means too much information and your model might get ""confused"" - but is there a mathematical explanation?

Note: I know you can just take students from different cities and make a sepperate model for students in the same city - but I am not interested in doing this.

CLARIFICATION: I am using zip as a CATEGORICAL variable!",t2_xtuyc,False,,0,False,is it common to use the ZIP code as a predictor variable in statistical models?,[],r/datascience,False,6,discussion,0,,,False,t3_n182tu,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1619720664.0,,[],{},,True,,1619742246.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you want to make a model that predicts if a student will drop out of university- you have historical information about many students and whether they dropped out or not. You also have access to the ZIP code (postal code) where they lived.&lt;/p&gt;

&lt;p&gt;1) Is it common to actually the zip code as an input variable (probably not, since there are too many categories)? Or, maybe use the first 3 numbers of the zip code as an input variable?&lt;/p&gt;

&lt;p&gt;2) I was always told to avoid using a predictor variable that has too many categories. Is there a mathematical reason behind this? From a mathematical standpoint: if your data has 1000 rows and one of your predictor has 450 categories - mathematically speaking, why might this harm your statistical model? I can understand it intuitively - having too many categories means too much information and your model might get &amp;quot;confused&amp;quot; - but is there a mathematical explanation?&lt;/p&gt;

&lt;p&gt;Note: I know you can just take students from different cities and make a sepperate model for students in the same city - but I am not interested in doing this.&lt;/p&gt;

&lt;p&gt;CLARIFICATION: I am using zip as a CATEGORICAL variable!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n182tu,True,,ottawalanguages,,19,True,all_ads,False,[],False,,/r/datascience/comments/n182tu/is_it_common_to_use_the_zip_code_as_a_predictor/,all_ads,False,https://www.reddit.com/r/datascience/comments/n182tu/is_it_common_to_use_the_zip_code_as_a_predictor/,515407,1619713446.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello,

I will soon get my PhD in Physics. Being a little underwhelmed by academia and physics I am thinking about making the transition to data-related fields (which seem really awesome and is also the only hiring market for scientists where I live).

My main issue is that my CV is hard to sell to the data world. I've got a paper on ML, been doing data analysis for almost all my PhD, and got decent analytics in Python etc. But I can't say my skills are at production level. The market also seems to have evolved rapidly: jobs qualifications are extremely tight, requiring advanced database management, data piping etc.

During my entire education I've been sold the idea that everybody hires physicists because they can learn anything pretty fast. Companies were supposed to hire and train us apparently. From what I understand now, this might not be the case as companies now have plethora of proper computer scientists at their disposal.

I still have \~1 year of funding left after my graduation, which I intend to ""use"" to search for a job and acquire the skills needed to enter the field. I was wondering if anyone had done this transition in the recent years ? What are the main things I should consider learning first ? From what I understand, git version control, SQL/noSQL are a must, is there anything else that comes to your mind ? How about ""soft"" skills ? How did you fit in with actual data engineers and analysts ?

I'm really looking for any information that comes to your mind and things you wished you knew beforehand.

Thanks!",t2_nucl8,False,,0,False,Physics PhD transitioning to data science: any advices?,[],r/datascience,False,6,career,0,,,False,t3_n04ga6,False,dark,0.95,,public,296,5,{},,,False,[],,False,False,,{},Career,False,296,,False,False,self,False,,[],{},,True,,1619604449.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I will soon get my PhD in Physics. Being a little underwhelmed by academia and physics I am thinking about making the transition to data-related fields (which seem really awesome and is also the only hiring market for scientists where I live).&lt;/p&gt;

&lt;p&gt;My main issue is that my CV is hard to sell to the data world. I&amp;#39;ve got a paper on ML, been doing data analysis for almost all my PhD, and got decent analytics in Python etc. But I can&amp;#39;t say my skills are at production level. The market also seems to have evolved rapidly: jobs qualifications are extremely tight, requiring advanced database management, data piping etc.&lt;/p&gt;

&lt;p&gt;During my entire education I&amp;#39;ve been sold the idea that everybody hires physicists because they can learn anything pretty fast. Companies were supposed to hire and train us apparently. From what I understand now, this might not be the case as companies now have plethora of proper computer scientists at their disposal.&lt;/p&gt;

&lt;p&gt;I still have ~1 year of funding left after my graduation, which I intend to &amp;quot;use&amp;quot; to search for a job and acquire the skills needed to enter the field. I was wondering if anyone had done this transition in the recent years ? What are the main things I should consider learning first ? From what I understand, git version control, SQL/noSQL are a must, is there anything else that comes to your mind ? How about &amp;quot;soft&amp;quot; skills ? How did you fit in with actual data engineers and analysts ?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m really looking for any information that comes to your mind and things you wished you knew beforehand.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n04ga6,True,,Valmishra,,140,True,all_ads,False,[],False,,/r/datascience/comments/n04ga6/physics_phd_transitioning_to_data_science_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/n04ga6/physics_phd_transitioning_to_data_science_any/,515407,1619575649.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Is it interesting to study onto itself like particles in article physics or computation?,t2_803tnh3u,False,,0,False,What is data philosophically speaking?,[],r/datascience,False,6,discussion,0,,,False,t3_n0vmnm,False,dark,0.56,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,1619667481.0,,[],{},,True,,1619694927.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it interesting to study onto itself like particles in article physics or computation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0vmnm,True,,Striking_Exchange659,,4,True,all_ads,False,[],False,,/r/datascience/comments/n0vmnm/what_is_data_philosophically_speaking/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0vmnm/what_is_data_philosophically_speaking/,515407,1619666127.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Suppose you have a dataset where some of the predictor variables are categorical and have hundreds of possible (discrete) values. Are there any common ways to ""bin"" all these values into general groups? 

E.g. suppose one of the variables is : 50% ""A"", 25% ""B"" , 20% ""C"", 0.5% D, 0.5% E, 0.5% F ....etc.

Could you reformat this variable as A, B, C, OTHER ?

Is this a common technique? Is this acceptable? Some statistical computing software can not always handle so many categories, other times perhaps it is advantageous to ""bin"" many low frequency entries together to facilitate statistical modelling?

thanks",t2_xtuyc,False,,0,False,"Standard approaches for ""binning"" data",[],r/datascience,False,6,discussion,0,,,False,t3_n0o5is,False,dark,0.6,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1619670902.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you have a dataset where some of the predictor variables are categorical and have hundreds of possible (discrete) values. Are there any common ways to &amp;quot;bin&amp;quot; all these values into general groups? &lt;/p&gt;

&lt;p&gt;E.g. suppose one of the variables is : 50% &amp;quot;A&amp;quot;, 25% &amp;quot;B&amp;quot; , 20% &amp;quot;C&amp;quot;, 0.5% D, 0.5% E, 0.5% F ....etc.&lt;/p&gt;

&lt;p&gt;Could you reformat this variable as A, B, C, OTHER ?&lt;/p&gt;

&lt;p&gt;Is this a common technique? Is this acceptable? Some statistical computing software can not always handle so many categories, other times perhaps it is advantageous to &amp;quot;bin&amp;quot; many low frequency entries together to facilitate statistical modelling?&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0o5is,True,,ottawalanguages,,4,True,all_ads,False,[],False,,/r/datascience/comments/n0o5is/standard_approaches_for_binning_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0o5is/standard_approaches_for_binning_data/,515407,1619642102.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Currently studying for a BS in data science, and I'd like to get some tattoos but I don't want to hurt my chances of success. I assume something easily visible like a hand tattoo would be a bad idea, but what about something like a forearm or ankle?

Sorry I know this is a ridiculous question, I just don't have any experience in an office environment haha",t2_43t7qlj3,False,,0,False,"Odd question, but will a tattoo hurt my chances of being hired in this field?",[],r/datascience,False,6,career,0,,,False,t3_n0xn9o,False,dark,0.38,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1619703054.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently studying for a BS in data science, and I&amp;#39;d like to get some tattoos but I don&amp;#39;t want to hurt my chances of success. I assume something easily visible like a hand tattoo would be a bad idea, but what about something like a forearm or ankle?&lt;/p&gt;

&lt;p&gt;Sorry I know this is a ridiculous question, I just don&amp;#39;t have any experience in an office environment haha&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0xn9o,True,,TheSmallestSteve,,12,True,all_ads,False,[],False,,/r/datascience/comments/n0xn9o/odd_question_but_will_a_tattoo_hurt_my_chances_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0xn9o/odd_question_but_will_a_tattoo_hurt_my_chances_of/,515407,1619674254.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello mates. I've been thinking for a while on how to approach this. Let me give you some context before anything else:
I have an array of piezoelectric sensors inside a vest that will signal analog values to a embd system every interval, lets call it 't'. This sensors will throw data when they are hitted by a BB, when a player crouches, whenever he pick ups something(basically whenever they receive a force).... The value of the analog signal will be as big as the force applied.

My toughts so far are on recording as much data as I can of different scenarios, label it whether the vest was hitted by a BB or not and fit it in an algorithm.

 All I could think about is to use a basic NN and feed it with lets say t, t-1..., t-5 iterations and see how well it does.

Any thoughts, ideas, criticism... Everything is welcome. Thanks in regards.

Edit: just to make sure everyone is on the same page. Remember that the controller will be receiving dats from multiple sensors.",t2_91e6lctk,False,,0,False,Labeling whether or not a vest has been shot with an airsoft BB using ML,[],r/datascience,False,6,discussion,0,,,False,t3_n0bpyg,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,1619616949.0,,[],{},,True,,1619634120.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello mates. I&amp;#39;ve been thinking for a while on how to approach this. Let me give you some context before anything else:
I have an array of piezoelectric sensors inside a vest that will signal analog values to a embd system every interval, lets call it &amp;#39;t&amp;#39;. This sensors will throw data when they are hitted by a BB, when a player crouches, whenever he pick ups something(basically whenever they receive a force).... The value of the analog signal will be as big as the force applied.&lt;/p&gt;

&lt;p&gt;My toughts so far are on recording as much data as I can of different scenarios, label it whether the vest was hitted by a BB or not and fit it in an algorithm.&lt;/p&gt;

&lt;p&gt;All I could think about is to use a basic NN and feed it with lets say t, t-1..., t-5 iterations and see how well it does.&lt;/p&gt;

&lt;p&gt;Any thoughts, ideas, criticism... Everything is welcome. Thanks in regards.&lt;/p&gt;

&lt;p&gt;Edit: just to make sure everyone is on the same page. Remember that the controller will be receiving dats from multiple sensors.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0bpyg,True,,devJaviortig,,19,True,all_ads,False,[],False,,/r/datascience/comments/n0bpyg/labeling_whether_or_not_a_vest_has_been_shot_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0bpyg/labeling_whether_or_not_a_vest_has_been_shot_with/,515407,1619605320.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi data people! Hopefully this sub is the appropriate place to post this, I posted to r/dataanalysis and r/dataengineering, but since this sub has about 10x the members of either of those, it would get a lot more attention and feedback I thought. 

Our company has been acquiring new companies at an increased rate lately, and - where possible and realistic - we're looking to streamline and combine our data streams and ETL process so that our team can be the central data go-to team for reporting.

As a part of this process, we're looking at potential new ETL processes we could adopt, and I'd love some suggestions from other people in the field. 

Currently, we use Visual Studio to manage .dtsx packages where we do the majority of the ETL work. These packages kick off various stored procedures in our SQL servers to load/combine/aggregate data wherever needed.

What systems or tools do you all use in your work environments, and do you have any suggestions on good processes I could look into? Thanks in advance!",t2_l8yrt,False,,0,False,Best ETL processes/tools?,[],r/datascience,False,6,discussion,0,,,False,t3_n0gzf8,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1619651491.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi data people! Hopefully this sub is the appropriate place to post this, I posted to &lt;a href=""/r/dataanalysis""&gt;r/dataanalysis&lt;/a&gt; and &lt;a href=""/r/dataengineering""&gt;r/dataengineering&lt;/a&gt;, but since this sub has about 10x the members of either of those, it would get a lot more attention and feedback I thought. &lt;/p&gt;

&lt;p&gt;Our company has been acquiring new companies at an increased rate lately, and - where possible and realistic - we&amp;#39;re looking to streamline and combine our data streams and ETL process so that our team can be the central data go-to team for reporting.&lt;/p&gt;

&lt;p&gt;As a part of this process, we&amp;#39;re looking at potential new ETL processes we could adopt, and I&amp;#39;d love some suggestions from other people in the field. &lt;/p&gt;

&lt;p&gt;Currently, we use Visual Studio to manage .dtsx packages where we do the majority of the ETL work. These packages kick off various stored procedures in our SQL servers to load/combine/aggregate data wherever needed.&lt;/p&gt;

&lt;p&gt;What systems or tools do you all use in your work environments, and do you have any suggestions on good processes I could look into? Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0gzf8,True,,UnkleWillard,,13,True,all_ads,False,[],False,,/r/datascience/comments/n0gzf8/best_etl_processestools/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0gzf8/best_etl_processestools/,515407,1619622691.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm currently working on all Deep Learning based projects, and tasks involve reading a fair bit of research papers then implementing them, experimenting, etc. However, I'm struggling to estimate the timelines, especially for research-oriented tasks, and communicate them effectively to the management.

Any advice on planning projects better? Do sprint cycles work for DS projects, if not, what works?

Are there any Data Science related design documents, etc that are publicly available which goes into more details into the planning of tasks?",t2_ujnqz,False,,0,False,"How do you plan, estimate, and communicate Data Science projects?",[],r/datascience,False,6,discussion,0,,,False,t3_n0gqgz,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1619650833.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently working on all Deep Learning based projects, and tasks involve reading a fair bit of research papers then implementing them, experimenting, etc. However, I&amp;#39;m struggling to estimate the timelines, especially for research-oriented tasks, and communicate them effectively to the management.&lt;/p&gt;

&lt;p&gt;Any advice on planning projects better? Do sprint cycles work for DS projects, if not, what works?&lt;/p&gt;

&lt;p&gt;Are there any Data Science related design documents, etc that are publicly available which goes into more details into the planning of tasks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,n0gqgz,True,,mln00b13,,9,True,all_ads,False,[],False,,/r/datascience/comments/n0gqgz/how_do_you_plan_estimate_and_communicate_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/n0gqgz/how_do_you_plan_estimate_and_communicate_data/,515407,1619622033.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This is a concept i always struggled with: in statistics, is ""more data always better""? 

Suppose you 50 years of data about hospital visits. You are interested in supervised classification. You have predictors such as age, height, weight, blood type, salary, etc. You are interested in predicting if the hospital stay will be less than 1 day or more than 1 day. This can be easily solved using random forest.

My dilemma is: using all 50 years of data might be able to capture a wide variety of patterns  ... but since we are interested in predicting future information, maybe some of the older data is less relevant and might surpress more current trends?

How do you deal with this problem?",t2_3f0i9m72,False,,0,False,How much data should you use in a model?,[],r/datascience,False,6,discussion,0,,,False,t3_mzqobo,False,dark,0.93,,public,147,0,{},,,False,[],,False,False,,{},Discussion,False,147,,False,False,self,False,,[],{},,True,,1619565218.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a concept i always struggled with: in statistics, is &amp;quot;more data always better&amp;quot;? &lt;/p&gt;

&lt;p&gt;Suppose you 50 years of data about hospital visits. You are interested in supervised classification. You have predictors such as age, height, weight, blood type, salary, etc. You are interested in predicting if the hospital stay will be less than 1 day or more than 1 day. This can be easily solved using random forest.&lt;/p&gt;

&lt;p&gt;My dilemma is: using all 50 years of data might be able to capture a wide variety of patterns  ... but since we are interested in predicting future information, maybe some of the older data is less relevant and might surpress more current trends?&lt;/p&gt;

&lt;p&gt;How do you deal with this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mzqobo,True,,SQL_beginner,,42,True,all_ads,False,[],False,,/r/datascience/comments/mzqobo/how_much_data_should_you_use_in_a_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/mzqobo/how_much_data_should_you_use_in_a_model/,515407,1619536418.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"TIL that changing the order of features when calling the fit method of sklearn random forest regressor  can lead to a different model, even if the seed and random\_state is the same.

For ex: 

1. Function call 1: RandomForestRegressor(random\_state=42).fit(dataframe1\[\[feat1,feat2,feat3\]\])
2. Function call 2: RandomForestRegressor(random\_state=42).fit(dataframe1\[\[feat3,feat2,feat1\]\])

The models you will get from the above two functions will be different! This could be down to the random nature of the algorithm and how it indexes features and considers them for making splits.

I spent an entire day trying to figure out as to why were my experiments not repeatable even when I set the random\_state and numpy seed at the top of the notebook, and at the end of the day it was this small thing.",t2_14zvnbe7,False,,0,False,TIL random forest randomness,[],r/datascience,False,6,fun,0,,,False,t3_mzvicb,False,dark,0.77,,public,9,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,9,,False,False,self,False,,[],{},,True,,1619577910.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;TIL that changing the order of features when calling the fit method of sklearn random forest regressor  can lead to a different model, even if the seed and random_state is the same.&lt;/p&gt;

&lt;p&gt;For ex: &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Function call 1: RandomForestRegressor(random_state=42).fit(dataframe1[[feat1,feat2,feat3]])&lt;/li&gt;
&lt;li&gt;Function call 2: RandomForestRegressor(random_state=42).fit(dataframe1[[feat3,feat2,feat1]])&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The models you will get from the above two functions will be different! This could be down to the random nature of the algorithm and how it indexes features and considers them for making splits.&lt;/p&gt;

&lt;p&gt;I spent an entire day trying to figure out as to why were my experiments not repeatable even when I set the random_state and numpy seed at the top of the notebook, and at the end of the day it was this small thing.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mzvicb,True,,pm_me_tap_ins,,6,True,all_ads,False,[],False,,/r/datascience/comments/mzvicb/til_random_forest_randomness/,all_ads,False,https://www.reddit.com/r/datascience/comments/mzvicb/til_random_forest_randomness/,515407,1619549110.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,Hello! I'm still learning about this field and trying to figure out how best to build out my skills as well as create a professional presence. My personal git has a handful of repos but nothing too exciting so I was looking to see how other people build theirs,t2_6bwv9lao,False,,0,False,Does anyone mind sharing their professional githubs? Or passing along some ideas on how to build one's out?,[],r/datascience,False,6,education,0,,,False,t3_mztf8j,False,dark,0.71,,public,7,0,{},,,False,[],,False,False,,{},Education,False,7,,False,False,self,False,,[],{},,True,,1619572471.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I&amp;#39;m still learning about this field and trying to figure out how best to build out my skills as well as create a professional presence. My personal git has a handful of repos but nothing too exciting so I was looking to see how other people build theirs&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mztf8j,True,,lost-and-all,,5,True,all_ads,False,[],False,,/r/datascience/comments/mztf8j/does_anyone_mind_sharing_their_professional/,all_ads,False,https://www.reddit.com/r/datascience/comments/mztf8j/does_anyone_mind_sharing_their_professional/,515407,1619543671.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I started intensively learning php a couple months back but still can't figure out why no one wants to use it for complex systems, everything seems to line up, it's decently fast, scalable with a good framework, very easy to mantain and develop in, yet its popularity is steadily decreasing, especially in complex areas like data science",t2_452t51ty,False,,0,False,Why is php nowhere to be found when talking about data science?,[],r/datascience,False,6,discussion,0,,,False,t3_mzqqee,False,dark,0.66,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1619565384.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I started intensively learning php a couple months back but still can&amp;#39;t figure out why no one wants to use it for complex systems, everything seems to line up, it&amp;#39;s decently fast, scalable with a good framework, very easy to mantain and develop in, yet its popularity is steadily decreasing, especially in complex areas like data science&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mzqqee,True,,Scratch9898,,17,True,all_ads,False,[],False,,/r/datascience/comments/mzqqee/why_is_php_nowhere_to_be_found_when_talking_about/,all_ads,False,https://www.reddit.com/r/datascience/comments/mzqqee/why_is_php_nowhere_to_be_found_when_talking_about/,515407,1619536584.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"In my \~6 years of working in the analytics domain, for most of the Fortune 10 clients, across geographies, one thing I've realized is while people may solve business problems using analytics, the journey is lost somewhere. At the risk of sounding cliche, ***'Enjoy the journey, not the destination"".*** So here's my attempt at creating the problem-solving journey from what I've experienced/learned/failed at.

The framework for problem-solving using analytics is a 3 step process. On we go:

1. **Break the business problem into an analytical problem**  
Let's start this with another cliche - *"" If I had an hour to solve a problem I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions"".* This is where a lot of analysts/consultants fail. As soon as a business problem falls into their ears, they straightaway get down to solution-ing, without even a bare attempt at understanding the problem at hand. To tackle this, I (and my team) follow what we call the **CS-FS framework** (extra marks to those who can come up with a better naming).  
The CS-FS framework stands for the Current State - Future State framework.In the CS-FS framework, the first step is to identify the **Current State** of the client, where they're at currently with the problem, followed by the next step, which is to identify the **Desired Future State**, where they want to be after the solution is provided - the insights, the behaviors driven by the insight and finally the outcome driven by the behavior.  
The final, and the most important step of the CS-FS framework is **to identify the gap**, that prevents the client from moving from the Current State to the Desired Future State. This becomes your Analytical Problem, and thus the input for the next step
2. **Find the Analytical Solution to the Analytical Problem**  
Now that you have the business problem converted to an analytical problem, let's look at the data, shall we? \*\*A BIG NO!\*\*  
We will start forming hypotheses around the problem, **WITHOUT BEING BIASED BY THE DATA.** I can't stress this point enough. The process of forming hypotheses should be independent of what data you have available. The correct method to this is after forming all possible hypotheses, you should be looking at the available data, and eliminating those hypotheses for which you don't have data.  
After the hypotheses are formed, you start looking at the data, and then the usual analytical solution follows - understand the data, do some EDA, test for hypotheses, do some ML (if the problem requires it), and yada yada yada. This is the part which most analysts are good at. For example - if the problem revolves around customer churn, this is the step where you'll go ahead with your classification modeling.Let me remind you, the output for this step is just an analytical solution - a classification model for your customer churn problem.   
Most of the time, the people for whom you're solving the problem would not be technically gifted, so they won't understand the Confusion Matrix output of a classification model or the output of an AUC ROC curve. They want you to talk in a language they understand. This is where we take the final road in our journey of problem-solving - the final step
3. **Convert the Analytical Solution to a Business Solution**  
An analytical solution is for computers, a business solution is for humans. And more or less, you'll be dealing with humans who want to understand what your many weeks' worth of effort has produced. You may have just created the most efficient and accurate ML model the world has ever seen, but if the final stakeholder is unable to interpret its meaning, then the whole exercise was useless.  
This is where you will use all your story-boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state. This is where visualization skills, dashboard creation, insight generation, creation of decks come into the picture. Again, when you create dashboards or reports, keep in mind that you're telling a story, and not just laying down a beautiful colored chart on a Power BI or a Tableau dashboard. Each chart, each number on a report should be action-oriented, and part of a larger story.  
Only when someone understands your story, are they most likely going to purchase another book from you. Only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders, will they travel with you again.

With that said, I've reached my destination. I hope you all do too. I'm totally open to criticism/suggestions/improvements that I can make to this journey. Looking forward to inputs from the community!",t2_5su5mzvm,False,,0,False,The Journey Of Problem Solving Using Analytics,[],r/datascience,False,6,projects,0,,,False,t3_myurtx,False,dark,0.98,,public,450,4,{},,,False,[],,False,False,,{},Projects,False,450,,False,False,self,False,,[],{'gid_1': 1},,True,,1619462432.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my ~6 years of working in the analytics domain, for most of the Fortune 10 clients, across geographies, one thing I&amp;#39;ve realized is while people may solve business problems using analytics, the journey is lost somewhere. At the risk of sounding cliche, &lt;strong&gt;&lt;em&gt;&amp;#39;Enjoy the journey, not the destination&amp;quot;.&lt;/em&gt;&lt;/strong&gt; So here&amp;#39;s my attempt at creating the problem-solving journey from what I&amp;#39;ve experienced/learned/failed at.&lt;/p&gt;

&lt;p&gt;The framework for problem-solving using analytics is a 3 step process. On we go:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Break the business problem into an analytical problem&lt;/strong&gt;&lt;br/&gt;
Let&amp;#39;s start this with another cliche - &lt;em&gt;&amp;quot; If I had an hour to solve a problem I&amp;#39;d spend 55 minutes thinking about the problem and 5 minutes thinking about solutions&amp;quot;.&lt;/em&gt; This is where a lot of analysts/consultants fail. As soon as a business problem falls into their ears, they straightaway get down to solution-ing, without even a bare attempt at understanding the problem at hand. To tackle this, I (and my team) follow what we call the &lt;strong&gt;CS-FS framework&lt;/strong&gt; (extra marks to those who can come up with a better naming).&lt;br/&gt;
The CS-FS framework stands for the Current State - Future State framework.In the CS-FS framework, the first step is to identify the &lt;strong&gt;Current State&lt;/strong&gt; of the client, where they&amp;#39;re at currently with the problem, followed by the next step, which is to identify the &lt;strong&gt;Desired Future State&lt;/strong&gt;, where they want to be after the solution is provided - the insights, the behaviors driven by the insight and finally the outcome driven by the behavior.&lt;br/&gt;
The final, and the most important step of the CS-FS framework is &lt;strong&gt;to identify the gap&lt;/strong&gt;, that prevents the client from moving from the Current State to the Desired Future State. This becomes your Analytical Problem, and thus the input for the next step&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Find the Analytical Solution to the Analytical Problem&lt;/strong&gt;&lt;br/&gt;
Now that you have the business problem converted to an analytical problem, let&amp;#39;s look at the data, shall we? **A BIG NO!**&lt;br/&gt;
We will start forming hypotheses around the problem, &lt;strong&gt;WITHOUT BEING BIASED BY THE DATA.&lt;/strong&gt; I can&amp;#39;t stress this point enough. The process of forming hypotheses should be independent of what data you have available. The correct method to this is after forming all possible hypotheses, you should be looking at the available data, and eliminating those hypotheses for which you don&amp;#39;t have data.&lt;br/&gt;
After the hypotheses are formed, you start looking at the data, and then the usual analytical solution follows - understand the data, do some EDA, test for hypotheses, do some ML (if the problem requires it), and yada yada yada. This is the part which most analysts are good at. For example - if the problem revolves around customer churn, this is the step where you&amp;#39;ll go ahead with your classification modeling.Let me remind you, the output for this step is just an analytical solution - a classification model for your customer churn problem.&lt;br/&gt;
Most of the time, the people for whom you&amp;#39;re solving the problem would not be technically gifted, so they won&amp;#39;t understand the Confusion Matrix output of a classification model or the output of an AUC ROC curve. They want you to talk in a language they understand. This is where we take the final road in our journey of problem-solving - the final step&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convert the Analytical Solution to a Business Solution&lt;/strong&gt;&lt;br/&gt;
An analytical solution is for computers, a business solution is for humans. And more or less, you&amp;#39;ll be dealing with humans who want to understand what your many weeks&amp;#39; worth of effort has produced. You may have just created the most efficient and accurate ML model the world has ever seen, but if the final stakeholder is unable to interpret its meaning, then the whole exercise was useless.&lt;br/&gt;
This is where you will use all your story-boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state. This is where visualization skills, dashboard creation, insight generation, creation of decks come into the picture. Again, when you create dashboards or reports, keep in mind that you&amp;#39;re telling a story, and not just laying down a beautiful colored chart on a Power BI or a Tableau dashboard. Each chart, each number on a report should be action-oriented, and part of a larger story.&lt;br/&gt;
Only when someone understands your story, are they most likely going to purchase another book from you. Only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders, will they travel with you again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With that said, I&amp;#39;ve reached my destination. I hope you all do too. I&amp;#39;m totally open to criticism/suggestions/improvements that I can make to this journey. Looking forward to inputs from the community!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,myurtx,True,,Acrobatic-Egg-,,50,True,all_ads,False,[],False,,/r/datascience/comments/myurtx/the_journey_of_problem_solving_using_analytics/,all_ads,False,https://www.reddit.com/r/datascience/comments/myurtx/the_journey_of_problem_solving_using_analytics/,515407,1619433632.0,1,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience," Hey people,

I'm working on my personal project which will be quite a challenge. One of its features is that the user can interact with an ""Open-domain question answering"" chatbot which will be trained on the data I provide it.

I want the model to resemble a specific person/group and it will be fed everything that that person/group wrote, said and etc. Have in mind that the model can answer questions in 1-4 sentences and it doesn't need to be based on pure facts. This means that the user won't ask the model questions like ""What is the capital of France?"" but more something along the lines of existential questions (""What is the meaning of &lt;thing&gt;?"").

Here are the questions I have as I didn't dabble into the NLP world of AI at all:

1. Are there any pre-trained or prebuilt models out there that I could use for this? I've found that the open-source Pavlov AI library has some interesting ones.
2. Which models would suit this task the best?
3. Are there any features I should watch out for or provide more information on?

The biggest part of the job will be to collect relevant data on the group I want the model to resemble. What would be some of the best practices when making the data as informative as it can be? Also, if I want there to be 4 groups that the model can resemble - Do I need to train 4 models or can I filter what a model learned into 4 categories?

Thanks for all replies and questions in advance. If some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great.",t2_4o6wucq4,False,,0,False,Question Answering AI,[],r/datascience,False,6,discussion,0,,,False,t3_mzk16l,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1619542124.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey people,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on my personal project which will be quite a challenge. One of its features is that the user can interact with an &amp;quot;Open-domain question answering&amp;quot; chatbot which will be trained on the data I provide it.&lt;/p&gt;

&lt;p&gt;I want the model to resemble a specific person/group and it will be fed everything that that person/group wrote, said and etc. Have in mind that the model can answer questions in 1-4 sentences and it doesn&amp;#39;t need to be based on pure facts. This means that the user won&amp;#39;t ask the model questions like &amp;quot;What is the capital of France?&amp;quot; but more something along the lines of existential questions (&amp;quot;What is the meaning of &amp;lt;thing&amp;gt;?&amp;quot;).&lt;/p&gt;

&lt;p&gt;Here are the questions I have as I didn&amp;#39;t dabble into the NLP world of AI at all:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Are there any pre-trained or prebuilt models out there that I could use for this? I&amp;#39;ve found that the open-source Pavlov AI library has some interesting ones.&lt;/li&gt;
&lt;li&gt;Which models would suit this task the best?&lt;/li&gt;
&lt;li&gt;Are there any features I should watch out for or provide more information on?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The biggest part of the job will be to collect relevant data on the group I want the model to resemble. What would be some of the best practices when making the data as informative as it can be? Also, if I want there to be 4 groups that the model can resemble - Do I need to train 4 models or can I filter what a model learned into 4 categories?&lt;/p&gt;

&lt;p&gt;Thanks for all replies and questions in advance. If some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mzk16l,True,,Ingvariuss,,5,True,all_ads,False,[],False,,/r/datascience/comments/mzk16l/question_answering_ai/,all_ads,False,https://www.reddit.com/r/datascience/comments/mzk16l/question_answering_ai/,515407,1619513324.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,Rediculous topic but my friend who is also a data scientist got a neural network tattooed on him and it got me wondering if anybody else had data science/ machine learning tattoos or ideas!,t2_68vll2wy,False,,0,False,Data Science Tattoos,[],r/datascience,False,6,fun,0,,,False,t3_mylony,False,dark,0.8,,public,149,1,{},,,False,[],,False,False,,{},Fun/Trivia,False,149,,False,False,self,False,,[],{'gid_1': 1},,True,,1619426281.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Rediculous topic but my friend who is also a data scientist got a neural network tattooed on him and it got me wondering if anybody else had data science/ machine learning tattoos or ideas!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mylony,True,,a_wsty,,109,True,all_ads,False,[],False,,/r/datascience/comments/mylony/data_science_tattoos/,all_ads,False,https://www.reddit.com/r/datascience/comments/mylony/data_science_tattoos/,515407,1619397481.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,"I’m a statistics student in school, and my goal is to become a “data scientist”, in quotations because that can mean many different things. Anyways, I’ve heard that data scientists tend to have very diverse backgrounds depending on the industry, ie. Statistics/computer science/Math/civil engineering (or some other engineering) / physics ie. List goes on and on.
Even saw someone who was a geospatial data scientist with a background in something GIS related. 
Point is it seems like the “data scientist” has very diverse backgrounds for the role.

However, for something like “machine learning engineer”, is this as diverse? I mean yea there is some machine learning involved, so some statistics, but 90% I’ve heard is SWE related, so most of the backgrounds are generally computer science? Am I right in saying that? For people who are non computer science, is there a higher barrier to entry to become a MLE than it is to be a data scientist? Can non-Cs backgrounds still be considered for MLE positions?",t2_5w4i5kd1,False,,0,False,Is there diversity in Machine Learning Engineer backgrounds like there is with DS?,[],r/datascience,False,6,discussion,0,,,False,t3_mylh4e,False,dark,0.82,,public,27,0,{},,,False,[],,False,False,,{},Discussion,False,27,,False,False,self,False,,[],{},,True,,1619425572.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a statistics student in school, and my goal is to become a “data scientist”, in quotations because that can mean many different things. Anyways, I’ve heard that data scientists tend to have very diverse backgrounds depending on the industry, ie. Statistics/computer science/Math/civil engineering (or some other engineering) / physics ie. List goes on and on.
Even saw someone who was a geospatial data scientist with a background in something GIS related. 
Point is it seems like the “data scientist” has very diverse backgrounds for the role.&lt;/p&gt;

&lt;p&gt;However, for something like “machine learning engineer”, is this as diverse? I mean yea there is some machine learning involved, so some statistics, but 90% I’ve heard is SWE related, so most of the backgrounds are generally computer science? Am I right in saying that? For people who are non computer science, is there a higher barrier to entry to become a MLE than it is to be a data scientist? Can non-Cs backgrounds still be considered for MLE positions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mylh4e,True,,veeeerain,,41,True,all_ads,False,[],False,,/r/datascience/comments/mylh4e/is_there_diversity_in_machine_learning_engineer/,all_ads,False,https://www.reddit.com/r/datascience/comments/mylh4e/is_there_diversity_in_machine_learning_engineer/,515407,1619396772.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I transitioned into data science from the life sciences in the past 3 yrs and I’m wondering how I can pronounce SQL so I don’t sound like a total novice. 

I have heard the developers in my company say it like ‘ES QUE EL’ but the data science/stat folk mostly say ‘SEQUEL’

Which is the correct one??",t2_66b51,False,,0,False,How to pronounce SQL?,[],r/datascience,False,6,meta,0,,,False,t3_myn5sa,False,dark,0.89,,public,15,0,{},,,False,[],,False,False,,{},Meta,False,15,,False,False,self,False,,[],{},,True,,1619431462.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I transitioned into data science from the life sciences in the past 3 yrs and I’m wondering how I can pronounce SQL so I don’t sound like a total novice. &lt;/p&gt;

&lt;p&gt;I have heard the developers in my company say it like ‘ES QUE EL’ but the data science/stat folk mostly say ‘SEQUEL’&lt;/p&gt;

&lt;p&gt;Which is the correct one??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,myn5sa,True,,miss_micropipette,,37,True,all_ads,False,[],False,,/r/datascience/comments/myn5sa/how_to_pronounce_sql/,all_ads,False,https://www.reddit.com/r/datascience/comments/myn5sa/how_to_pronounce_sql/,515407,1619402662.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,
,datascience,"Hi, I posted over the weekend in r/datascience about how someone might navigate the data science field without masters or a PhD. Within 15 minutes my question was removed, and I’m hoping someone can help me understand why this happened? Thanks,",t2_cdiejit,False,,0,False,Why was my career question on the data science industry removed?,[],r/datascience,False,6,discussion,0,,,False,t3_mz3s2z,False,dark,0.42,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1619488903.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I posted over the weekend in &lt;a href=""/r/datascience""&gt;r/datascience&lt;/a&gt; about how someone might navigate the data science field without masters or a PhD. Within 15 minutes my question was removed, and I’m hoping someone can help me understand why this happened? Thanks,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mz3s2z,True,,nocturnalhustler,,3,True,all_ads,False,[],False,,/r/datascience/comments/mz3s2z/why_was_my_career_question_on_the_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/mz3s2z/why_was_my_career_question_on_the_data_science/,515407,1619460103.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am in a graduate level program Social Sciences program and leaning towards data analyst / data science fields when I am finished. I am currently evaluating a course I would like to take on Applied Mathematical Methods. This particular course is taught in the economics college, but the methods should be applicable in a broader socioeconomic context. Here are the mathematical methods listed:

Matrix algebra, differentiation, unconstrained and constrained optimization, integration and linear programming.

My question: how much math do you use in your daily? Would knowing any of these concepts bolster your skills? If not, what mathematical methods would take your game to the next level in a data science role?",t2_88lrckw,False,,0,False,Applied Mathematical Methods: Are they useful?,[],r/datascience,False,6,education,0,,,False,t3_mxuojo,False,dark,0.95,,public,176,1,{},,,False,[],,False,False,,{},Education,False,176,,False,False,self,False,,[],{},,True,,1619332352.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am in a graduate level program Social Sciences program and leaning towards data analyst / data science fields when I am finished. I am currently evaluating a course I would like to take on Applied Mathematical Methods. This particular course is taught in the economics college, but the methods should be applicable in a broader socioeconomic context. Here are the mathematical methods listed:&lt;/p&gt;

&lt;p&gt;Matrix algebra, differentiation, unconstrained and constrained optimization, integration and linear programming.&lt;/p&gt;

&lt;p&gt;My question: how much math do you use in your daily? Would knowing any of these concepts bolster your skills? If not, what mathematical methods would take your game to the next level in a data science role?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mxuojo,True,,AskIT_qa,,55,True,all_ads,False,[],False,,/r/datascience/comments/mxuojo/applied_mathematical_methods_are_they_useful/,all_ads,False,https://www.reddit.com/r/datascience/comments/mxuojo/applied_mathematical_methods_are_they_useful/,515407,1619303552.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 25 Apr 2021 - 02 May 2021,[],r/datascience,False,6,,0,,,False,t3_my6w3q,False,dark,0.81,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1619380830.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,my6w3q,True,,datascience-bot,,145,False,all_ads,False,[],False,dark,/r/datascience/comments/my6w3q/weekly_entering_transitioning_thread_25_apr_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/my6w3q/weekly_entering_transitioning_thread_25_apr_2021/,515407,1619352030.0,0,,False,,,,,,,,
,datascience,"I work for the government as a data scientist, as my first (and so far only) post-masters job. I like my job, but I don't like where I have to live to do my job at my specific branch of the government (and there is no chance of this kind of work being done remotely from my preferred living location). I may change my mind over time, but I think maybe in two or three years when I have more on my resume, I may go back to the private sector (preferably a big and more stable company, not a startup) so I will be able to go back to where I'm from (which is NYC and there is a lot of tech and DS there so that shouldn't be a problem). However I do hear a fair amount of chatter amongst DS people both inside and outside the government that there is somewhat of a stigma against government employees amongst big tech corps when looking to hire programmers &amp; DS, primarily for two reasons: 1, that gov employees get too used to working exactly 40 hours/week less willing to put up with being pressured/forced to do unpaid overtime frequently, and 2, that gov handles DS and SWE very differently than FAANG/etc and that the skill set is too different. I don't know about #2, but I think #1 maybe be true, because I know a couple of people (SWE and DS mostly) who left, went private, and came back because they discovered that they cared more about reasonable workloads and good PTO than making 30-50% more money &amp; getting bonuses. And honestly, I may too,  but for me it's primarily the location. 

I realize the answer is probably ""it really depends on the company and it really depends what part of the government"", but any general takes on this?",t2_5qdqpamk,False,,0,False,Do you think there is a stigma against (US) government employees trying to get data science jobs in the private sector?,[],r/datascience,False,6,career,0,,,False,t3_mxvzwl,False,dark,0.79,,public,9,0,{},,,False,[],,False,False,,{},Career,False,9,,False,False,self,1619307976.0,,[],{},,True,,1619336579.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work for the government as a data scientist, as my first (and so far only) post-masters job. I like my job, but I don&amp;#39;t like where I have to live to do my job at my specific branch of the government (and there is no chance of this kind of work being done remotely from my preferred living location). I may change my mind over time, but I think maybe in two or three years when I have more on my resume, I may go back to the private sector (preferably a big and more stable company, not a startup) so I will be able to go back to where I&amp;#39;m from (which is NYC and there is a lot of tech and DS there so that shouldn&amp;#39;t be a problem). However I do hear a fair amount of chatter amongst DS people both inside and outside the government that there is somewhat of a stigma against government employees amongst big tech corps when looking to hire programmers &amp;amp; DS, primarily for two reasons: 1, that gov employees get too used to working exactly 40 hours/week less willing to put up with being pressured/forced to do unpaid overtime frequently, and 2, that gov handles DS and SWE very differently than FAANG/etc and that the skill set is too different. I don&amp;#39;t know about #2, but I think #1 maybe be true, because I know a couple of people (SWE and DS mostly) who left, went private, and came back because they discovered that they cared more about reasonable workloads and good PTO than making 30-50% more money &amp;amp; getting bonuses. And honestly, I may too,  but for me it&amp;#39;s primarily the location. &lt;/p&gt;

&lt;p&gt;I realize the answer is probably &amp;quot;it really depends on the company and it really depends what part of the government&amp;quot;, but any general takes on this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mxvzwl,True,,ui_throwaway_354,,17,True,all_ads,False,[],False,,/r/datascience/comments/mxvzwl/do_you_think_there_is_a_stigma_against_us/,all_ads,False,https://www.reddit.com/r/datascience/comments/mxvzwl/do_you_think_there_is_a_stigma_against_us/,515407,1619307779.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"When it comes to real-world data, how accurate were the statistical models you developed? Were these models able to consistently and accurately make predictions? 

E.g. for supervised binary classification, has anyone been able to develop a model that had high accuracy, high sensitivity and high specificity?",t2_3tosvccj,False,,0,False,[D] how accurate were the statistical models you developed on real-world data?,[],r/datascience,False,6,discussion,0,,,False,t3_mxy6bp,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1619344090.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When it comes to real-world data, how accurate were the statistical models you developed? Were these models able to consistently and accurately make predictions? &lt;/p&gt;

&lt;p&gt;E.g. for supervised binary classification, has anyone been able to develop a model that had high accuracy, high sensitivity and high specificity?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mxy6bp,True,,jj4646,,6,True,all_ads,False,[],False,,/r/datascience/comments/mxy6bp/d_how_accurate_were_the_statistical_models_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/mxy6bp/d_how_accurate_were_the_statistical_models_you/,515407,1619315290.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, I've seen enough of this trend that every big company (especially in north Africa) is forcing the inclusion of machine learning in every aspect of its activity. 

People are literally misunderstanding how things work, the state of art of how to tackle every subject in hand hence creating problems that don't exist. It's solutionism at its worst.

They  dumbing down machines that are inherently superior. ( Gilfoyle's quote from SV)",t2_5i5vbzw,False,,0,False,Machine learning is not always the best answer,[],r/datascience,False,6,discussion,0,,,False,t3_mwur7p,False,dark,0.95,,public,461,2,{},,,False,[],,False,False,,{},Discussion,False,461,,False,False,self,False,,[],{'gid_1': 1},,True,,1619211738.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;ve seen enough of this trend that every big company (especially in north Africa) is forcing the inclusion of machine learning in every aspect of its activity. &lt;/p&gt;

&lt;p&gt;People are literally misunderstanding how things work, the state of art of how to tackle every subject in hand hence creating problems that don&amp;#39;t exist. It&amp;#39;s solutionism at its worst.&lt;/p&gt;

&lt;p&gt;They  dumbing down machines that are inherently superior. ( Gilfoyle&amp;#39;s quote from SV)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwur7p,True,,postal__dude,,141,True,all_ads,False,[],False,,/r/datascience/comments/mwur7p/machine_learning_is_not_always_the_best_answer/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwur7p/machine_learning_is_not_always_the_best_answer/,515407,1619182938.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in the process of interviewing for a Data Scientist role. I had taken a two hour Python/SQL technical evaluation and passed it. One week later, I'm in a late-stage 90 minute interview with several team members. With 45 minutes left, they suddenly had me do a screen share with everyone and bombarded me with SQL/Python questions. This effectively left no time for me to ask questions I had for the team. It was stressful, and in no way reflects a typical coding environment (that I've been in). 

I didn't botch the surprise technical assessment, but didn't ace it either. Certainly wasn't an environment to do my best work. I'll be honest, it was a huge turn-off.

I know data science is technical oriented, but I felt that I had little opportunity to absorb/ask about team and company culture.

Bit of a rant, but also curious if anyone else has experienced this, and what your experience was like?",t2_5nsm9nbe,False,,0,False,Surprise 45 minute technical assessment in late-stage interview with several of company's team members,[],r/datascience,False,6,,0,,,False,t3_mwxyac,False,dark,0.79,,public,13,0,{},,,False,[],,False,False,,{},Job Search,False,13,,False,False,self,False,,[],{},,True,,1619220872.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of interviewing for a Data Scientist role. I had taken a two hour Python/SQL technical evaluation and passed it. One week later, I&amp;#39;m in a late-stage 90 minute interview with several team members. With 45 minutes left, they suddenly had me do a screen share with everyone and bombarded me with SQL/Python questions. This effectively left no time for me to ask questions I had for the team. It was stressful, and in no way reflects a typical coding environment (that I&amp;#39;ve been in). &lt;/p&gt;

&lt;p&gt;I didn&amp;#39;t botch the surprise technical assessment, but didn&amp;#39;t ace it either. Certainly wasn&amp;#39;t an environment to do my best work. I&amp;#39;ll be honest, it was a huge turn-off.&lt;/p&gt;

&lt;p&gt;I know data science is technical oriented, but I felt that I had little opportunity to absorb/ask about team and company culture.&lt;/p&gt;

&lt;p&gt;Bit of a rant, but also curious if anyone else has experienced this, and what your experience was like?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,mwxyac,True,,800rob,,23,True,all_ads,False,[],False,,/r/datascience/comments/mwxyac/surprise_45_minute_technical_assessment_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwxyac/surprise_45_minute_technical_assessment_in/,515407,1619192072.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"In python/sklearn, most of the time the defaults produce the best (or very close to it) performing model (F1 score), and doing a gridsearch over 6,000 combinations or whatever rarely improves anything. The only thing I've found to be helpful is building new features. Is this typical?",t2_15xuhx,False,,0,False,Do you often find hyperparam tuning does very little?,[],r/datascience,False,6,tooling,0,,,False,t3_mwl2zj,False,dark,0.98,,public,125,0,{},,,False,[],,False,False,,{},Tooling,False,125,,False,True,self,False,,[],{},,True,,1619173385.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In python/sklearn, most of the time the defaults produce the best (or very close to it) performing model (F1 score), and doing a gridsearch over 6,000 combinations or whatever rarely improves anything. The only thing I&amp;#39;ve found to be helpful is building new features. Is this typical?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwl2zj,True,,question_23,,51,True,all_ads,False,[],False,,/r/datascience/comments/mwl2zj/do_you_often_find_hyperparam_tuning_does_very/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwl2zj/do_you_often_find_hyperparam_tuning_does_very/,515407,1619144585.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I’m curious what various ways this group has found to use data science, or data science-like skills to make their day to day job better? 

I’ll start - I wrote a script using primarily regular expressions to turn code log files back into code. I don’t use it a lot, but every now and then I’ll get my hands on a log file without access to the code, and this lets me easily back door my way into the code.",t2_afgeic8,False,,0,False,Simple ways you use data science to improve your day-to-day job?,[],r/datascience,False,6,discussion,0,,,False,t3_mwvk2p,False,dark,0.85,,public,13,2,{},,,False,[],,False,False,,{},Discussion,False,13,,False,False,self,False,,[],{'gid_1': 1},,True,,1619214204.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m curious what various ways this group has found to use data science, or data science-like skills to make their day to day job better? &lt;/p&gt;

&lt;p&gt;I’ll start - I wrote a script using primarily regular expressions to turn code log files back into code. I don’t use it a lot, but every now and then I’ll get my hands on a log file without access to the code, and this lets me easily back door my way into the code.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'award_3267ca1c-127a-49e9-9a3d-4ba96224af18', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=16&amp;height=16&amp;auto=webp&amp;s=6ce62fa40de4c6b72859d2cbdf22af5c0e012233', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=32&amp;height=32&amp;auto=webp&amp;s=26297b024da3e9bd6507e7b8553507493b5e6606', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=48&amp;height=48&amp;auto=webp&amp;s=0763517837b22d5e414dd330d5006c0d89ccb499', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a2154561daa83678f3f9e6e2a627629ee2a2bcc', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=128&amp;height=128&amp;auto=webp&amp;s=96897549f634fd6324e1338a98b9778733ea4813', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Let's sip to good health and good company"", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': ""I'll Drink to That"", 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=16&amp;height=16&amp;auto=webp&amp;s=6ce62fa40de4c6b72859d2cbdf22af5c0e012233', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=32&amp;height=32&amp;auto=webp&amp;s=26297b024da3e9bd6507e7b8553507493b5e6606', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=48&amp;height=48&amp;auto=webp&amp;s=0763517837b22d5e414dd330d5006c0d89ccb499', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a2154561daa83678f3f9e6e2a627629ee2a2bcc', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png?width=128&amp;height=128&amp;auto=webp&amp;s=96897549f634fd6324e1338a98b9778733ea4813', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/45aeu8mzvsj51_IllDrinktoThat.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwvk2p,True,,jarkeb,,8,True,all_ads,False,[],False,,/r/datascience/comments/mwvk2p/simple_ways_you_use_data_science_to_improve_your/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwvk2p/simple_ways_you_use_data_science_to_improve_your/,515407,1619185404.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been a data analyst with some data science-ish work for years and now have finally secured a job as a data scientist and I don't want to F it up. I'm freaking out. Everyone on the team seems nice but it's also small so I assume I'll need to be pretty independent. I'm going to have a few weeks off before starting and wondering if anyone has any advice on things I can do to refresh my skills in this area so I don't seem like a complete idiot when I start? It's been a while since I did any academic DS work.
Tia!",t2_2om1gcma,False,,0,False,Starting a job as a data scientist in a month and freaking out!,[],r/datascience,False,6,career,0,,,False,t3_mwtor5,False,dark,0.8,,public,12,0,{},,,False,[],,False,False,,{},Career,False,12,,False,False,self,False,,[],{},,True,,1619208166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been a data analyst with some data science-ish work for years and now have finally secured a job as a data scientist and I don&amp;#39;t want to F it up. I&amp;#39;m freaking out. Everyone on the team seems nice but it&amp;#39;s also small so I assume I&amp;#39;ll need to be pretty independent. I&amp;#39;m going to have a few weeks off before starting and wondering if anyone has any advice on things I can do to refresh my skills in this area so I don&amp;#39;t seem like a complete idiot when I start? It&amp;#39;s been a while since I did any academic DS work.
Tia!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwtor5,True,,zaznoba03,,24,True,all_ads,False,[],False,,/r/datascience/comments/mwtor5/starting_a_job_as_a_data_scientist_in_a_month_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwtor5/starting_a_job_as_a_data_scientist_in_a_month_and/,515407,1619179366.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"We are using (in production) an ML for text classification. We trained our model using some custom English text corpus. Currently, the model is working acceptable level of accuracy for our purpose. Now we want to extend it to handle French language as well. We are planning to investigate the following two approaches.

1. We have a French-language corpus. Therefore, we would like to train a new model for handling French text.
2. Use the same model trained with English corpus. But use a third-party language translation service (such as Google Translator) to translate French text to English before inputting it into the ML model.

So I would like to know your thoughts regarding these two approaches.",t2_8l1x8,False,,0,False,[D] How to extend a text classification ML model to work with more than one language?,[],r/datascience,False,6,discussion,0,,,False,t3_mx20vs,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1619231921.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We are using (in production) an ML for text classification. We trained our model using some custom English text corpus. Currently, the model is working acceptable level of accuracy for our purpose. Now we want to extend it to handle French language as well. We are planning to investigate the following two approaches.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We have a French-language corpus. Therefore, we would like to train a new model for handling French text.&lt;/li&gt;
&lt;li&gt;Use the same model trained with English corpus. But use a third-party language translation service (such as Google Translator) to translate French text to English before inputting it into the ML model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So I would like to know your thoughts regarding these two approaches.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mx20vs,True,,upulbandara,,4,True,all_ads,False,[],False,,/r/datascience/comments/mx20vs/d_how_to_extend_a_text_classification_ml_model_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/mx20vs/d_how_to_extend_a_text_classification_ml_model_to/,515407,1619203121.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Obviously you’ll need post/comment history here. 

Post in this thread and we’ll check you out.",t2_fs0bt,False,,0,False,Anyone interested in being a mod?,[],r/datascience,False,6,,0,,,False,t3_mwfrbq,False,dark,0.95,,public,106,1,{},,,False,[],,False,False,,{},,False,106,,False,False,self,False,modflair,[],{},,True,,1619156905.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Obviously you’ll need post/comment history here. &lt;/p&gt;

&lt;p&gt;Post in this thread and we’ll check you out.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,True,MS | Data Scientist | Healthcare,[],False,,,,t5_2sptq,,,,mwfrbq,True,,patrickSwayzeNU,,91,True,all_ads,False,[],False,dark,/r/datascience/comments/mwfrbq/anyone_interested_in_being_a_mod/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwfrbq/anyone_interested_in_being_a_mod/,515407,1619128105.0,0,,False,,,,,,,,
,datascience,"I want to start sending a list of tasks for the day to my manager in the morning, and at the end of the day give a status update on all the tasks. 

Is there a good way to do this other than sending out emails?

Mainly want to keep myself accountable in the WFH situation. We don't do daily standup meetings so I've found myself slacking more and more.",t2_qinw9,False,,0,False,How to do Daily Check Up?,[],r/datascience,False,6,tooling,0,,,False,t3_mx245j,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1619232170.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to start sending a list of tasks for the day to my manager in the morning, and at the end of the day give a status update on all the tasks. &lt;/p&gt;

&lt;p&gt;Is there a good way to do this other than sending out emails?&lt;/p&gt;

&lt;p&gt;Mainly want to keep myself accountable in the WFH situation. We don&amp;#39;t do daily standup meetings so I&amp;#39;ve found myself slacking more and more.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mx245j,True,,monkeyunited,,11,True,all_ads,False,[],False,,/r/datascience/comments/mx245j/how_to_do_daily_check_up/,all_ads,False,https://www.reddit.com/r/datascience/comments/mx245j/how_to_do_daily_check_up/,515407,1619203370.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I work at a network of charter schools. We have 50 schools, thousands of kids, and to keep it simple, let's call it 4 main reading tests per year, plus the typical, everyday grades kids get on reading assignments. How do we aggregate all this info to get to one measure of a child's reading ability? 

In my mind, we could standardize the scores, then weigh them based on what we feel are the most valuable assessments and output one measure, a ""Reading KPI"". But is there a way to mathematically calculate these weights? Or just rely on SMEs to guide us? 

Would a regression model help us isolate the features (in this case assessments) that are most important to predicting their Reading KPI?  

Can we use ML to predict their score on an upcoming test, and take action if they are +1 SD below it?  

Should this metric consider the child's progress over time, or only compare their performance against their peers?",t2_pr6lcu2,False,,0,False,DS to Find Kids Who Read Good,[],r/datascience,False,6,discussion,0,,,False,t3_mwwulj,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1619217877.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work at a network of charter schools. We have 50 schools, thousands of kids, and to keep it simple, let&amp;#39;s call it 4 main reading tests per year, plus the typical, everyday grades kids get on reading assignments. How do we aggregate all this info to get to one measure of a child&amp;#39;s reading ability? &lt;/p&gt;

&lt;p&gt;In my mind, we could standardize the scores, then weigh them based on what we feel are the most valuable assessments and output one measure, a &amp;quot;Reading KPI&amp;quot;. But is there a way to mathematically calculate these weights? Or just rely on SMEs to guide us? &lt;/p&gt;

&lt;p&gt;Would a regression model help us isolate the features (in this case assessments) that are most important to predicting their Reading KPI?  &lt;/p&gt;

&lt;p&gt;Can we use ML to predict their score on an upcoming test, and take action if they are +1 SD below it?  &lt;/p&gt;

&lt;p&gt;Should this metric consider the child&amp;#39;s progress over time, or only compare their performance against their peers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwwulj,True,,most_humblest_ever,,11,True,all_ads,False,[],False,,/r/datascience/comments/mwwulj/ds_to_find_kids_who_read_good/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwwulj/ds_to_find_kids_who_read_good/,515407,1619189077.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_1798hj,False,,0,False,What's your best use of AutoML?,[],r/datascience,False,6,discussion,0,,,False,t3_mw3hi6,False,dark,0.93,,public,109,0,{},,,False,[],,False,False,,{},Discussion,False,109,,False,False,self,False,,[],{},,True,,1619122757.0,text,6,,,text,self.datascience,True,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mw3hi6,True,,pp314159,,38,True,all_ads,False,[],False,,/r/datascience/comments/mw3hi6/whats_your_best_use_of_automl/,all_ads,False,https://www.reddit.com/r/datascience/comments/mw3hi6/whats_your_best_use_of_automl/,515407,1619093957.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Data Science is continually changing and is unlikely to ""settle down"" and stay in one spot for a long time. What industries, sectors, etc. that only exist in concept or even ones that are just starting to emerge now do you see data science dominating?",t2_ct0iy,False,,0,False,"What ""futuristic"" industries do you see data science playing an integral role in the next 50 years?",[],r/datascience,False,6,discussion,0,,,False,t3_mwf3h1,False,dark,0.95,,public,18,0,{},,,False,[],,False,False,,{},Discussion,False,18,,False,False,self,False,,[],{},,True,,1619155103.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Data Science is continually changing and is unlikely to &amp;quot;settle down&amp;quot; and stay in one spot for a long time. What industries, sectors, etc. that only exist in concept or even ones that are just starting to emerge now do you see data science dominating?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwf3h1,True,,marbledduck,,32,True,all_ads,False,[],False,,/r/datascience/comments/mwf3h1/what_futuristic_industries_do_you_see_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwf3h1/what_futuristic_industries_do_you_see_data/,515407,1619126303.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’ve been working as a data science/data engineer hybrid for about 5+ years. I would consider myself a practitioner – I’m not looking for cutting-edge methods to address common problems, rather I’m just trying to deliver as much business value as possible to a client. Currently I’m in a consulting role but previously I worked for a big enterprise.

I’m in a situation where my consulting group does not have work for me at the moment. Obviously what you do in the consulting world is heavily dependent on what sales can pull in and they don’t seem to be doing a great job. I’m salaried, so even if they don’t have work for me, I’m still getting paid (lack of work does affect my bonus, but overall not such a big deal &amp; I am content with my level of pay even without my bonus). It’s been like this for a month or so, but there are no clear signs of a big project in the future so this could continue for many more months.

While I’m on the bench they literally have nothing for me to do and my boss has told me so himself. He said you can do trainings if you want to make good use of your time, but otherwise there’s not much else to do. I’ve been taking him up on that.

I’ve come to the realization that selling data science is a difficult chore for salespeople for a variety of reasons (lack of technical knowledge, can’t guarantee ROI in many cases, difficulties in pricing, etc…). For this reason I sympathize for them, but it makes me worry.

My questions to the community:

1) Have you experienced anything like this at your work? Is this common and what should I do?

2) Should I be thankful to have this sort of “problem” or is this a worrisome position? My company has made it clear that they have no intentions of laying me off any time soon (and seem to have a history of keeping people on salary even without work)

3) Do you agree with the assertion that selling DS is hard? Or could this just be an issue I’ve seen with the companies I’ve worked for?

Edit: I do currently help with sales in a pre-sales role. Sales generates leads and I'm brought in to talk about the technical details and help sell the service.",t2_1hy2flk,False,,0,False,Salaried DS: No Work &amp; Kept on the Bench,[],r/datascience,False,6,career,0,,,False,t3_mw8bzl,False,dark,0.91,,public,37,0,{},,,False,[],,False,False,,{},Career,False,37,,False,False,self,1619110854.0,,[],{},,True,,1619136905.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’ve been working as a data science/data engineer hybrid for about 5+ years. I would consider myself a practitioner – I’m not looking for cutting-edge methods to address common problems, rather I’m just trying to deliver as much business value as possible to a client. Currently I’m in a consulting role but previously I worked for a big enterprise.&lt;/p&gt;

&lt;p&gt;I’m in a situation where my consulting group does not have work for me at the moment. Obviously what you do in the consulting world is heavily dependent on what sales can pull in and they don’t seem to be doing a great job. I’m salaried, so even if they don’t have work for me, I’m still getting paid (lack of work does affect my bonus, but overall not such a big deal &amp;amp; I am content with my level of pay even without my bonus). It’s been like this for a month or so, but there are no clear signs of a big project in the future so this could continue for many more months.&lt;/p&gt;

&lt;p&gt;While I’m on the bench they literally have nothing for me to do and my boss has told me so himself. He said you can do trainings if you want to make good use of your time, but otherwise there’s not much else to do. I’ve been taking him up on that.&lt;/p&gt;

&lt;p&gt;I’ve come to the realization that selling data science is a difficult chore for salespeople for a variety of reasons (lack of technical knowledge, can’t guarantee ROI in many cases, difficulties in pricing, etc…). For this reason I sympathize for them, but it makes me worry.&lt;/p&gt;

&lt;p&gt;My questions to the community:&lt;/p&gt;

&lt;p&gt;1) Have you experienced anything like this at your work? Is this common and what should I do?&lt;/p&gt;

&lt;p&gt;2) Should I be thankful to have this sort of “problem” or is this a worrisome position? My company has made it clear that they have no intentions of laying me off any time soon (and seem to have a history of keeping people on salary even without work)&lt;/p&gt;

&lt;p&gt;3) Do you agree with the assertion that selling DS is hard? Or could this just be an issue I’ve seen with the companies I’ve worked for?&lt;/p&gt;

&lt;p&gt;Edit: I do currently help with sales in a pre-sales role. Sales generates leads and I&amp;#39;m brought in to talk about the technical details and help sell the service.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mw8bzl,True,,weareglenn,,24,True,all_ads,False,[],False,,/r/datascience/comments/mw8bzl/salaried_ds_no_work_kept_on_the_bench/,all_ads,False,https://www.reddit.com/r/datascience/comments/mw8bzl/salaried_ds_no_work_kept_on_the_bench/,515407,1619108105.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi, sorry for the newbie question. We are getting 4 years of donation data sent to us as we can no longer use the service portal to pull specific reports and search through it. I need to set up a substitute that will allow members of our team (who have no experience outside of basic excel) to search through the data and if possible save spreadsheets based on reporting parameters. 

&amp;#x200B;

What is the best solution for something like this? Could I use Power BI? Am I right in thinking SQL would probably be best but the person submitting the queries would need to know SQL (which would rule it out for this use case). The size of the data would crash most of the team's excel programs. 

&amp;#x200B;

Thanks for your help!",t2_7t2l9,False,,0,False,"Getting sent 4 years of Data, what is the best way to make this searchable for the whole team independently?",[],r/datascience,False,6,discussion,0,,,False,t3_mwqwfv,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1619196391.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, sorry for the newbie question. We are getting 4 years of donation data sent to us as we can no longer use the service portal to pull specific reports and search through it. I need to set up a substitute that will allow members of our team (who have no experience outside of basic excel) to search through the data and if possible save spreadsheets based on reporting parameters. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What is the best solution for something like this? Could I use Power BI? Am I right in thinking SQL would probably be best but the person submitting the queries would need to know SQL (which would rule it out for this use case). The size of the data would crash most of the team&amp;#39;s excel programs. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mwqwfv,True,,jboyd88,,5,True,all_ads,False,[],False,,/r/datascience/comments/mwqwfv/getting_sent_4_years_of_data_what_is_the_best_way/,all_ads,False,https://www.reddit.com/r/datascience/comments/mwqwfv/getting_sent_4_years_of_data_what_is_the_best_way/,515407,1619167591.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello fellow community,

So...While we might love jupyter and all our fancy tools when getting results into the hands of customers Webapps seem to be the deal.

Currently I am developing a few frontends, calling them “data driven” for now. Whatever that means, but it’s trendy.

Basically they are CRUD Interfaces with a lot of sugar.

Collapsible lists with tooltips, maybe a summary row, icons, colors, basically presenting data in a way that people will like to pay for.

Currently I decided to go with a Django backend and a react frontend.

Overall I have to admit I hate frontend dev almost as much as I hate Webapps. Still I thought react was a reasonable choice for a great user experience with a modern toolset.

Right now the frontends authenticate against the backends and fetches data using GraphQL instead of traditional REST. Which sounded like a great idea at the time.

But actually I feel like this was a terrible approach. When fetching data there needs to be a ton of transformation and looping over arrays done in the frontend to bringt the pieces of fetched data together in a format suitable to render tables.
Which in my opinion is a mess; fiddling with arrays in JS while there is a Python backend at my fingertips that could use pandas to do it in the fraction of the time. But that seems just how this works.

I also got fed up with react. It provides a lot of great advantages, but honestly I am not happy having tons of packages for simple stuff that might get compromised with incompatible versions and stuff down the road. Also I feel bad about the packages available to create those tables in general.
It just feels extremely inefficient, and that’s coming from someone usually writhing Python ;)

Overall what I like:
- beautiful frontend
- great structure
- single page applications just feel so good
- easy to use (mainly)

What I just can’t stand anymore:
- way too much logic inside the frontend
- way too much data transformation inside the frontend (well, all of it)
- too much packages that don’t feel reliable in the long run
- sometimes clunky to debug depending on what packages are used
- I somehow never get the exact visual results rendered that I want
- I somehow create a memory leak daily that I have to fix then (call me incompetent but I can’t figure out why this always happens to me)


So I have been talking to a few other DS and Devs and...GraphQL and React seem to be really popular and others don’t seem to mind it too much.

What are your experiences? Similar problems? Do you use something else?
I would love to ditch react in favor of something more suitable.

Overall I feel like providing a crud interface with “advanced” stuff like icons in cells, tool tips, and collapsible rows (tree structure tables) should be a common challenge, I just can’t find the proper tool for the job.

Best regards and would love to hear your thoughts",t2_26l8ajby,False,,0,False,Data driven Web Frontends....looking at React and beyond for CRUD,[],r/datascience,False,6,projects,0,,,False,t3_mvcuae,False,dark,0.96,,public,127,0,{},,,False,[],,False,False,,{},Projects,False,127,,False,False,self,False,,[],{},,True,,1619030155.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello fellow community,&lt;/p&gt;

&lt;p&gt;So...While we might love jupyter and all our fancy tools when getting results into the hands of customers Webapps seem to be the deal.&lt;/p&gt;

&lt;p&gt;Currently I am developing a few frontends, calling them “data driven” for now. Whatever that means, but it’s trendy.&lt;/p&gt;

&lt;p&gt;Basically they are CRUD Interfaces with a lot of sugar.&lt;/p&gt;

&lt;p&gt;Collapsible lists with tooltips, maybe a summary row, icons, colors, basically presenting data in a way that people will like to pay for.&lt;/p&gt;

&lt;p&gt;Currently I decided to go with a Django backend and a react frontend.&lt;/p&gt;

&lt;p&gt;Overall I have to admit I hate frontend dev almost as much as I hate Webapps. Still I thought react was a reasonable choice for a great user experience with a modern toolset.&lt;/p&gt;

&lt;p&gt;Right now the frontends authenticate against the backends and fetches data using GraphQL instead of traditional REST. Which sounded like a great idea at the time.&lt;/p&gt;

&lt;p&gt;But actually I feel like this was a terrible approach. When fetching data there needs to be a ton of transformation and looping over arrays done in the frontend to bringt the pieces of fetched data together in a format suitable to render tables.
Which in my opinion is a mess; fiddling with arrays in JS while there is a Python backend at my fingertips that could use pandas to do it in the fraction of the time. But that seems just how this works.&lt;/p&gt;

&lt;p&gt;I also got fed up with react. It provides a lot of great advantages, but honestly I am not happy having tons of packages for simple stuff that might get compromised with incompatible versions and stuff down the road. Also I feel bad about the packages available to create those tables in general.
It just feels extremely inefficient, and that’s coming from someone usually writhing Python ;)&lt;/p&gt;

&lt;p&gt;Overall what I like:
- beautiful frontend
- great structure
- single page applications just feel so good
- easy to use (mainly)&lt;/p&gt;

&lt;p&gt;What I just can’t stand anymore:
- way too much logic inside the frontend
- way too much data transformation inside the frontend (well, all of it)
- too much packages that don’t feel reliable in the long run
- sometimes clunky to debug depending on what packages are used
- I somehow never get the exact visual results rendered that I want
- I somehow create a memory leak daily that I have to fix then (call me incompetent but I can’t figure out why this always happens to me)&lt;/p&gt;

&lt;p&gt;So I have been talking to a few other DS and Devs and...GraphQL and React seem to be really popular and others don’t seem to mind it too much.&lt;/p&gt;

&lt;p&gt;What are your experiences? Similar problems? Do you use something else?
I would love to ditch react in favor of something more suitable.&lt;/p&gt;

&lt;p&gt;Overall I feel like providing a crud interface with “advanced” stuff like icons in cells, tool tips, and collapsible rows (tree structure tables) should be a common challenge, I just can’t find the proper tool for the job.&lt;/p&gt;

&lt;p&gt;Best regards and would love to hear your thoughts&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mvcuae,True,,ReactCereals,,49,True,all_ads,False,[],False,,/r/datascience/comments/mvcuae/data_driven_web_frontendslooking_at_react_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/mvcuae/data_driven_web_frontendslooking_at_react_and/,515407,1619001355.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"What is the product or tool that analysed or summarised your data in a way that provided you with a memorable user experience (eg very useful, pleasant, easy to use)?

Why was that?

I am interested in *products that analysed your data for you*, such as Google Analytics for data of your website, Apple Health for data about your sleep, Apple Screen Time for data about your device usage, etc.

I am not interested in products or tools that allowed you to analyse your data, like MS Excel.",t2_172hfn,False,,0,False,What is the best data analytics product that you have ever used?,[],r/datascience,False,6,discussion,0,,,False,t3_mw79sm,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1619134081.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is the product or tool that analysed or summarised your data in a way that provided you with a memorable user experience (eg very useful, pleasant, easy to use)?&lt;/p&gt;

&lt;p&gt;Why was that?&lt;/p&gt;

&lt;p&gt;I am interested in &lt;em&gt;products that analysed your data for you&lt;/em&gt;, such as Google Analytics for data of your website, Apple Health for data about your sleep, Apple Screen Time for data about your device usage, etc.&lt;/p&gt;

&lt;p&gt;I am not interested in products or tools that allowed you to analyse your data, like MS Excel.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mw79sm,True,,ricvolpe,,3,True,all_ads,False,[],False,,/r/datascience/comments/mw79sm/what_is_the_best_data_analytics_product_that_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/mw79sm/what_is_the_best_data_analytics_product_that_you/,515407,1619105281.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Greetings DS community,

&amp;#x200B;

**Please help me out here.** 

I have a dataset provided by a company which is a daily tabulation of sales data over 6 months. I want to create a program which would predict and showcase the next 'n' values(n as an input).

The data has no trend, although it shows seasonality on a weekly basis. The data rejects the ADCF test and is stationary without any differencing. Even if I difference it over a shift of 1, the p-value just drops heavily below 0.05. 

Furthermore, I have applied autoarima for understanding the best parameters for the SARIMA model but I still can't get good predictions. 

I even used fb-prophet but I don't know how to actually maximise the accuracy of these models. 

Can someone please help me understand what exactly should I do to get tangible results? 

If someone can spend some time, I'll reach out to them via PM and share the PACF, ACF and the decomposition plots. 

**PLEASE HELP ME, THE DEADLINE IS TONIGHT.**",t2_84nw0237,False,,0,False,"Time series forecasting of sales data - NO TREND, ONLY SEASONALITY *URGENT*",[],r/datascience,False,6,discussion,0,,,False,t3_mw60gc,False,dark,0.24,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1619130573.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings DS community,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please help me out here.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;I have a dataset provided by a company which is a daily tabulation of sales data over 6 months. I want to create a program which would predict and showcase the next &amp;#39;n&amp;#39; values(n as an input).&lt;/p&gt;

&lt;p&gt;The data has no trend, although it shows seasonality on a weekly basis. The data rejects the ADCF test and is stationary without any differencing. Even if I difference it over a shift of 1, the p-value just drops heavily below 0.05. &lt;/p&gt;

&lt;p&gt;Furthermore, I have applied autoarima for understanding the best parameters for the SARIMA model but I still can&amp;#39;t get good predictions. &lt;/p&gt;

&lt;p&gt;I even used fb-prophet but I don&amp;#39;t know how to actually maximise the accuracy of these models. &lt;/p&gt;

&lt;p&gt;Can someone please help me understand what exactly should I do to get tangible results? &lt;/p&gt;

&lt;p&gt;If someone can spend some time, I&amp;#39;ll reach out to them via PM and share the PACF, ACF and the decomposition plots. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PLEASE HELP ME, THE DEADLINE IS TONIGHT.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mw60gc,True,,TypicalQueryMan,,6,True,all_ads,False,[],False,,/r/datascience/comments/mw60gc/time_series_forecasting_of_sales_data_no_trend/,all_ads,False,https://www.reddit.com/r/datascience/comments/mw60gc/time_series_forecasting_of_sales_data_no_trend/,515407,1619101773.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Curious if anyone has managed to land a job where they actually can frolic in their free time. Currently all of my code, any models I make for research or a hobby, and all stocks I want to buy are monitored by my company.

I was very careful to negotiate in my initial contract such that all work done for school would be owned by school/ me (because otherwise my company forces us to send any academic papers we want to publish through a review process where they edit the document and review it in corporate first...).

I've had to deal with gnarly contracts like this for the last ten years and they're always a bit off-putting. Curious to hear if anyone has had any luck not ending up in this situation.

(I should mention I also had to take down my Github when I started at this company and cannot have a blog or social media presence...)",t2_9l54wyqa,False,,0,False,Are there any companies out there that don't insist on owning everything you do in your free time anymore? Or is it standard practice to assume you're a slave 100% of the time as a data scientist these days?,[],r/datascience,False,6,discussion,0,,,False,t3_muqo6t,False,dark,0.94,,public,388,0,{},,,False,[],,False,False,,{},Discussion,False,388,,False,False,self,False,,[],{},,True,,1618955169.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Curious if anyone has managed to land a job where they actually can frolic in their free time. Currently all of my code, any models I make for research or a hobby, and all stocks I want to buy are monitored by my company.&lt;/p&gt;

&lt;p&gt;I was very careful to negotiate in my initial contract such that all work done for school would be owned by school/ me (because otherwise my company forces us to send any academic papers we want to publish through a review process where they edit the document and review it in corporate first...).&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve had to deal with gnarly contracts like this for the last ten years and they&amp;#39;re always a bit off-putting. Curious to hear if anyone has had any luck not ending up in this situation.&lt;/p&gt;

&lt;p&gt;(I should mention I also had to take down my Github when I started at this company and cannot have a blog or social media presence...)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,muqo6t,True,,kiwipineapplebug2,,121,True,all_ads,False,[],False,,/r/datascience/comments/muqo6t/are_there_any_companies_out_there_that_dont/,all_ads,False,https://www.reddit.com/r/datascience/comments/muqo6t/are_there_any_companies_out_there_that_dont/,515407,1618926369.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Depending on boss or colleague, they introduce me various ways - data scientist, researcher, statistician. Those are the main titles I'm introduced as, but there are more. 

On paper I'm titled as a statistician according to HR. But I think the title of researcher is probably the best fit. The majority of my duties, design research studies (experimental and survey), collect data (from a research study and/or public data), clean, structure, analyze and visualize data, create papers, decks, and reports, present findings. 

What separates the jobs and fields in and related to data science? Statistician, researcher, data analyst, data scientist, et cetera...",t2_5dpllzer,False,,0,False,What separates the jobs and fields in and related to data science?,[],r/datascience,False,6,career,0,,,False,t3_mvhyxz,False,dark,0.54,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,True,self,1619030892.0,,[],{},,True,,1619047254.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Depending on boss or colleague, they introduce me various ways - data scientist, researcher, statistician. Those are the main titles I&amp;#39;m introduced as, but there are more. &lt;/p&gt;

&lt;p&gt;On paper I&amp;#39;m titled as a statistician according to HR. But I think the title of researcher is probably the best fit. The majority of my duties, design research studies (experimental and survey), collect data (from a research study and/or public data), clean, structure, analyze and visualize data, create papers, decks, and reports, present findings. &lt;/p&gt;

&lt;p&gt;What separates the jobs and fields in and related to data science? Statistician, researcher, data analyst, data scientist, et cetera...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mvhyxz,True,,Weird_Surname,,7,True,all_ads,False,[],False,,/r/datascience/comments/mvhyxz/what_separates_the_jobs_and_fields_in_and_related/,all_ads,False,https://www.reddit.com/r/datascience/comments/mvhyxz/what_separates_the_jobs_and_fields_in_and_related/,515407,1619018454.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I swear, every time I decide to work with a 3rd party, I'm reminded of why I never want to work with 3rd parties - even if they say they specifically place data science folks. Anyone here had luck working with a 3rd party recruitment firm and, if so, which one(s)?",t2_15h4ul,False,,0,False,Are there any 3rd party tech/data sci recruiting firms worth their salt?,[],r/datascience,False,6,,0,,,False,t3_mv35ze,False,dark,0.82,,public,15,0,{},,,False,[],,False,False,,{},Job Search,False,15,,False,False,self,False,,[],{},,True,,1618989646.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I swear, every time I decide to work with a 3rd party, I&amp;#39;m reminded of why I never want to work with 3rd parties - even if they say they specifically place data science folks. Anyone here had luck working with a 3rd party recruitment firm and, if so, which one(s)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,mv35ze,True,,BlackJack5027,,10,True,all_ads,False,[],False,,/r/datascience/comments/mv35ze/are_there_any_3rd_party_techdata_sci_recruiting/,all_ads,False,https://www.reddit.com/r/datascience/comments/mv35ze/are_there_any_3rd_party_techdata_sci_recruiting/,515407,1618960846.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I’m a data scientists at a very large corporation and after hitting three years at the company, I’ve noticed I get at least 5 emails/LinkedIn messages from recruiters about different DS positions available. Normally I ignore them, but lately recruiters from the companies themselves have been reaching out (rather than contracts or recruiting companies) and I am tempted to respond. Has anyone else worked with recruiters for data science positions and if so, what was your experience? I’m happy at my job but I’m getting so curious about these. Thanks for any insight!",t2_3fqxm3d7,False,,0,False,Recruiters reaching out,[],r/datascience,False,6,career,0,,,False,t3_mutr8a,False,dark,0.84,,public,15,0,{},,,False,[],,False,False,,{},Career,False,15,,False,False,self,False,,[],{},,True,,1618963626.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a data scientists at a very large corporation and after hitting three years at the company, I’ve noticed I get at least 5 emails/LinkedIn messages from recruiters about different DS positions available. Normally I ignore them, but lately recruiters from the companies themselves have been reaching out (rather than contracts or recruiting companies) and I am tempted to respond. Has anyone else worked with recruiters for data science positions and if so, what was your experience? I’m happy at my job but I’m getting so curious about these. Thanks for any insight!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mutr8a,True,,jellotothetree,,17,True,all_ads,False,[],False,,/r/datascience/comments/mutr8a/recruiters_reaching_out/,all_ads,False,https://www.reddit.com/r/datascience/comments/mutr8a/recruiters_reaching_out/,515407,1618934826.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Or any companies/products aimed in the nonprofit direction?

I’m just curious if due to the cost of data science any nonprofits employ it in any way.",t2_1s89t2xm,False,,0,False,Are there any data science applications in the nonprofit space?,[],r/datascience,False,6,discussion,0,,,False,t3_mv2l0a,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1618987873.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Or any companies/products aimed in the nonprofit direction?&lt;/p&gt;

&lt;p&gt;I’m just curious if due to the cost of data science any nonprofits employ it in any way.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mv2l0a,True,,mfb1274,,8,True,all_ads,False,[],False,,/r/datascience/comments/mv2l0a/are_there_any_data_science_applications_in_the/,all_ads,False,https://www.reddit.com/r/datascience/comments/mv2l0a/are_there_any_data_science_applications_in_the/,515407,1618959073.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I somehow landed a job right out of undergrad as a data scientist. Over the last few years I’ve spent a ton of effort to get up to speed with the masters/PhD level DS that I work with and about a year ago I feel like I got to their level and started teaching them new things. The issue is, I’m really starting to hate working in the corporate environment. They’re so much pressure to perform perfectly and little room for experimentation. I love the data science but I end up spending most of my time finding data, fighting with IT to get access, finding SMEs to work with who have business knowledge of the data, and so on. I really like building models and working with data but the time I spend coding is so little it doesn’t feel worth it. It also doesn’t help that I’m not the career driven type, I care about how much money I make, but couldn’t care less about what title I have. So I guess my question is, does any one have any suggestions of alternatives to working in corporate as a data scientist? I’ve looked into consulting work but that takes some years of building a network. Considering I’ve only been in the industry a few years, I have some room to grow. Thanks everyone!",t2_3fqxm3d7,False,,0,False,Leaving corporate data science,[],r/datascience,False,6,career,0,,,False,t3_mu9lj7,False,dark,0.96,,public,326,2,{},,,False,[],,False,False,,{},Career,False,326,,False,False,self,1618894669.0,,[],{'gid_1': 1},,True,,1618892503.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I somehow landed a job right out of undergrad as a data scientist. Over the last few years I’ve spent a ton of effort to get up to speed with the masters/PhD level DS that I work with and about a year ago I feel like I got to their level and started teaching them new things. The issue is, I’m really starting to hate working in the corporate environment. They’re so much pressure to perform perfectly and little room for experimentation. I love the data science but I end up spending most of my time finding data, fighting with IT to get access, finding SMEs to work with who have business knowledge of the data, and so on. I really like building models and working with data but the time I spend coding is so little it doesn’t feel worth it. It also doesn’t help that I’m not the career driven type, I care about how much money I make, but couldn’t care less about what title I have. So I guess my question is, does any one have any suggestions of alternatives to working in corporate as a data scientist? I’ve looked into consulting work but that takes some years of building a network. Considering I’ve only been in the industry a few years, I have some room to grow. Thanks everyone!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mu9lj7,True,,jellotothetree,,103,True,all_ads,False,[],False,,/r/datascience/comments/mu9lj7/leaving_corporate_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/mu9lj7/leaving_corporate_data_science/,515407,1618863703.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I know many of you have jobs and don’t really have the time to learn new things, but do any of you get bored or tired of just doing “data” related coding, whether it be training models/data cleaning/data visualization and just try and learn how to build a website or an app? I’m a student and ive just gotten bored of doing sklearn/tidyverse/pandas/tidymodels all day or doing data stuff with Python or R and just feel like learning react or something random like that for a change of pace. I don’t actually intend on getting into web development, I want to be a DS but sometimes too much data stuff bores me. Anyone feel the same way? And if so what did you learn?",t2_5w4i5kd1,False,,0,False,Any of you learn Web Dev or App Dev for a change?,[],r/datascience,False,6,discussion,0,,,False,t3_mtpp09,False,dark,0.98,,public,165,0,{},,,False,[],,False,False,,{},Discussion,False,165,,False,False,self,False,,[],{},,True,,1618822731.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know many of you have jobs and don’t really have the time to learn new things, but do any of you get bored or tired of just doing “data” related coding, whether it be training models/data cleaning/data visualization and just try and learn how to build a website or an app? I’m a student and ive just gotten bored of doing sklearn/tidyverse/pandas/tidymodels all day or doing data stuff with Python or R and just feel like learning react or something random like that for a change of pace. I don’t actually intend on getting into web development, I want to be a DS but sometimes too much data stuff bores me. Anyone feel the same way? And if so what did you learn?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mtpp09,True,,veeeerain,,153,True,all_ads,False,[],False,,/r/datascience/comments/mtpp09/any_of_you_learn_web_dev_or_app_dev_for_a_change/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtpp09/any_of_you_learn_web_dev_or_app_dev_for_a_change/,515407,1618793931.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have been reading more about the theoretical backgrounds of neural networks (e.g. ""universal approximation theorem"") and have seen several authors demonstrate that even a simple (few layers, many neurons) neural network can (theoretically) approximate the variable of interest (i.e. the response variable) to a ""decent"" level of precision.  However, the implication being that to use simple neural networks in order to achieve good results, this would require a very large number of neurons. Therefore, deeper neural networks have been developed over the years, which attempt to provide good results with more layers but a fewer number of neurons.

This brings me to my situation: I have never been able to successfully fit a neural network to any real-world data that I have used. I have always gotten really bad results with neural networks (after trying all sorts of combinations of number of neurons, number of layers, learning rate, activation function, ""drop out"" regularization, etc.). This seems to be a hyperparameter-grid search problem.

(Ironically, models like CART decision trees have good results on the same data (supervised binary classification) and random forest has produced even better - this data is not ""small by any means"", contains around 30 columns and over 300,000 rows of data).

 Does anyone know if routines have been written (e.g. in tensorflow keras) that can assist in this problem of deciding the number of layers and the number of neurons? Is there a ""ground rule"" for deciding how many layers and how many neurons to begin with? Is there something around that can ""intelligently"" point you in the right direction for how many neurons/layers to choose?",t2_xtuyc,False,,0,False,Effective ways of choosing number of neurons/layers in a neural network?,[],r/datascience,False,6,discussion,0,,,False,t3_mual4y,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1618895245.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been reading more about the theoretical backgrounds of neural networks (e.g. &amp;quot;universal approximation theorem&amp;quot;) and have seen several authors demonstrate that even a simple (few layers, many neurons) neural network can (theoretically) approximate the variable of interest (i.e. the response variable) to a &amp;quot;decent&amp;quot; level of precision.  However, the implication being that to use simple neural networks in order to achieve good results, this would require a very large number of neurons. Therefore, deeper neural networks have been developed over the years, which attempt to provide good results with more layers but a fewer number of neurons.&lt;/p&gt;

&lt;p&gt;This brings me to my situation: I have never been able to successfully fit a neural network to any real-world data that I have used. I have always gotten really bad results with neural networks (after trying all sorts of combinations of number of neurons, number of layers, learning rate, activation function, &amp;quot;drop out&amp;quot; regularization, etc.). This seems to be a hyperparameter-grid search problem.&lt;/p&gt;

&lt;p&gt;(Ironically, models like CART decision trees have good results on the same data (supervised binary classification) and random forest has produced even better - this data is not &amp;quot;small by any means&amp;quot;, contains around 30 columns and over 300,000 rows of data).&lt;/p&gt;

&lt;p&gt;Does anyone know if routines have been written (e.g. in tensorflow keras) that can assist in this problem of deciding the number of layers and the number of neurons? Is there a &amp;quot;ground rule&amp;quot; for deciding how many layers and how many neurons to begin with? Is there something around that can &amp;quot;intelligently&amp;quot; point you in the right direction for how many neurons/layers to choose?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mual4y,True,,ottawalanguages,,20,True,all_ads,False,[],False,,/r/datascience/comments/mual4y/effective_ways_of_choosing_number_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/mual4y/effective_ways_of_choosing_number_of/,515407,1618866445.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_6nta6wnn,False,,0,False,What is the best tech stack/ data pipeline for apache superset?,[],r/datascience,False,6,discussion,0,,,False,t3_mu43ek,False,dark,0.76,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1618877625.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mu43ek,True,,sackafackaboomboom,,0,True,all_ads,False,[],False,,/r/datascience/comments/mu43ek/what_is_the_best_tech_stack_data_pipeline_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/mu43ek/what_is_the_best_tech_stack_data_pipeline_for/,515407,1618848825.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone,

My field is not Web Development but data science. With that in mind I have the following questions.

I have developed a machine learning model in Python's SKlearn. I would like to build a website where users provide inputs for predictions (e.g. their age) and output some prediction (e.g. expected income) .

**My objective is not just to make a ""dashboard"". I could easily use Python's Streamlit or R's Shiny to deploy my model and make it accessible by anyone.** I want to use this as an opportunity to learn about Web Development and build a website from the ground up to solve this problem and have full control over it. For instance , I would like to be able to have ads on the website and control how many predictions are requested by each user per minute.

So what would be **a step-by step clear approach/'syllabus'** to do this ? What specific libraries and tools would I need to use in each step ? Where should the model be ""stored"" ? Where should I host my website and make it call the model for predictions ? How can I make it safe so that my model building code is private ? What are the most cost-effective solutions ?

Further details : I know some CSS, HTML and JS. Never created any API. Not much experience in deploying website or web app.

Thanks in advance.",t2_1fyghm,False,,0,False,Deploy Machine Learning model on website,[],r/datascience,False,6,projects,0,,,False,t3_mua4ky,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1618893979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;My field is not Web Development but data science. With that in mind I have the following questions.&lt;/p&gt;

&lt;p&gt;I have developed a machine learning model in Python&amp;#39;s SKlearn. I would like to build a website where users provide inputs for predictions (e.g. their age) and output some prediction (e.g. expected income) .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My objective is not just to make a &amp;quot;dashboard&amp;quot;. I could easily use Python&amp;#39;s Streamlit or R&amp;#39;s Shiny to deploy my model and make it accessible by anyone.&lt;/strong&gt; I want to use this as an opportunity to learn about Web Development and build a website from the ground up to solve this problem and have full control over it. For instance , I would like to be able to have ads on the website and control how many predictions are requested by each user per minute.&lt;/p&gt;

&lt;p&gt;So what would be &lt;strong&gt;a step-by step clear approach/&amp;#39;syllabus&amp;#39;&lt;/strong&gt; to do this ? What specific libraries and tools would I need to use in each step ? Where should the model be &amp;quot;stored&amp;quot; ? Where should I host my website and make it call the model for predictions ? How can I make it safe so that my model building code is private ? What are the most cost-effective solutions ?&lt;/p&gt;

&lt;p&gt;Further details : I know some CSS, HTML and JS. Never created any API. Not much experience in deploying website or web app.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mua4ky,True,,nothingveryserious,,8,True,all_ads,False,[],False,,/r/datascience/comments/mua4ky/deploy_machine_learning_model_on_website/,all_ads,False,https://www.reddit.com/r/datascience/comments/mua4ky/deploy_machine_learning_model_on_website/,515407,1618865179.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m plotting data in R that consists of which countries have A, which countries have B, and which countries have A and B.

I feel like a Venn diagram is the most obvious choice but was wondering if anyone might recommend any alternatives?",t2_2kd49gzz,False,,0,False,How might you display overlapping binary data other than a Venn diagram?,[],r/datascience,False,6,discussion,0,,,False,t3_mtanu3,False,dark,0.91,,public,89,0,{},,,False,[],,False,False,,{},Discussion,False,89,,False,False,self,False,,[],{},,True,,1618772158.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m plotting data in R that consists of which countries have A, which countries have B, and which countries have A and B.&lt;/p&gt;

&lt;p&gt;I feel like a Venn diagram is the most obvious choice but was wondering if anyone might recommend any alternatives?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mtanu3,True,,-Khlerik-,,29,True,all_ads,False,[],False,,/r/datascience/comments/mtanu3/how_might_you_display_overlapping_binary_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtanu3/how_might_you_display_overlapping_binary_data/,515407,1618743358.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm curious to hear how many Data Science/AI/ML or other adjacent analysts find themselves working in tasks outside of what would generally be considered analytics. I'm at my second job now where the majority of my work comes from ticketing systems. How common is this? 

I can't tell if this is an issue that I keep walking into, maybe it's how I'm marketing myself, or maybe it's normal. In my previous role, I was hired and worked on big data projects for around 5 years in an IT department before slowly being shifted over into fielding support calls for confused users. My current role (Senior in DS team) is almost 100% ticket-based work (ex: ""Check that junior resource copied cells correctly between sheets""). It feels like I'm being intentionally deskilled wherever I go and I'm not sure if it's normal. For reference, I'm an American with 7 years of experience and a BSc in CS and MS in AI.",t2_4loyjzmr,False,,0,False,Data Scientist == Ancillary Analyst,[],r/datascience,False,6,career,0,,,False,t3_mtj0ch,False,dark,0.83,,public,18,0,{},,,False,[],,False,False,,{},Career,False,18,,False,False,self,False,,[],{},,True,,1618801135.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m curious to hear how many Data Science/AI/ML or other adjacent analysts find themselves working in tasks outside of what would generally be considered analytics. I&amp;#39;m at my second job now where the majority of my work comes from ticketing systems. How common is this? &lt;/p&gt;

&lt;p&gt;I can&amp;#39;t tell if this is an issue that I keep walking into, maybe it&amp;#39;s how I&amp;#39;m marketing myself, or maybe it&amp;#39;s normal. In my previous role, I was hired and worked on big data projects for around 5 years in an IT department before slowly being shifted over into fielding support calls for confused users. My current role (Senior in DS team) is almost 100% ticket-based work (ex: &amp;quot;Check that junior resource copied cells correctly between sheets&amp;quot;). It feels like I&amp;#39;m being intentionally deskilled wherever I go and I&amp;#39;m not sure if it&amp;#39;s normal. For reference, I&amp;#39;m an American with 7 years of experience and a BSc in CS and MS in AI.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mtj0ch,True,,iqzium,,15,True,all_ads,False,[],False,,/r/datascience/comments/mtj0ch/data_scientist_ancillary_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtj0ch/data_scientist_ancillary_analyst/,515406,1618772335.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"My approach would be to explain how it's a modeling technique that identifies the line of best fit between the inputs and the desired output that's being predicted. I'd probably also mention that the framework can explain relationships between features and the individual contribution each feature has towards final output.

Anything else you'd add or mention? These people are truly non-technical, so anything to simplify this would help. Thanks in advance.",t2_7a1de321,False,,0,False,How would you explain linear regression to a non-technical business partner?,[],r/datascience,False,6,career,0,,,False,t3_mts5h2,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1618831796.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My approach would be to explain how it&amp;#39;s a modeling technique that identifies the line of best fit between the inputs and the desired output that&amp;#39;s being predicted. I&amp;#39;d probably also mention that the framework can explain relationships between features and the individual contribution each feature has towards final output.&lt;/p&gt;

&lt;p&gt;Anything else you&amp;#39;d add or mention? These people are truly non-technical, so anything to simplify this would help. Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mts5h2,True,,Environmental-Bar572,,12,True,all_ads,False,[],False,,/r/datascience/comments/mts5h2/how_would_you_explain_linear_regression_to_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/mts5h2/how_would_you_explain_linear_regression_to_a/,515406,1618802996.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello, I am trying to automate some manual tasks prone to human error. I have some ideas, but they are all so janky, and I hope some one smarter than me can offer an elegant solution. 

The workflow is like this:

I have a series of binary files in a proprietary format on a redhat server. On this server is a c++ binary application that can decode this file into .csv format. I log into the linux machines, run this application on the binary files,  then copy these files over locally to a windows laptop and run a python-based data analysis. Putting python+analysis software on these linux servers is not an option. I have mounted the linux server's home directory as a mapped drive on my windows machine for easy copying of files. but I still have to log in to the linux host to run the binary decoder application. I want to automate the whole process where I can run a python script on my windows laptop and point it to a file (or folder) on the mapped linux drive, have it execute the decoder script remotely, then read in the csv files and perform the data analysis. 

The problem is that the decoder script is compiled in the native linux env, so it needs to be ran via its native environment. Is there an elegant way to execute a binary remotely on linux from windows?

&amp;#x200B;

P.S. My current proposed solution is to have python launch a Cygwin application, and then have Cygwin try to 'ssh user@host &lt;command&gt;' to remotely execute the decoder binary.",t2_131z0v,False,,0,False,How can I make this workflow better?,[],r/datascience,False,6,discussion,0,,,False,t3_mtj6dq,False,dark,1.0,,public,3,0,{},,,True,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1618801623.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am trying to automate some manual tasks prone to human error. I have some ideas, but they are all so janky, and I hope some one smarter than me can offer an elegant solution. &lt;/p&gt;

&lt;p&gt;The workflow is like this:&lt;/p&gt;

&lt;p&gt;I have a series of binary files in a proprietary format on a redhat server. On this server is a c++ binary application that can decode this file into .csv format. I log into the linux machines, run this application on the binary files,  then copy these files over locally to a windows laptop and run a python-based data analysis. Putting python+analysis software on these linux servers is not an option. I have mounted the linux server&amp;#39;s home directory as a mapped drive on my windows machine for easy copying of files. but I still have to log in to the linux host to run the binary decoder application. I want to automate the whole process where I can run a python script on my windows laptop and point it to a file (or folder) on the mapped linux drive, have it execute the decoder script remotely, then read in the csv files and perform the data analysis. &lt;/p&gt;

&lt;p&gt;The problem is that the decoder script is compiled in the native linux env, so it needs to be ran via its native environment. Is there an elegant way to execute a binary remotely on linux from windows?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;P.S. My current proposed solution is to have python launch a Cygwin application, and then have Cygwin try to &amp;#39;ssh user@host &amp;lt;command&amp;gt;&amp;#39; to remotely execute the decoder binary.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mtj6dq,True,,rednirgskizzif,,4,True,all_ads,False,[],False,,/r/datascience/comments/mtj6dq/how_can_i_make_this_workflow_better/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtj6dq/how_can_i_make_this_workflow_better/,515406,1618772823.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 18 Apr 2021 - 25 Apr 2021,[],r/datascience,False,6,,0,,,False,t3_mtbi4s,False,dark,0.78,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1618776030.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,mtbi4s,True,,datascience-bot,,138,False,all_ads,False,[],False,dark,/r/datascience/comments/mtbi4s/weekly_entering_transitioning_thread_18_apr_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtbi4s/weekly_entering_transitioning_thread_18_apr_2021/,515406,1618747230.0,0,,False,,,,,,,,
,datascience,"In my current DS job I am free to experiment with models and my boss lets me do some R&amp;D. However, the production side is deficient: they use old tools, and very outdated pipelines and technologies. Most of the time we do ad hoc analyses for clients that require no production at all. I feel the need to grow my skills on production, and that's something I can't do where I am now.

That's why I'm looking for another job, even though I like my current company, boss and colleagues.

I was contacted for a new opportunity by a cool non-profit organization: they want to start a DS project that sounds very interesting to me (and useful for society IMHO), but there's one caveat: I'd be the first DS ever there. I would basically need to build everything from scratch, all by myself.

One one side, I could build a good career, I'd be plenty of room to do things my way. On the other side I'd be all alone, without anyone to learn with and from.

In career terms, would that be a suicidal move? Is there a risk to ""cut myself out"" the DS job market?

Have you every been the first DS in a company? Did you ever move from ""multi DS"" to ""single DS"" companies? And what have your learned from that?

I hope this might help also other readers too. Any advice is very welcome.",t2_3fcw5pgq,False,,0,False,Moving to a company where you are the only DS: growth opportunity or suicidal move?,[],r/datascience,False,6,discussion,0,,,False,t3_mspnso,False,dark,0.98,,public,334,2,{},,,False,[],,False,False,,{},Discussion,False,334,,False,False,self,False,,[],{'gid_1': 1},,True,,1618690650.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my current DS job I am free to experiment with models and my boss lets me do some R&amp;amp;D. However, the production side is deficient: they use old tools, and very outdated pipelines and technologies. Most of the time we do ad hoc analyses for clients that require no production at all. I feel the need to grow my skills on production, and that&amp;#39;s something I can&amp;#39;t do where I am now.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s why I&amp;#39;m looking for another job, even though I like my current company, boss and colleagues.&lt;/p&gt;

&lt;p&gt;I was contacted for a new opportunity by a cool non-profit organization: they want to start a DS project that sounds very interesting to me (and useful for society IMHO), but there&amp;#39;s one caveat: I&amp;#39;d be the first DS ever there. I would basically need to build everything from scratch, all by myself.&lt;/p&gt;

&lt;p&gt;One one side, I could build a good career, I&amp;#39;d be plenty of room to do things my way. On the other side I&amp;#39;d be all alone, without anyone to learn with and from.&lt;/p&gt;

&lt;p&gt;In career terms, would that be a suicidal move? Is there a risk to &amp;quot;cut myself out&amp;quot; the DS job market?&lt;/p&gt;

&lt;p&gt;Have you every been the first DS in a company? Did you ever move from &amp;quot;multi DS&amp;quot; to &amp;quot;single DS&amp;quot; companies? And what have your learned from that?&lt;/p&gt;

&lt;p&gt;I hope this might help also other readers too. Any advice is very welcome.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mspnso,True,,Le2vo,,61,True,all_ads,False,[],False,,/r/datascience/comments/mspnso/moving_to_a_company_where_you_are_the_only_ds/,all_ads,False,https://www.reddit.com/r/datascience/comments/mspnso/moving_to_a_company_where_you_are_the_only_ds/,515406,1618661850.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’m sorry if this post is breaking any rules on this subreddit. 

I just wanted to know how each one of you Data Scientists would define this field.",t2_756bflr6,False,,0,False,What is your definition of Data Science?,[],r/datascience,False,6,discussion,0,,,False,t3_mtbrwb,False,dark,0.44,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1618777151.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m sorry if this post is breaking any rules on this subreddit. &lt;/p&gt;

&lt;p&gt;I just wanted to know how each one of you Data Scientists would define this field.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mtbrwb,True,,sarlfage,,14,True,all_ads,False,[],False,,/r/datascience/comments/mtbrwb/what_is_your_definition_of_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/mtbrwb/what_is_your_definition_of_data_science/,515406,1618748351.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_5tmwc1m9,False,,0,False,"I'm burnt out with learning, can't find work. How do you guys keep pushing forward?",[],r/datascience,False,6,career,0,,,False,t3_msc85z,False,dark,0.95,,public,374,3,{},,,False,[],,False,False,,{},Career,False,374,,False,False,self,False,,[],{},,True,,1618635096.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,msc85z,True,,teeqtee33,,115,True,all_ads,False,[],False,,/r/datascience/comments/msc85z/im_burnt_out_with_learning_cant_find_work_how_do/,all_ads,False,https://www.reddit.com/r/datascience/comments/msc85z/im_burnt_out_with_learning_cant_find_work_how_do/,515406,1618606296.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,I am doing my post-grad in data science and do a lot of projects that I think I could structure better from start to finish. I look at top submissions on kaggle for reference. What is the project you use for reference when doing your projects? What is your general structure?,t2_4i2w5,False,,0,False,What is the best structured ds project you have seen?,[],r/datascience,False,6,discussion,0,,,False,t3_mrwzkq,False,dark,0.96,,public,240,0,{},,,False,[],,False,False,,{},Discussion,False,240,,False,False,self,False,,[],{},,True,,1618581349.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing my post-grad in data science and do a lot of projects that I think I could structure better from start to finish. I look at top submissions on kaggle for reference. What is the project you use for reference when doing your projects? What is your general structure?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,MS | Student,[],False,,,,t5_2sptq,,,,mrwzkq,True,,browneyesays,,32,True,all_ads,False,[],False,dark,/r/datascience/comments/mrwzkq/what_is_the_best_structured_ds_project_you_have/,all_ads,False,https://www.reddit.com/r/datascience/comments/mrwzkq/what_is_the_best_structured_ds_project_you_have/,515406,1618552549.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know this has been asked before, but I figured I'd bring it up again to compile some more responses. I've been asked tons of terrible interview questions that are more reflective of if I'm capable of memorizing things about DS than if I'm actually capable at all. So I'm curious: What questions do you think are really good at illuminating whether or not someone is / would be a good data scientist? 

&amp;#x200B;

And, along the same lines: What traits, skills, etc. are you trying to select for when hiring new data scientists?",t2_15v03egd,False,,0,False,What interview questions do you think are really good indicators of DS skill / ability? And what makes a good DS?,[],r/datascience,False,6,discussion,0,,,False,t3_ms365m,False,dark,0.86,,public,25,0,{},,,False,[],,False,False,,{},Discussion,False,25,,False,False,self,False,,[],{},,True,,1618608581.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know this has been asked before, but I figured I&amp;#39;d bring it up again to compile some more responses. I&amp;#39;ve been asked tons of terrible interview questions that are more reflective of if I&amp;#39;m capable of memorizing things about DS than if I&amp;#39;m actually capable at all. So I&amp;#39;m curious: What questions do you think are really good at illuminating whether or not someone is / would be a good data scientist? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And, along the same lines: What traits, skills, etc. are you trying to select for when hiring new data scientists?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ms365m,True,,eipi-10,,42,True,all_ads,False,[],False,,/r/datascience/comments/ms365m/what_interview_questions_do_you_think_are_really/,all_ads,False,https://www.reddit.com/r/datascience/comments/ms365m/what_interview_questions_do_you_think_are_really/,515406,1618579781.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I currently work as a data scientist, currently interviewing for a data analyst role at a company I am interested in.  Despite the job title, the job description sounds like more of a data science job than a data analyst job.  Would it be impolite to request a title change for the job as part of the hiring process?



I have heard that job titles in data science don't mean much, and people often do go between data science, data engineering, and data analyst jobs as part of their career.  How true is that?",t2_2u9mels3,False,,0,False,Is it impolite to ask a hiring manager to change the job title?,[],r/datascience,False,6,discussion,0,,,False,t3_mrg4ag,False,dark,0.94,,public,194,0,{},,,False,[],,False,False,,{},Discussion,False,194,,False,False,self,False,,[],{},,True,,1618526600.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I currently work as a data scientist, currently interviewing for a data analyst role at a company I am interested in.  Despite the job title, the job description sounds like more of a data science job than a data analyst job.  Would it be impolite to request a title change for the job as part of the hiring process?&lt;/p&gt;

&lt;p&gt;I have heard that job titles in data science don&amp;#39;t mean much, and people often do go between data science, data engineering, and data analyst jobs as part of their career.  How true is that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mrg4ag,True,,ComputationalStats,,107,True,all_ads,False,[],False,,/r/datascience/comments/mrg4ag/is_it_impolite_to_ask_a_hiring_manager_to_change/,all_ads,False,https://www.reddit.com/r/datascience/comments/mrg4ag/is_it_impolite_to_ask_a_hiring_manager_to_change/,515406,1618497800.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm a data scientist at a medium sized financial company. Lately we've gotten quite concerned about customer privacy in data science and analytics, and while I have a high level of personal ethics on the topic, I'm not familiar with what is typical at other companies in how the privacy of customers is handled while doing data science tasks. 

For internal data science projects, how do you reconcile the need to restrict access to some personally identifiable information with the need for as much data as possible in a model? Do you have names/addresses hidden to most by default with an exception process for when this information is needed?

How is consent handled normally? Does a data scientist assume that there are processes in place to ensure that whatever data is available in the warehouse/pipeline/whatever has adequate consent for the purpose you're using it for? Do you need to do a ""check"" on consent before any development can begin?

Thanks for any discussion on the topic.",t2_8pqb1,False,,0,False,"For data scientists and BI Analysts at companies large and small, how do you deal with customer privacy when building models and/or dashboards?",[],r/datascience,False,6,discussion,0,,,False,t3_mrqoix,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1618557658.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a data scientist at a medium sized financial company. Lately we&amp;#39;ve gotten quite concerned about customer privacy in data science and analytics, and while I have a high level of personal ethics on the topic, I&amp;#39;m not familiar with what is typical at other companies in how the privacy of customers is handled while doing data science tasks. &lt;/p&gt;

&lt;p&gt;For internal data science projects, how do you reconcile the need to restrict access to some personally identifiable information with the need for as much data as possible in a model? Do you have names/addresses hidden to most by default with an exception process for when this information is needed?&lt;/p&gt;

&lt;p&gt;How is consent handled normally? Does a data scientist assume that there are processes in place to ensure that whatever data is available in the warehouse/pipeline/whatever has adequate consent for the purpose you&amp;#39;re using it for? Do you need to do a &amp;quot;check&amp;quot; on consent before any development can begin?&lt;/p&gt;

&lt;p&gt;Thanks for any discussion on the topic.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mrqoix,True,,atron306,,11,True,all_ads,False,[],False,,/r/datascience/comments/mrqoix/for_data_scientists_and_bi_analysts_at_companies/,all_ads,False,https://www.reddit.com/r/datascience/comments/mrqoix/for_data_scientists_and_bi_analysts_at_companies/,515406,1618528858.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"We send an email to our customers to advertise 3/4 products every day.

Supposing that today's email is advertising three products (product A, product B, product C), the customer is given four options: 

1) purchase A
2) purchase B
3) purchase C
4) make no purchase. 

Each choice is mutually exclusive i.e. each customer can make at most one purchase per day. (EDIT: This is because of the nature of the business, which is subscription-based).

Every day we advertise different products and some products are advertised with more frequency than others.

We want to test if certain products cannibalize others, for example if customers that like A also like B, it would be wise not to advertise both products on the same day.

I am exploring affinity analysis to measure support, confidence and lift for each product pair - the issue is however that some products are more likely to be purchased simply because they are advertised more frequently than others.

Any suggestions on how to go about it?",t2_2oixipth,False,,0,False,Product cannibalization - Would affinity analysis help?,[],r/datascience,False,6,discussion,0,,,False,t3_mrdttx,False,dark,1.0,,public,12,0,{},,,False,[],,False,False,,{},Discussion,False,12,,False,False,self,1618562812.0,,[],{},,True,,1618519273.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We send an email to our customers to advertise 3/4 products every day.&lt;/p&gt;

&lt;p&gt;Supposing that today&amp;#39;s email is advertising three products (product A, product B, product C), the customer is given four options: &lt;/p&gt;

&lt;p&gt;1) purchase A
2) purchase B
3) purchase C
4) make no purchase. &lt;/p&gt;

&lt;p&gt;Each choice is mutually exclusive i.e. each customer can make at most one purchase per day. (EDIT: This is because of the nature of the business, which is subscription-based).&lt;/p&gt;

&lt;p&gt;Every day we advertise different products and some products are advertised with more frequency than others.&lt;/p&gt;

&lt;p&gt;We want to test if certain products cannibalize others, for example if customers that like A also like B, it would be wise not to advertise both products on the same day.&lt;/p&gt;

&lt;p&gt;I am exploring affinity analysis to measure support, confidence and lift for each product pair - the issue is however that some products are more likely to be purchased simply because they are advertised more frequently than others.&lt;/p&gt;

&lt;p&gt;Any suggestions on how to go about it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mrdttx,True,,GoodDot9,,15,True,all_ads,False,[],False,,/r/datascience/comments/mrdttx/product_cannibalization_would_affinity_analysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/mrdttx/product_cannibalization_would_affinity_analysis/,515406,1618490473.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am a civil engineering graduate considering starting a master's degree in geomatics engineering at a Canadian university. I know one of the applications of geomatics has to do with geospatial data science, things like spatial databases, data mining, and spatial statistics. For those who work in this field, how good is it? What are some typical tasks you perform? Does it have good future prospects? Does it require as much math and stats as ""regular"" data science?",t2_3q66js10,False,,0,False,Is Geospatial Data Science a good field to get into?,[],r/datascience,False,6,career,0,,,False,t3_mr1989,False,dark,0.95,,public,168,0,{},,,False,[],,False,False,,{},Career,False,168,,False,False,self,False,,[],{},,True,,1618466898.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a civil engineering graduate considering starting a master&amp;#39;s degree in geomatics engineering at a Canadian university. I know one of the applications of geomatics has to do with geospatial data science, things like spatial databases, data mining, and spatial statistics. For those who work in this field, how good is it? What are some typical tasks you perform? Does it have good future prospects? Does it require as much math and stats as &amp;quot;regular&amp;quot; data science?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,mr1989,True,,Pedro9870,,45,True,all_ads,False,[],False,,/r/datascience/comments/mr1989/is_geospatial_data_science_a_good_field_to_get/,all_ads,False,https://www.reddit.com/r/datascience/comments/mr1989/is_geospatial_data_science_a_good_field_to_get/,515406,1618438098.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 13 Jun 2021 - 20 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_nyussm,False,dark,0.87,,public,16,0,{},,,False,[],,False,False,,{},Discussion,False,16,,False,False,self,False,,[],{},,True,,1623614432.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,nyussm,True,,datascience-bot,,108,False,all_ads,False,[],False,dark,/r/datascience/comments/nyussm/weekly_entering_transitioning_thread_13_jun_2021/,all_ads,True,https://www.reddit.com/r/datascience/comments/nyussm/weekly_entering_transitioning_thread_13_jun_2021/,515407,1623585632.0,0,,False,,,,,,,,
,datascience,"Hi Everyone, I am an industry data scientist. One of the problems that I find is that while working at a large company,  there is some adoption lag with some new tools + libraries. Could anyone help point me in the right direction for software tools + libraries that are picking up steam this year? I remember hearing stuff about the Julia Programming language a couple of years ago but not sure if that has risen in popularity",t2_89mmv5s6,False,,0,False,What are some exciting new tools/libraries in 2021?,[],r/datascience,False,6,tooling,0,,,False,t3_o3l9o0,False,dark,0.97,,public,202,0,{},,,False,[],,False,False,,{},Tooling,False,202,,False,False,self,False,,[],{},,True,,1624152781.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Everyone, I am an industry data scientist. One of the problems that I find is that while working at a large company,  there is some adoption lag with some new tools + libraries. Could anyone help point me in the right direction for software tools + libraries that are picking up steam this year? I remember hearing stuff about the Julia Programming language a couple of years ago but not sure if that has risen in popularity&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3l9o0,True,,nyc_brand,,49,True,all_ads,False,[],False,,/r/datascience/comments/o3l9o0/what_are_some_exciting_new_toolslibraries_in_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3l9o0/what_are_some_exciting_new_toolslibraries_in_2021/,515407,1624123981.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_2mezzo8f,False,,0,False,"Python or R, which programming language is better for data science and which has better scope in the future?",[],r/datascience,False,6,discussion,0,,,False,t3_o3u1dn,False,dark,0.69,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,False,,[],{},,True,,1624179303.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3u1dn,True,,iam_syndrome,,22,True,all_ads,False,[],False,,/r/datascience/comments/o3u1dn/python_or_r_which_programming_language_is_better/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3u1dn/python_or_r_which_programming_language_is_better/,515407,1624150503.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am a beginner and have learned Python, Pandas, Excel and the Basics of Hypothesis Testing and wanted to know what should I learn and what certifications I should prepare for If I want to get a Career in Business Analytics or Data Analytics so please can anyone tell me what should I do and also what type of projects I should do?",t2_cqf2yo7g,False,,0,False,"What tools, language and technologies to learn as a beginner in the field of data analytics?",[],r/datascience,False,6,education,0,,,False,t3_o3zdd3,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624199789.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a beginner and have learned Python, Pandas, Excel and the Basics of Hypothesis Testing and wanted to know what should I learn and what certifications I should prepare for If I want to get a Career in Business Analytics or Data Analytics so please can anyone tell me what should I do and also what type of projects I should do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3zdd3,True,,Emperor-SuN-5,,6,True,all_ads,False,[],False,,/r/datascience/comments/o3zdd3/what_tools_language_and_technologies_to_learn_as/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3zdd3/what_tools_language_and_technologies_to_learn_as/,515407,1624170989.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hey guys. Hope you are doing good today.

I am a python developer and I have very little experience in ML and Big data. I am also a professional stock trader.

So I want some technical advice or an idea  from the redditors here.

I wanna a build a full stack data science project something related to the stock market.

When I say a full stack I mean right from the start from getting data or extracting data and going through all the big data pipelines and building a predictive model and deploying it.

I mean I have some ideas but I am not able to technically produce it in a way like you guys could do. I am asking you because with experience you know something compared to me

I want answers like 

""Scrap financial data from a website, live stream process it using spark and setup in airflow and then take the data do a reinforcement learning algorithm and deploy it showing something in the frontend""

Forgive me if I am demanding too much,but please do your best. It would be helpful for others too",t2_6hyleuj8,False,,0,False,Full Data engineering/ Data science Pipeline on stock market idea,[],r/datascience,False,6,projects,0,,,False,t3_o3z9j9,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1624199253.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Hope you are doing good today.&lt;/p&gt;

&lt;p&gt;I am a python developer and I have very little experience in ML and Big data. I am also a professional stock trader.&lt;/p&gt;

&lt;p&gt;So I want some technical advice or an idea  from the redditors here.&lt;/p&gt;

&lt;p&gt;I wanna a build a full stack data science project something related to the stock market.&lt;/p&gt;

&lt;p&gt;When I say a full stack I mean right from the start from getting data or extracting data and going through all the big data pipelines and building a predictive model and deploying it.&lt;/p&gt;

&lt;p&gt;I mean I have some ideas but I am not able to technically produce it in a way like you guys could do. I am asking you because with experience you know something compared to me&lt;/p&gt;

&lt;p&gt;I want answers like &lt;/p&gt;

&lt;p&gt;&amp;quot;Scrap financial data from a website, live stream process it using spark and setup in airflow and then take the data do a reinforcement learning algorithm and deploy it showing something in the frontend&amp;quot;&lt;/p&gt;

&lt;p&gt;Forgive me if I am demanding too much,but please do your best. It would be helpful for others too&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3z9j9,True,,Shyprogrammer1,,1,True,all_ads,False,[],False,,/r/datascience/comments/o3z9j9/full_data_engineering_data_science_pipeline_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3z9j9/full_data_engineering_data_science_pipeline_on/,515407,1624170453.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I read a lot of posts about how to land a data science position and what the biggest differences are between doing research in academia and working as a Data Scientist. Since I did the same journey myself, a couple of years ago, I thought it might be helpful if I summarised my experience and my thoughts about how to successfully do the transfer. My first post on the subject is here: [https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e](https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e)

&amp;#x200B;

Let me know what you think! Thanks!",t2_1tvky02i,False,,0,False,Some tips on how to go from academia to a Data Science position,[],r/datascience,False,6,career,0,,,False,t3_o3oppz,False,dark,0.78,,public,10,0,{},,,False,[],,False,False,,{},Career,False,10,,False,False,self,False,,[],{},,True,,1624162144.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read a lot of posts about how to land a data science position and what the biggest differences are between doing research in academia and working as a Data Scientist. Since I did the same journey myself, a couple of years ago, I thought it might be helpful if I summarised my experience and my thoughts about how to successfully do the transfer. My first post on the subject is here: &lt;a href=""https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e""&gt;https://ml-joe.medium.com/from-ph-d-to-research-data-scientist-the-journey-from-academia-to-industry-part-1-2d29200dc5c1?sk=1dc4115b6bc3213377e3e75667d6f25e&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Let me know what you think! Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3oppz,True,,_StunningStar_,,7,True,all_ads,False,[],False,,/r/datascience/comments/o3oppz/some_tips_on_how_to_go_from_academia_to_a_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3oppz/some_tips_on_how_to_go_from_academia_to_a_data/,515407,1624133344.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?auto=webp&amp;s=3ebe93d24dd00b154937c50e6cf0a5fceddb0b19', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff942eacf52a64eac70a062af72c51bc80bb732a', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7185268e63073e097e152401b14eee82bded5b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b02300caae4026c79be942f7ece883a973823af', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a436a18d2eb5a55a1e2908a28c94b5eabf2dd95', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a42e8c3ea7a81551be5f4148e509b5ad7e27633f', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/cFsEEH2PBbEzbot03MHt6JWtK71QHASlf6TPF_SF9o0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=63434c9348ecc0216b810f017f77c2bbf973a450', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'nztKV3duavl5jt7iucQG6Q7vBNiwBwK-q1RBQde4U3E'}], 'enabled': False}",,,,,
,datascience,"Github link (includes a link to a Kaggle notebook to run it directly) -  [shreyansh26/ML-Optimizers-JAX](https://github.com/shreyansh26/ML-Optimizers-JAX)

Implementations of some popular optimizers from scratch for a simple model like Linear Regression. The goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the weights and the bias without explicitly writing their derivatives as a separate function. 

This can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail.",t2_5xzd9om,False,,0,False,ML Optimizers from scratch using JAX,[],r/datascience,False,6,projects,0,,,True,t3_o40pui,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,1624177108.0,,[],{},,True,,1624205580.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Github link (includes a link to a Kaggle notebook to run it directly) -  &lt;a href=""https://github.com/shreyansh26/ML-Optimizers-JAX""&gt;shreyansh26/ML-Optimizers-JAX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Implementations of some popular optimizers from scratch for a simple model like Linear Regression. The goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the weights and the bias without explicitly writing their derivatives as a separate function. &lt;/p&gt;

&lt;p&gt;This can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o40pui,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/datascience/comments/o40pui/ml_optimizers_from_scratch_using_jax/,all_ads,False,https://www.reddit.com/r/datascience/comments/o40pui/ml_optimizers_from_scratch_using_jax/,515407,1624176780.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?auto=webp&amp;s=c2c00ee3482e5bc793d82475409a54c07a9fb967', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb2880799dffc0a9f5cbceca5c08c501f0c14c0c', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f2b430ab9d9ff7108e05ff1d2b6975a1082147', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbe0616a95721cfa473210b7a8eca2955cb23ef9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47795bc9e153b29fd6828f6e70fee6e2b64a4ebb', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f9285692be2a3c7ee8cb6e6098f1b87b5643a4', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/0DGZJxQggnKi-8PReQj-spkQnj0ffepVeI8UB9lM1Ng.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28021321b3765655bc5a62440e664221eb0d50d2', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Kz19YO1AEVVgU7yWFoST8FAepTIsUeAUmioW0RN4T94'}], 'enabled': False}",,,,,
,datascience,"Hi,

I have posted this on other subreddits last week but I didn't get a response so I thought I'd give this one a try. 

I work as a data analyst at a theme park (kind of like disney) but we operate only during 4-6 months in an year. I was asked to analyze the F&amp;B sales at the different locations in the park and compare this year's sales with last year's sales.

Couple of constraints and issues:

1. The number of outlets in an area keep changing every year. Last year we might have had 10 outlets in Area A but this year we only have 3. - To overcome this, I divided the sales by the number of outlets so I will be comparing the revenue per outlet for different years.
2. Different outlets operate for different days. For example, this year, outlet A might have operated only for 30 days but outlet B would have operated for 90 days. So outlet B's revenue would be higher. - To overcome this, I divided the revenue per outlet by the number of days the park operated this year. So let's say the park operated for a total of 100 days then I will divide the revenue per outlet/100 to get the revenue per outlet per day. Someone from the management pointed out that it would be better to divide the revenue per outlet by the number of days each outlet is operating to get the exact figures. That approach makes sense but it seems a bit more time consuming. Would it be worth redoing the analysis to change this?
3. Since we operate on a season basis, we operate for different days every season. For example, last time we operated from Oct'19 - Mar'20 but this time we operated from Oct'20-May'21. I thought that displaying the revenue per outlet per day would overcome this but someone from management mentioned that they would rather have me forecast the expected revenue from Apr'20 - May'20  so that we are comparing Oct'19-May'20 vs Oct'20-May'21. And then I can use this new forecasted revenue to get the revenue per outlet per day.
4. Because of covid, we had lesser footfall and hence lesser revenue this time. Someone suggested to normalize this year's sales by taking the footfall into account - i.e. take the drop in footfall from last season to this season (let's say the drop is 30%) then take 70% of last year's sales to accommodate the drop in footfall. Is this the right approach to it? Wouldn't comparing the revenue per outlet per day take care of the drop in footfall?
5. All of this needs to be in a dashboard and they want a functionality where they can make a like to like comparison - We will be reopening in Oct'21 and at that moment they want to compare their performance with previous season (i.e Oct '20). There are 2 problems with this:
   1. In Oct'20, we started on a monday (weekday) whereas in Oct'21 we might start on a saturday(weekend). Our weekend performance is higher than weekday so comparing monday vs saturday wouldn't be correct.
   2. As mentioned earlier we operate for different # of days across different season. For eg, this time we operated for 191 days whereas last time we operated for 161 days. Even if I compare  the 1st sunday of last time with 1st sunday of this time and so on, I will not have anything to compare with after we finish 161 days this season. How do I accurately make this comparison?

Sorry for the long post but I have not come across such constraints before so I'm not sure how to deal with all of it. I would really appreciate any help! Thanks in advance!",t2_5wjm49wo,False,,0,False,What approach do I use to analyze this F&amp;B Sales data?,[],r/datascience,False,6,discussion,0,,,True,t3_o40jxj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624205010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have posted this on other subreddits last week but I didn&amp;#39;t get a response so I thought I&amp;#39;d give this one a try. &lt;/p&gt;

&lt;p&gt;I work as a data analyst at a theme park (kind of like disney) but we operate only during 4-6 months in an year. I was asked to analyze the F&amp;amp;B sales at the different locations in the park and compare this year&amp;#39;s sales with last year&amp;#39;s sales.&lt;/p&gt;

&lt;p&gt;Couple of constraints and issues:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The number of outlets in an area keep changing every year. Last year we might have had 10 outlets in Area A but this year we only have 3. - To overcome this, I divided the sales by the number of outlets so I will be comparing the revenue per outlet for different years.&lt;/li&gt;
&lt;li&gt;Different outlets operate for different days. For example, this year, outlet A might have operated only for 30 days but outlet B would have operated for 90 days. So outlet B&amp;#39;s revenue would be higher. - To overcome this, I divided the revenue per outlet by the number of days the park operated this year. So let&amp;#39;s say the park operated for a total of 100 days then I will divide the revenue per outlet/100 to get the revenue per outlet per day. Someone from the management pointed out that it would be better to divide the revenue per outlet by the number of days each outlet is operating to get the exact figures. That approach makes sense but it seems a bit more time consuming. Would it be worth redoing the analysis to change this?&lt;/li&gt;
&lt;li&gt;Since we operate on a season basis, we operate for different days every season. For example, last time we operated from Oct&amp;#39;19 - Mar&amp;#39;20 but this time we operated from Oct&amp;#39;20-May&amp;#39;21. I thought that displaying the revenue per outlet per day would overcome this but someone from management mentioned that they would rather have me forecast the expected revenue from Apr&amp;#39;20 - May&amp;#39;20  so that we are comparing Oct&amp;#39;19-May&amp;#39;20 vs Oct&amp;#39;20-May&amp;#39;21. And then I can use this new forecasted revenue to get the revenue per outlet per day.&lt;/li&gt;
&lt;li&gt;Because of covid, we had lesser footfall and hence lesser revenue this time. Someone suggested to normalize this year&amp;#39;s sales by taking the footfall into account - i.e. take the drop in footfall from last season to this season (let&amp;#39;s say the drop is 30%) then take 70% of last year&amp;#39;s sales to accommodate the drop in footfall. Is this the right approach to it? Wouldn&amp;#39;t comparing the revenue per outlet per day take care of the drop in footfall?&lt;/li&gt;
&lt;li&gt;All of this needs to be in a dashboard and they want a functionality where they can make a like to like comparison - We will be reopening in Oct&amp;#39;21 and at that moment they want to compare their performance with previous season (i.e Oct &amp;#39;20). There are 2 problems with this:

&lt;ol&gt;
&lt;li&gt;In Oct&amp;#39;20, we started on a monday (weekday) whereas in Oct&amp;#39;21 we might start on a saturday(weekend). Our weekend performance is higher than weekday so comparing monday vs saturday wouldn&amp;#39;t be correct.&lt;/li&gt;
&lt;li&gt;As mentioned earlier we operate for different # of days across different season. For eg, this time we operated for 191 days whereas last time we operated for 161 days. Even if I compare  the 1st sunday of last time with 1st sunday of this time and so on, I will not have anything to compare with after we finish 161 days this season. How do I accurately make this comparison?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sorry for the long post but I have not come across such constraints before so I&amp;#39;m not sure how to deal with all of it. I would really appreciate any help! Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o40jxj,True,,Professional_Crazy49,,0,True,all_ads,False,[],False,,/r/datascience/comments/o40jxj/what_approach_do_i_use_to_analyze_this_fb_sales/,all_ads,False,https://www.reddit.com/r/datascience/comments/o40jxj/what_approach_do_i_use_to_analyze_this_fb_sales/,515407,1624176210.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have a dataset containing the monthly number of shipments for the years 2014 to 2019. I used the Facebook Prophet algorithm to fit a basic model. However, I am not sure how to implement cross-validation on this data. I could not find any blogs/GitHub files that perform cross-validation on monthly data. Is there a workaround for this?

Thanks in advance",t2_55fnfe2p,False,,0,False,Crossvalidation using Facebook Prophet,[],r/datascience,False,6,projects,0,,,False,t3_o3sjcm,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,self,False,,[],{},,True,,1624174117.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have a dataset containing the monthly number of shipments for the years 2014 to 2019. I used the Facebook Prophet algorithm to fit a basic model. However, I am not sure how to implement cross-validation on this data. I could not find any blogs/GitHub files that perform cross-validation on monthly data. Is there a workaround for this?&lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3sjcm,True,,xnxrxdh,,4,True,all_ads,False,[],False,,/r/datascience/comments/o3sjcm/crossvalidation_using_facebook_prophet/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3sjcm/crossvalidation_using_facebook_prophet/,515407,1624145317.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m considering a switch to data engineering as I really enjoy the engineering side of things I’ve had to do as a Data Scientist. 

As a Data Scientist what were you doing before? How do you like it? Any regrets?",t2_4z85fo82,False,,0,False,"Data Scientists who switched to Data Engineer, how is it going?",[],r/datascience,False,6,career,0,,,False,t3_o33nut,False,dark,0.96,,public,192,3,{},,,False,[],,False,False,,{},Career,False,192,,False,False,self,False,,[],{'gid_1': 2},,True,,1624089487.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m considering a switch to data engineering as I really enjoy the engineering side of things I’ve had to do as a Data Scientist. &lt;/p&gt;

&lt;p&gt;As a Data Scientist what were you doing before? How do you like it? Any regrets?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o33nut,True,,Dismal-Variation-12,,50,True,all_ads,False,[],False,,/r/datascience/comments/o33nut/data_scientists_who_switched_to_data_engineer_how/,all_ads,False,https://www.reddit.com/r/datascience/comments/o33nut/data_scientists_who_switched_to_data_engineer_how/,515407,1624060687.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all,

I was just wondering, is there a good place to read some good quality actual data science projects end to end?

Ideally, I think it would be good to just have a place with a good set of Jupyter notebooks of varying length and complexity that I could read to get a better understanding of the whole end to end process of a 'typical' data science project.

I understand that that code can be found on Kaggle and GitHub, but it feels like it is kind of hard work to be able to find them all organised in one place. Its hard to discern the good from the amateur, and a lot of the time its just small little 'tricks' and tutorials, with not much descriptive text of what the problem is and what they are trying to carry out. I'm kind of looking for some 'gold standard' data science projects I can read to get a better understanding of what is required and what to strive for.

Maybe I am just doing it wrong, but does any one have any suggestions?",t2_3z5dxa7s,False,,0,False,Where to find good Quality DS project examples?,[],r/datascience,False,6,discussion,0,,,False,t3_o3brf2,False,dark,0.9,,public,24,0,{},,,False,[],,False,False,,{},Discussion,False,24,,False,False,self,False,,[],{},,True,,1624121755.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I was just wondering, is there a good place to read some good quality actual data science projects end to end?&lt;/p&gt;

&lt;p&gt;Ideally, I think it would be good to just have a place with a good set of Jupyter notebooks of varying length and complexity that I could read to get a better understanding of the whole end to end process of a &amp;#39;typical&amp;#39; data science project.&lt;/p&gt;

&lt;p&gt;I understand that that code can be found on Kaggle and GitHub, but it feels like it is kind of hard work to be able to find them all organised in one place. Its hard to discern the good from the amateur, and a lot of the time its just small little &amp;#39;tricks&amp;#39; and tutorials, with not much descriptive text of what the problem is and what they are trying to carry out. I&amp;#39;m kind of looking for some &amp;#39;gold standard&amp;#39; data science projects I can read to get a better understanding of what is required and what to strive for.&lt;/p&gt;

&lt;p&gt;Maybe I am just doing it wrong, but does any one have any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3brf2,True,,beneath_the_knees,,11,True,all_ads,False,[],False,,/r/datascience/comments/o3brf2/where_to_find_good_quality_ds_project_examples/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3brf2/where_to_find_good_quality_ds_project_examples/,515407,1624092955.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,True,,,,
,datascience,"I know that GDBT model cannot be parallelized during training because the trees are built sequentially. What about its scoring / making prediction phase for one single sample? 

As far as I understand, we can evaluate the outputs of all trees at once (hence parallelizable), and the final prediction is just a linear combination. Is that correct?",t2_9aqvgklw,False,,0,False,Can the prediction of gradient boosted decision trees be parallelized?,[],r/datascience,False,6,discussion,0,,,False,t3_o3rbc2,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1624170153.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that GDBT model cannot be parallelized during training because the trees are built sequentially. What about its scoring / making prediction phase for one single sample? &lt;/p&gt;

&lt;p&gt;As far as I understand, we can evaluate the outputs of all trees at once (hence parallelizable), and the final prediction is just a linear combination. Is that correct?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3rbc2,True,,populus27,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3rbc2/can_the_prediction_of_gradient_boosted_decision/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3rbc2/can_the_prediction_of_gradient_boosted_decision/,515407,1624141353.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"After much thought, I have decided that getting a masters in data science is the correct career choice for me. I begin my studies this fall, and my first class is python for data analysis. 

Does anyone have advice they’d like to share? I know the basics of Python and I took an online udemy course for Python with data analysis back in 2019. 

My goal is to maximize what I learn in this course because I’d really like to increase my use of Python (for data science and just in general)",t2_8wi0rk4h,False,,0,False,"Starting my masters in DS this fall, any advice on my first class?",[],r/datascience,False,6,education,0,,,False,t3_o3ybbf,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1624195222.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After much thought, I have decided that getting a masters in data science is the correct career choice for me. I begin my studies this fall, and my first class is python for data analysis. &lt;/p&gt;

&lt;p&gt;Does anyone have advice they’d like to share? I know the basics of Python and I took an online udemy course for Python with data analysis back in 2019. &lt;/p&gt;

&lt;p&gt;My goal is to maximize what I learn in this course because I’d really like to increase my use of Python (for data science and just in general)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ybbf,True,,pizzagarrett,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3ybbf/starting_my_masters_in_ds_this_fall_any_advice_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ybbf/starting_my_masters_in_ds_this_fall_any_advice_on/,515407,1624166422.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Within Australia, large companies have something called grad programs. These grads rotate through the business 3 or 4 times in 6 months stints. These grads have to apply to an area of the business to work.
I need to write a pitch to get these new grads to apply to my area of the business.

Question is, if you are new to analytics/data science, why did you want to get into it? What did you hear about it for you to think yeah that’s what I want to do?",t2_a8wit5hx,False,,0,False,Sales pitch for DS,[],r/datascience,False,6,discussion,0,,,False,t3_o3u9v8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624180117.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Within Australia, large companies have something called grad programs. These grads rotate through the business 3 or 4 times in 6 months stints. These grads have to apply to an area of the business to work.
I need to write a pitch to get these new grads to apply to my area of the business.&lt;/p&gt;

&lt;p&gt;Question is, if you are new to analytics/data science, why did you want to get into it? What did you hear about it for you to think yeah that’s what I want to do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3u9v8,True,,EmergencyContact2016,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3u9v8/sales_pitch_for_ds/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3u9v8/sales_pitch_for_ds/,515407,1624151317.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My list:

1. Documentation
2. The summary of code logic as flowchart/diagram format
3. Code convention
4. Grab some general significant number to check if each step output is making sense
5. Prepare some common preproc function for the task with similar procedures.
6. Put the date/time as prefix for each simulation testing",t2_5czjyjhi,False,,0,False,What are your top things that are usually being overlooked but super important and can save lots of time and money,[],r/datascience,False,6,discussion,0,,,False,t3_o2x2me,False,dark,1.0,,public,200,0,{},,,False,[],,False,False,,{},Discussion,False,200,,False,False,self,1624043112.0,,[],{},,True,,1624070457.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My list:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;The summary of code logic as flowchart/diagram format&lt;/li&gt;
&lt;li&gt;Code convention&lt;/li&gt;
&lt;li&gt;Grab some general significant number to check if each step output is making sense&lt;/li&gt;
&lt;li&gt;Prepare some common preproc function for the task with similar procedures.&lt;/li&gt;
&lt;li&gt;Put the date/time as prefix for each simulation testing&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2x2me,True,,vietlinh12hoa,,37,True,all_ads,False,[],False,,/r/datascience/comments/o2x2me/what_are_your_top_things_that_are_usually_being/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2x2me/what_are_your_top_things_that_are_usually_being/,515407,1624041657.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’m a 2 year decision scientist and I’m on my own. I went from knowing little excel, let alone python or sql, to writing code all the time. But there are still things I don’t know. I was wondering if there was someone who wouldn’t mind being friends on discord or something where I can ask questions when they pop in my head. For the most part, I think Google answers the base line of my questions, but it would be nice to be able to tailor my question based on my role. 

An example is something like: it’s me, my laptop, and the company’s SQL server. Should I get Hadoop? Should I run pyspark for these pesky, longer operations when I don’t have multiple machines to run operations on? How can I grab weather data  for all US zip codes and store it, as it’s over 25-50 GB of data? 

Being the sole guy for these kinds of developments is challenging. But anything to make my life easier would be a blessing.",t2_9l80yrty,False,,0,False,Mentorship or a discord to ask questions?,[],r/datascience,False,6,career,0,,,False,t3_o3o5lm,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1624160502.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a 2 year decision scientist and I’m on my own. I went from knowing little excel, let alone python or sql, to writing code all the time. But there are still things I don’t know. I was wondering if there was someone who wouldn’t mind being friends on discord or something where I can ask questions when they pop in my head. For the most part, I think Google answers the base line of my questions, but it would be nice to be able to tailor my question based on my role. &lt;/p&gt;

&lt;p&gt;An example is something like: it’s me, my laptop, and the company’s SQL server. Should I get Hadoop? Should I run pyspark for these pesky, longer operations when I don’t have multiple machines to run operations on? How can I grab weather data  for all US zip codes and store it, as it’s over 25-50 GB of data? &lt;/p&gt;

&lt;p&gt;Being the sole guy for these kinds of developments is challenging. But anything to make my life easier would be a blessing.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3o5lm,True,,cruelbankai,,3,True,all_ads,False,[],False,,/r/datascience/comments/o3o5lm/mentorship_or_a_discord_to_ask_questions/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3o5lm/mentorship_or_a_discord_to_ask_questions/,515407,1624131702.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"(Disclaimer: I'm not affiliated with the project)

[https://github.com/mc2-project](https://github.com/mc2-project/mc2)

MC2 is a project that aims at helping data scientists and engineers collaborate and process collective data without sharing the content of the data with one another, not even the cloud provider. I think the project is very interesting and I'd like about the use cases and what you will do with it.",t2_wpnyl,False,,0,False,MC2: A Platform for Secure Analytics and Machine Learning,[],r/datascience,False,6,projects,0,,,False,t3_o3ofbu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1624161318.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(Disclaimer: I&amp;#39;m not affiliated with the project)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/mc2-project/mc2""&gt;https://github.com/mc2-project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MC2 is a project that aims at helping data scientists and engineers collaborate and process collective data without sharing the content of the data with one another, not even the cloud provider. I think the project is very interesting and I&amp;#39;d like about the use cases and what you will do with it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ofbu,True,,jchasinga,,0,True,all_ads,False,[],False,,/r/datascience/comments/o3ofbu/mc2_a_platform_for_secure_analytics_and_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ofbu/mc2_a_platform_for_secure_analytics_and_machine/,515407,1624132518.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?auto=webp&amp;s=293d3b0321b4403bf0a59d6719a0fb2088904dbf', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=daa811d09a0acefb7b703773f3a1be4381f0c32e', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=686e1183b1f5a22ae2b77efb7d8084768d08b5fb', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1fac4b7b236fbdb639a1cf852a17a38f9d99d68', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b005e3d8f7511b9ca6f7cea7ee5b5e0588acb8d', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=31a3c73d523da7224f4974de990cf6b518c2cd8b', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/9jE4bHOmx1J6LKfTz6iR-T1GfhUb-PRosQO-09Qbm24.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09c413698eb66f9f4e95c9abec0bfc08e0d6b610', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Hq9BWX3G702TK0LNyn1QLKGeYG4MP29m4Km9Hj6gIsc'}], 'enabled': False}",,,,,
,datascience,"My company hired a junior BI analyst a couple of months back to handle the backlog of dashboarding/analysis demands I was not able to juggle in a timely manner due to more pressing data science/engineering concerns (I was the sole data person in the team at the time).

She's done a great job, but now that most of the overdue demands she was given are done, she has expressed interest in gradually transitioning to a more broad data role within the company. Most of her previous job experience is in a strict BI role (dashboard building, reporting, etc) but she has done a few python courses on her own.

Under my supervision, she has begun working on a few basic tasks like data cleaning, pipeline building, etc. I can tell she's interested, but her progress has been remarkably slow and underwhelming.   
People who have more experience with mentoring juniors, what are some things I can do to be a better mentor and help her develop in the field? I try to give detailed feedback in each of her deliveries but as I said progress is slow so far.  


\-------  
EDIT: I didn't expect this many good responses, thank you so much! Starting next week I'll try to come up with a more active plan of mentoring based on the input you guys gave. Here's hoping this post can help more people who are or might eventually be in a similar situation.  
",t2_8mqzapd7,False,,0,False,Tips on how to properly mentor and train junior data people?,[],r/datascience,False,6,career,0,,,False,t3_o2q05f,False,dark,0.97,,public,159,1,{},,,False,[],,False,False,,{},Career,False,159,,False,False,self,1624061082.0,,[],{'gid_1': 1},,True,,1624054295.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My company hired a junior BI analyst a couple of months back to handle the backlog of dashboarding/analysis demands I was not able to juggle in a timely manner due to more pressing data science/engineering concerns (I was the sole data person in the team at the time).&lt;/p&gt;

&lt;p&gt;She&amp;#39;s done a great job, but now that most of the overdue demands she was given are done, she has expressed interest in gradually transitioning to a more broad data role within the company. Most of her previous job experience is in a strict BI role (dashboard building, reporting, etc) but she has done a few python courses on her own.&lt;/p&gt;

&lt;p&gt;Under my supervision, she has begun working on a few basic tasks like data cleaning, pipeline building, etc. I can tell she&amp;#39;s interested, but her progress has been remarkably slow and underwhelming.&lt;br/&gt;
People who have more experience with mentoring juniors, what are some things I can do to be a better mentor and help her develop in the field? I try to give detailed feedback in each of her deliveries but as I said progress is slow so far.  &lt;/p&gt;

&lt;p&gt;-------&lt;br/&gt;
EDIT: I didn&amp;#39;t expect this many good responses, thank you so much! Starting next week I&amp;#39;ll try to come up with a more active plan of mentoring based on the input you guys gave. Here&amp;#39;s hoping this post can help more people who are or might eventually be in a similar situation.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2q05f,True,,false-shrimp,,21,True,all_ads,False,[],False,,/r/datascience/comments/o2q05f/tips_on_how_to_properly_mentor_and_train_junior/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2q05f/tips_on_how_to_properly_mentor_and_train_junior/,515407,1624025495.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi, I am a CS major. I graduated in 2019 and have been working in a company since then.

When I joined, they had just started an AI team. In fact, my manager was the first data science hire and I was the second one. We have been working on creating an MVP, and don't have any customers for it yet.

My manager is more inclined towards the business side of data science (so he's good in storytelling in demos, working with Sales and Product Management for the requirements etc.). We don't have any expert for guidance on ML product pipelines, algorithm improvement etc. so anything I have implemented so far has been by taking help from the internet.

As a college fresher, I got this job through a Python interview. I even left an offer of 2.5x pay in software development because I wanted to pursue data science.

I haven't interviewed for any other company since then. I don't really know what is usually involved in data science jobs. Have I gained any useful data science experience in order to get the next job?

My work involves:

\- setting up internal products on demo servers and preparing a story through dummy data/insights for customer demos or security conferences (40%)

\- designing database tables and creating reports and dashboards for the product (initially it was in PowerBI, then Jasper Reports and now in an internal product framework) (25%)

\- developing and maintaining Python jobs and integrating model training/detection with the main Java product through Flask (20%)

\- exploring, drawing analogies and implementing unsupervised ML/DL algorithms that can solve the use cases provided by Product Management. (15%)

 Is this what is usually involved in a real data science job? 

Also, I read that you should add in your resume how your work helped in making business value. But since we don't have any customers, what should I add?",t2_47gxdu5o,False,,0,False,Is this a data science job for real?,[],r/datascience,False,6,career,0,,,False,t3_o2wsh8,False,dark,0.93,,public,40,0,{},,,False,[],,False,False,,{},Career,False,40,,False,False,self,False,,[],{},,True,,1624069732.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am a CS major. I graduated in 2019 and have been working in a company since then.&lt;/p&gt;

&lt;p&gt;When I joined, they had just started an AI team. In fact, my manager was the first data science hire and I was the second one. We have been working on creating an MVP, and don&amp;#39;t have any customers for it yet.&lt;/p&gt;

&lt;p&gt;My manager is more inclined towards the business side of data science (so he&amp;#39;s good in storytelling in demos, working with Sales and Product Management for the requirements etc.). We don&amp;#39;t have any expert for guidance on ML product pipelines, algorithm improvement etc. so anything I have implemented so far has been by taking help from the internet.&lt;/p&gt;

&lt;p&gt;As a college fresher, I got this job through a Python interview. I even left an offer of 2.5x pay in software development because I wanted to pursue data science.&lt;/p&gt;

&lt;p&gt;I haven&amp;#39;t interviewed for any other company since then. I don&amp;#39;t really know what is usually involved in data science jobs. Have I gained any useful data science experience in order to get the next job?&lt;/p&gt;

&lt;p&gt;My work involves:&lt;/p&gt;

&lt;p&gt;- setting up internal products on demo servers and preparing a story through dummy data/insights for customer demos or security conferences (40%)&lt;/p&gt;

&lt;p&gt;- designing database tables and creating reports and dashboards for the product (initially it was in PowerBI, then Jasper Reports and now in an internal product framework) (25%)&lt;/p&gt;

&lt;p&gt;- developing and maintaining Python jobs and integrating model training/detection with the main Java product through Flask (20%)&lt;/p&gt;

&lt;p&gt;- exploring, drawing analogies and implementing unsupervised ML/DL algorithms that can solve the use cases provided by Product Management. (15%)&lt;/p&gt;

&lt;p&gt;Is this what is usually involved in a real data science job? &lt;/p&gt;

&lt;p&gt;Also, I read that you should add in your resume how your work helped in making business value. But since we don&amp;#39;t have any customers, what should I add?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2wsh8,True,,divyagupta25,,27,True,all_ads,False,[],False,,/r/datascience/comments/o2wsh8/is_this_a_data_science_job_for_real/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2wsh8/is_this_a_data_science_job_for_real/,515407,1624040932.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I took a data science with Python course on Udemy that was created in 2017. I’m wondering, how much do pandas and other classic DS libraries change (matplotlib, scikitlearn, seaborn etc.)? Should I take a refresher course?",t2_8wi0rk4h,False,,0,False,Do pandas and other Python data science libraries change a lot?,[],r/datascience,False,6,discussion,0,,,False,t3_o360o3,False,dark,0.77,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1624098022.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I took a data science with Python course on Udemy that was created in 2017. I’m wondering, how much do pandas and other classic DS libraries change (matplotlib, scikitlearn, seaborn etc.)? Should I take a refresher course?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o360o3,True,,pizzagarrett,,7,True,all_ads,False,[],False,,/r/datascience/comments/o360o3/do_pandas_and_other_python_data_science_libraries/,all_ads,False,https://www.reddit.com/r/datascience/comments/o360o3/do_pandas_and_other_python_data_science_libraries/,515407,1624069222.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Not necessarily data engineering (ETL/ELT) work, but architecture and development work inside of the database/DWH.",t2_3uoce3bn,False,,0,False,How often do you find yourself performing analytics engineering duties?,[],r/datascience,False,6,discussion,0,,,False,t3_o3g1q0,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1624137912.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Not necessarily data engineering (ETL/ELT) work, but architecture and development work inside of the database/DWH.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3g1q0,True,,Tender_Figs,,2,True,all_ads,False,[],False,,/r/datascience/comments/o3g1q0/how_often_do_you_find_yourself_performing/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3g1q0/how_often_do_you_find_yourself_performing/,515407,1624109112.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have 0 SWE background and my coding skills include SQL, R and Python (pandas, numpy etc). My work is on the analytics side so I rarely think in terms of OOP, and I dont do a lot of functions.

Even looking at the most basic leetcode problems, I get stuck with even where to start.  Terms binary trees, hashes, sorted lists etc is very foreign to me.

I want to start doing leetcode, but is there any other resources I should start with first just to kind of get the basics of the basics down?  The idea is that if I see a leetcode problem, I want to have some sort of reference of where to start.",t2_i6go6an,False,,0,False,‘Primer’ for leetcode?,[],r/datascience,False,6,education,0,,,False,t3_o31fkc,False,dark,0.94,,public,14,0,{},,,False,[],,False,False,,{},Education,False,14,,False,False,self,False,,[],{},,True,,1624082673.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 0 SWE background and my coding skills include SQL, R and Python (pandas, numpy etc). My work is on the analytics side so I rarely think in terms of OOP, and I dont do a lot of functions.&lt;/p&gt;

&lt;p&gt;Even looking at the most basic leetcode problems, I get stuck with even where to start.  Terms binary trees, hashes, sorted lists etc is very foreign to me.&lt;/p&gt;

&lt;p&gt;I want to start doing leetcode, but is there any other resources I should start with first just to kind of get the basics of the basics down?  The idea is that if I see a leetcode problem, I want to have some sort of reference of where to start.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o31fkc,True,,mrdlau,,18,True,all_ads,False,[],False,,/r/datascience/comments/o31fkc/primer_for_leetcode/,all_ads,False,https://www.reddit.com/r/datascience/comments/o31fkc/primer_for_leetcode/,515407,1624053873.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I work on a ""Data Science"" team that does work that is hybrid of typical DS/DE/DA work.

Some days it's simple SQL and Tableau, other times its building pipelines through real time ingestion with kafka or batch ETLs using Airflow, sometimes it's hard statistical analysis and occasionally we do actual Machine Learning... 

We are all Data Scientists by title but play the hybrid of just being good with data, regardless of specifics coming to DA versus DE versus DS..

Recently I've felt like I've been missing the DS side of things. I get the occasional one off request for building data models but don't ever really deploy anything with them... Usually just local models and then sharing the results.

I am wanting to start putting some models ""in production"" as a way to show my usual stakeholders the value that ML can have for the business and I'm curious in all the ways you guys do this on your team? I specifically am curious about two scenarios...

**Scenario 1: Model only needs to be running when the ETL runs.** 

Most of our work is batch processing data at some interval. I typically utilize airflow with kubernetes pod operators to spin up python containers that have the code I need for the ETL. I figured a super simple way to do this was to just store a trained model in the container itself and have the scripts run the models the same way I would locally whenever the airflow job is triggered. This seems simple, but I'm sure my idea is flawed. I know a major limitation here is that to retrain the model, I need to pull my container back to my local machine, retrain the model locally, rebuild the container and push it back to artifactory so the ETL can now use the updated version. There's also the limitation that other people can't use the model since its only present ""in production"" when airflow triggers the pod to spin up.

**Scenario 2: Model needs to be available and running at all times**

I often utilize kafka triggers deployed in kubernetes pods to listen to topics of interest. While they usually just write data to a db or send notifications to stakeholders about the event that occurred, I see a lot of potential in being able to point data from an event into a classification or regression model in real time and then store the results of the model for later use. To do this, I think my approach would be to build a flask API that is deployed in the same kubernetes cluster that I could point my kafka trigger towards... I have played with flask before, but am not as great with the whole devops side in terms of configuring IPs and making sure my flask API is available for other ""apps"" to use... 

I would love your feedback on how your team deploys models and code, especially if you run into the above two scenarios :)",t2_33en6nux,False,,0,False,How does your team deploy models?,[],r/datascience,False,6,discussion,0,,,False,t3_o2u9dy,False,dark,0.97,,public,34,0,{},,,False,[],,False,False,,{},Discussion,False,34,,False,False,self,False,,[],{},,True,,1624064600.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work on a &amp;quot;Data Science&amp;quot; team that does work that is hybrid of typical DS/DE/DA work.&lt;/p&gt;

&lt;p&gt;Some days it&amp;#39;s simple SQL and Tableau, other times its building pipelines through real time ingestion with kafka or batch ETLs using Airflow, sometimes it&amp;#39;s hard statistical analysis and occasionally we do actual Machine Learning... &lt;/p&gt;

&lt;p&gt;We are all Data Scientists by title but play the hybrid of just being good with data, regardless of specifics coming to DA versus DE versus DS..&lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;ve felt like I&amp;#39;ve been missing the DS side of things. I get the occasional one off request for building data models but don&amp;#39;t ever really deploy anything with them... Usually just local models and then sharing the results.&lt;/p&gt;

&lt;p&gt;I am wanting to start putting some models &amp;quot;in production&amp;quot; as a way to show my usual stakeholders the value that ML can have for the business and I&amp;#39;m curious in all the ways you guys do this on your team? I specifically am curious about two scenarios...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1: Model only needs to be running when the ETL runs.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Most of our work is batch processing data at some interval. I typically utilize airflow with kubernetes pod operators to spin up python containers that have the code I need for the ETL. I figured a super simple way to do this was to just store a trained model in the container itself and have the scripts run the models the same way I would locally whenever the airflow job is triggered. This seems simple, but I&amp;#39;m sure my idea is flawed. I know a major limitation here is that to retrain the model, I need to pull my container back to my local machine, retrain the model locally, rebuild the container and push it back to artifactory so the ETL can now use the updated version. There&amp;#39;s also the limitation that other people can&amp;#39;t use the model since its only present &amp;quot;in production&amp;quot; when airflow triggers the pod to spin up.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2: Model needs to be available and running at all times&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I often utilize kafka triggers deployed in kubernetes pods to listen to topics of interest. While they usually just write data to a db or send notifications to stakeholders about the event that occurred, I see a lot of potential in being able to point data from an event into a classification or regression model in real time and then store the results of the model for later use. To do this, I think my approach would be to build a flask API that is deployed in the same kubernetes cluster that I could point my kafka trigger towards... I have played with flask before, but am not as great with the whole devops side in terms of configuring IPs and making sure my flask API is available for other &amp;quot;apps&amp;quot; to use... &lt;/p&gt;

&lt;p&gt;I would love your feedback on how your team deploys models and code, especially if you run into the above two scenarios :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2u9dy,True,,Beertarian,,15,True,all_ads,False,[],False,,/r/datascience/comments/o2u9dy/how_does_your_team_deploy_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2u9dy/how_does_your_team_deploy_models/,515407,1624035800.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello.

I am an undergraduate student looking for more potential career opportunities.

Are there data scientists here from actuarial background who can share more experience on it?

Thanks in advance.",t2_1rkw6x8n,False,,0,False,Any Actuary transition to data scientists?,[],r/datascience,False,6,career,0,,,False,t3_o395o5,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1624109860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello.&lt;/p&gt;

&lt;p&gt;I am an undergraduate student looking for more potential career opportunities.&lt;/p&gt;

&lt;p&gt;Are there data scientists here from actuarial background who can share more experience on it?&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o395o5,True,,markpreston54,,4,True,all_ads,False,[],False,,/r/datascience/comments/o395o5/any_actuary_transition_to_data_scientists/,all_ads,False,https://www.reddit.com/r/datascience/comments/o395o5/any_actuary_transition_to_data_scientists/,515407,1624081060.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," Hello all,

I have been admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan to start in Fall 2021:

[https://www.si.umich.edu/programs/master-applied-data-science-online](https://www.si.umich.edu/programs/master-applied-data-science-online)

Does anyone have any insights on the program? If I understand correctly, the program is very new and the first cohort of students must be about to graduate now. I would love to read the experience of some people that have taken the program and whether they thought it was worth it as well as any objective opinions in general.

I have found very few first-hand opinions on the MADS at Michigan out there, and the opinions that I have read (mostly here on reddit) were on the negative side. Given that other schools such as Georgia Tech have tons of overwhelmingly positive opinions -both on the quality of the classes as well as the price-, I am scared this is not the right move for me. It is a big investment in terms of time and money after all.

My background:

&amp;#x200B;

* Bachelor in Economics (10 years ago)
* 5+ years of Data Analytics work in large tech company in Bay AreA
* Obtained 2 professional certificates at UC Berkeley Extension: 1 in Programming, 1 in Data Science
   * Medium Proficiency in Python, SQL + Statistics/Probability
* I intend to take degree in 3 years (while working full-time)
* I can afford the tuition (\~$45k before employer contributions) if program is worth it. It won't put me into hardship or debt. 
* What I want to get from the degree: applied knowledge that I can use at the workplace to move onto more technical roles. 
   * Also, a degree that will 'officially' open the doors to the mentioned roles, as many positions state that a Masters Degree is the minimum required qualification.
* If I were to decline the offer, my current options are not many:
   * Master of Science in Data Science at Colorado Boulder (everyone is 'admitted' through their Introduction courses that take place every 3 months): https://www.colorado.edu/program/data-science/coursera-overview
   * Apply to other schools for the Spring 2022 semester (Georgia Tech would be the first target; any other recommendations?)",t2_51engwuk,False,,0,False,Admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan - Thoughts?,[],r/datascience,False,6,education,0,,,False,t3_o3ajx5,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1624116206.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I have been admitted to the Master of Applied Data Science (MADS) - Online at the University of Michigan to start in Fall 2021:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.si.umich.edu/programs/master-applied-data-science-online""&gt;https://www.si.umich.edu/programs/master-applied-data-science-online&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Does anyone have any insights on the program? If I understand correctly, the program is very new and the first cohort of students must be about to graduate now. I would love to read the experience of some people that have taken the program and whether they thought it was worth it as well as any objective opinions in general.&lt;/p&gt;

&lt;p&gt;I have found very few first-hand opinions on the MADS at Michigan out there, and the opinions that I have read (mostly here on reddit) were on the negative side. Given that other schools such as Georgia Tech have tons of overwhelmingly positive opinions -both on the quality of the classes as well as the price-, I am scared this is not the right move for me. It is a big investment in terms of time and money after all.&lt;/p&gt;

&lt;p&gt;My background:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bachelor in Economics (10 years ago)&lt;/li&gt;
&lt;li&gt;5+ years of Data Analytics work in large tech company in Bay AreA&lt;/li&gt;
&lt;li&gt;Obtained 2 professional certificates at UC Berkeley Extension: 1 in Programming, 1 in Data Science

&lt;ul&gt;
&lt;li&gt;Medium Proficiency in Python, SQL + Statistics/Probability&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;I intend to take degree in 3 years (while working full-time)&lt;/li&gt;
&lt;li&gt;I can afford the tuition (~$45k before employer contributions) if program is worth it. It won&amp;#39;t put me into hardship or debt. &lt;/li&gt;
&lt;li&gt;What I want to get from the degree: applied knowledge that I can use at the workplace to move onto more technical roles. 

&lt;ul&gt;
&lt;li&gt;Also, a degree that will &amp;#39;officially&amp;#39; open the doors to the mentioned roles, as many positions state that a Masters Degree is the minimum required qualification.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I were to decline the offer, my current options are not many:

&lt;ul&gt;
&lt;li&gt;Master of Science in Data Science at Colorado Boulder (everyone is &amp;#39;admitted&amp;#39; through their Introduction courses that take place every 3 months): &lt;a href=""https://www.colorado.edu/program/data-science/coursera-overview""&gt;https://www.colorado.edu/program/data-science/coursera-overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Apply to other schools for the Spring 2022 semester (Georgia Tech would be the first target; any other recommendations?)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3ajx5,True,,rakhed1,,18,True,all_ads,False,[],False,,/r/datascience/comments/o3ajx5/admitted_to_the_master_of_applied_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3ajx5/admitted_to_the_master_of_applied_data_science/,515407,1624087406.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?auto=webp&amp;s=21967ac2e4e8d2260dec86533b2daf1d668e4569', 'width': 500, 'height': 281}, 'resolutions': [{'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc5389a13a598c72a09381953f4c6a38829ba222', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c55a38a70fbe75184ab51833b9e04e51170c0759', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/5EPpb3bsxTW-KZFE0cbwgFMyop7Hk4-8lIB57QdkulA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74994dc942a0fd65b3c3a8383d2454e13d9f762a', 'width': 320, 'height': 179}], 'variants': {}, 'id': 'OG8afLvNUP-fqr1Uh-23YpAEqhmL_QnNnPnkzAZPWKM'}], 'enabled': False}",,,,,
,datascience,"I have a great opportunity in a company, it's a B2B company. The CEO of this company really want to hire me but he wants to test me one day to be sure. This company use a call center and the CEO ask me to find a way to increase the pourcentage of people accept to take the call. In this moment the call center get 30% of people taking the phone call. The compagny wants an increase of 5%. So i need to build a model. What kind of variables should i take according to you to build it (sexe, age, professional activity,...) ?  I have no work experiences but just theorical knowledge. 

I need some advices. 
Thanks 

Ps: Sorry for my Frenchglish",t2_7mv7a33f,False,,0,False,Create a model to improve a business issue,[],r/datascience,False,6,career,0,,,False,t3_o3d3jm,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1624127423.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a great opportunity in a company, it&amp;#39;s a B2B company. The CEO of this company really want to hire me but he wants to test me one day to be sure. This company use a call center and the CEO ask me to find a way to increase the pourcentage of people accept to take the call. In this moment the call center get 30% of people taking the phone call. The compagny wants an increase of 5%. So i need to build a model. What kind of variables should i take according to you to build it (sexe, age, professional activity,...) ?  I have no work experiences but just theorical knowledge. &lt;/p&gt;

&lt;p&gt;I need some advices. 
Thanks &lt;/p&gt;

&lt;p&gt;Ps: Sorry for my Frenchglish&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o3d3jm,True,,battlefieldanalytica,,7,True,all_ads,False,[],False,,/r/datascience/comments/o3d3jm/create_a_model_to_improve_a_business_issue/,all_ads,False,https://www.reddit.com/r/datascience/comments/o3d3jm/create_a_model_to_improve_a_business_issue/,515407,1624098623.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"So, for the past 3 years, I have worked in multiple dead-end jobs due to having no goal or career in mind, leading to not going to college or university, which I really do hate myself for. Worked in a bar, two warehouse jobs, and now work in an apprenticeship in a financial services company as a Quality Assurance Analyst. 

As part of my apprenticeship, I take part in courses where they are relevant to our jobs and I was introduced to Data Science which caught my attention. We get told what it is, it’s purpose, the jobs that come under it, and I don’t think I ever felt in love with such a thing before in my life. 

Fast forward three months, I have signed up for online courses, been reading articles, programming languages, and it’s never gone away, and that’s rare for someone like me with Autism and ADHD. But one thing I keep seeing is articles and people saying “How to Become a Data Analyst with No Degree or Experience”. 

This has me interested but also very wary due to the current work climate and strict job requirements. So, I want to ask everyone: Is it really possible to land a job as a Data Analyst with a portfolio background, with no experience, and no degree? 

I’m happy for the job I am grateful to have kept since pre-pandemic but I know I wouldn’t like to stay in a job like that, or in the same company which is pretty terrible to staff. 

If any advice or knowledge can be given as well, it would be much appreciated in my career path. Thank you.",t2_881086cq,False,,0,False,Is it really possible?,[],r/datascience,False,6,career,0,,,False,t3_o32fsq,False,dark,0.72,,public,6,0,{},,,False,[],,False,False,,{},Career,False,6,,False,False,self,False,,[],{},,True,,1624085673.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, for the past 3 years, I have worked in multiple dead-end jobs due to having no goal or career in mind, leading to not going to college or university, which I really do hate myself for. Worked in a bar, two warehouse jobs, and now work in an apprenticeship in a financial services company as a Quality Assurance Analyst. &lt;/p&gt;

&lt;p&gt;As part of my apprenticeship, I take part in courses where they are relevant to our jobs and I was introduced to Data Science which caught my attention. We get told what it is, it’s purpose, the jobs that come under it, and I don’t think I ever felt in love with such a thing before in my life. &lt;/p&gt;

&lt;p&gt;Fast forward three months, I have signed up for online courses, been reading articles, programming languages, and it’s never gone away, and that’s rare for someone like me with Autism and ADHD. But one thing I keep seeing is articles and people saying “How to Become a Data Analyst with No Degree or Experience”. &lt;/p&gt;

&lt;p&gt;This has me interested but also very wary due to the current work climate and strict job requirements. So, I want to ask everyone: Is it really possible to land a job as a Data Analyst with a portfolio background, with no experience, and no degree? &lt;/p&gt;

&lt;p&gt;I’m happy for the job I am grateful to have kept since pre-pandemic but I know I wouldn’t like to stay in a job like that, or in the same company which is pretty terrible to staff. &lt;/p&gt;

&lt;p&gt;If any advice or knowledge can be given as well, it would be much appreciated in my career path. Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o32fsq,True,,mintcoffeebug,,6,True,all_ads,False,[],False,,/r/datascience/comments/o32fsq/is_it_really_possible/,all_ads,False,https://www.reddit.com/r/datascience/comments/o32fsq/is_it_really_possible/,515406,1624056873.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hey guys so I’m undecided yet on what I want to major in but data science has caught my eye. I was just wondering what specific entry level jobs could I get out of college? Is it so in demand that you can work in pretty much any sector? Is there such thing as an associates  in data science?

Im not very good at math but I plan on taking remedial math in college, assuming they have that; will I be fine? 

What kind of internships can benefit me to set myself up for success?",t2_3bthqqi7,False,,0,False,What jobs and sectors can I get with a data science degree?,[],r/datascience,False,6,education,0,,,False,t3_o36yvp,False,dark,0.59,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1624101623.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys so I’m undecided yet on what I want to major in but data science has caught my eye. I was just wondering what specific entry level jobs could I get out of college? Is it so in demand that you can work in pretty much any sector? Is there such thing as an associates  in data science?&lt;/p&gt;

&lt;p&gt;Im not very good at math but I plan on taking remedial math in college, assuming they have that; will I be fine? &lt;/p&gt;

&lt;p&gt;What kind of internships can benefit me to set myself up for success?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o36yvp,True,,jaylenbrowny,,3,True,all_ads,False,[],False,,/r/datascience/comments/o36yvp/what_jobs_and_sectors_can_i_get_with_a_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/o36yvp/what_jobs_and_sectors_can_i_get_with_a_data/,515406,1624072823.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Untill college, I was kind of extrovert who eventually with age started becoming introvert. I started my career in sales but one thing after another made me switch to Data science. After 5 years of job, I'm mostly introvert and data science actually helps me this as many other career options do indeed need extrovert characteristics.
To break down on how it helps 2 major examples are:
•Communication:
Its mostly through email, this helps me avoid unnecessary talking. Moreover any verbal meeting mostly translate into me listening to the person and identify the problem statement and then let my work speak.

•Technology
Working on laptop, pushing code on git for collaboration etc all further helps me with least human interaction.

Things were it hurts me most.
•Switching job
Terrible time at interview since now you have to speak and make eye contact

•Office friends
Many a times it had happened that I'm the last person to get to know of office gossip. In my past jobs it was kind off necessity since it helped me know if company will shutdown or not. My last 4 company where I worked all shutdown and in 2, I was among last to know that it was going down.",t2_9zy06zws,False,,0,False,How many people is data science are introverts?,[],r/datascience,False,6,fun,0,,,False,t3_o34mzt,False,dark,0.67,,public,4,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,4,,False,False,self,False,,[],{},,True,,1624092917.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Untill college, I was kind of extrovert who eventually with age started becoming introvert. I started my career in sales but one thing after another made me switch to Data science. After 5 years of job, I&amp;#39;m mostly introvert and data science actually helps me this as many other career options do indeed need extrovert characteristics.
To break down on how it helps 2 major examples are:
•Communication:
Its mostly through email, this helps me avoid unnecessary talking. Moreover any verbal meeting mostly translate into me listening to the person and identify the problem statement and then let my work speak.&lt;/p&gt;

&lt;p&gt;•Technology
Working on laptop, pushing code on git for collaboration etc all further helps me with least human interaction.&lt;/p&gt;

&lt;p&gt;Things were it hurts me most.
•Switching job
Terrible time at interview since now you have to speak and make eye contact&lt;/p&gt;

&lt;p&gt;•Office friends
Many a times it had happened that I&amp;#39;m the last person to get to know of office gossip. In my past jobs it was kind off necessity since it helped me know if company will shutdown or not. My last 4 company where I worked all shutdown and in 2, I was among last to know that it was going down.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o34mzt,True,,data_hop,,6,True,all_ads,False,[],False,,/r/datascience/comments/o34mzt/how_many_people_is_data_science_are_introverts/,all_ads,False,https://www.reddit.com/r/datascience/comments/o34mzt/how_many_people_is_data_science_are_introverts/,515406,1624064117.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_3mpdgvl7,False,,0,False,What course/book/talk etc. added the most value to your career as a data scientist?,[],r/datascience,False,6,discussion,0,,,False,t3_o2hiye,False,dark,0.99,,public,149,1,{},,,False,[],,False,False,,{},Discussion,False,149,,False,False,self,False,,[],{},,True,,1624022739.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2hiye,True,,benthecoderX,,32,True,all_ads,False,[],False,,/r/datascience/comments/o2hiye/what_coursebooktalk_etc_added_the_most_value_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2hiye/what_coursebooktalk_etc_added_the_most_value_to/,515406,1623993939.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[https://martin-thoma.com/images/2016/01/ml-classifiers-2.png](https://martin-thoma.com/images/2016/01/ml-classifiers-2.png)

These pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: ""red"" and ""blue"".

Shouldn't all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean ""an overlapping decision boundary""?

Thanks",t2_3tosvccj,False,,0,False,"can someone please explain what the ""white color shades"" mean in this picture?",[],r/datascience,False,6,discussion,0,,,False,t3_o2qr6i,False,dark,0.96,,public,21,0,{},,,False,[],,False,False,,{},Discussion,False,21,,False,False,self,False,,[],{},,True,,1624056352.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://martin-thoma.com/images/2016/01/ml-classifiers-2.png""&gt;https://martin-thoma.com/images/2016/01/ml-classifiers-2.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: &amp;quot;red&amp;quot; and &amp;quot;blue&amp;quot;.&lt;/p&gt;

&lt;p&gt;Shouldn&amp;#39;t all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean &amp;quot;an overlapping decision boundary&amp;quot;?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2qr6i,True,,jj4646,,5,True,all_ads,False,[],False,,/r/datascience/comments/o2qr6i/can_someone_please_explain_what_the_white_color/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2qr6i/can_someone_please_explain_what_the_white_color/,515406,1624027552.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?auto=webp&amp;s=6c851a26b932f89c3893032e66e34b3d707d10bf', 'width': 988, 'height': 718}, 'resolutions': [{'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8b8bb013f48747bd1299d307627a8ead0b79895', 'width': 108, 'height': 78}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81487fe3975442ef3106ad9c62c22516022d13a3', 'width': 216, 'height': 156}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e565a4aa170327c35d05952e2e5ea1ae298fb7ae', 'width': 320, 'height': 232}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a70a531fb9b09544521400d244898c2ea26812c0', 'width': 640, 'height': 465}, {'url': 'https://external-preview.redd.it/EVvhyjXalC0ZkGcUrZBbwrIgrSbiBGejGcDdw-d40S4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=39eabd46817c4651e0bbbbb9f64e2752c2e83c8d', 'width': 960, 'height': 697}], 'variants': {}, 'id': 'UTdvJd2FNGWl3ZnR5XDXOxEiR_Z5BOBaqJWXE-QQN8w'}], 'enabled': False}",,,,,
,datascience,"I have a couple projects I’d like to work on. But I’m terrible at holding myself accountable to making progress on projects. I’d like to get together with a handful of people to work on our own projects, but we’d meet every couple weeks to give updates and feedback.

If anyone else is in the Chicago area, I’d love to meet in person. (I’ve spent enough time cooped up over the past year.)

If you’re interested, PM me.

EDIT: Wow! Thanks everyone for the interest! We started a discord server for the group. I don't want to post it directly on the sub, but if you're interested, send me a PM and I'll respond with the discord link. I'm logging off for the night, so I may not get back to you until tomorrow.",t2_jqnnc,False,,0,False,Anyone interested on getting together to focus on personal projects?,[],r/datascience,False,6,projects,0,,,False,t3_o2cr6e,False,dark,0.95,,public,236,3,{},,,False,[],,False,False,,{},Projects,False,236,,False,False,self,1623992291.0,,[],{},,True,,1624006814.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a couple projects I’d like to work on. But I’m terrible at holding myself accountable to making progress on projects. I’d like to get together with a handful of people to work on our own projects, but we’d meet every couple weeks to give updates and feedback.&lt;/p&gt;

&lt;p&gt;If anyone else is in the Chicago area, I’d love to meet in person. (I’ve spent enough time cooped up over the past year.)&lt;/p&gt;

&lt;p&gt;If you’re interested, PM me.&lt;/p&gt;

&lt;p&gt;EDIT: Wow! Thanks everyone for the interest! We started a discord server for the group. I don&amp;#39;t want to post it directly on the sub, but if you&amp;#39;re interested, send me a PM and I&amp;#39;ll respond with the discord link. I&amp;#39;m logging off for the night, so I may not get back to you until tomorrow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2cr6e,True,,MindlessTime,,96,True,all_ads,False,[],False,,/r/datascience/comments/o2cr6e/anyone_interested_on_getting_together_to_focus_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2cr6e/anyone_interested_on_getting_together_to_focus_on/,515406,1623978014.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m a beginner and I’m planning on creating a career in cloud services. I currently have little to no knowledge on cloud platforms and I am wanting to learn more. As a first step I’m planning to prepare for the basic certification offered by AWS I.e. Cloud Practitioner.
It would be great if anyone can provide me resources to begin with. I’m more of a video person than book person so if you have any online MOOC course that will be perfect for my goal. Thanks!",t2_9ga7xr3c,False,,0,False,How to start preparing for AWS cloud practitioner certification,[],r/datascience,False,6,discussion,0,,,False,t3_o33taz,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1624089965.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a beginner and I’m planning on creating a career in cloud services. I currently have little to no knowledge on cloud platforms and I am wanting to learn more. As a first step I’m planning to prepare for the basic certification offered by AWS I.e. Cloud Practitioner.
It would be great if anyone can provide me resources to begin with. I’m more of a video person than book person so if you have any online MOOC course that will be perfect for my goal. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o33taz,True,,ElNinoo9,,2,True,all_ads,False,[],False,,/r/datascience/comments/o33taz/how_to_start_preparing_for_aws_cloud_practitioner/,all_ads,False,https://www.reddit.com/r/datascience/comments/o33taz/how_to_start_preparing_for_aws_cloud_practitioner/,515406,1624061165.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have CS Bachelors and am interested in going thru a curriculum or learning the core fundamentals of data engineering and gaining practical experience doing projects. I'm at stage 0 right now. Have reasonable familiarity with python, pandas, basic experience with sql (relational).

Does anyone have any resources (articles, online classes/courses or tracks, books, etc..) they can share so I may learn the core fundamentals for data engineering starting as a beginner? A lot of what I've seen online is like cloudera, google, aws, etc.. offering some classes tailored to their proprietary systems but idk which softwares/programs are most relevant in Data Engineering. So its hard to tell what is a good resource for learning core fundamentals versus Company A wanting to grow users in their proprietary ecosystem (which may not be used much by many companies).

**In short, I'd like to know what the most widely used skills/softwares/programs are for data engineering and what the best resources are to learn &amp; apply said learning pragmatically.** Ideally cheap/free but is not necessarily a requirement.

I appreciate any help in advance. :)",t2_yxicn,False,,0,False,Resources for Learning Data Engineering?,[],r/datascience,False,6,education,0,,,False,t3_o31p08,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,1624054919.0,,[],{},,True,,1624083481.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have CS Bachelors and am interested in going thru a curriculum or learning the core fundamentals of data engineering and gaining practical experience doing projects. I&amp;#39;m at stage 0 right now. Have reasonable familiarity with python, pandas, basic experience with sql (relational).&lt;/p&gt;

&lt;p&gt;Does anyone have any resources (articles, online classes/courses or tracks, books, etc..) they can share so I may learn the core fundamentals for data engineering starting as a beginner? A lot of what I&amp;#39;ve seen online is like cloudera, google, aws, etc.. offering some classes tailored to their proprietary systems but idk which softwares/programs are most relevant in Data Engineering. So its hard to tell what is a good resource for learning core fundamentals versus Company A wanting to grow users in their proprietary ecosystem (which may not be used much by many companies).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In short, I&amp;#39;d like to know what the most widely used skills/softwares/programs are for data engineering and what the best resources are to learn &amp;amp; apply said learning pragmatically.&lt;/strong&gt; Ideally cheap/free but is not necessarily a requirement.&lt;/p&gt;

&lt;p&gt;I appreciate any help in advance. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o31p08,True,,zeroskater45,,7,True,all_ads,False,[],False,,/r/datascience/comments/o31p08/resources_for_learning_data_engineering/,all_ads,False,https://www.reddit.com/r/datascience/comments/o31p08/resources_for_learning_data_engineering/,515406,1624054681.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"What software did the author use to create the wonderful visualizations in this Transformer writeup? I would love to recreate his work. 

[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)",t2_2ich5u4y,False,,0,False,ML Visualization Software,[],r/datascience,False,6,education,0,,,False,t3_o32wxx,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624087138.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What software did the author use to create the wonderful visualizations in this Transformer writeup? I would love to recreate his work. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://jalammar.github.io/illustrated-transformer/""&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o32wxx,True,,Stutoucan12,,4,True,all_ads,False,[],False,,/r/datascience/comments/o32wxx/ml_visualization_software/,all_ads,False,https://www.reddit.com/r/datascience/comments/o32wxx/ml_visualization_software/,515406,1624058338.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?auto=webp&amp;s=6f463f6bff513c6d017fa5538a00f4548a2dcfe1', 'width': 1436, 'height': 804}, 'resolutions': [{'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8a7e804fa309d37c49f00a733d9224f0be49337', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b16204a4e79f791d02291fe76812185b9020ae5f', 'width': 216, 'height': 120}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4574a3af653fcbecd95c46b4b76706aef9b447a1', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=236b712e35df9668aa6ac8750ddd6b46b6ce24cf', 'width': 640, 'height': 358}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78c4a9af8900e812f85c7fa529f53043e236922a', 'width': 960, 'height': 537}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c049be5f511c9d47b872329f22723838d608ee3a', 'width': 1080, 'height': 604}], 'variants': {}, 'id': 'c1mFuTYSarvnl5bzqbH7vE9qZMX_5O-mA_uV6IM3pBM'}], 'enabled': False}",,,,,
,datascience,"With \\things starting to reopen my company is restarting training and conference travelling.

I was taking a look at the Disney Conference on Data and Analytics. I know that most people find the Disney conferences very useful and I was wondering if anyone in DS had attended this conference and if so, what they thought of it.

[https://disneydataconference.com/](https://disneydataconference.com/)

FYI - not affiliated in with Disney in any manner, just a Canadian looking for a winter trip to somewhere warm ;)

Or are there any good ones looking to be in person in the fall? I know a lot of this may be still be subject to cancellations depending on what happens again in the fall.",t2_6nl3f37f,False,,0,False,Conferences for 2021 - Disney Conference,[],r/datascience,False,6,education,0,,,False,t3_o316ni,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1624081910.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With \things starting to reopen my company is restarting training and conference travelling.&lt;/p&gt;

&lt;p&gt;I was taking a look at the Disney Conference on Data and Analytics. I know that most people find the Disney conferences very useful and I was wondering if anyone in DS had attended this conference and if so, what they thought of it.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://disneydataconference.com/""&gt;https://disneydataconference.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;FYI - not affiliated in with Disney in any manner, just a Canadian looking for a winter trip to somewhere warm ;)&lt;/p&gt;

&lt;p&gt;Or are there any good ones looking to be in person in the fall? I know a lot of this may be still be subject to cancellations depending on what happens again in the fall.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o316ni,True,,data_chameleon,,1,True,all_ads,False,[],False,,/r/datascience/comments/o316ni/conferences_for_2021_disney_conference/,all_ads,False,https://www.reddit.com/r/datascience/comments/o316ni/conferences_for_2021_disney_conference/,515406,1624053110.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?auto=webp&amp;s=0cf4aa92a5ea30a8c9cd895126b35ed845bc94d4', 'width': 752, 'height': 395}, 'resolutions': [{'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61f23680e1b9a9b7e4d226f31a6dcedbf1f99866', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00f8a00c6ad70f30e19afcf16bbd310c645fed16', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e6b6b1d8c4344113fa620266387bc03acdd9b5b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/46G56dnnM3aepXXaDtw8NLkX3kULEiAQOx5RHFhEo78.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32f50b936d570fd5cd0f35ae6d6eb5487f2a84d5', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'K1siLmi-ySXB--VuJx92KjBEFmUnwSLBbgpH-DiI5kk'}], 'enabled': False}",,,,,
,datascience,"Hey all!

I setup a short survey **print(**[https://forms.gle/2XGL3q8RGpZjmLi78](https://forms.gle/2XGL3q8RGpZjmLi78)**)** to learn more about problems that other developers (data engineers, data scientists) face when it comes to documentation. It would help me out if you'd be willing to give some feedback about the challenges that you face. This is a problem that I face when working on new projects and on new teams and something that I want to build and solve for.

Challenges could include like: time that it takes to create, knowledge transfer of what you work on, having to explain to non-technical people vs. new developers.",t2_127oi7,False,,0,False,survey for data scientists: issues faced around documentation / on-boarding / KT,[],r/datascience,False,6,discussion,0,,,False,t3_o36h26,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624099770.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all!&lt;/p&gt;

&lt;p&gt;I setup a short survey &lt;strong&gt;print(&lt;/strong&gt;&lt;a href=""https://forms.gle/2XGL3q8RGpZjmLi78""&gt;https://forms.gle/2XGL3q8RGpZjmLi78&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt; to learn more about problems that other developers (data engineers, data scientists) face when it comes to documentation. It would help me out if you&amp;#39;d be willing to give some feedback about the challenges that you face. This is a problem that I face when working on new projects and on new teams and something that I want to build and solve for.&lt;/p&gt;

&lt;p&gt;Challenges could include like: time that it takes to create, knowledge transfer of what you work on, having to explain to non-technical people vs. new developers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o36h26,True,,guru223,,1,True,all_ads,False,[],False,,/r/datascience/comments/o36h26/survey_for_data_scientists_issues_faced_around/,all_ads,False,https://www.reddit.com/r/datascience/comments/o36h26/survey_for_data_scientists_issues_faced_around/,515406,1624070970.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?auto=webp&amp;s=6295ba82373dab020feb7eae12a61705ebe044bc', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ec2b17d66ce9ca67a1c87f9179de8e907e27aee', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17598c88788d482122b5cfe2a79163908774960d', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a6fd85b5bb7a6ba3992167181da32d340c39db1', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3db12eaf4eaace3c7ef8fe5e4ef3e7eaa1af42e', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42ae751cb04df084b056a18daaf2d7d07b8c4fe9', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/DLUel2kxUTVk_8lq4KC_rwEtADXa1JDfexiz2T3ytvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6073ac1b229d6db9325306f4b33bb59b544af50b', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '_mfYyVSEkOcMSL-fSbM3-1BCr-m3sawG63jbtsML9-s'}], 'enabled': False}",,,,,
,datascience,"Here's the hypothetical from the interviewer:

FB launched a Zoom-like feature. It was generally well-accepted and its usage is growing.

You work at Instagram. How would you evaluate if IG should add that Zoom-like feature?

(in other words, a synchronous communication app within a heterogeneous network (FB) is being evaluted for launch in an otherwise asynchronous homogenous network (IG).

My response:

Clarifying questions:

\- Can some people use the FB Zoom feature with a higher / different  access level than others?

\- What requirements, minimums, or thresholds must be achieved in order to obtain higher access (such as:

o a Facebook business page,

o a Facebook business page with  &gt;1,000 followers

o a Facebook user with &gt;500 edges (relationships since [Facebook can process one trillion edge graphs)](https://research.fb.com/publications/one-trillion-edges-graph-processing-at-facebook-scale/) which the people (nodes) desiring higher access must have people

\- Higher access might include: the ability to invite more than 20 people (20 being the number the hypothetical provided); the ability to place other companies’ advertisements in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; the ability to place advertisements of the host’s company in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; a longer Zoom meeting time (&gt;60 minutes for example)

(clarifying questions partially answered but  mostly deferred)

I would review the data from the A/B experiment on this feature's usage and adoption at FB. I imagine this A/B test would have a dependent variable / control group / training set in machine learning (ML) (user behavior before Zoom feature) and an independent variable / test group / test set in ML (user behavior after Zoom feature).  user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).

I'd also analyze datasets which tracked:

\- how many invitees became hosts and set up their own FB Zoom meeting within 1,2,3,4 and +4 weeks;

\- How many invitees purchased one of the host’s products that were advertised in the Zoom meeting?

\- How many invitees purchased one of the non-host products that were advertised in the Zoom meeting?

The A/B experiment I'd design at IG:

Dependent variable / control group / training set: IG user behavior before Zoom feature

Independent variable / test group / test set: IG user behavior after Zoom feature

user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).

Last, I'd run an SQL inner join of the networks of people who used FB Zoom  and the networks of those same people on IG. I'd use clustering ML (entropy weight k-means) to then look for trends or patterns which might explain why the Zoom feature was used more (or less) on FB than IG; which gender uses the Zoom feature more; which network (IG or FB) leads to more Zoom invites being sent out; do IG or FB Zoom meetings lead to more purchases? What type of purchases (category)? What price range? How does price, category and frequency of purchase vary based on GIS or location data of Zoom host and location of invitees? Do the data suggest this might be growing into something like the Influencers feature?",t2_80vvyzaz,False,,0,False,FAANG interview prep: A/B testing - please be merciless in your reply,[],r/datascience,False,6,,0,,,False,t3_o35vgb,False,dark,0.62,,public,2,0,{},,,False,[],,False,False,,{},Job Search,False,2,,False,False,self,1624109659.0,,[],{},,True,,1624097478.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here&amp;#39;s the hypothetical from the interviewer:&lt;/p&gt;

&lt;p&gt;FB launched a Zoom-like feature. It was generally well-accepted and its usage is growing.&lt;/p&gt;

&lt;p&gt;You work at Instagram. How would you evaluate if IG should add that Zoom-like feature?&lt;/p&gt;

&lt;p&gt;(in other words, a synchronous communication app within a heterogeneous network (FB) is being evaluted for launch in an otherwise asynchronous homogenous network (IG).&lt;/p&gt;

&lt;p&gt;My response:&lt;/p&gt;

&lt;p&gt;Clarifying questions:&lt;/p&gt;

&lt;p&gt;- Can some people use the FB Zoom feature with a higher / different  access level than others?&lt;/p&gt;

&lt;p&gt;- What requirements, minimums, or thresholds must be achieved in order to obtain higher access (such as:&lt;/p&gt;

&lt;p&gt;o a Facebook business page,&lt;/p&gt;

&lt;p&gt;o a Facebook business page with  &amp;gt;1,000 followers&lt;/p&gt;

&lt;p&gt;o a Facebook user with &amp;gt;500 edges (relationships since &lt;a href=""https://research.fb.com/publications/one-trillion-edges-graph-processing-at-facebook-scale/""&gt;Facebook can process one trillion edge graphs)&lt;/a&gt; which the people (nodes) desiring higher access must have people&lt;/p&gt;

&lt;p&gt;- Higher access might include: the ability to invite more than 20 people (20 being the number the hypothetical provided); the ability to place other companies’ advertisements in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; the ability to place advertisements of the host’s company in the Zoom invitation, the Zoom meeting, and/or the Zoom follow up notification; a longer Zoom meeting time (&amp;gt;60 minutes for example)&lt;/p&gt;

&lt;p&gt;(clarifying questions partially answered but  mostly deferred)&lt;/p&gt;

&lt;p&gt;I would review the data from the A/B experiment on this feature&amp;#39;s usage and adoption at FB. I imagine this A/B test would have a dependent variable / control group / training set in machine learning (ML) (user behavior before Zoom feature) and an independent variable / test group / test set in ML (user behavior after Zoom feature).  user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).&lt;/p&gt;

&lt;p&gt;I&amp;#39;d also analyze datasets which tracked:&lt;/p&gt;

&lt;p&gt;- how many invitees became hosts and set up their own FB Zoom meeting within 1,2,3,4 and +4 weeks;&lt;/p&gt;

&lt;p&gt;- How many invitees purchased one of the host’s products that were advertised in the Zoom meeting?&lt;/p&gt;

&lt;p&gt;- How many invitees purchased one of the non-host products that were advertised in the Zoom meeting?&lt;/p&gt;

&lt;p&gt;The A/B experiment I&amp;#39;d design at IG:&lt;/p&gt;

&lt;p&gt;Dependent variable / control group / training set: IG user behavior before Zoom feature&lt;/p&gt;

&lt;p&gt;Independent variable / test group / test set: IG user behavior after Zoom feature&lt;/p&gt;

&lt;p&gt;user behavior here would include the following variables which would be available in SQL tables for further analysis: frequency (did it increase or decrease after using this feature; in what ways did it increase or decrease (likes, comments, shares, marketplace (buying or selling), direct chat w/ other nodes (users), creation or removal of certain edges (relationships).&lt;/p&gt;

&lt;p&gt;Last, I&amp;#39;d run an SQL inner join of the networks of people who used FB Zoom  and the networks of those same people on IG. I&amp;#39;d use clustering ML (entropy weight k-means) to then look for trends or patterns which might explain why the Zoom feature was used more (or less) on FB than IG; which gender uses the Zoom feature more; which network (IG or FB) leads to more Zoom invites being sent out; do IG or FB Zoom meetings lead to more purchases? What type of purchases (category)? What price range? How does price, category and frequency of purchase vary based on GIS or location data of Zoom host and location of invitees? Do the data suggest this might be growing into something like the Influencers feature?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,o35vgb,True,,Objective-Patient-37,,3,True,all_ads,False,[],False,,/r/datascience/comments/o35vgb/faang_interview_prep_ab_testing_please_be/,all_ads,False,https://www.reddit.com/r/datascience/comments/o35vgb/faang_interview_prep_ab_testing_please_be/,515406,1624068678.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi All!

Could I get your opinions on the best value data science bootcamps out there? It's something I've looked at for a long time, but I've been hesitant to pick one because there seems to be so many of them.

The first one that comes to mind is General Assembly, and I also see Springboard. Let me know your experience with these and any others out there.

Thanks in advance!",t2_218h5k2r,False,,0,False,What Are The Best Data Science Bootcamps?,[],r/datascience,False,6,education,0,,,False,t3_o347ji,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1624091365.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All!&lt;/p&gt;

&lt;p&gt;Could I get your opinions on the best value data science bootcamps out there? It&amp;#39;s something I&amp;#39;ve looked at for a long time, but I&amp;#39;ve been hesitant to pick one because there seems to be so many of them.&lt;/p&gt;

&lt;p&gt;The first one that comes to mind is General Assembly, and I also see Springboard. Let me know your experience with these and any others out there.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o347ji,True,,RegularAd1,,7,True,all_ads,False,[],False,,/r/datascience/comments/o347ji/what_are_the_best_data_science_bootcamps/,all_ads,False,https://www.reddit.com/r/datascience/comments/o347ji/what_are_the_best_data_science_bootcamps/,515406,1624062565.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I'm a newbie who just started to learn data science via Kaggle courses and the above-said parameter was used when defining the model.   
e.g - `data_model = data_model.DecisionTreeRegressor(random_state=1)`

They mention that this was to get the same results every time. But even after I changed values, or just completely removed the parameter, the same results were generated, or at least that's what it seems like, looking at the `predictions.head()` 

So what's the use of that parameter here? Why do we even use it when it brings no change?

.

Also, please give me any advice you'd liek to as I've only started learning like a week ago.",t2_6phv9ewx,False,,0,False,What is the random_state parameter for DecisionTreeRegressor?,[],r/datascience,False,6,education,0,,,False,t3_o319vj,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1624082192.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a newbie who just started to learn data science via Kaggle courses and the above-said parameter was used when defining the model.&lt;br/&gt;
e.g - &lt;code&gt;data_model = data_model.DecisionTreeRegressor(random_state=1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;They mention that this was to get the same results every time. But even after I changed values, or just completely removed the parameter, the same results were generated, or at least that&amp;#39;s what it seems like, looking at the &lt;code&gt;predictions.head()&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;So what&amp;#39;s the use of that parameter here? Why do we even use it when it brings no change?&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Also, please give me any advice you&amp;#39;d liek to as I&amp;#39;ve only started learning like a week ago.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o319vj,True,,AtomicAnnihilator,,4,True,all_ads,False,[],False,,/r/datascience/comments/o319vj/what_is_the_random_state_parameter_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/o319vj/what_is_the_random_state_parameter_for/,515406,1624053392.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Non-native English speaker here. 

What is the difference between a Data Analyst and a Database Analyst? 

Is the occupation of a Data Analyst part of occupation unit group [2172--Database Analysts and Data Administrators](https://noc.esdc.gc.ca/Structure/NocProfile?objectid=xYpcSE6sKP672q64fVjEXwuRuW7o0pqx1PT2WTqtRT1u0YlGq3so6qLpM%2FO7M0Ic4ZDQ9YoZIGfvucsB9Lr7ohU%2BkbKG85oz8uuocyqCXV0%3D)? If not, which occupational unit does it fall into in the Canadian National Occupational Classification System?",t2_24o20tpg,False,,0,False,Is the occupation of a Data Analyst and Database Analyst same or are they different? Under what occupational group does a Data Analyst fall into?,[],r/datascience,False,6,career,0,,,False,t3_o2p1cp,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1624051508.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Non-native English speaker here. &lt;/p&gt;

&lt;p&gt;What is the difference between a Data Analyst and a Database Analyst? &lt;/p&gt;

&lt;p&gt;Is the occupation of a Data Analyst part of occupation unit group &lt;a href=""https://noc.esdc.gc.ca/Structure/NocProfile?objectid=xYpcSE6sKP672q64fVjEXwuRuW7o0pqx1PT2WTqtRT1u0YlGq3so6qLpM%2FO7M0Ic4ZDQ9YoZIGfvucsB9Lr7ohU%2BkbKG85oz8uuocyqCXV0%3D""&gt;2172--Database Analysts and Data Administrators&lt;/a&gt;? If not, which occupational unit does it fall into in the Canadian National Occupational Classification System?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2p1cp,True,,Experimentalphone,,4,True,all_ads,False,[],False,,/r/datascience/comments/o2p1cp/is_the_occupation_of_a_data_analyst_and_database/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2p1cp/is_the_occupation_of_a_data_analyst_and_database/,515406,1624022708.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello!

I am just finishing my first year at my first job after grad school. In the past year I have realized my manager most of the times overestimate the deadline of the task he gives me. I'm not talking about building ML models but Data cleaning, prep or mostly building dashboards. Just to give you a context on overestimation, sometimes he'll ask me to build a dashboard in a week but I end up developing in a day or two. It's actually nice because the rest of the time I use it for my upskilling.

Similar thing just happened today as well so I was just wondering if you guys also come across similar situations and what do you do in cases like these?

Thanks!",t2_bv171ji2,False,,0,False,How common is it for managers in Analytics field to overestimate the project/task deadline?,[],r/datascience,False,6,discussion,0,,,False,t3_o2x0t5,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624070326.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am just finishing my first year at my first job after grad school. In the past year I have realized my manager most of the times overestimate the deadline of the task he gives me. I&amp;#39;m not talking about building ML models but Data cleaning, prep or mostly building dashboards. Just to give you a context on overestimation, sometimes he&amp;#39;ll ask me to build a dashboard in a week but I end up developing in a day or two. It&amp;#39;s actually nice because the rest of the time I use it for my upskilling.&lt;/p&gt;

&lt;p&gt;Similar thing just happened today as well so I was just wondering if you guys also come across similar situations and what do you do in cases like these?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2x0t5,True,,quite--average,,7,True,all_ads,False,[],False,,/r/datascience/comments/o2x0t5/how_common_is_it_for_managers_in_analytics_field/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2x0t5/how_common_is_it_for_managers_in_analytics_field/,515406,1624041526.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. He's been having trouble with installing tensorflow on his M1 mac. I was sure there would be an online utility to do something like that without the need to train locally, but I can't seem to find a decent resource for him to use.

Any suggestions?",t2_4i1hm,False,,0,False,There has to be an online tool for text generation right?,[],r/datascience,False,6,discussion,0,,,False,t3_o2w4qb,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624068041.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. He&amp;#39;s been having trouble with installing tensorflow on his M1 mac. I was sure there would be an online utility to do something like that without the need to train locally, but I can&amp;#39;t seem to find a decent resource for him to use.&lt;/p&gt;

&lt;p&gt;Any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2w4qb,True,,AI52487963,,0,True,all_ads,False,[],False,,/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/,515406,1624039241.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What are others’ thoughts on paying your own way to attend conferences directly related to your job? Is it common practice?  Are there salary ranges or other criteria you consider when making this kind of choice?

Back story: I used to work as a government contractor and we were only allowed to attend one conference per year. On that salary, I really couldn’t afford to pay my own way so I only went and presented at one. I recently changed jobs to a small startup, with a 25% pay increase but no real benefits to speak of yet (VERY small and new startup). There’s a conference that is squarely in line with my job description and potentially really important because of both the subject matter and the focus.  The boss is leaning towards only sending one of us and having that person take screenshots. Honestly, I don’t think this is functional for two reasons: 1) if I’m at a conference I want to focus on it and not be taking screenshots like crazy for 4 days, and 2) if I’m not at the conference, screenshots are not going to be useful to me really.  So I’m considering just paying for it myself ($500).  It will mean I can’t get (right away) the new computer I also need for my job, but this conference is really important for both me and the person who signaled it to the boss (and would get to go...that’s only fair and I’m totally ok with it).  However, paying my own way also sends a signal that I’m happy to do this going forward - which I am not. Conferences are expensive, and as a W2 employee they are not tax-deductible. What would you guys do?",t2_47ysknnw,False,,0,False,Conferences - paying your own way,[],r/datascience,False,6,career,0,,,False,t3_o2vm51,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1624066738.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are others’ thoughts on paying your own way to attend conferences directly related to your job? Is it common practice?  Are there salary ranges or other criteria you consider when making this kind of choice?&lt;/p&gt;

&lt;p&gt;Back story: I used to work as a government contractor and we were only allowed to attend one conference per year. On that salary, I really couldn’t afford to pay my own way so I only went and presented at one. I recently changed jobs to a small startup, with a 25% pay increase but no real benefits to speak of yet (VERY small and new startup). There’s a conference that is squarely in line with my job description and potentially really important because of both the subject matter and the focus.  The boss is leaning towards only sending one of us and having that person take screenshots. Honestly, I don’t think this is functional for two reasons: 1) if I’m at a conference I want to focus on it and not be taking screenshots like crazy for 4 days, and 2) if I’m not at the conference, screenshots are not going to be useful to me really.  So I’m considering just paying for it myself ($500).  It will mean I can’t get (right away) the new computer I also need for my job, but this conference is really important for both me and the person who signaled it to the boss (and would get to go...that’s only fair and I’m totally ok with it).  However, paying my own way also sends a signal that I’m happy to do this going forward - which I am not. Conferences are expensive, and as a W2 employee they are not tax-deductible. What would you guys do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2vm51,True,,sovrappensiero1,,2,True,all_ads,False,[],False,,/r/datascience/comments/o2vm51/conferences_paying_your_own_way/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2vm51/conferences_paying_your_own_way/,515406,1624037938.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I often see in textbooks and blogs, when dealing with Bayesian Models, popular choices of priors seem to be well-known distributions (e.g. the gaussian prior). But how would you go about selecting priors for a specific model?

For example, suppose you are interested in making some bayesian model (e.g. for causal inference) on some medical data. One of your variables - 'smoking status', you have knowledge that patients who smoke vs patients who don't smoke have different life expectancies. 

Using historical data, how would you choose a ""prior"" for this model? 

Thanks",t2_o4xj9,False,,0,False,Has anyone had any experience selecting priors for Bayesian Models in real life?,[],r/datascience,False,6,discussion,0,,,False,t3_o2gg88,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1624018825.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I often see in textbooks and blogs, when dealing with Bayesian Models, popular choices of priors seem to be well-known distributions (e.g. the gaussian prior). But how would you go about selecting priors for a specific model?&lt;/p&gt;

&lt;p&gt;For example, suppose you are interested in making some bayesian model (e.g. for causal inference) on some medical data. One of your variables - &amp;#39;smoking status&amp;#39;, you have knowledge that patients who smoke vs patients who don&amp;#39;t smoke have different life expectancies. &lt;/p&gt;

&lt;p&gt;Using historical data, how would you choose a &amp;quot;prior&amp;quot; for this model? &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2gg88,True,,blueest,,10,True,all_ads,False,[],False,,/r/datascience/comments/o2gg88/has_anyone_had_any_experience_selecting_priors/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2gg88/has_anyone_had_any_experience_selecting_priors/,515406,1623990025.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What exactly does a data scientist at a company like Amazon or Google does? What are the must have skills for survival in the industry? And where should a complete noob startm,t2_4tfitpj6,False,,0,False,Getting started with data science,[],r/datascience,False,6,education,0,,,False,t3_o320dd,False,dark,0.3,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1624084479.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What exactly does a data scientist at a company like Amazon or Google does? What are the must have skills for survival in the industry? And where should a complete noob startm&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o320dd,True,,notrealkhushi,,0,True,all_ads,False,[],False,,/r/datascience/comments/o320dd/getting_started_with_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o320dd/getting_started_with_data_science/,515406,1624055679.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_63ay634e,False,,0,False,"Straight from Science Fiction! 🤯😍 ""robot can mimic varieties of human expressions across many human subjects""",[],r/datascience,False,6,projects,0,42.0,,False,t3_o30gtg,False,dark,0.17,,public,0,0,{},140.0,,False,[],,False,False,,{},Projects,False,0,,False,False,https://b.thumbs.redditmedia.com/8-hHr0Ij-JOzgti1uKlHhkv14DKpNauH6bs931CVvwY.jpg,False,,[],{},,False,,1624079782.0,text,6,,,text,self.LatestInML,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o30gtg,True,,cv2020br,,0,True,all_ads,False,[],False,,/r/datascience/comments/o30gtg/straight_from_science_fiction_robot_can_mimic/,all_ads,False,/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/,515406,1624050982.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""[link to paper](https://www.catalyzex.com/paper/arxiv:2105.12724)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1t39a8iw83671.png?width=1974&amp;format=png&amp;auto=webp&amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29\n\n👇 Free extension to get code for ML papers (❤️'d by Andrew Ng)\n\nChrome: https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil\n\nFirefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex"", 'author_fullname': 't2_63ay634e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Straight from Science Fiction! 🤯😍 ""robot can mimic varieties of human expressions across many human subjects""', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 42, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'1t39a8iw83671': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 32, 'x': 108, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed0cd1c5a3fcb3c54fe025a94affaa9c861d9a54'}, {'y': 64, 'x': 216, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734e006c59841135dbd8d8691e01f24e5c45c0e7'}, {'y': 96, 'x': 320, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c25af8dde7bb0e1ded71b622a55ae50572c9164c'}, {'y': 192, 'x': 640, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54a891e6ea076a294c25b718d3dd57d8b56f5454'}, {'y': 288, 'x': 960, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd6a64f8802c5b582896db4e0fd9e91ccc469ad4'}, {'y': 324, 'x': 1080, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7231d2c28aa14ba7d2d150ae3895a1416ec18cef'}], 's': {'y': 594, 'x': 1974, 'u': 'https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;format=png&amp;auto=webp&amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29'}, 'id': '1t39a8iw83671'}}, 'name': 't3_o304e9', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/8-hHr0Ij-JOzgti1uKlHhkv14DKpNauH6bs931CVvwY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1624078788.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.catalyzex.com/paper/arxiv:2105.12724""&gt;link to paper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29""&gt;https://preview.redd.it/1t39a8iw83671.png?width=1974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70bda2be51623f4a4eb58c0f8eed35d6ceaabc29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39;d by Andrew Ng)&lt;/p&gt;\n\n&lt;p&gt;Chrome: &lt;a href=""https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'o304e9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cv2020br', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/', 'subreddit_subscribers': 7080, 'created_utc': 1624049988.0, 'num_crossposts': 6, 'media': None, 'is_video': False}]",/r/LatestInML/comments/o304e9/straight_from_science_fiction_robot_can_mimic/,t3_o304e9,
,datascience,"Most of the stock price prediction tutorials use a single stock as example with LSTM models. Although stocks have been proved to be random walks and containing to use LSTM for real time predictions is ""not recommended"".

Say that you stuck to LSTM, how does it happen in real world for 10 different stocks (it could be 20,50, etc.)?

Do you train 10 different LSTM models? Or use a different approach? 

The Idea is to tell which amongst these 10 stocks would rise and which ones will dip.

Thanks",t2_2mmql89p,False,,0,False,Multiple stock predictions,[],r/datascience,False,6,discussion,0,,,False,t3_o2eq7y,False,dark,0.7,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1624013137.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most of the stock price prediction tutorials use a single stock as example with LSTM models. Although stocks have been proved to be random walks and containing to use LSTM for real time predictions is &amp;quot;not recommended&amp;quot;.&lt;/p&gt;

&lt;p&gt;Say that you stuck to LSTM, how does it happen in real world for 10 different stocks (it could be 20,50, etc.)?&lt;/p&gt;

&lt;p&gt;Do you train 10 different LSTM models? Or use a different approach? &lt;/p&gt;

&lt;p&gt;The Idea is to tell which amongst these 10 stocks would rise and which ones will dip.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2eq7y,True,,grid_world,,12,True,all_ads,False,[],False,,/r/datascience/comments/o2eq7y/multiple_stock_predictions/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2eq7y/multiple_stock_predictions/,515406,1623984337.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Does anyone know what are the most modern statistical models being used for time series analysis? I have heard of transformer and attention mechanisms models that are used for modelling sequential data - but these seem to be more relevant for modelling data from the NLP domain. When it comes to classical time series modelling (e.g. a vector of temperature measurements) : does anyone know what are some of the more modern models being used for this? I did some searching online : it seems like ARIMA style models were some of the first ones, followed by state space models/hidden markov, and the more recent ones being RNN and LSTM. 

Are LSTM and RNN the most modern models that are being used for classical time series problems?

Thanks",t2_xtuyc,False,,0,False,Modern Time Series Models,[],r/datascience,False,6,discussion,0,,,False,t3_o2i7b9,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1624025208.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know what are the most modern statistical models being used for time series analysis? I have heard of transformer and attention mechanisms models that are used for modelling sequential data - but these seem to be more relevant for modelling data from the NLP domain. When it comes to classical time series modelling (e.g. a vector of temperature measurements) : does anyone know what are some of the more modern models being used for this? I did some searching online : it seems like ARIMA style models were some of the first ones, followed by state space models/hidden markov, and the more recent ones being RNN and LSTM. &lt;/p&gt;

&lt;p&gt;Are LSTM and RNN the most modern models that are being used for classical time series problems?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2i7b9,True,,ottawalanguages,,2,True,all_ads,False,[],False,,/r/datascience/comments/o2i7b9/modern_time_series_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2i7b9/modern_time_series_models/,515406,1623996408.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am currently a sr. analyst doing research on business questions and support our DS team as a SME. It's a fairly relaxed but unsatisfying job. The pay is ok but enough. We will have hybrid model when going back and the commute is only 10 minutes.

I went to my (grad) school networking event, pitched myself to a company, and got offered a managerial role. My responsibility is building out data solutions such as dashboards, analytics, and ML models, in addition to overseeing a small data process/ETL team.

Asides from the increased workload, this role requires 5 days in-office and the commute is 40+ min one way. There's no plan to move to hybrid in near future. I also have no experience in managing a team, let alone doing that while building out data functions from scratch.

Now the plus side is, it's a 20% increase in total comp. I have a lot of say in how the data analytics will run. Lastly, the chance of managerial experience seems to be valuable and rare.

I'm not too worried about delivering the technical aspect of work. I have experience building out dashboard, db, and pipeline. I have also delivered 1 ML model into production and currently delivering another one, granted they are off-the-shelf models. I'll essentially be repeating what I had been doing, given that new company is in the same business.

I think the company is sincere in matching my ask and trusting me with a managerial role. That said, would you consider this a good move? Do you think there's a high chance of failure considering that I had no prior experience in managing and have to build a new function at the same time? Do you think it's worth the quality-of-life drop for advancement in career?",t2_qinw9,False,,0,False,Offered managerial role for likely a big hit on QOL,[],r/datascience,False,6,career,0,,,False,t3_o2ew30,False,dark,0.77,,public,7,0,{},,,False,[],,False,False,,{},Career,False,7,,False,False,self,False,,[],{},,True,,1624013670.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am currently a sr. analyst doing research on business questions and support our DS team as a SME. It&amp;#39;s a fairly relaxed but unsatisfying job. The pay is ok but enough. We will have hybrid model when going back and the commute is only 10 minutes.&lt;/p&gt;

&lt;p&gt;I went to my (grad) school networking event, pitched myself to a company, and got offered a managerial role. My responsibility is building out data solutions such as dashboards, analytics, and ML models, in addition to overseeing a small data process/ETL team.&lt;/p&gt;

&lt;p&gt;Asides from the increased workload, this role requires 5 days in-office and the commute is 40+ min one way. There&amp;#39;s no plan to move to hybrid in near future. I also have no experience in managing a team, let alone doing that while building out data functions from scratch.&lt;/p&gt;

&lt;p&gt;Now the plus side is, it&amp;#39;s a 20% increase in total comp. I have a lot of say in how the data analytics will run. Lastly, the chance of managerial experience seems to be valuable and rare.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not too worried about delivering the technical aspect of work. I have experience building out dashboard, db, and pipeline. I have also delivered 1 ML model into production and currently delivering another one, granted they are off-the-shelf models. I&amp;#39;ll essentially be repeating what I had been doing, given that new company is in the same business.&lt;/p&gt;

&lt;p&gt;I think the company is sincere in matching my ask and trusting me with a managerial role. That said, would you consider this a good move? Do you think there&amp;#39;s a high chance of failure considering that I had no prior experience in managing and have to build a new function at the same time? Do you think it&amp;#39;s worth the quality-of-life drop for advancement in career?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2ew30,True,,monkeyunited,,16,True,all_ads,False,[],False,,/r/datascience/comments/o2ew30/offered_managerial_role_for_likely_a_big_hit_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2ew30/offered_managerial_role_for_likely_a_big_hit_on/,515406,1623984870.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"PLEASE TELL ME IF THIS VIOLATES RULE 9 AND ILL REMOVE IT THANKS

So I really don't get what data science is, and how its different from statistics. I've read a bunch of websites but still can't come up with like a simple explanation of what it is. From my understanding, you get the meaning from data, while in statistics you make the data? Also, I don't get why coding is needed for data? What does an average day look like for a data scientist? Do you like to make the data and then say what it means or something? Sorry if I sound really dumb.",t2_84h8gjfg,False,,0,False,Can you guys help me understand what data science is?,[],r/datascience,False,6,discussion,0,,,False,t3_o2q9o8,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1624055010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;PLEASE TELL ME IF THIS VIOLATES RULE 9 AND ILL REMOVE IT THANKS&lt;/p&gt;

&lt;p&gt;So I really don&amp;#39;t get what data science is, and how its different from statistics. I&amp;#39;ve read a bunch of websites but still can&amp;#39;t come up with like a simple explanation of what it is. From my understanding, you get the meaning from data, while in statistics you make the data? Also, I don&amp;#39;t get why coding is needed for data? What does an average day look like for a data scientist? Do you like to make the data and then say what it means or something? Sorry if I sound really dumb.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2q9o8,True,,thumbsgloved,,6,True,all_ads,False,[],False,,/r/datascience/comments/o2q9o8/can_you_guys_help_me_understand_what_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2q9o8/can_you_guys_help_me_understand_what_data_science/,515406,1624026210.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone,

Did anyone work as a Data scientist in ecommerce industry? I am currently working with Bloomreach, which can provide full customer experience without really knowing advanced programming.

I am looking for something cheaper alternative tool for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. What tools/combination of tools are best for you?

I tried almost all CDP demos, but i want to know some experiences from e-commerce segment. For info, we are small agency focused on automation and data.",t2_31exp9bt,False,,0,False,CDP / XCDP for beginner data scientist,[],r/datascience,False,6,tooling,0,,,False,t3_o2jtgd,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1624031872.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;Did anyone work as a Data scientist in ecommerce industry? I am currently working with Bloomreach, which can provide full customer experience without really knowing advanced programming.&lt;/p&gt;

&lt;p&gt;I am looking for something cheaper alternative tool for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. What tools/combination of tools are best for you?&lt;/p&gt;

&lt;p&gt;I tried almost all CDP demos, but i want to know some experiences from e-commerce segment. For info, we are small agency focused on automation and data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2jtgd,True,,Sonny-Orkidea,,0,True,all_ads,False,[],False,,/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/,515407,1624003072.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_9xi78ijs,False,,0,False,How to Scrape Q&amp;A Sites like Quora,[],r/datascience,False,6,fun,0,47.0,,False,t3_o2jj7g,False,dark,0.4,,public,0,0,{},140.0,,False,[],,False,False,,{},Fun/Trivia,False,0,,False,False,https://b.thumbs.redditmedia.com/ZPruvwEjwMO1_p3Xg81GIuydJf5kHTp4sPMyISuq7Po.jpg,False,,[],{},,False,,1624030708.0,text,6,,,text,reddit.com,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o2jj7g,True,,Octoparseideas,,1,True,all_ads,False,[],False,,/r/datascience/comments/o2jj7g/how_to_scrape_qa_sites_like_quora/,all_ads,False,https://www.reddit.com/user/Octoparseideas/comments/o2jfp6/how_to_scrape_qa_sites_like_quora/?utm_source=share&amp;utm_medium=web2x&amp;context=3,515407,1624001908.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,https://www.reddit.com/user/Octoparseideas/comments/o2jfp6/how_to_scrape_qa_sites_like_quora/?utm_source=share&amp;utm_medium=web2x&amp;context=3,,
,datascience,"Mid Level Director in early 40s working in large Financial Data company making a decent living responsible for Change an Risk for a product within Operations Group.  Also have a small team of 3 Data Scientist/Engineers and double the headcount next year with focus on New Data Integration, Data Source/Scraping, RPA, AI/NLP Modeling and Advanced Analytics/Visualization with Tableau/PBI.  

While this is 1/3 of my responsibilities, its where my interest and curiosity lies.  My background is more Finance and Operations, intermediate SQL.  My involvement in all this is more around coming up with ideas, push on execution, project manage it all while tasking my guys on execution etc.  But because I know nothing about python, the different packages etc.  I always wanted to learn more.   Since I'm on an operations team and not Technology, are use cases are fairly basic etc but it's still fun to find opportunities to apply such tech to automate and reduce risk etc.   Our company offer tuition reimbursement of 20k per calendar year.  So I can pick any MSC degree over 3 calendar years but in 2 full years and it would be free.  I wouldnt do it for the ""starting Salary"" since I make above any MSC or MBA starting salary.  It's more for trying to further my career (sharpening the toolbox) in prep for any potential career/company changes if at all and staying relevant. 

Question for you guys who have done such degrees and are working in such field.  Would this make sense for me?  Should it be Data Science?  Or would Data Analytics make more sense based on what I do and what level I'm at?  Is DS too nitty gritty?  Too close to the actual work?  Would it further it further my career?  Considering I have a team of Data Scientists or Data Engineers etc, does it make sense to learn it all from the group up?  Thoughts?",t2_1lhphj92,False,,0,False,"40s Mid Level Manager in FinTech with Interest in DS, should I do a part time MS Degree?",[],r/datascience,False,6,education,0,,,False,t3_o22b58,False,dark,0.71,,public,7,0,{},,,False,[],,False,False,,{},Education,False,7,,False,False,self,False,,[],{},,True,,1623978757.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Mid Level Director in early 40s working in large Financial Data company making a decent living responsible for Change an Risk for a product within Operations Group.  Also have a small team of 3 Data Scientist/Engineers and double the headcount next year with focus on New Data Integration, Data Source/Scraping, RPA, AI/NLP Modeling and Advanced Analytics/Visualization with Tableau/PBI.  &lt;/p&gt;

&lt;p&gt;While this is 1/3 of my responsibilities, its where my interest and curiosity lies.  My background is more Finance and Operations, intermediate SQL.  My involvement in all this is more around coming up with ideas, push on execution, project manage it all while tasking my guys on execution etc.  But because I know nothing about python, the different packages etc.  I always wanted to learn more.   Since I&amp;#39;m on an operations team and not Technology, are use cases are fairly basic etc but it&amp;#39;s still fun to find opportunities to apply such tech to automate and reduce risk etc.   Our company offer tuition reimbursement of 20k per calendar year.  So I can pick any MSC degree over 3 calendar years but in 2 full years and it would be free.  I wouldnt do it for the &amp;quot;starting Salary&amp;quot; since I make above any MSC or MBA starting salary.  It&amp;#39;s more for trying to further my career (sharpening the toolbox) in prep for any potential career/company changes if at all and staying relevant. &lt;/p&gt;

&lt;p&gt;Question for you guys who have done such degrees and are working in such field.  Would this make sense for me?  Should it be Data Science?  Or would Data Analytics make more sense based on what I do and what level I&amp;#39;m at?  Is DS too nitty gritty?  Too close to the actual work?  Would it further it further my career?  Considering I have a team of Data Scientists or Data Engineers etc, does it make sense to learn it all from the group up?  Thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o22b58,True,,ddavid1101,,32,True,all_ads,False,[],False,,/r/datascience/comments/o22b58/40s_mid_level_manager_in_fintech_with_interest_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/o22b58/40s_mid_level_manager_in_fintech_with_interest_in/,515407,1623949957.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So I come from different industry (\~5 years experience working in energy) and then move to the new company in eCommerce since February (so technically I'm like a grad in eCommerce). I'm the first data scientist in the company, so there is no team and I likely spend a lot of time working with business guys. Before that, the company hired external consultants for analytics. And, the best I can have from them it's only Excel file, code, without any official business or technical documentation. So the only way to find out the logic calculation is to look at the code.

The business lead I usually reported to also take a maternity leave since April.

The company are in rush to produce Annual Customer Report on the my first day, which even after 4 months I hardly knows anything. Today this morning the top manager reviewed my performance, and they are not quite pleased with what I'm delivering. Then they decide not to extend my contract. They expect a data scientist to lead them to the place they want, coming up with ideas, questions and making decision. This is in my opinion a little bit more for business analysts who are already in the company for years, and also a little bit unrealisitc if the company do not have a proper technical team. (The good things is that the company uses SAS, and if they terminate then I can come back with Python.)

I wonder what's your thought in this situation? Usually, do you think 4 - 6 months are good time enough to onboard data scientists (if he didn't have any relevant domain knowledge before)?",t2_5czjyjhi,False,,0,False,"So, being fired after 4 month",[],r/datascience,False,6,career,0,,,False,t3_o1ulr0,False,dark,0.85,,public,23,0,{},,,False,[],,False,False,,{},Career,False,23,,False,False,self,1623929726.0,,[],{},,True,,1623956731.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I come from different industry (~5 years experience working in energy) and then move to the new company in eCommerce since February (so technically I&amp;#39;m like a grad in eCommerce). I&amp;#39;m the first data scientist in the company, so there is no team and I likely spend a lot of time working with business guys. Before that, the company hired external consultants for analytics. And, the best I can have from them it&amp;#39;s only Excel file, code, without any official business or technical documentation. So the only way to find out the logic calculation is to look at the code.&lt;/p&gt;

&lt;p&gt;The business lead I usually reported to also take a maternity leave since April.&lt;/p&gt;

&lt;p&gt;The company are in rush to produce Annual Customer Report on the my first day, which even after 4 months I hardly knows anything. Today this morning the top manager reviewed my performance, and they are not quite pleased with what I&amp;#39;m delivering. Then they decide not to extend my contract. They expect a data scientist to lead them to the place they want, coming up with ideas, questions and making decision. This is in my opinion a little bit more for business analysts who are already in the company for years, and also a little bit unrealisitc if the company do not have a proper technical team. (The good things is that the company uses SAS, and if they terminate then I can come back with Python.)&lt;/p&gt;

&lt;p&gt;I wonder what&amp;#39;s your thought in this situation? Usually, do you think 4 - 6 months are good time enough to onboard data scientists (if he didn&amp;#39;t have any relevant domain knowledge before)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o1ulr0,True,,vietlinh12hoa,,18,True,all_ads,False,[],False,,/r/datascience/comments/o1ulr0/so_being_fired_after_4_month/,all_ads,False,https://www.reddit.com/r/datascience/comments/o1ulr0/so_being_fired_after_4_month/,515407,1623927931.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," I graduated with a degree in ML/dataScience 2 years ago and since then been working for a small company as a ML Engineer in their R&amp;D team. Unfortunately, I’m not getting the growth I expected I would in this role. It could be because I’m the only ML engineer, no other data scientist or anyone working on the things I’m working on. 
 Additionally, there’s also no data engineers and no data pipelines, no cloud, the data is all scattered in files and folders. As mentioned, the company is not in Cloud yet, so no computing power either. Working as an ML engineer right out of bachelors and not having a team or even a mentor has been kind of rough to say the least. 

 So here I am asking for advise- I really am not sure what to do next. Whether to get an MS in a similar field (data analytics/ML) and then switch my job or to switch first and get some more experience and then think about doing MS. 

I can tell you where I want to be though- currently I’m doing core algorithm building and prediction modelling on moving parts but I think I want to venture towards the customer facing stuff – more on the lines of Market analysis or Product analysis. 

I guess I’m here because I need some advice- any advice helps. Tell me about your journey into ML/ data analytics- what has worked for you what hasn’t. Any certifications or online programs you recommend to thrive and get noticed for more opportunities and grow in the field. Any MS programs that are good and worth exploring. Any advice that allows me to figure out what to do next helps. 

Thank you for taking your time reading this and leaving a comment!",t2_11x3qj,False,,0,False,Recent graduate in Data Science,[],r/datascience,False,6,career,0,,,False,t3_o233zs,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1623980794.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I graduated with a degree in ML/dataScience 2 years ago and since then been working for a small company as a ML Engineer in their R&amp;amp;D team. Unfortunately, I’m not getting the growth I expected I would in this role. It could be because I’m the only ML engineer, no other data scientist or anyone working on the things I’m working on. 
 Additionally, there’s also no data engineers and no data pipelines, no cloud, the data is all scattered in files and folders. As mentioned, the company is not in Cloud yet, so no computing power either. Working as an ML engineer right out of bachelors and not having a team or even a mentor has been kind of rough to say the least. &lt;/p&gt;

&lt;p&gt;So here I am asking for advise- I really am not sure what to do next. Whether to get an MS in a similar field (data analytics/ML) and then switch my job or to switch first and get some more experience and then think about doing MS. &lt;/p&gt;

&lt;p&gt;I can tell you where I want to be though- currently I’m doing core algorithm building and prediction modelling on moving parts but I think I want to venture towards the customer facing stuff – more on the lines of Market analysis or Product analysis. &lt;/p&gt;

&lt;p&gt;I guess I’m here because I need some advice- any advice helps. Tell me about your journey into ML/ data analytics- what has worked for you what hasn’t. Any certifications or online programs you recommend to thrive and get noticed for more opportunities and grow in the field. Any MS programs that are good and worth exploring. Any advice that allows me to figure out what to do next helps. &lt;/p&gt;

&lt;p&gt;Thank you for taking your time reading this and leaving a comment!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o233zs,True,,goketchumall,,10,True,all_ads,False,[],False,,/r/datascience/comments/o233zs/recent_graduate_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/o233zs/recent_graduate_in_data_science/,515407,1623951994.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all,

The project that I’m working right now is about image recognition. For the data pipeline we are using the following pipeline:
- Take photos with our cellphones
- Divide them into batches of 100 (called tasks), Upload them into CVAT (hosted in a aws machine) and label them
- Download the images/labels to our local machines
- Add some additional metadata to the labels and upload them to a aws s3 bucket. 

This has some manual labor as there is label happening all the time and it would be desirable that the s3 bucket would have the updated information without much of an hassle. 

What would you suggest in this case? What tools that you would think to automate this process?

I think that the bottleneck here is CVAT which only works with the creation of “tasks” and each one of them can’t have a lot of images since it will make the application slow. 

Much thanks.",t2_e7fj1,False,,0,False,What would be the best workflow for this image dataset use case,[],r/datascience,False,6,discussion,0,,,False,t3_o271l3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623990653.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;The project that I’m working right now is about image recognition. For the data pipeline we are using the following pipeline:
- Take photos with our cellphones
- Divide them into batches of 100 (called tasks), Upload them into CVAT (hosted in a aws machine) and label them
- Download the images/labels to our local machines
- Add some additional metadata to the labels and upload them to a aws s3 bucket. &lt;/p&gt;

&lt;p&gt;This has some manual labor as there is label happening all the time and it would be desirable that the s3 bucket would have the updated information without much of an hassle. &lt;/p&gt;

&lt;p&gt;What would you suggest in this case? What tools that you would think to automate this process?&lt;/p&gt;

&lt;p&gt;I think that the bottleneck here is CVAT which only works with the creation of “tasks” and each one of them can’t have a lot of images since it will make the application slow. &lt;/p&gt;

&lt;p&gt;Much thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o271l3,True,,NewKlear09,,0,True,all_ads,False,[],False,,/r/datascience/comments/o271l3/what_would_be_the_best_workflow_for_this_image/,all_ads,False,https://www.reddit.com/r/datascience/comments/o271l3/what_would_be_the_best_workflow_for_this_image/,515407,1623961853.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all. I’m working on a project that requires a story or vision. I don’t want it to look crude but don’t need to spend a ton of time on it. What’s a good compromise to start with?

I figure a good starting point is a step up from PowerPoint. No disrespect to PPT but…

Thanks!",t2_2o0q5m4h,False,,0,False,How do you create storyboards?,[],r/datascience,False,6,tooling,0,,,False,t3_o29mq6,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1623997503.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I’m working on a project that requires a story or vision. I don’t want it to look crude but don’t need to spend a ton of time on it. What’s a good compromise to start with?&lt;/p&gt;

&lt;p&gt;I figure a good starting point is a step up from PowerPoint. No disrespect to PPT but…&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o29mq6,True,,rotterdamn8,,5,True,all_ads,False,[],False,,/r/datascience/comments/o29mq6/how_do_you_create_storyboards/,all_ads,False,https://www.reddit.com/r/datascience/comments/o29mq6/how_do_you_create_storyboards/,515407,1623968703.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"So I'm actually applying to Data Engineering jobs, so may not even be relevant.

However I'm never sure when it's okay to actually add something to your resume.

In the case of R, I only spent maybe a couple months on it. I learned the Tidyverse package, including Dplyr.

I also created a single project which included various graphs, along with decision trees, linear regression, and clustering algorithms using the caret package.

But I wouldn't say I'm proficient I don't think (I'm just good at Googling stuff).

I don't know if it's still okay to just stick it in with my list of skills.",t2_cmy227mg,False,,0,False,How good at R or Python do you have to be before you add it to your resume?,[],r/datascience,False,6,,0,,,False,t3_o12a74,False,dark,0.95,,public,296,0,{},,,False,[],,False,False,,{},Job Search,False,296,,False,False,self,False,,[],{},,True,,1623870317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m actually applying to Data Engineering jobs, so may not even be relevant.&lt;/p&gt;

&lt;p&gt;However I&amp;#39;m never sure when it&amp;#39;s okay to actually add something to your resume.&lt;/p&gt;

&lt;p&gt;In the case of R, I only spent maybe a couple months on it. I learned the Tidyverse package, including Dplyr.&lt;/p&gt;

&lt;p&gt;I also created a single project which included various graphs, along with decision trees, linear regression, and clustering algorithms using the caret package.&lt;/p&gt;

&lt;p&gt;But I wouldn&amp;#39;t say I&amp;#39;m proficient I don&amp;#39;t think (I&amp;#39;m just good at Googling stuff).&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s still okay to just stick it in with my list of skills.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,o12a74,True,,SWE-Aaron,,107,True,all_ads,False,[],False,,/r/datascience/comments/o12a74/how_good_at_r_or_python_do_you_have_to_be_before/,all_ads,False,https://www.reddit.com/r/datascience/comments/o12a74/how_good_at_r_or_python_do_you_have_to_be_before/,515407,1623841517.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,When using XGBregressor to predict propensity scores on a binary classification (eg propensity to convert/churn etc). Would the best performance metric be a regression metric like mae or a classification metric like log loss?,t2_3wvr93n,False,,0,False,Best performance metric for XGBregressor?,[],r/datascience,False,6,discussion,0,,,False,t3_o26s33,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623989971.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When using XGBregressor to predict propensity scores on a binary classification (eg propensity to convert/churn etc). Would the best performance metric be a regression metric like mae or a classification metric like log loss?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o26s33,True,,andrew__jason,,5,True,all_ads,False,[],False,,/r/datascience/comments/o26s33/best_performance_metric_for_xgbregressor/,all_ads,False,https://www.reddit.com/r/datascience/comments/o26s33/best_performance_metric_for_xgbregressor/,515407,1623961171.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’ve been working in this field at a consulting firm for 2 years now, and a question that always rests heavy on my mind is whether I should be applying my skills to a sector that has more direct social impact. The need for data analytics/science seems to follow the money trail, as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it. 

There are definitely options to do data science in a more impactful way (e.g. non-profits, environmental data, life sciences data), but I have the overall impression that these fields pay less. More importantly, it can be frustrating to do DS for due to the lack of work done previously, leading to messy and lots of unclean/scattered data. In other words, because the majority of people take the higher paying jobs over the ones with higher social impact, the high social impact data roles and ecosystems remain underdeveloped.

My question is: how do you grapple with this reality?

Do you grind at a corporate job and donate part of your income? Do you teach on the side? Answer a lot of Stack Overflow questions? Invest it in your children’s future? Some pro bono DS work for non-profits in your free time? 

Or just ignore all of it, because life is short, and you worked incredibly hard to get to where you’re at?",t2_138g5d,False,,0,False,Grappling with the social impact of data-related careers,[],r/datascience,False,6,discussion,0,,,False,t3_o1abco,False,dark,0.79,,public,16,1,{},,,False,[],,False,False,,{},Discussion,False,16,,False,False,self,False,,[],{},,True,,1623893319.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’ve been working in this field at a consulting firm for 2 years now, and a question that always rests heavy on my mind is whether I should be applying my skills to a sector that has more direct social impact. The need for data analytics/science seems to follow the money trail, as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it. &lt;/p&gt;

&lt;p&gt;There are definitely options to do data science in a more impactful way (e.g. non-profits, environmental data, life sciences data), but I have the overall impression that these fields pay less. More importantly, it can be frustrating to do DS for due to the lack of work done previously, leading to messy and lots of unclean/scattered data. In other words, because the majority of people take the higher paying jobs over the ones with higher social impact, the high social impact data roles and ecosystems remain underdeveloped.&lt;/p&gt;

&lt;p&gt;My question is: how do you grapple with this reality?&lt;/p&gt;

&lt;p&gt;Do you grind at a corporate job and donate part of your income? Do you teach on the side? Answer a lot of Stack Overflow questions? Invest it in your children’s future? Some pro bono DS work for non-profits in your free time? &lt;/p&gt;

&lt;p&gt;Or just ignore all of it, because life is short, and you worked incredibly hard to get to where you’re at?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'award_1f0462ee-18f5-4f33-89cf-f1f79336a452', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=16&amp;height=16&amp;auto=webp&amp;s=3ca7dc1f4e12ca386a561446e72f772d38ba49d8', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=32&amp;height=32&amp;auto=webp&amp;s=c19d1e661e4aa6a9326a9f0b74b3ebf5d9f7a75e', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=48&amp;height=48&amp;auto=webp&amp;s=ed063580825e72b0ae63fe30c807b453b1362694', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=64&amp;height=64&amp;auto=webp&amp;s=7176b4b72b850e3e052138fe8b3967c4c5b52dae', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=128&amp;height=128&amp;auto=webp&amp;s=f7b307840995777f9ae04699d019740658ba0e77', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing. Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome (Pro)', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=16&amp;height=16&amp;auto=webp&amp;s=3ca7dc1f4e12ca386a561446e72f772d38ba49d8', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=32&amp;height=32&amp;auto=webp&amp;s=c19d1e661e4aa6a9326a9f0b74b3ebf5d9f7a75e', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=48&amp;height=48&amp;auto=webp&amp;s=ed063580825e72b0ae63fe30c807b453b1362694', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=64&amp;height=64&amp;auto=webp&amp;s=7176b4b72b850e3e052138fe8b3967c4c5b52dae', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png?width=128&amp;height=128&amp;auto=webp&amp;s=f7b307840995777f9ae04699d019740658ba0e77', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/0o2j782f00e41_WholesomeSuperpro.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o1abco,True,,SubtleCoconut,,22,True,all_ads,False,[],False,,/r/datascience/comments/o1abco/grappling_with_the_social_impact_of_datarelated/,all_ads,False,https://www.reddit.com/r/datascience/comments/o1abco/grappling_with_the_social_impact_of_datarelated/,515407,1623864519.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

I have worked with R as my primary language dealing with everything from econometrics/ML applications, GPS analysis using APIs, NLP contract analysis and dealing with unstructured data. I would say if I have a problem and can use R then I can likely devise some way to solve it.

&amp;#x200B;

However, I want to move into a new role and I am finding despite my 5+ years with R as my primary language that Python is more preferred when I talk with recruiters, so I have picked it up and I was hoping to get some guidance with good examples of code. In R it is very easy and code 'flows' between statements using the pipe operator, but python I am finding it less intuitive from trial and error to do multistep aggregations, data summarization, etc. 

&amp;#x200B;

Anyone have a good guide of how to use python for complex data summaries? I want to be able to something like ifelse(col1 %in% \[""phrase1"", ""phrase2""\], Col2, NA) and get a count of how many unique values pass that logic. And I might have a half dozen conditional things like this as I have to evaluate many similar statistics for different time periods quite often. Kaggle has some data cleaning, but that data is perfect compared to some of my sources as a consultant.

&amp;#x200B;

&amp;#x200B;

Obviously this below chunk of code should be broken up across multiple lines, correct?

`df_BorState = df[df.BorrowerCity==""San Francisco""].groupby( [""BorrowerState"", ""BorrowerCity"", ""BorrowerZip""] ).agg( Tot_Amount = ('CurrentApprovalAmount', 'sum') ).sort_values(""Tot_Amount"", ascending = False)`

&amp;#x200B;

Is something like this standardized?

`df_BorState = df[df.BorrowerCity==""San Francisco""].groupby(` 

`[""BorrowerState"", ""BorrowerCity"", ""BorrowerZip""] ).agg(` 

`Tot_Amount = ('CurrentApprovalAmount', 'sum') ).sort_values(`

`""Tot_Amount"", ascending = False)`",t2_6phog,False,,0,False,Data Wrangling - Multiline Python Statements? Trying to learn best syntax for Python coming from R,[],r/datascience,False,6,projects,0,,,False,t3_o18wu3,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},Projects,False,7,,False,False,self,False,,[],{},,True,,1623889817.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have worked with R as my primary language dealing with everything from econometrics/ML applications, GPS analysis using APIs, NLP contract analysis and dealing with unstructured data. I would say if I have a problem and can use R then I can likely devise some way to solve it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, I want to move into a new role and I am finding despite my 5+ years with R as my primary language that Python is more preferred when I talk with recruiters, so I have picked it up and I was hoping to get some guidance with good examples of code. In R it is very easy and code &amp;#39;flows&amp;#39; between statements using the pipe operator, but python I am finding it less intuitive from trial and error to do multistep aggregations, data summarization, etc. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyone have a good guide of how to use python for complex data summaries? I want to be able to something like ifelse(col1 %in% [&amp;quot;phrase1&amp;quot;, &amp;quot;phrase2&amp;quot;], Col2, NA) and get a count of how many unique values pass that logic. And I might have a half dozen conditional things like this as I have to evaluate many similar statistics for different time periods quite often. Kaggle has some data cleaning, but that data is perfect compared to some of my sources as a consultant.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Obviously this below chunk of code should be broken up across multiple lines, correct?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df_BorState = df[df.BorrowerCity==&amp;quot;San Francisco&amp;quot;].groupby( [&amp;quot;BorrowerState&amp;quot;, &amp;quot;BorrowerCity&amp;quot;, &amp;quot;BorrowerZip&amp;quot;] ).agg( Tot_Amount = (&amp;#39;CurrentApprovalAmount&amp;#39;, &amp;#39;sum&amp;#39;) ).sort_values(&amp;quot;Tot_Amount&amp;quot;, ascending = False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is something like this standardized?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df_BorState = df[df.BorrowerCity==&amp;quot;San Francisco&amp;quot;].groupby(&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;[&amp;quot;BorrowerState&amp;quot;, &amp;quot;BorrowerCity&amp;quot;, &amp;quot;BorrowerZip&amp;quot;] ).agg(&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tot_Amount = (&amp;#39;CurrentApprovalAmount&amp;#39;, &amp;#39;sum&amp;#39;) ).sort_values(&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;Tot_Amount&amp;quot;, ascending = False)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o18wu3,True,,Unhelpful_Scientist,,14,True,all_ads,False,[],False,,/r/datascience/comments/o18wu3/data_wrangling_multiline_python_statements_trying/,all_ads,False,https://www.reddit.com/r/datascience/comments/o18wu3/data_wrangling_multiline_python_statements_trying/,515407,1623861017.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"EDIT:  /u/pedrosorio Makes good points.  I've changed the prior rankings to reflect the team ELOs from the start of qualifying, and have also included UEFA nations games.  What I really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  The model is good enough for me for now.  I appreciate all your comments though, so please share.

Last week, I posted some predictions for the 2020 Euro.  Now that the first round is over, we can examine some of my performance.

My predictions and results for the first round are shown in [this](https://i.imgur.com/qkY6SQc.png) table (sorry it isn't prettier).  I achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  My multiclass ROC for predicting the outcome is 0.77.  In short, in the first 12 games I perform slightly better than random guessing (which is honestly fine for me).  However, most people who have watched international football wouldn't assign all match events as equally likely (is Italy drawing Turkey really as probable as Italy losing to Turkey?  No).  Its hard for me to measure against a ""reasonable guesser"".  My work pool records all our guesses, and so at the end of the group stage I can use that as a sort of ensemble method to compare against.  We'll see.

[Here](https://i.imgur.com/mZaivJY.png) are match predictions for the remaining group stage games conditioned on the results of the first games.  The model is not perfect, and still makes some weird predictions.  For example, Portugal is given higher probability to beat France than they are to beat Germany even though France beat Germany in the first round.  If you subscribe to some sort of sports law of transitivity, this may sound weird.

My predictions for the second round and the results of the first can be found [here](https://github.com/Dpananos/Euro2021Predictions/tree/main/predictions).",t2_131vu3d,False,,0,False,Euro 2020 Predictions Update,[],r/datascience,False,6,discussion,0,,,False,t3_o13xow,False,dark,0.85,,public,14,0,{},,,False,[],,False,False,,{},Discussion,False,14,,False,False,self,1623862907.0,modflair,[],{},,True,,1623876063.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;EDIT:  &lt;a href=""/u/pedrosorio""&gt;/u/pedrosorio&lt;/a&gt; Makes good points.  I&amp;#39;ve changed the prior rankings to reflect the team ELOs from the start of qualifying, and have also included UEFA nations games.  What I really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  The model is good enough for me for now.  I appreciate all your comments though, so please share.&lt;/p&gt;

&lt;p&gt;Last week, I posted some predictions for the 2020 Euro.  Now that the first round is over, we can examine some of my performance.&lt;/p&gt;

&lt;p&gt;My predictions and results for the first round are shown in &lt;a href=""https://i.imgur.com/qkY6SQc.png""&gt;this&lt;/a&gt; table (sorry it isn&amp;#39;t prettier).  I achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  My multiclass ROC for predicting the outcome is 0.77.  In short, in the first 12 games I perform slightly better than random guessing (which is honestly fine for me).  However, most people who have watched international football wouldn&amp;#39;t assign all match events as equally likely (is Italy drawing Turkey really as probable as Italy losing to Turkey?  No).  Its hard for me to measure against a &amp;quot;reasonable guesser&amp;quot;.  My work pool records all our guesses, and so at the end of the group stage I can use that as a sort of ensemble method to compare against.  We&amp;#39;ll see.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.imgur.com/mZaivJY.png""&gt;Here&lt;/a&gt; are match predictions for the remaining group stage games conditioned on the results of the first games.  The model is not perfect, and still makes some weird predictions.  For example, Portugal is given higher probability to beat France than they are to beat Germany even though France beat Germany in the first round.  If you subscribe to some sort of sports law of transitivity, this may sound weird.&lt;/p&gt;

&lt;p&gt;My predictions for the second round and the results of the first can be found &lt;a href=""https://github.com/Dpananos/Euro2021Predictions/tree/main/predictions""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,Data Scientist,[],False,,,,t5_2sptq,,,,o13xow,True,,__compactsupport__,,9,True,all_ads,False,[],False,dark,/r/datascience/comments/o13xow/euro_2020_predictions_update/,all_ads,False,https://www.reddit.com/r/datascience/comments/o13xow/euro_2020_predictions_update/,515407,1623847263.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?auto=webp&amp;s=913ae94abd4249ac0a8a5ef9514b9ecd6ceca699', 'width': 1974, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4826127422dcc33c904fdcfb14d072c69817bb3d', 'width': 108, 'height': 22}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=923efe3de1aa23f8264eb6160d91d6e8a2dc57d9', 'width': 216, 'height': 45}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61db3dd1f07d2ce17d4e8772e879653c0b0daa88', 'width': 320, 'height': 68}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4aa2160d74c2461538cc50bf34a3cdff25daae9e', 'width': 640, 'height': 136}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=400bd8683be55b7d294d2e6f8febc13b553231ce', 'width': 960, 'height': 204}, {'url': 'https://external-preview.redd.it/JIQZcK9XR1R-EUzcm-_RAglAzxwBk5BXLz4K2KKKOrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37b16a58d5f1a7c84fb8ad12cb25743e5f6237dd', 'width': 1080, 'height': 229}], 'variants': {}, 'id': 'Q3ECQTNHwHDcXOJH3MMWHh6eWyeUz-FxweRRKGRtIWU'}], 'enabled': False}",,,,,
,datascience,"Hello!

Sorry for another R, Python post. 

Recently I got a task at work in which I had to read through multiple sheets of excel, clean, transform, reshape it and make it into a single dataframe. I hadn't done this type of task in either R or Python. Since, it was not really a time constraint task, I decided to do it in both and learn how to do it in both languages. I am better with R than Python. I barely know Python actually. So I started doing it in R and comfortably (with google) did it without taking much time. After that I tried it in Python but I'm still struggling to finish it. I will be able to do it but it's taking me significantly more time than R.

Is that just the learning curve of Python since I barely know the language or some things are just easier in R and I should just do it in the language I'm comfortable with? I'm afraid that that I'll never be able to learn Python like this and won't getting any interviews since I don't know how to do stuff in Python.

Thanks! Sorry for the long post.",t2_bv171ji2,False,,0,False,Does knowing R instead of Python makes you unhireable?,[],r/datascience,False,6,discussion,0,,,False,t3_o0neg0,False,dark,0.86,,public,260,2,{},,,False,[],,False,False,,{},Discussion,False,260,,False,False,self,False,,[],{},,True,,1623816582.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Sorry for another R, Python post. &lt;/p&gt;

&lt;p&gt;Recently I got a task at work in which I had to read through multiple sheets of excel, clean, transform, reshape it and make it into a single dataframe. I hadn&amp;#39;t done this type of task in either R or Python. Since, it was not really a time constraint task, I decided to do it in both and learn how to do it in both languages. I am better with R than Python. I barely know Python actually. So I started doing it in R and comfortably (with google) did it without taking much time. After that I tried it in Python but I&amp;#39;m still struggling to finish it. I will be able to do it but it&amp;#39;s taking me significantly more time than R.&lt;/p&gt;

&lt;p&gt;Is that just the learning curve of Python since I barely know the language or some things are just easier in R and I should just do it in the language I&amp;#39;m comfortable with? I&amp;#39;m afraid that that I&amp;#39;ll never be able to learn Python like this and won&amp;#39;t getting any interviews since I don&amp;#39;t know how to do stuff in Python.&lt;/p&gt;

&lt;p&gt;Thanks! Sorry for the long post.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0neg0,True,,quite--average,,182,True,all_ads,False,[],False,,/r/datascience/comments/o0neg0/does_knowing_r_instead_of_python_makes_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0neg0/does_knowing_r_instead_of_python_makes_you/,515407,1623787782.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Suppose you have a predictive model that has been used for a few years - at what point do people decide to make changes to the model (e.g. add new variables, retrain the model with new data)? Is there a standard procedure for doing this? Are old and new models often run in parallel? 

I would be curious to hear how this problem is being handled across the industry.

Thanks",t2_o4xj9,False,,0,False,When to update models?,[],r/datascience,False,6,discussion,0,,,False,t3_o17b6n,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623885520.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you have a predictive model that has been used for a few years - at what point do people decide to make changes to the model (e.g. add new variables, retrain the model with new data)? Is there a standard procedure for doing this? Are old and new models often run in parallel? &lt;/p&gt;

&lt;p&gt;I would be curious to hear how this problem is being handled across the industry.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o17b6n,True,,blueest,,9,True,all_ads,False,[],False,,/r/datascience/comments/o17b6n/when_to_update_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/o17b6n/when_to_update_models/,515407,1623856720.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Do you think that remote working for data science or analytics roles will be a thing post-covid too? I know that it cannot be like the past year that every job was remote, but will many jobs keep the remote status in the following years regardless of covid?",t2_2h9gdmeu,False,,0,False,remote work,[],r/datascience,False,6,discussion,0,,,False,t3_o11rn9,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623868311.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you think that remote working for data science or analytics roles will be a thing post-covid too? I know that it cannot be like the past year that every job was remote, but will many jobs keep the remote status in the following years regardless of covid?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o11rn9,True,,Hipocampus777,,9,True,all_ads,False,[],False,,/r/datascience/comments/o11rn9/remote_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/o11rn9/remote_work/,515407,1623839511.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a list of x,y coordinates  that represent the location of cells in a slice of tissue.  I can generate a scatterplot of that data and make a map.  Now I want to remove a bunch of those points because they are not trustworthy.   I know how to do that 1 point at a time, but I have thousands of points that need to be removed on each of hundreds of scatterplots.  

Do you know if there is any way to highlight a lot of points at once on a scatterplot and then just delete them from the data set?

It doesn't matter what program/platform is used to do this.

It seems like a straightforward question but I can't find any way to do it, nor anyone who does.",t2_3u89r,False,,0,False,How to remove data from a scatterplot,[],r/datascience,False,6,discussion,0,,,False,t3_o0ztez,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623859863.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a list of x,y coordinates  that represent the location of cells in a slice of tissue.  I can generate a scatterplot of that data and make a map.  Now I want to remove a bunch of those points because they are not trustworthy.   I know how to do that 1 point at a time, but I have thousands of points that need to be removed on each of hundreds of scatterplots.  &lt;/p&gt;

&lt;p&gt;Do you know if there is any way to highlight a lot of points at once on a scatterplot and then just delete them from the data set?&lt;/p&gt;

&lt;p&gt;It doesn&amp;#39;t matter what program/platform is used to do this.&lt;/p&gt;

&lt;p&gt;It seems like a straightforward question but I can&amp;#39;t find any way to do it, nor anyone who does.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0ztez,True,,synzeta,,19,True,all_ads,False,[],False,,/r/datascience/comments/o0ztez/how_to_remove_data_from_a_scatterplot/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0ztez/how_to_remove_data_from_a_scatterplot/,515407,1623831063.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

As the title suggests, off late, I have been struggling to communicate with non-technical stakeholders efficiently. For example, there was this one time where one of the stakeholders asked me to explain what a model did and I ended up combining a few technical terms together in a sentence which ended up confusing the stakeholder even more.

It is not that I suck at communicating in general. If I were to have a conversation regarding a simple analysis/explaining some viz, I do it pretty well. But, when it comes to explaining something more technical to someone with a non-technical background (like the inner workings of the model), I mess up big time.

My initial thought was that I might be messing up the explanations because I don't understand the models well enough, and I have been reading up all the basics from scratch in hopes that it would help me become better at explaining concepts. 

While I do continue my re-reading of the basics, are there any other ways that I could improve my technical explanations? Thanks!",t2_68s0c143,False,,0,False,How does one become a better communicator?,[],r/datascience,False,6,career,0,,,False,t3_o0koo8,False,dark,0.9,,public,42,0,{},,,False,[],,False,False,,{},Career,False,42,,False,False,self,False,,[],{},,True,,1623809367.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;As the title suggests, off late, I have been struggling to communicate with non-technical stakeholders efficiently. For example, there was this one time where one of the stakeholders asked me to explain what a model did and I ended up combining a few technical terms together in a sentence which ended up confusing the stakeholder even more.&lt;/p&gt;

&lt;p&gt;It is not that I suck at communicating in general. If I were to have a conversation regarding a simple analysis/explaining some viz, I do it pretty well. But, when it comes to explaining something more technical to someone with a non-technical background (like the inner workings of the model), I mess up big time.&lt;/p&gt;

&lt;p&gt;My initial thought was that I might be messing up the explanations because I don&amp;#39;t understand the models well enough, and I have been reading up all the basics from scratch in hopes that it would help me become better at explaining concepts. &lt;/p&gt;

&lt;p&gt;While I do continue my re-reading of the basics, are there any other ways that I could improve my technical explanations? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0koo8,True,,poplunoir,,23,True,all_ads,False,[],False,,/r/datascience/comments/o0koo8/how_does_one_become_a_better_communicator/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0koo8/how_does_one_become_a_better_communicator/,515407,1623780567.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"at a first glance LP seems promising...but honestly the techniques seem really dated....there must be modern tools vs using a simplex method to optimize.

appreciate any insight, and experiences shared

thanks",t2_4jdxx,False,,0,False,Is linear programming part of DS?,[],r/datascience,False,6,discussion,0,,,False,t3_o06z5u,False,dark,0.88,,public,128,0,{},,,False,[],,False,False,,{},Discussion,False,128,,False,False,self,False,,[],{},,True,,1623764863.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;at a first glance LP seems promising...but honestly the techniques seem really dated....there must be modern tools vs using a simplex method to optimize.&lt;/p&gt;

&lt;p&gt;appreciate any insight, and experiences shared&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o06z5u,True,,DeeJayCruiser,,60,True,all_ads,False,[],False,,/r/datascience/comments/o06z5u/is_linear_programming_part_of_ds/,all_ads,False,https://www.reddit.com/r/datascience/comments/o06z5u/is_linear_programming_part_of_ds/,515407,1623736063.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've started a new job as a Sr. Data Analyst. I am ramping up and am about a month into the position. Ramping up in the pandemic is a different experience than when I ramped up in a previous role in the office (obviously). The most notable part is knowing when to reach out to others and seek out that sort of 'whiteboard' session.

I feel that I am missing that now and don't have a great feel for my team. I have a project now but ultimately I feel disconnected. I am looking to do more catch ups and understand that with time rapport builds. Slack is part of the culture, but the group chat isn't too active.

When I have ramped up other individuals in the past I try to meet often and be available for their questions. I try to also have specific times to check in to help facilitate communication and let the person know there isn't a 'quota' to their questions.

What are ways others have been able to ramp up in the pandemic or fully remote? Any tips?

Edit: grammar",t2_d1rr0,False,,0,False,Ramping Up Help from Coworkers,[],r/datascience,False,6,discussion,0,,,False,t3_o0oz27,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,1623953680.0,,[],{},,True,,1623821035.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve started a new job as a Sr. Data Analyst. I am ramping up and am about a month into the position. Ramping up in the pandemic is a different experience than when I ramped up in a previous role in the office (obviously). The most notable part is knowing when to reach out to others and seek out that sort of &amp;#39;whiteboard&amp;#39; session.&lt;/p&gt;

&lt;p&gt;I feel that I am missing that now and don&amp;#39;t have a great feel for my team. I have a project now but ultimately I feel disconnected. I am looking to do more catch ups and understand that with time rapport builds. Slack is part of the culture, but the group chat isn&amp;#39;t too active.&lt;/p&gt;

&lt;p&gt;When I have ramped up other individuals in the past I try to meet often and be available for their questions. I try to also have specific times to check in to help facilitate communication and let the person know there isn&amp;#39;t a &amp;#39;quota&amp;#39; to their questions.&lt;/p&gt;

&lt;p&gt;What are ways others have been able to ramp up in the pandemic or fully remote? Any tips?&lt;/p&gt;

&lt;p&gt;Edit: grammar&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0oz27,True,,NameNumber7,,1,True,all_ads,False,[],False,,/r/datascience/comments/o0oz27/ramping_up_help_from_coworkers/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0oz27/ramping_up_help_from_coworkers/,515407,1623792235.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_34qgdyb6,False,,0,False,"I have a large dataset (100 mil rows) in Russian, I want to translate it into English. I was using Googletranslate API, which was showing error coz of a high number of requests. Is there anything else I can do??",[],r/datascience,False,6,projects,0,,,False,t3_o08ue4,False,dark,0.83,,public,20,0,{},,,False,[],,False,False,,{},Projects,False,20,,False,False,self,False,,[],{},,True,,1623772043.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o08ue4,True,,yaakarsh1011,,18,True,all_ads,False,[],False,,/r/datascience/comments/o08ue4/i_have_a_large_dataset_100_mil_rows_in_russian_i/,all_ads,False,https://www.reddit.com/r/datascience/comments/o08ue4/i_have_a_large_dataset_100_mil_rows_in_russian_i/,515407,1623743243.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I’m a rising senior currently interning at a decent company as swe, but I recently got offered a fall internship at a lower tier company in data science. I’m really interested in data science and haven’t really been exposed to it, but I’m worried this will look like a step down resume wise. The pay for the ds role is about half as much as my current swe role too for context. If I took the ds role, I would have less time on school as well since it’s in the fall. So would it just be more worthwhile to focus on school and maybe research there than this internship? I would still apply to data science jobs in the future regardless. Any thoughts would be appreciated!",t2_5bpl12gd,False,,0,False,Is it worth taking a lower tier DS internship after mid tier SWE internship if I’m interested in DS?,[],r/datascience,False,6,career,0,,,False,t3_o0m6w9,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623813326.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a rising senior currently interning at a decent company as swe, but I recently got offered a fall internship at a lower tier company in data science. I’m really interested in data science and haven’t really been exposed to it, but I’m worried this will look like a step down resume wise. The pay for the ds role is about half as much as my current swe role too for context. If I took the ds role, I would have less time on school as well since it’s in the fall. So would it just be more worthwhile to focus on school and maybe research there than this internship? I would still apply to data science jobs in the future regardless. Any thoughts would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0m6w9,True,,piledriver42069,,6,True,all_ads,False,[],False,,/r/datascience/comments/o0m6w9/is_it_worth_taking_a_lower_tier_ds_internship/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0m6w9/is_it_worth_taking_a_lower_tier_ds_internship/,515407,1623784526.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"If given some say in setting a job title, for someone who does both (and will be expected to do both in the role), would you go for a merged title like ""Data Scientist &amp; Engineer"", or just pick one? Context: data scientist roles are already on the resume, so goal is to highlight the additional skill/responsibility, if appropriate.",t2_64vd4,False,,0,False,Choosing a combined title? E.g. Data Scientist &amp; Engineer,[],r/datascience,False,6,career,0,,,False,t3_o0p64q,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623821599.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If given some say in setting a job title, for someone who does both (and will be expected to do both in the role), would you go for a merged title like &amp;quot;Data Scientist &amp;amp; Engineer&amp;quot;, or just pick one? Context: data scientist roles are already on the resume, so goal is to highlight the additional skill/responsibility, if appropriate.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0p64q,True,,le_sacre,,5,True,all_ads,False,[],False,,/r/datascience/comments/o0p64q/choosing_a_combined_title_eg_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0p64q/choosing_a_combined_title_eg_data_scientist/,515407,1623792799.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm pretty happy with my full time job as a senior model developer (but I do a lot of analytics on model results as well), including the steady paycheck and job security it comes with. But sometimes I don't get to do the most interesting work so I think about getting into part-time freelancing. I had one person in my network pitch me an app idea for his small business. I had a conversation with their ceo, he seemed super interested, pitched me some requirements, answered some questions. I then roughed out the specific work required, how much time it would take, and used $50/hour as my baseline pay needed even though that's below what my regular job pays. Normally I'd need more for working beyond my full-time job because it would require sacrificing my social life, but the project was extremely interesting and could have led to a good relationship and more work down the road. But when he saw my estimate that it'd take 200 hours to complete aka $10k he said my estimates all made sense but thanks but no thanks. The fact that he didn't even attempt to counter makes me think he was expecting to pay like $1k for it, even though the guy who pitched the idea to me said that for $10k they'd recoup costs in a few months because they paid contractors to manually do the work my app would have automated, plus they lose money on mistakes the contractors regularly make.

&amp;#x200B;

So my question is what's the market like for work like this? Are people actually able to build custom data-driven apps where they're paid 5 figures for completed projects? Or is my experience more typical where there's just a sticker shock? What kind of clients are paying this? How do you get your foot in the door? Would a good github or personal page with personal project examples be valuable?",t2_6xi2clnm,False,,0,False,Finding Freelancing Opportunities,[],r/datascience,False,6,network,0,,,False,t3_o0oxpm,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Networking,False,1,,False,False,self,False,,[],{},,True,,1623820932.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m pretty happy with my full time job as a senior model developer (but I do a lot of analytics on model results as well), including the steady paycheck and job security it comes with. But sometimes I don&amp;#39;t get to do the most interesting work so I think about getting into part-time freelancing. I had one person in my network pitch me an app idea for his small business. I had a conversation with their ceo, he seemed super interested, pitched me some requirements, answered some questions. I then roughed out the specific work required, how much time it would take, and used $50/hour as my baseline pay needed even though that&amp;#39;s below what my regular job pays. Normally I&amp;#39;d need more for working beyond my full-time job because it would require sacrificing my social life, but the project was extremely interesting and could have led to a good relationship and more work down the road. But when he saw my estimate that it&amp;#39;d take 200 hours to complete aka $10k he said my estimates all made sense but thanks but no thanks. The fact that he didn&amp;#39;t even attempt to counter makes me think he was expecting to pay like $1k for it, even though the guy who pitched the idea to me said that for $10k they&amp;#39;d recoup costs in a few months because they paid contractors to manually do the work my app would have automated, plus they lose money on mistakes the contractors regularly make.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So my question is what&amp;#39;s the market like for work like this? Are people actually able to build custom data-driven apps where they&amp;#39;re paid 5 figures for completed projects? Or is my experience more typical where there&amp;#39;s just a sticker shock? What kind of clients are paying this? How do you get your foot in the door? Would a good github or personal page with personal project examples be valuable?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0oxpm,True,,yoi12321,,6,True,all_ads,False,[],False,,/r/datascience/comments/o0oxpm/finding_freelancing_opportunities/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0oxpm/finding_freelancing_opportunities/,515407,1623792132.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I joined a new organization in January, and I am now the only member of the data team remaining. 

The bad - My boss, the data engineer, and two analysts (the whole team besides me) have all already resigned or are going to be leaving by the end of the month.  Some senior leaders in other departments have also resigned in the last few weeks. Retention has apparently been an issue here for many years, although I have had a pretty pleasant experience working with my colleagues and stakeholders so far. I have only met lots of bright, motivated people, so maybe they are all just getting poached by other companies. My pay is below market-rate. 

The good - We have an embedded consulting group that handles a lot of our project management, data engineering, and data analysis, and they are staying, if not expanding in the short-term. We also have a new team lead and department director starting soon. I interviewed the team lead, and have seen the resume of the director, and am confident I can learn a lot from both of them. The work we do here is incredibly interesting and meaningful and I am motivated to build great data products. 

Obviously this situation is a cause for concern. Has anyone weathered a similar situation that turned out for the best? Or is this likely going to be just miserable plodding along for the next year or two if I stay?",t2_pr6lcu2,False,,0,False,100% Turnover on Data Team,[],r/datascience,False,6,discussion,0,,,False,t3_nzsoi4,False,dark,0.97,,public,109,0,{},,,False,[],,False,False,,{},Discussion,False,109,,False,False,self,False,,[],{},,True,,1623722135.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I joined a new organization in January, and I am now the only member of the data team remaining. &lt;/p&gt;

&lt;p&gt;The bad - My boss, the data engineer, and two analysts (the whole team besides me) have all already resigned or are going to be leaving by the end of the month.  Some senior leaders in other departments have also resigned in the last few weeks. Retention has apparently been an issue here for many years, although I have had a pretty pleasant experience working with my colleagues and stakeholders so far. I have only met lots of bright, motivated people, so maybe they are all just getting poached by other companies. My pay is below market-rate. &lt;/p&gt;

&lt;p&gt;The good - We have an embedded consulting group that handles a lot of our project management, data engineering, and data analysis, and they are staying, if not expanding in the short-term. We also have a new team lead and department director starting soon. I interviewed the team lead, and have seen the resume of the director, and am confident I can learn a lot from both of them. The work we do here is incredibly interesting and meaningful and I am motivated to build great data products. &lt;/p&gt;

&lt;p&gt;Obviously this situation is a cause for concern. Has anyone weathered a similar situation that turned out for the best? Or is this likely going to be just miserable plodding along for the next year or two if I stay?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzsoi4,True,,most_humblest_ever,,34,True,all_ads,False,[],False,,/r/datascience/comments/nzsoi4/100_turnover_on_data_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzsoi4/100_turnover_on_data_team/,515407,1623693335.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What do you focus on now, and how do you approach new ideas? 

Do you have a framework to protect yourself and your team from risky work?",t2_5e34w9d2,False,,0,False,"Data scientists in leadership positions, what are your strengths?",[],r/datascience,False,6,discussion,0,,,False,t3_nzhzxq,False,dark,0.99,,public,349,3,{},,,False,[],,False,False,,{},Discussion,False,349,,False,False,self,False,,[],{'gid_1': 1},,True,,1623687740.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you focus on now, and how do you approach new ideas? &lt;/p&gt;

&lt;p&gt;Do you have a framework to protect yourself and your team from risky work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzhzxq,True,,expatwithajetpack,,62,True,all_ads,False,[],False,,/r/datascience/comments/nzhzxq/data_scientists_in_leadership_positions_what_are/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzhzxq/data_scientists_in_leadership_positions_what_are/,515407,1623658940.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am looking for Python coding example for - recommender system for medicines. The closest I have come to is [this](https://www.kaggle.com/chocozzz/recommendation-medicines-by-using-a-review).

Can you suggest something better?

Thanks!",t2_2mmql89p,False,,0,False,Recommender System for Medicines - Code Example,[],r/datascience,False,6,discussion,0,,,False,t3_o0fxws,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623796860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for Python coding example for - recommender system for medicines. The closest I have come to is &lt;a href=""https://www.kaggle.com/chocozzz/recommendation-medicines-by-using-a-review""&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Can you suggest something better?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0fxws,True,,grid_world,,1,True,all_ads,False,[],False,,/r/datascience/comments/o0fxws/recommender_system_for_medicines_code_example/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0fxws/recommender_system_for_medicines_code_example/,515406,1623768060.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G7-gjDBRxBZWaTaVTvB0T4nZxHmSe__JeUiMaix8mbA.jpg?auto=webp&amp;s=c94aa6553da546b50c91ac16027480b89d933ee4', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'AV9ukwzWnwzwB0dFAXmSFzM_xMdA1yCD6CZAK1Cjyro'}], 'enabled': False}",,,,,
,datascience,"Suppose you are working on a supervised binary classification task. You have patient medical information (e.g. age, weight, gender, height, blood pressure, etc) and whether they have a certain disease or not (this is the response variable, ""yes"" or ""no""). Let's imagine that determining if patients have this disease is time consuming and costly - so a machine learning approach is being considered.

Let's assume that this disease is very rare. In your data set, only 1% of patients have this disease. Thus, the dataset is imbalanced. 

Intuitively, we know that any machine learning algorithm trained on this data will likely perform poorly. That is, the performance will likely be deceptive: you might get an accuracy of 99%, but misclassify all of the patients who have the disease. 

Mathematically speaking: is there any mathematical explanation for this very logical concept?

 E.g. if only study 1 hour for a chemistry exam, I might only learn how to solve 2-3 types of problems - thus, on a true/false style chemistry exam, there will be many questions that I don't know how to answer because I never saw them before, and I will be likely to perform badly on material that I have not prepared for. Do machine learning models work the same way?

For popular algorithms like neural networks, xgboost and random forest - can it be shown that for classification problems, you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance? 

On a more abstract side, I have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples. E.g. a 5 year old child can ""learn"" what is an ""elephant"" after seeing a few pictures of an elephant (e.g. it's perfectly reasonable to expect that a young child would see a picture of the cartoon character Dumbo and identify Dumbo as an elephant after coming back from a zoo), but a machine learning algorithm would likely need thousands and thousands of pictures of elephants (and likely require to see the same pictures upside down, inverted, with added noise, different color scheme, etc) prior to be able to generalize and learn the concept of an elephant. Perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease, since there are so few of them?

Does the above concept have anything to do with the ""bias-variance tradeoff""? Or is it just logic - if there is not enough variability and information within the data, the machine learning model just learns the ""noise"" within the dataset? I am really curious to see if such a threshold for measuring ""minimum level of variability within the data"" has ever been studied?

PS: in a 1 dimensional sense, on a number line, if you have a ""point"" at 3 and another ""point"" at 5 - you could consider all inferences outside of 3 and 5 as ""extrapolation"" and all inferences between 3 and 5 as  ""interpolation"". When dealing with higher dimensional data, could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as ""interpolation"" and observstions that are farther away as ""extrapolation""? In reality, can you just consider all prediction as extrapolation - small scale extrapolation for closer points, large scale extrapolation for further points?

Thanks",t2_3f0i9m72,False,,0,False,Dealing with imbalanced datasets,[],r/datascience,False,6,discussion,0,,,False,t3_o0exje,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623794111.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you are working on a supervised binary classification task. You have patient medical information (e.g. age, weight, gender, height, blood pressure, etc) and whether they have a certain disease or not (this is the response variable, &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot;). Let&amp;#39;s imagine that determining if patients have this disease is time consuming and costly - so a machine learning approach is being considered.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s assume that this disease is very rare. In your data set, only 1% of patients have this disease. Thus, the dataset is imbalanced. &lt;/p&gt;

&lt;p&gt;Intuitively, we know that any machine learning algorithm trained on this data will likely perform poorly. That is, the performance will likely be deceptive: you might get an accuracy of 99%, but misclassify all of the patients who have the disease. &lt;/p&gt;

&lt;p&gt;Mathematically speaking: is there any mathematical explanation for this very logical concept?&lt;/p&gt;

&lt;p&gt;E.g. if only study 1 hour for a chemistry exam, I might only learn how to solve 2-3 types of problems - thus, on a true/false style chemistry exam, there will be many questions that I don&amp;#39;t know how to answer because I never saw them before, and I will be likely to perform badly on material that I have not prepared for. Do machine learning models work the same way?&lt;/p&gt;

&lt;p&gt;For popular algorithms like neural networks, xgboost and random forest - can it be shown that for classification problems, you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance? &lt;/p&gt;

&lt;p&gt;On a more abstract side, I have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples. E.g. a 5 year old child can &amp;quot;learn&amp;quot; what is an &amp;quot;elephant&amp;quot; after seeing a few pictures of an elephant (e.g. it&amp;#39;s perfectly reasonable to expect that a young child would see a picture of the cartoon character Dumbo and identify Dumbo as an elephant after coming back from a zoo), but a machine learning algorithm would likely need thousands and thousands of pictures of elephants (and likely require to see the same pictures upside down, inverted, with added noise, different color scheme, etc) prior to be able to generalize and learn the concept of an elephant. Perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease, since there are so few of them?&lt;/p&gt;

&lt;p&gt;Does the above concept have anything to do with the &amp;quot;bias-variance tradeoff&amp;quot;? Or is it just logic - if there is not enough variability and information within the data, the machine learning model just learns the &amp;quot;noise&amp;quot; within the dataset? I am really curious to see if such a threshold for measuring &amp;quot;minimum level of variability within the data&amp;quot; has ever been studied?&lt;/p&gt;

&lt;p&gt;PS: in a 1 dimensional sense, on a number line, if you have a &amp;quot;point&amp;quot; at 3 and another &amp;quot;point&amp;quot; at 5 - you could consider all inferences outside of 3 and 5 as &amp;quot;extrapolation&amp;quot; and all inferences between 3 and 5 as  &amp;quot;interpolation&amp;quot;. When dealing with higher dimensional data, could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as &amp;quot;interpolation&amp;quot; and observstions that are farther away as &amp;quot;extrapolation&amp;quot;? In reality, can you just consider all prediction as extrapolation - small scale extrapolation for closer points, large scale extrapolation for further points?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0exje,True,,SQL_beginner,,7,True,all_ads,False,[],False,,/r/datascience/comments/o0exje/dealing_with_imbalanced_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0exje/dealing_with_imbalanced_datasets/,515406,1623765311.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in the situation where SME refuse to believe the model I created despite the good score (&gt;70% f1 score). Now Im clueless on how to convince them. 

As a background, I did my degree in the related field.  So, I have good understanding about the input data. Plus, via EDA, I can clearly see the seperation between classes in output. So, I'm not using Deep Learning to develop thr model, just simple logistic regression, hence the result is pretty easy to intepret and present.

Have you encountered this situation? How you go about convincing SME?",t2_8w8sc9b,False,,0,False,Convincing SME depite good result model,[],r/datascience,False,6,discussion,0,,,False,t3_o07hrm,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623766773.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the situation where SME refuse to believe the model I created despite the good score (&amp;gt;70% f1 score). Now Im clueless on how to convince them. &lt;/p&gt;

&lt;p&gt;As a background, I did my degree in the related field.  So, I have good understanding about the input data. Plus, via EDA, I can clearly see the seperation between classes in output. So, I&amp;#39;m not using Deep Learning to develop thr model, just simple logistic regression, hence the result is pretty easy to intepret and present.&lt;/p&gt;

&lt;p&gt;Have you encountered this situation? How you go about convincing SME?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o07hrm,True,,ampang_boy,,8,True,all_ads,False,[],False,,/r/datascience/comments/o07hrm/convincing_sme_depite_good_result_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/o07hrm/convincing_sme_depite_good_result_model/,515406,1623737973.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," 

There is a well-known algorithm in statistics called SMOTE (Synthetic Minority Over Sampling Technique) which is often used to ""balance"" and ""imbalanced"" data set:

[https://en.wikipedia.org/wiki/Oversampling\_and\_undersampling\_in\_data\_analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)

[https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/#:~:text=SMOTE%20(synthetic%20minority%20oversampling%20technique)%20is%20one%20of%20the%20most,instances%20between%20existing%20minority%20instances)

[https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)

If I have understood correctly, the premise of the SMOTE algorithm is as follows: Suppose you have a dataset containing information for medical patients that are ""healthy"" and ""not healthy"". But, let's assume that the majority of the patients within your dataset are ""healthy"" (the composition of healthy: not healthy being 95:5) . If you want to make a statistical model for this data, the data does not contain enough information for ""not healthy"" patients, and it will be very challenging to build a reliable statistical model that can make accurate predictions for ""not healthy"" patients. Thus, the SMOTE algorithm can fix this problem by:

1. ""rebalancing"" the data set (e.g. after SMOTE, your data set can have a composition of 70:30)
2. creating ""new"" data points from the ""existing"" data : as I understand, this is done by multiplying a given vector corresponding to a randomly selected individual observation, by some random number between 0 and 1.

This leads me to my question: Suppose you have already have a balanced dataset (e.g. the healthy: not healthy has a composition of 60:40), but let's assume that you have a relatively small dat set to begin with (e.g. 500 rows). Can you use the SMOTE algorithm to create new data points so that your dataset is bigger? I understand that no algorithm can magically compensate for data quality issues, but at the same time I don't see any major flaws with using SMOTE on already balanced data?

For reference, I illustrated this process below using R:

I would be interested in hearing a second opinion - Thanks!

    #load and install libraries 
    remotes::install_version(""DMwR"", version=""0.4.1"") 
    library(DMwR) 
    
     #create some fake data and put them into a data frame called ""f""  
    
    var_1&lt;- rnorm(100,1,4) 
    var_2 &lt;-rnorm(100,10,5)
     var_3&lt;- c(""0"",""2"", ""4"")
     var_3 &lt;- sample(var_3, 100, replace=TRUE, prob=c(0.3, 0.6, 0.1)) 
    
     response&lt;- c(""1"",""0"") 
    response &lt;- sample(response, 100, replace=TRUE, prob=c(0.3, 0.7)) 
    
     #put them into a data frame called ""f"" 
    
    f &lt;- data.frame(var_1, var_2, var_3, response) 
    
     #declare var_3 and response_variable as factors 
    f$var_3 = as.factor(f$var_3)
     f$response = as.factor(f$response)  
    
    #SMOTE algorithm  
    #simulate new points from the first class
    
     smoted_data_over &lt;- SMOTE(response~., f, perc.over=100) 
    
     #simulate new points from the second class 
    smoted_data_under &lt;- SMOTE(response~., f, perc.under=100)  
    
    #combine everything together into a final new data file 
    final &lt;-rbind(f, smoted_data_over, smoted_data_under)",t2_3f0i9m72,False,,0,False,Generating New Data Points with SMOTE,[],r/datascience,False,6,discussion,0,,,False,t3_o0e56q,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623791864.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There is a well-known algorithm in statistics called SMOTE (Synthetic Minority Over Sampling Technique) which is often used to &amp;quot;balance&amp;quot; and &amp;quot;imbalanced&amp;quot; data set:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis""&gt;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/#:%7E:text=SMOTE%20(synthetic%20minority%20oversampling%20technique""&gt;https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python&lt;/a&gt;%20is%20one%20of%20the%20most,instances%20between%20existing%20minority%20instances)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/""&gt;https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If I have understood correctly, the premise of the SMOTE algorithm is as follows: Suppose you have a dataset containing information for medical patients that are &amp;quot;healthy&amp;quot; and &amp;quot;not healthy&amp;quot;. But, let&amp;#39;s assume that the majority of the patients within your dataset are &amp;quot;healthy&amp;quot; (the composition of healthy: not healthy being 95:5) . If you want to make a statistical model for this data, the data does not contain enough information for &amp;quot;not healthy&amp;quot; patients, and it will be very challenging to build a reliable statistical model that can make accurate predictions for &amp;quot;not healthy&amp;quot; patients. Thus, the SMOTE algorithm can fix this problem by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&amp;quot;rebalancing&amp;quot; the data set (e.g. after SMOTE, your data set can have a composition of 70:30)&lt;/li&gt;
&lt;li&gt;creating &amp;quot;new&amp;quot; data points from the &amp;quot;existing&amp;quot; data : as I understand, this is done by multiplying a given vector corresponding to a randomly selected individual observation, by some random number between 0 and 1.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This leads me to my question: Suppose you have already have a balanced dataset (e.g. the healthy: not healthy has a composition of 60:40), but let&amp;#39;s assume that you have a relatively small dat set to begin with (e.g. 500 rows). Can you use the SMOTE algorithm to create new data points so that your dataset is bigger? I understand that no algorithm can magically compensate for data quality issues, but at the same time I don&amp;#39;t see any major flaws with using SMOTE on already balanced data?&lt;/p&gt;

&lt;p&gt;For reference, I illustrated this process below using R:&lt;/p&gt;

&lt;p&gt;I would be interested in hearing a second opinion - Thanks!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#load and install libraries 
remotes::install_version(&amp;quot;DMwR&amp;quot;, version=&amp;quot;0.4.1&amp;quot;) 
library(DMwR) 

 #create some fake data and put them into a data frame called &amp;quot;f&amp;quot;  

var_1&amp;lt;- rnorm(100,1,4) 
var_2 &amp;lt;-rnorm(100,10,5)
 var_3&amp;lt;- c(&amp;quot;0&amp;quot;,&amp;quot;2&amp;quot;, &amp;quot;4&amp;quot;)
 var_3 &amp;lt;- sample(var_3, 100, replace=TRUE, prob=c(0.3, 0.6, 0.1)) 

 response&amp;lt;- c(&amp;quot;1&amp;quot;,&amp;quot;0&amp;quot;) 
response &amp;lt;- sample(response, 100, replace=TRUE, prob=c(0.3, 0.7)) 

 #put them into a data frame called &amp;quot;f&amp;quot; 

f &amp;lt;- data.frame(var_1, var_2, var_3, response) 

 #declare var_3 and response_variable as factors 
f$var_3 = as.factor(f$var_3)
 f$response = as.factor(f$response)  

#SMOTE algorithm  
#simulate new points from the first class

 smoted_data_over &amp;lt;- SMOTE(response~., f, perc.over=100) 

 #simulate new points from the second class 
smoted_data_under &amp;lt;- SMOTE(response~., f, perc.under=100)  

#combine everything together into a final new data file 
final &amp;lt;-rbind(f, smoted_data_over, smoted_data_under)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o0e56q,True,,SQL_beginner,,3,True,all_ads,False,[],False,,/r/datascience/comments/o0e56q/generating_new_data_points_with_smote/,all_ads,False,https://www.reddit.com/r/datascience/comments/o0e56q/generating_new_data_points_with_smote/,515406,1623763064.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/apkmo5zs_-YmFFDjmG62TyIS5jvdW18a790XKORnEu8.jpg?auto=webp&amp;s=d430e4efd42baf58b4bc71c9549944b7bdf40715', 'width': 50, 'height': 39}, 'resolutions': [], 'variants': {}, 'id': 'K8Lxcv5aHkjcbujO-2N7cVXtdXkwYe8BHCOouTCCjb8'}], 'enabled': False}",,,,,
,datascience,"I\`ve been struggling to maintain version control of Jupyter Notebooks through pure Git because of all the issues of git dffing detecting cell output changes and stuff.

&amp;#x200B;

Do you use any specific tools to keep up a good gitlfow-like version control scheme of your data science jupyter notebooks?",t2_nkoag,False,,0,False,What do you use to version control Jupyter Notebooks?,[],r/datascience,False,6,discussion,0,,,False,t3_nzo5lk,False,dark,0.9,,public,28,0,{},,,False,[],,False,False,,{},Discussion,False,28,,False,False,self,False,,[],{},,True,,1623710009.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I`ve been struggling to maintain version control of Jupyter Notebooks through pure Git because of all the issues of git dffing detecting cell output changes and stuff.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Do you use any specific tools to keep up a good gitlfow-like version control scheme of your data science jupyter notebooks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzo5lk,True,,RoyalScores,,21,True,all_ads,False,[],False,,/r/datascience/comments/nzo5lk/what_do_you_use_to_version_control_jupyter/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzo5lk/what_do_you_use_to_version_control_jupyter/,515406,1623681209.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I’m very much a beginner and learning R and Python but goodness do I prefer R. In future roles, will I be able to say that I prefer to use R over Python or is there not that kind of flexibility?",t2_5bg0t3mm,False,,0,False,Can I set a language preference?,[],r/datascience,False,6,career,0,,,False,t3_o01iho,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623746715.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m very much a beginner and learning R and Python but goodness do I prefer R. In future roles, will I be able to say that I prefer to use R over Python or is there not that kind of flexibility?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,o01iho,True,,ThrowRA-11789,,25,True,all_ads,False,[],False,,/r/datascience/comments/o01iho/can_i_set_a_language_preference/,all_ads,False,https://www.reddit.com/r/datascience/comments/o01iho/can_i_set_a_language_preference/,515406,1623717915.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Do you have any favourite bloggers that are professionals and have high quality content that you love reading? Would love to add them to my reading list. Cheers.,t2_3mpdgvl7,False,,0,False,Any good personal blogs that has quality data science content?,[],r/datascience,False,6,discussion,0,,,False,t3_nzqoeo,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623716815.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you have any favourite bloggers that are professionals and have high quality content that you love reading? Would love to add them to my reading list. Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzqoeo,True,,benthecoderX,,15,True,all_ads,False,[],False,,/r/datascience/comments/nzqoeo/any_good_personal_blogs_that_has_quality_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzqoeo/any_good_personal_blogs_that_has_quality_data/,515406,1623688015.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I was looking into tableau online which seems fantastic, but is a little expensive for my needs.  Ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.

Anyone have another suggestions?

&amp;#x200B;

Thanks",t2_9s3bg,False,,0,False,Suggestions for BI visualization tool that supports live feed connection from BQ?,[],r/datascience,False,6,tooling,0,,,False,t3_nzyady,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623736965.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was looking into tableau online which seems fantastic, but is a little expensive for my needs.  Ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.&lt;/p&gt;

&lt;p&gt;Anyone have another suggestions?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzyady,True,,tcbjj,,8,True,all_ads,False,[],False,,/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/,515406,1623708165.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi 

Recently I'm doing my EDA and I found that my analyst is my weaknesses.

My friend said textbooks are the best and I can read more articles published on Towards Datascience, etc secondly. 

Just wondering how you guys improve analysis

I'd like to model some great analysts or collect those best analytical writings.

Would you like to share any best analytical articles or your favourite authors?


Thanks",t2_2xpkxipc,False,,0,False,How do you improve analytical thinking/writing?,[],r/datascience,False,6,discussion,0,,,False,t3_nzxqxm,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1623709743.0,,[],{},,True,,1623735494.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;m doing my EDA and I found that my analyst is my weaknesses.&lt;/p&gt;

&lt;p&gt;My friend said textbooks are the best and I can read more articles published on Towards Datascience, etc secondly. &lt;/p&gt;

&lt;p&gt;Just wondering how you guys improve analysis&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to model some great analysts or collect those best analytical writings.&lt;/p&gt;

&lt;p&gt;Would you like to share any best analytical articles or your favourite authors?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzxqxm,True,,homchange,,13,True,all_ads,False,[],False,,/r/datascience/comments/nzxqxm/how_do_you_improve_analytical_thinkingwriting/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzxqxm/how_do_you_improve_analytical_thinkingwriting/,515406,1623706694.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let’s say we’re a company like Spotify and want to recommend songs to you based on songs you’ve listened to.

Say we have a constraint that we don’t want to funnel all of our impressions to a small % of songs (which is likely to occur with user based collaborative filtering), but rather would prefer broad coverage of viewership across our whole library.

We want this to be personalized to each user and we want to iteratively learn as you continue to listen to new songs. 

How would you go about building this architecture? What does it look like? What models do you opt to use?

Everything I’ve thought of seems to have some sort of hold up so I feel I’m missing something. I assume we can’t train an RNN for every user due to likely limited sample data (they’ve only listened to so many songs themselves) and computational cost of maintaining hundreds of thousands or millions of NNs. Traditional content based recommendations using like simple cosine similarity may not be able to capture some of the more complex nonlinear relationships without exceptional upfront feature engineering (e.g. I like electronic songs, but only if they’re between X and Y BPM, with female vocalists). 

What am I missing? Do we have a good solution for this type of problem?",t2_87cp2,False,,0,False,How would you go about building a content based recommendation system with reinforcement?,[],r/datascience,False,6,discussion,0,,,False,t3_nz22z5,False,dark,0.98,,public,119,0,{},,,False,[],,False,False,,{},Discussion,False,119,,False,False,self,1623607767.0,,[],{},,True,,1623636113.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let’s say we’re a company like Spotify and want to recommend songs to you based on songs you’ve listened to.&lt;/p&gt;

&lt;p&gt;Say we have a constraint that we don’t want to funnel all of our impressions to a small % of songs (which is likely to occur with user based collaborative filtering), but rather would prefer broad coverage of viewership across our whole library.&lt;/p&gt;

&lt;p&gt;We want this to be personalized to each user and we want to iteratively learn as you continue to listen to new songs. &lt;/p&gt;

&lt;p&gt;How would you go about building this architecture? What does it look like? What models do you opt to use?&lt;/p&gt;

&lt;p&gt;Everything I’ve thought of seems to have some sort of hold up so I feel I’m missing something. I assume we can’t train an RNN for every user due to likely limited sample data (they’ve only listened to so many songs themselves) and computational cost of maintaining hundreds of thousands or millions of NNs. Traditional content based recommendations using like simple cosine similarity may not be able to capture some of the more complex nonlinear relationships without exceptional upfront feature engineering (e.g. I like electronic songs, but only if they’re between X and Y BPM, with female vocalists). &lt;/p&gt;

&lt;p&gt;What am I missing? Do we have a good solution for this type of problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nz22z5,True,,reddstaw,,12,True,all_ads,False,[],False,,/r/datascience/comments/nz22z5/how_would_you_go_about_building_a_content_based/,all_ads,False,https://www.reddit.com/r/datascience/comments/nz22z5/how_would_you_go_about_building_a_content_based/,515406,1623607313.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What do you think about a cluster analysis to segment customers? I feel like a manual segmentation is often times better, especially when it comes to more personalized marketing. 

I think that clustering makes sense when there are distinct groups in the population. However, I think that these groups can easily be identified by EDA (finding thresholds of certain variables) in most cases. There is a possibility for identifying relevant groups only through cluster analysis, but I think that a) those cases are rare and b) the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication.

Does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value (e.g. because of more personalized communication)? I am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds/criteria for clusters.",t2_41ms80a8,False,,0,False,Cluster Analysis for Customer Segmentation,[],r/datascience,False,6,discussion,0,,,False,t3_nzkpwk,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623699104.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you think about a cluster analysis to segment customers? I feel like a manual segmentation is often times better, especially when it comes to more personalized marketing. &lt;/p&gt;

&lt;p&gt;I think that clustering makes sense when there are distinct groups in the population. However, I think that these groups can easily be identified by EDA (finding thresholds of certain variables) in most cases. There is a possibility for identifying relevant groups only through cluster analysis, but I think that a) those cases are rare and b) the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication.&lt;/p&gt;

&lt;p&gt;Does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value (e.g. because of more personalized communication)? I am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds/criteria for clusters.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzkpwk,True,,tstr2609,,10,True,all_ads,False,[],False,,/r/datascience/comments/nzkpwk/cluster_analysis_for_customer_segmentation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzkpwk/cluster_analysis_for_customer_segmentation/,515406,1623670304.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Topic. Given the lack of future covariates, can one use (S)ARIMAX to predict *k* (instead of just one) time steps ahead?

Another related question: Can we use SARIMAX to predict non-negative integer series, such as count of item sale? If yes, how? If not, which alternatives do you recommend?",t2_9aqvgklw,False,,0,False,Is it possible to use (S)ARIMAX to predict multiple time steps ahead?,[],r/datascience,False,6,discussion,0,,,False,t3_nznbfn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,1623681163.0,,[],{},,True,,1623707674.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Topic. Given the lack of future covariates, can one use (S)ARIMAX to predict &lt;em&gt;k&lt;/em&gt; (instead of just one) time steps ahead?&lt;/p&gt;

&lt;p&gt;Another related question: Can we use SARIMAX to predict non-negative integer series, such as count of item sale? If yes, how? If not, which alternatives do you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nznbfn,True,,populus27,,5,True,all_ads,False,[],False,,/r/datascience/comments/nznbfn/is_it_possible_to_use_sarimax_to_predict_multiple/,all_ads,False,https://www.reddit.com/r/datascience/comments/nznbfn/is_it_possible_to_use_sarimax_to_predict_multiple/,515406,1623678874.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What made you realize that you wanted to pursue data science? Did you go to school for it or did you wind up the field by accident? Was there a certain class you took the sparked your interest?,t2_2gujwkgv,False,,0,False,When did you know the data science was for you?,[],r/datascience,False,6,discussion,0,,,False,t3_nzf1fb,False,dark,0.67,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623675425.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What made you realize that you wanted to pursue data science? Did you go to school for it or did you wind up the field by accident? Was there a certain class you took the sparked your interest?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzf1fb,True,,J24C,,16,True,all_ads,False,[],False,,/r/datascience/comments/nzf1fb/when_did_you_know_the_data_science_was_for_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzf1fb/when_did_you_know_the_data_science_was_for_you/,515406,1623646625.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey everyone. I'm currently working actively on 2 projects, with 3 more coming up in the next weeks. I've been working with SQL trying to define patient cohorts for later statistical analysis in R. The SQL work has been slow to my liking but I only started working here in January and on a new database, of which I had to move out from after the first 2 months, to another database (same data source but different structure) so I had to redo a good chunk of the initial progress. I would say I've been only making real progress during the past 4 months. Add to that that I have at least 2 meetings a week.

My current contract is supposed to expire by the end of this month, and they'll extend it for another 3 months for me to deliver the first 2 projects, but they still expect me to work to some degree on the other 3 that are coming.  

My question is if you feel like they're setting me up to fail on this position or working on 5 projects is actually doable? Obviously I'm going to focus on the ones that they're going to evaluate me on, but considering I'm the first data scientist here I think they're not used to having the vast majority of the time defining the data extraction parameters and cleaning the data, and only a small portion on the actual statistical analysis or ML modelling.

At the time of starting this job I had 9 months of experience, but this is my first job after getting my MSc. I already had some SQL experience but mainly from internet tutorials and some simple joins from my previous position.

What do you think?",t2_5u939xo,False,,0,False,How many projects are you able to work efficiently on?,[],r/datascience,False,6,career,0,,,False,t3_nzlzih,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623703599.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone. I&amp;#39;m currently working actively on 2 projects, with 3 more coming up in the next weeks. I&amp;#39;ve been working with SQL trying to define patient cohorts for later statistical analysis in R. The SQL work has been slow to my liking but I only started working here in January and on a new database, of which I had to move out from after the first 2 months, to another database (same data source but different structure) so I had to redo a good chunk of the initial progress. I would say I&amp;#39;ve been only making real progress during the past 4 months. Add to that that I have at least 2 meetings a week.&lt;/p&gt;

&lt;p&gt;My current contract is supposed to expire by the end of this month, and they&amp;#39;ll extend it for another 3 months for me to deliver the first 2 projects, but they still expect me to work to some degree on the other 3 that are coming.  &lt;/p&gt;

&lt;p&gt;My question is if you feel like they&amp;#39;re setting me up to fail on this position or working on 5 projects is actually doable? Obviously I&amp;#39;m going to focus on the ones that they&amp;#39;re going to evaluate me on, but considering I&amp;#39;m the first data scientist here I think they&amp;#39;re not used to having the vast majority of the time defining the data extraction parameters and cleaning the data, and only a small portion on the actual statistical analysis or ML modelling.&lt;/p&gt;

&lt;p&gt;At the time of starting this job I had 9 months of experience, but this is my first job after getting my MSc. I already had some SQL experience but mainly from internet tutorials and some simple joins from my previous position.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzlzih,True,,Trucomallica,,13,True,all_ads,False,[],False,,/r/datascience/comments/nzlzih/how_many_projects_are_you_able_to_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzlzih/how_many_projects_are_you_able_to_work/,515406,1623674799.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hey Guys,

I am currently working on my thesis. In this thesis I work with time series data and use a model based on fused regression estimated on a rolling window. Unfortunately I do not habe enough obervations to utilize cross-validation or even any kind of validation. If someone here could point me on something I could use instead to choose my hyperparameters I would be incredibly thankful.

Thanks in advance and have a nice day.

Edit:
To clarify a little

The task is basically to evaluate penalized regression models against a standard factor model (essentially linear regression) in hedge fund replication. 

At every point t I estimate a model based on the data in the rolling window abd use the coefficients as portfolio weights in t+1.",t2_5avdhsjx,False,,0,False,Alternatives to Cross-Validation,[],r/datascience,False,6,discussion,0,,,False,t3_nzie21,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1623669100.0,,[],{},,True,,1623689547.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;

&lt;p&gt;I am currently working on my thesis. In this thesis I work with time series data and use a model based on fused regression estimated on a rolling window. Unfortunately I do not habe enough obervations to utilize cross-validation or even any kind of validation. If someone here could point me on something I could use instead to choose my hyperparameters I would be incredibly thankful.&lt;/p&gt;

&lt;p&gt;Thanks in advance and have a nice day.&lt;/p&gt;

&lt;p&gt;Edit:
To clarify a little&lt;/p&gt;

&lt;p&gt;The task is basically to evaluate penalized regression models against a standard factor model (essentially linear regression) in hedge fund replication. &lt;/p&gt;

&lt;p&gt;At every point t I estimate a model based on the data in the rolling window abd use the coefficients as portfolio weights in t+1.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzie21,True,,0din23,,8,True,all_ads,False,[],False,,/r/datascience/comments/nzie21/alternatives_to_crossvalidation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzie21/alternatives_to_crossvalidation/,515406,1623660747.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone,

for a university project we were given the following simple algorithm and I was wondering if it has a name or is similar to an existing method. I'd be glad if you could leave some comments.

So we have data points with 2 features in 3 evenly distributed categories. One can easily see 3 clusters but there is quite some overlap.

The suggested method wants us to determine 3 centres, one for each category, and categorize data points by determining the closest centre. These centres should be optimized so that the number of wrong categorizations is minimized. (Any optimization methods recommended? There seem to be a lot of local minima so we used scipys dual-annealing and most likely found the optimum but maybe there is a faster way?).

So is there any name for this categorization Method? Or a very similar one? All the methodes we looked at (sklearn) are much more sohphisticated.

We wanted to also use knn as an alternative method and compare the two using cross-validation.

If you have any other ideas I'd be very happy to hear them.

Thanks for reading and have a nice day!

P.S.: Cool to see this subreddit exists. subscribed!",t2_fm7fx,False,,0,False,Is there a name for this simple Machine Learning Method? Or a similar one?,[],r/datascience,False,6,education,0,,,False,t3_nzi1lx,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623687949.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;for a university project we were given the following simple algorithm and I was wondering if it has a name or is similar to an existing method. I&amp;#39;d be glad if you could leave some comments.&lt;/p&gt;

&lt;p&gt;So we have data points with 2 features in 3 evenly distributed categories. One can easily see 3 clusters but there is quite some overlap.&lt;/p&gt;

&lt;p&gt;The suggested method wants us to determine 3 centres, one for each category, and categorize data points by determining the closest centre. These centres should be optimized so that the number of wrong categorizations is minimized. (Any optimization methods recommended? There seem to be a lot of local minima so we used scipys dual-annealing and most likely found the optimum but maybe there is a faster way?).&lt;/p&gt;

&lt;p&gt;So is there any name for this categorization Method? Or a very similar one? All the methodes we looked at (sklearn) are much more sohphisticated.&lt;/p&gt;

&lt;p&gt;We wanted to also use knn as an alternative method and compare the two using cross-validation.&lt;/p&gt;

&lt;p&gt;If you have any other ideas I&amp;#39;d be very happy to hear them.&lt;/p&gt;

&lt;p&gt;Thanks for reading and have a nice day!&lt;/p&gt;

&lt;p&gt;P.S.: Cool to see this subreddit exists. subscribed!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzi1lx,True,,pitano,,11,True,all_ads,False,[],False,,/r/datascience/comments/nzi1lx/is_there_a_name_for_this_simple_machine_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzi1lx/is_there_a_name_for_this_simple_machine_learning/,515406,1623659149.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Supposed we have a linear transformation 
A = [[2,1], [1,2]] 
Why does when i use linalg in python and input this matrix it produces eigenvector and eigenvalues when this linear transformation has both changed the direction(amplitude) of the default coordinate system?",t2_67s1slvl,False,,0,False,Why does this linear transformation produces any eigenvector,[],r/datascience,False,6,education,0,,,False,t3_nzmrwc,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1623706048.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Supposed we have a linear transformation 
A = [[2,1], [1,2]] 
Why does when i use linalg in python and input this matrix it produces eigenvector and eigenvalues when this linear transformation has both changed the direction(amplitude) of the default coordinate system?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzmrwc,True,,izner82,,2,True,all_ads,False,[],False,,/r/datascience/comments/nzmrwc/why_does_this_linear_transformation_produces_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzmrwc/why_does_this_linear_transformation_produces_any/,515406,1623677248.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_43ojyjs,False,,0,False,Is going from a data science job to people analytics (corporate human resources) in the same company a demotion?,[],r/datascience,False,6,career,0,,,False,t3_nzigaa,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1623689830.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nzigaa,True,,the_siloviki,,9,True,all_ads,False,[],False,,/r/datascience/comments/nzigaa/is_going_from_a_data_science_job_to_people/,all_ads,False,https://www.reddit.com/r/datascience/comments/nzigaa/is_going_from_a_data_science_job_to_people/,515406,1623661030.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am looking to study recommendation system techniques (such as collaborative filtering), and how to build + evaluate machine learning models for ranking (instead of classification) problems. Google search results have been messy so far.

Does anyone have a good overview for these two topics?",t2_9aqvgklw,False,,0,False,Is there a comprehensive overview of ML ranking + recommendation system techniques?,[],r/datascience,False,6,discussion,0,,,False,t3_nyses8,False,dark,0.98,,public,53,0,{},,,False,[],,False,False,,{},Discussion,False,53,,False,False,self,False,,[],{},,True,,1623603981.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to study recommendation system techniques (such as collaborative filtering), and how to build + evaluate machine learning models for ranking (instead of classification) problems. Google search results have been messy so far.&lt;/p&gt;

&lt;p&gt;Does anyone have a good overview for these two topics?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyses8,True,,populus27,,7,True,all_ads,False,[],False,,/r/datascience/comments/nyses8/is_there_a_comprehensive_overview_of_ml_ranking/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyses8/is_there_a_comprehensive_overview_of_ml_ranking/,515406,1623575181.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Noob here.  I have very basic skills in Python using PyCharm.  

I just picked up Python for Data Science for Dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.

In this book, the author uses Jupyter Notebook.  Before I go and install another program and head down the path of learning it, I'm wondering if this is the right tool to be using. 

My goals: Well, I guess I'd just like to expand my knowledge of Python.  I don't use it for work or anything, yet...  I'd like to move into an FP&amp;A role and I know understanding Python is sometimes advantageous.  I do realize that doing data science with Python is probably more than would be needed in an FP&amp;A role, and that's OK.  I think I may just like to learn how to use Python more because I'm just a very analytical person by nature and maybe someday I'll use it to put together analyses of Coronavirus data.  But since I am new with learning coding languages, if Jupyter is good as a starting point, that's OK too.  Have to admit that the CLI screenshots in the book intimidated me,  but I'm OK learning it since I know CLI is kind of a part of being a techy and it's probably about time I got more comfortable with it.",t2_4800s9po,False,,0,False,Using Jupyter Notebook vs something else?,[],r/datascience,False,6,education,0,,,False,t3_nyizb5,False,dark,0.95,,public,138,0,{},,,False,[],,False,False,,{},Education,False,138,,False,False,self,False,,[],{},,True,,1623567522.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Noob here.  I have very basic skills in Python using PyCharm.  &lt;/p&gt;

&lt;p&gt;I just picked up Python for Data Science for Dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.&lt;/p&gt;

&lt;p&gt;In this book, the author uses Jupyter Notebook.  Before I go and install another program and head down the path of learning it, I&amp;#39;m wondering if this is the right tool to be using. &lt;/p&gt;

&lt;p&gt;My goals: Well, I guess I&amp;#39;d just like to expand my knowledge of Python.  I don&amp;#39;t use it for work or anything, yet...  I&amp;#39;d like to move into an FP&amp;amp;A role and I know understanding Python is sometimes advantageous.  I do realize that doing data science with Python is probably more than would be needed in an FP&amp;amp;A role, and that&amp;#39;s OK.  I think I may just like to learn how to use Python more because I&amp;#39;m just a very analytical person by nature and maybe someday I&amp;#39;ll use it to put together analyses of Coronavirus data.  But since I am new with learning coding languages, if Jupyter is good as a starting point, that&amp;#39;s OK too.  Have to admit that the CLI screenshots in the book intimidated me,  but I&amp;#39;m OK learning it since I know CLI is kind of a part of being a techy and it&amp;#39;s probably about time I got more comfortable with it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyizb5,True,,lljc00,,109,True,all_ads,False,[],False,,/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/,515406,1623538722.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Are there any other types of data that we interpret with programming?

I assume theirs multiple different types of tabular data.. sequential is anything with a time series such as audio, &amp; image is a 2D picture is that all their is?


Thankyou",t2_2v31gwfg,False,,0,False,Tabular sequential and image data. Is there any other type of data?,[],r/datascience,False,6,discussion,0,,,False,t3_nyxfbk,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623623134.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any other types of data that we interpret with programming?&lt;/p&gt;

&lt;p&gt;I assume theirs multiple different types of tabular data.. sequential is anything with a time series such as audio, &amp;amp; image is a 2D picture is that all their is?&lt;/p&gt;

&lt;p&gt;Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyxfbk,True,,quantumwoooo,,9,True,all_ads,False,[],False,,/r/datascience/comments/nyxfbk/tabular_sequential_and_image_data_is_there_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyxfbk/tabular_sequential_and_image_data_is_there_any/,515406,1623594334.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi,

Not sure if this post violates the on-topic rule, it is DS-related in the applied sense in terms of being a practicioner, sorry if it is considered OT.

I usually work locally on my machine, which has always been a laptop with an i7 or i9 with a lower-end nvidia GPU for small to medium-sized modeling tasks.  I'm going to start a new job soon and will have my choice of work laptop. Big compute tasks can be performed on the cloud, however for prototype/POC work with limited datasets that don't require very intense hyperparameter searches, I typically work locally.

I've been reading some interesting things about the performance of ML libraries on M1 machines and it looks like deep learning packages as well as low level vector libraries and libraries built on top of them such as numpy are very quick these days with the m1.

Is anybody using an M1 machine these days for DS? I won't have time to mess around with complex builds and such, I'm generally somebody who just relies on anaconda to install what I need and make sure all of the packages work nicely together. Is the M1 ""There yet"" in terms of being ready to hit the road for DS work with minimal fuss?

My other question/concern is memory allocation for the gpu cores when using DL libraries. Since the memory is ""unified"", if I have 16 gigs, how is that split between general system use and GPU use?

Thanks!",t2_1dikvtfr,False,,0,False,Anybody using a M1 Apple product for local modeling work?,[],r/datascience,False,6,tooling,0,,,False,t3_ny7wy5,False,dark,0.9,,public,96,0,{},,,False,[],,False,False,,{},Tooling,False,96,,False,False,self,False,,[],{},,True,,1623536706.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Not sure if this post violates the on-topic rule, it is DS-related in the applied sense in terms of being a practicioner, sorry if it is considered OT.&lt;/p&gt;

&lt;p&gt;I usually work locally on my machine, which has always been a laptop with an i7 or i9 with a lower-end nvidia GPU for small to medium-sized modeling tasks.  I&amp;#39;m going to start a new job soon and will have my choice of work laptop. Big compute tasks can be performed on the cloud, however for prototype/POC work with limited datasets that don&amp;#39;t require very intense hyperparameter searches, I typically work locally.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been reading some interesting things about the performance of ML libraries on M1 machines and it looks like deep learning packages as well as low level vector libraries and libraries built on top of them such as numpy are very quick these days with the m1.&lt;/p&gt;

&lt;p&gt;Is anybody using an M1 machine these days for DS? I won&amp;#39;t have time to mess around with complex builds and such, I&amp;#39;m generally somebody who just relies on anaconda to install what I need and make sure all of the packages work nicely together. Is the M1 &amp;quot;There yet&amp;quot; in terms of being ready to hit the road for DS work with minimal fuss?&lt;/p&gt;

&lt;p&gt;My other question/concern is memory allocation for the gpu cores when using DL libraries. Since the memory is &amp;quot;unified&amp;quot;, if I have 16 gigs, how is that split between general system use and GPU use?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ny7wy5,True,,ShmDoubleO,,32,True,all_ads,False,[],False,,/r/datascience/comments/ny7wy5/anybody_using_a_m1_apple_product_for_local/,all_ads,False,https://www.reddit.com/r/datascience/comments/ny7wy5/anybody_using_a_m1_apple_product_for_local/,515406,1623507906.0,1,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all, 

there is certainly a lot of hype still about Data Science and Machine learning, as somebody who tried ""sour side"" as well (few failed projects because quality of data, not really significant results) 

I was wondering if somebody had any success stories, with quick ML projects that had real business impact and were set into process to bring value. Also if it was worth to go down Python way (I have experience doing my master thesis where I used ML) or much quicker way with ""pre-defined"" tools, like Power BI, or those online engines.  
   
Now I got promoted to Business Intelligence manager position, only successful part of ML I did so far was with engine inside of Power BI. Down the line there are certainly some ideas I have, with speech recognition, better assigning employees to customers, finding right time to contact right customer.   


But uncertainity of ML and its timing to get it done scares me (for some thing I know I can bring 50-60% of value by doing visual analysis, solving the big chunks of decisions in 10% of the time).  


So generally I am advocate of ""ML is hype that is not worth it, and it will stay like this until we have done majority of analysis visually first"". I can see how they are failing in other business unit, where they hired niche data analysts and data scientist, before even having data in db, so it only enforces my thinking.  


Also if somebody has in mind employee churn project, thats a no-go with quality of data in HR (not updating positions, wrong assigned managers, not standardized position names...)

tl;dr: is e2e ML deliverable by single person in company as side project, with bringing real value (relatively quickly)?",t2_827ll6b9,False,,0,False,Any successful 1 man end to end stories?,[],r/datascience,False,6,projects,0,,,False,t3_nyhh3s,False,dark,0.86,,public,21,0,{},,,False,[],,False,False,,{},Projects,False,21,,False,False,self,False,,[],{},,True,,1623563166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, &lt;/p&gt;

&lt;p&gt;there is certainly a lot of hype still about Data Science and Machine learning, as somebody who tried &amp;quot;sour side&amp;quot; as well (few failed projects because quality of data, not really significant results) &lt;/p&gt;

&lt;p&gt;I was wondering if somebody had any success stories, with quick ML projects that had real business impact and were set into process to bring value. Also if it was worth to go down Python way (I have experience doing my master thesis where I used ML) or much quicker way with &amp;quot;pre-defined&amp;quot; tools, like Power BI, or those online engines.  &lt;/p&gt;

&lt;p&gt;Now I got promoted to Business Intelligence manager position, only successful part of ML I did so far was with engine inside of Power BI. Down the line there are certainly some ideas I have, with speech recognition, better assigning employees to customers, finding right time to contact right customer.   &lt;/p&gt;

&lt;p&gt;But uncertainity of ML and its timing to get it done scares me (for some thing I know I can bring 50-60% of value by doing visual analysis, solving the big chunks of decisions in 10% of the time).  &lt;/p&gt;

&lt;p&gt;So generally I am advocate of &amp;quot;ML is hype that is not worth it, and it will stay like this until we have done majority of analysis visually first&amp;quot;. I can see how they are failing in other business unit, where they hired niche data analysts and data scientist, before even having data in db, so it only enforces my thinking.  &lt;/p&gt;

&lt;p&gt;Also if somebody has in mind employee churn project, thats a no-go with quality of data in HR (not updating positions, wrong assigned managers, not standardized position names...)&lt;/p&gt;

&lt;p&gt;tl;dr: is e2e ML deliverable by single person in company as side project, with bringing real value (relatively quickly)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyhh3s,True,,UnderstandingFit9152,,13,True,all_ads,False,[],False,,/r/datascience/comments/nyhh3s/any_successful_1_man_end_to_end_stories/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyhh3s/any_successful_1_man_end_to_end_stories/,515406,1623534366.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm a new grad MS working as an entry-level data scientist. Most of the PhDs at my company are either at the senior level or managerial, but most also have a few years of experience too.

In general how much years of experience do you need to go from my current level to a senior data scientist and then to the principal/managerial level? I know a few new grad colleagues in SWE who got to senior engineer shortly after one year, but it seems like that trajectory is not exactly the same for DS.

I know this will also vary by company (with some tech companies requiring a PhD for data scientists at the lowest level).",t2_3ahwym06,False,,0,False,YoE to go from entry-level/MS data scientist to mid-level/senior data scientist? (USA),[],r/datascience,False,6,career,0,,,False,t3_nyegax,False,dark,0.6,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623555139.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a new grad MS working as an entry-level data scientist. Most of the PhDs at my company are either at the senior level or managerial, but most also have a few years of experience too.&lt;/p&gt;

&lt;p&gt;In general how much years of experience do you need to go from my current level to a senior data scientist and then to the principal/managerial level? I know a few new grad colleagues in SWE who got to senior engineer shortly after one year, but it seems like that trajectory is not exactly the same for DS.&lt;/p&gt;

&lt;p&gt;I know this will also vary by company (with some tech companies requiring a PhD for data scientists at the lowest level).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyegax,True,,bigchungusmode96,,12,True,all_ads,False,[],False,,/r/datascience/comments/nyegax/yoe_to_go_from_entrylevelms_data_scientist_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyegax/yoe_to_go_from_entrylevelms_data_scientist_to/,515406,1623526339.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm a senior at university and I got a pretty nice internship somehow. I keep getting assigned work with nlp stuff that I don't know how to do. I read the theory behind it some time ago and I watch youtube videos, but I haven't had the opportunity to practice yet. Is this feeling of not knowing what you're doing normal? I've mainly worked with basic machine learning in the past, not much deep learning.

&amp;#x200B;

EDIT: Thanks for all the responses guys. I had some anxiety coming in this week and its settling down. Lots of great advice here as well.",t2_3ygdf0hp,False,,0,False,Is it common to feel like you have no idea what you're doing in an internship?,[],r/datascience,False,6,discussion,0,,,False,t3_nxi5db,False,dark,0.97,,public,355,2,{},,,False,[],,False,False,,{},Discussion,False,355,,False,False,self,1623432685.0,,[],{},,True,,1623452785.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a senior at university and I got a pretty nice internship somehow. I keep getting assigned work with nlp stuff that I don&amp;#39;t know how to do. I read the theory behind it some time ago and I watch youtube videos, but I haven&amp;#39;t had the opportunity to practice yet. Is this feeling of not knowing what you&amp;#39;re doing normal? I&amp;#39;ve mainly worked with basic machine learning in the past, not much deep learning.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: Thanks for all the responses guys. I had some anxiety coming in this week and its settling down. Lots of great advice here as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxi5db,True,,trippygrass,,66,True,all_ads,False,[],False,,/r/datascience/comments/nxi5db/is_it_common_to_feel_like_you_have_no_idea_what/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxi5db/is_it_common_to_feel_like_you_have_no_idea_what/,515406,1623423985.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am offered a data scientist job at a budding health startup. As per the CEO job is ""not just about prediction"" but helping them to make a better product. At this new start up: i would be involved in deciding which product would fit vertically/horizontally with the current product, finding and discussing with right stakeholders out side the company, finding/buying data, making models. I sense the CEO is keen to license other models to integrate within the product. 

I am fine with product development part but I would like to build model myself. I am afraid if the CEO just lincense other model then i would be end up setting infrastructure to use the models within current products. 

Another key point is I am currently working in non mathematics/physics academic lab (where i do not do any modeling), and would like to transition to data science, learned data science through bootcamp/personal projects.

 My concern is would I get stuck within product infrastructure without making my hands dirty!

Edited: for clarity",t2_de03u,False,,0,False,Should I take this Data scientist job offer,[],r/datascience,False,6,career,0,,,False,t3_nye9ya,False,dark,0.36,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623539463.0,,[],{},,True,,1623554645.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am offered a data scientist job at a budding health startup. As per the CEO job is &amp;quot;not just about prediction&amp;quot; but helping them to make a better product. At this new start up: i would be involved in deciding which product would fit vertically/horizontally with the current product, finding and discussing with right stakeholders out side the company, finding/buying data, making models. I sense the CEO is keen to license other models to integrate within the product. &lt;/p&gt;

&lt;p&gt;I am fine with product development part but I would like to build model myself. I am afraid if the CEO just lincense other model then i would be end up setting infrastructure to use the models within current products. &lt;/p&gt;

&lt;p&gt;Another key point is I am currently working in non mathematics/physics academic lab (where i do not do any modeling), and would like to transition to data science, learned data science through bootcamp/personal projects.&lt;/p&gt;

&lt;p&gt;My concern is would I get stuck within product infrastructure without making my hands dirty!&lt;/p&gt;

&lt;p&gt;Edited: for clarity&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nye9ya,True,,karanphosphatase,,14,True,all_ads,False,[],False,,/r/datascience/comments/nye9ya/should_i_take_this_data_scientist_job_offer/,all_ads,False,https://www.reddit.com/r/datascience/comments/nye9ya/should_i_take_this_data_scientist_job_offer/,515406,1623525845.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Change my mind.,t2_pyilr,False,,0,False,Julia has rendered Pandas obsolete,[],r/datascience,False,6,discussion,0,,,False,t3_nyj11a,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623567657.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Change my mind.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nyj11a,True,,LornartheBreton,,10,True,all_ads,False,[],False,,/r/datascience/comments/nyj11a/julia_has_rendered_pandas_obsolete/,all_ads,False,https://www.reddit.com/r/datascience/comments/nyj11a/julia_has_rendered_pandas_obsolete/,515406,1623538857.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What steps would you take to make sense of a large new dataset that has been sent to you? 



Had the above question pop up in an interview. So from a data science perspective, what would you do?",t2_4kym3qu,False,,0,False,Steps when working with a new database,[],r/datascience,False,6,discussion,0,,,False,t3_nxru9w,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,1623450706.0,,[],{},,True,,1623478820.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What steps would you take to make sense of a large new dataset that has been sent to you? &lt;/p&gt;

&lt;p&gt;Had the above question pop up in an interview. So from a data science perspective, what would you do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxru9w,True,,Plebn,,5,True,all_ads,False,[],False,,/r/datascience/comments/nxru9w/steps_when_working_with_a_new_database/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxru9w/steps_when_working_with_a_new_database/,515406,1623450020.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,True,,,,
,datascience,"There was some good discussion in a previous post about not ""working"" the full day.  Im curious to hear for days when there is work to do that isnt entirely motivating, what do you do to keep energy up?  Since I've been with jobs that involve sitting all day, I usually hit a slump around 2pm and am trying to find more techniques for picking up my energy and focus.

Sometimes I'll take my dog for a walk or give myself a 10 minute Reddit break, but what works for you?",t2_1xf97mwo,False,,0,False,What do you do to combat the afternoon slump?,[],r/datascience,False,6,discussion,0,,,False,t3_nwww2s,False,dark,0.96,,public,278,1,{},,,False,[],,False,False,,{},Discussion,False,278,,False,False,self,False,,[],{},,True,,1623383881.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There was some good discussion in a previous post about not &amp;quot;working&amp;quot; the full day.  Im curious to hear for days when there is work to do that isnt entirely motivating, what do you do to keep energy up?  Since I&amp;#39;ve been with jobs that involve sitting all day, I usually hit a slump around 2pm and am trying to find more techniques for picking up my energy and focus.&lt;/p&gt;

&lt;p&gt;Sometimes I&amp;#39;ll take my dog for a walk or give myself a 10 minute Reddit break, but what works for you?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwww2s,True,,Beekle1014,,115,True,all_ads,False,[],False,,/r/datascience/comments/nwww2s/what_do_you_do_to_combat_the_afternoon_slump/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwww2s/what_do_you_do_to_combat_the_afternoon_slump/,515406,1623355081.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win Kaggle competitions (i.e. perform well for medium sized tabular datasets) compared to deep neural networks and linear regression models?

Heuristically, here are my general conclusions:

1) GLM (general linear models) perform best on smaller sized datasets, provided certain statistical assumptions are met.

2) boosting and bagging algorithms (e.g. random forest and xgboost) perform best on larger tabular datasets, and do not require many statistical assumptions.

3) deep neural networks perform best on very large datasets, preferably on non tabular datasets (e.g. tensors, pictures, audio, computer vision, text/nlp).

But can there be any mathematical reasons that try to explain these general conclusions (provided these conclusions are correct)? 

For instance, suppose there is one response variable and one predictor variable, and when graphed together they look like a sine wave - it seems unlikely that a linear regression model could perform well. Perhaps this is because a linear model can only capture a linear trend? Perhaps it is too hard to understand the exact assumptions required for GLM models to work on real world data, or they are too prone to overfit on complex data?

The same way, is there any math that explains why alphaGO, self driving cars and Google's BERT NLP model are all based on neural networks - and not using random forest and xgboost? Is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets? Perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data, which is just not computationally possible ... or would surely result in overfitting?

And the same way, is there any math that explains why deep neural networks aren't as successful as random forest and xgboost on medium sized tabluar datasets? Do deep neural networks simply require too much effort to select the right combinations of hyperparameters, and its just not worth it for medium sized datasets when random forests work well given significantly less effort? Are deep neural networks to prone to overfit medium datasets? 

Of course, all of this comes to down to trial and error: if a certain model fits the training and test data well - then use that model. But just using mathematical logic and intuition, can we develop some general guidelines that tell us which conditions and types/size of data are favorable for specific algorithms? This could potentially save us a lot of time by directly trying better suited models for the task at hand (e.g. not even trying to use logistic regression for alphaGO). 

So in the end: Beyond empirical results, could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks? And beyond empirical results, could there be any reasons why random forest and xgboost are not chosen for the ImageNet competition?

Thanks",t2_3f0i9m72,False,,0,False,"Can we begin to understand possible mathematical reasons as to why algorithms like ""xgboost"" and ""random forest"" win Kaggle Competitions, instead of neural networks?",[],r/datascience,False,6,discussion,0,,,False,t3_nxn6we,False,dark,0.57,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623465962.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win Kaggle competitions (i.e. perform well for medium sized tabular datasets) compared to deep neural networks and linear regression models?&lt;/p&gt;

&lt;p&gt;Heuristically, here are my general conclusions:&lt;/p&gt;

&lt;p&gt;1) GLM (general linear models) perform best on smaller sized datasets, provided certain statistical assumptions are met.&lt;/p&gt;

&lt;p&gt;2) boosting and bagging algorithms (e.g. random forest and xgboost) perform best on larger tabular datasets, and do not require many statistical assumptions.&lt;/p&gt;

&lt;p&gt;3) deep neural networks perform best on very large datasets, preferably on non tabular datasets (e.g. tensors, pictures, audio, computer vision, text/nlp).&lt;/p&gt;

&lt;p&gt;But can there be any mathematical reasons that try to explain these general conclusions (provided these conclusions are correct)? &lt;/p&gt;

&lt;p&gt;For instance, suppose there is one response variable and one predictor variable, and when graphed together they look like a sine wave - it seems unlikely that a linear regression model could perform well. Perhaps this is because a linear model can only capture a linear trend? Perhaps it is too hard to understand the exact assumptions required for GLM models to work on real world data, or they are too prone to overfit on complex data?&lt;/p&gt;

&lt;p&gt;The same way, is there any math that explains why alphaGO, self driving cars and Google&amp;#39;s BERT NLP model are all based on neural networks - and not using random forest and xgboost? Is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets? Perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data, which is just not computationally possible ... or would surely result in overfitting?&lt;/p&gt;

&lt;p&gt;And the same way, is there any math that explains why deep neural networks aren&amp;#39;t as successful as random forest and xgboost on medium sized tabluar datasets? Do deep neural networks simply require too much effort to select the right combinations of hyperparameters, and its just not worth it for medium sized datasets when random forests work well given significantly less effort? Are deep neural networks to prone to overfit medium datasets? &lt;/p&gt;

&lt;p&gt;Of course, all of this comes to down to trial and error: if a certain model fits the training and test data well - then use that model. But just using mathematical logic and intuition, can we develop some general guidelines that tell us which conditions and types/size of data are favorable for specific algorithms? This could potentially save us a lot of time by directly trying better suited models for the task at hand (e.g. not even trying to use logistic regression for alphaGO). &lt;/p&gt;

&lt;p&gt;So in the end: Beyond empirical results, could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks? And beyond empirical results, could there be any reasons why random forest and xgboost are not chosen for the ImageNet competition?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxn6we,True,,SQL_beginner,,7,True,all_ads,False,[],False,,/r/datascience/comments/nxn6we/can_we_begin_to_understand_possible_mathematical/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxn6we/can_we_begin_to_understand_possible_mathematical/,515406,1623437162.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_c8kps30y,False,,0,False,How many of you bioinformaticians or related (working or studying) know or have used Docker and BioPortainer?,[],r/datascience,False,6,discussion,0,,,False,t3_nxr7dl,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,default,False,,[],{},,False,,1623477009.0,text,6,,,text,self.bioinformatics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxr7dl,True,,DavidAciole,,1,True,all_ads,False,[],False,,/r/datascience/comments/nxr7dl/how_many_of_you_bioinformaticians_or_related/,all_ads,False,/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/,515406,1623448209.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?auto=webp&amp;s=225830c14f9c675a2d6936716884c914be25234b', 'width': 1587, 'height': 1123}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5bc7475a8a74e6f6ac488cfb9e89c09a7d9f55d', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc1899399640458afe2aeb59b65945468fa4bd68', 'width': 216, 'height': 152}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f0110fb3f94a17146d11ed01f341a7f0596c6a8', 'width': 320, 'height': 226}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4676d9baeb5de5d8400122da2a7ee74174f7678e', 'width': 640, 'height': 452}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0285b27f19659b2cd877138d29f54817b18f24c7', 'width': 960, 'height': 679}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1c1a8be22952bfc6606c9b9a9b8d06daf0708b7', 'width': 1080, 'height': 764}], 'variants': {}, 'id': '4BTJwXlrWtbTBSDy62N6aTx6FtLxgiNw-XQdSpoTzu0'}], 'enabled': False}",,"[{'approved_at_utc': None, 'subreddit': 'bioinformatics', 'selftext': '[BioPortainer](https://bioportainer.github.io/BioPortainer/) and [Docker](https://www.docker.com) can be used by bioinformaticians to elegantly manage virtual environments. How many of you know/work with [Docker](https://www.docker.com)? And how many use bioinformatics-directed platforms to manage it, like [BioPortainer](https://bioportainer.github.io/BioPortainer/)? For those who used it, what you think about it?\n\n[View Poll](https://www.reddit.com/poll/nxr27j)', 'author_fullname': 't2_c8kps30y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How many of you bioinformaticians or related (working or studying) know or have used Docker and BioPortainer?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/bioinformatics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nxr27j', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.64, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'discussion', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1623448511.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1623476604.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.bioinformatics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://bioportainer.github.io/BioPortainer/""&gt;BioPortainer&lt;/a&gt; and &lt;a href=""https://www.docker.com""&gt;Docker&lt;/a&gt; can be used by bioinformaticians to elegantly manage virtual environments. How many of you know/work with &lt;a href=""https://www.docker.com""&gt;Docker&lt;/a&gt;? And how many use bioinformatics-directed platforms to manage it, like &lt;a href=""https://bioportainer.github.io/BioPortainer/""&gt;BioPortainer&lt;/a&gt;? For those who used it, what you think about it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nxr27j""&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?auto=webp&amp;s=225830c14f9c675a2d6936716884c914be25234b', 'width': 1587, 'height': 1123}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5bc7475a8a74e6f6ac488cfb9e89c09a7d9f55d', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc1899399640458afe2aeb59b65945468fa4bd68', 'width': 216, 'height': 152}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f0110fb3f94a17146d11ed01f341a7f0596c6a8', 'width': 320, 'height': 226}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4676d9baeb5de5d8400122da2a7ee74174f7678e', 'width': 640, 'height': 452}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0285b27f19659b2cd877138d29f54817b18f24c7', 'width': 960, 'height': 679}, {'url': 'https://external-preview.redd.it/ZZq4q0uSmxDYbpIF5TMJzC5kO4Rmn43kyPT04SWZna0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1c1a8be22952bfc6606c9b9a9b8d06daf0708b7', 'width': 1080, 'height': 764}], 'variants': {}, 'id': '4BTJwXlrWtbTBSDy62N6aTx6FtLxgiNw-XQdSpoTzu0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '20e12ad6-7f51-11e4-a315-22000b3617ab', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh0x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nxr27j', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'DavidAciole', 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': {'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1624052604338, 'options': [{'text': 'Wtf is Docker?', 'vote_count': 57, 'id': '8508428'}, {'text': 'Wtf is BioPortainer?', 'vote_count': 145, 'id': '8508429'}, {'text': 'I know Docker, but never used it', 'vote_count': 163, 'id': '8508430'}, {'text': 'I use Docker with BioPortainer', 'vote_count': 7, 'id': '8508431'}, {'text': 'I use Docker without BioPortainer', 'vote_count': 200, 'id': '8508432'}, {'text': ""I have no idea what I'm doing here"", 'vote_count': 186, 'id': '8508433'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 758, 'total_stake_amount': None}, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/', 'subreddit_subscribers': 65941, 'created_utc': 1623447804.0, 'num_crossposts': 5, 'media': None, 'is_video': False}]",/r/bioinformatics/comments/nxr27j/how_many_of_you_bioinformaticians_or_related/,t3_nxr27j,
,datascience,"Currently I’m working for a customer, which only allows visual basic and java. The project I got is to make visualisation, create a report in powerpoint automatically. This should be done easily in python if using matplotlib and pptx-python. Has anyone done data analytics in java like this before?",t2_a1bnnkle,False,,0,False,Data Analytics with java,[],r/datascience,False,6,discussion,0,,,False,t3_nxlv5l,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623462424.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I’m working for a customer, which only allows visual basic and java. The project I got is to make visualisation, create a report in powerpoint automatically. This should be done easily in python if using matplotlib and pptx-python. Has anyone done data analytics in java like this before?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxlv5l,True,,Individual-Sweet-734,,3,True,all_ads,False,[],False,,/r/datascience/comments/nxlv5l/data_analytics_with_java/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxlv5l/data_analytics_with_java/,515406,1623433624.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," Is  there any good idea of a mobile app which would require on-device  inference? I can't think of any. In my opinion, MobileNet times have  passed. Now, creating mobile-efficient models are only useful to surpass  some benchmarks. A weird flex and that's all in my view.

It  seems all useful ML-based mobile apps have already been implemented:  face recognition, image filters, sound classification, voice  recognition...

Is there any need for deploying a ML model on a mobile device and not using an external server + API?",t2_cl8ig5la,False,,0,False,Are mobile neural nets still relevant?,[],r/datascience,False,6,discussion,0,,,False,t3_nxjdcb,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623455985.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is  there any good idea of a mobile app which would require on-device  inference? I can&amp;#39;t think of any. In my opinion, MobileNet times have  passed. Now, creating mobile-efficient models are only useful to surpass  some benchmarks. A weird flex and that&amp;#39;s all in my view.&lt;/p&gt;

&lt;p&gt;It  seems all useful ML-based mobile apps have already been implemented:  face recognition, image filters, sound classification, voice  recognition...&lt;/p&gt;

&lt;p&gt;Is there any need for deploying a ML model on a mobile device and not using an external server + API?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxjdcb,True,,adenml,,5,True,all_ads,False,[],False,,/r/datascience/comments/nxjdcb/are_mobile_neural_nets_still_relevant/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxjdcb/are_mobile_neural_nets_still_relevant/,515406,1623427185.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I remember Andrew Gelman had a nice little Stan model for the World Cup.  Since the Euro starts tomorrow, I thought I would try my hand at forecasting some of the games.  I'm not a sports data scientist (I'm more of a biostats guy), but I do like Bayes and I do like international football, so I thought what the hell.

To keep myself honest, I'd like to post a few predictions here.  To keep in the spirit of the sub, I am willing to discuss data, models, critiques of either, with you all.

[Here](https://github.com/Dpananos/Euro2021Predictions/blob/main/predictions/predictions.csv) are my predictions for the group stage.  Since its on github, you can see if I cheat and push predictions after the fact.

I plan to make predictions for each major stage and then update the model after that (so the group stage will pass, I'll calculate my Brier score and cross-entropy loss, I'll refit the model, and go from there).

But the real point of the post is to let you all know you're free to piggy-back off my efforts.  Clone the repo and try your hand.  I'd love to learn a thing or two about these sorts of modelling problems from the community.",t2_131vu3d,False,,0,False,Euro 2020 Predictions,[],r/datascience,False,6,discussion,0,,,False,t3_nx1guv,False,dark,0.95,,public,45,0,{},,,False,[],,False,False,,{},Discussion,False,45,,False,False,self,False,modflair,[],{},,True,,1623395906.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I remember Andrew Gelman had a nice little Stan model for the World Cup.  Since the Euro starts tomorrow, I thought I would try my hand at forecasting some of the games.  I&amp;#39;m not a sports data scientist (I&amp;#39;m more of a biostats guy), but I do like Bayes and I do like international football, so I thought what the hell.&lt;/p&gt;

&lt;p&gt;To keep myself honest, I&amp;#39;d like to post a few predictions here.  To keep in the spirit of the sub, I am willing to discuss data, models, critiques of either, with you all.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/Dpananos/Euro2021Predictions/blob/main/predictions/predictions.csv""&gt;Here&lt;/a&gt; are my predictions for the group stage.  Since its on github, you can see if I cheat and push predictions after the fact.&lt;/p&gt;

&lt;p&gt;I plan to make predictions for each major stage and then update the model after that (so the group stage will pass, I&amp;#39;ll calculate my Brier score and cross-entropy loss, I&amp;#39;ll refit the model, and go from there).&lt;/p&gt;

&lt;p&gt;But the real point of the post is to let you all know you&amp;#39;re free to piggy-back off my efforts.  Clone the repo and try your hand.  I&amp;#39;d love to learn a thing or two about these sorts of modelling problems from the community.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,Data Scientist,[],False,,,,t5_2sptq,,,,nx1guv,True,,__compactsupport__,,15,True,all_ads,False,[],False,dark,/r/datascience/comments/nx1guv/euro_2020_predictions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx1guv/euro_2020_predictions/,515406,1623367106.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?auto=webp&amp;s=0417942dffa68513b288d4522285b20ab33421d0', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b490b45b3c3c81707e0c2e51074f9d79ca125f67', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ec142b1e0b92d6bbacf20aaee0ef6c1765af46b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4d81256dd502a9fda572cc6a2db719d5e383ae1', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd7f9bd78fdf9576c2e15278e6d38efd8d0cb776', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f1f817575ddc8b9f914d3f80c84c6b52d74403c', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/oDDj061ZbQdaimZ86FcKIOPo42mPmhQ48NyTQx3vY-E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79a9c64aff6057d14c9ade6f77bf4cf7630a8fca', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'qLoV4x39urq-R60R0FGRs1usiDBtr5KQCPn6tdleV6Y'}], 'enabled': False}",,,,,
,datascience,Hi all. I have two offers. One is from a big4bank for a data analyst internship (pays less) but I believe they use state of art technology and there is a lot to learn; The work is all for internal stuff.  The second offer is to work as a project analyst in which I will be working with clients. The company is big but not in the top10 or anything. The advantage of this company is that I’ll be working with clients which means more exposure and connections. They also use near top tech. I'm not sure which one will provide most growth. Any advice appreciated,t2_7ciqprlz,False,,0,False,Comparing a data analyst intern (Bank) offer with IT project analyst (Consultancy) offer,[],r/datascience,False,6,,0,,,False,t3_nxc78c,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,1623411455.0,,[],{},,True,,1623433997.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I have two offers. One is from a big4bank for a data analyst internship (pays less) but I believe they use state of art technology and there is a lot to learn; The work is all for internal stuff.  The second offer is to work as a project analyst in which I will be working with clients. The company is big but not in the top10 or anything. The advantage of this company is that I’ll be working with clients which means more exposure and connections. They also use near top tech. I&amp;#39;m not sure which one will provide most growth. Any advice appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nxc78c,True,,MalangChic,,19,True,all_ads,False,[],False,,/r/datascience/comments/nxc78c/comparing_a_data_analyst_intern_bank_offer_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxc78c/comparing_a_data_analyst_intern_bank_offer_with/,515406,1623405197.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hello guys, I'd like to discuss about data product concept or situation in a growing company. 

So here is the situation. I am is a one of the data lead in a fast growing company in southeast asia. I bet in this year, we've doubled our MAU and getting the attention of market.

But, right now, in our data team, there are a lot of different spectrum that need to be achieved, ranging from data warehousing, data collection (ingestion), government, into creating data product like recommendation system, fraud detection, etc.

The problem here is, the data team, doesn’t have the skill like project management or PM skill related like scrum or agile methodologies to perform those wide ranging tasks.

Does anyone in this Data Science forum has a problem to integrate your expertise into your product ? Or even doesn't have any idea to breakdown your analysis or model creation into several part of tasks?

Cause, I feel that we are a bunch of experts that having no strategical or tactical solution for managing project.

Does anyone here has a same experience when serving data for product team or your company?",t2_42bjrsld,False,,0,False,Question about Data Product,[],r/datascience,False,6,discussion,0,,,False,t3_nxkfwj,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623458727.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, I&amp;#39;d like to discuss about data product concept or situation in a growing company. &lt;/p&gt;

&lt;p&gt;So here is the situation. I am is a one of the data lead in a fast growing company in southeast asia. I bet in this year, we&amp;#39;ve doubled our MAU and getting the attention of market.&lt;/p&gt;

&lt;p&gt;But, right now, in our data team, there are a lot of different spectrum that need to be achieved, ranging from data warehousing, data collection (ingestion), government, into creating data product like recommendation system, fraud detection, etc.&lt;/p&gt;

&lt;p&gt;The problem here is, the data team, doesn’t have the skill like project management or PM skill related like scrum or agile methodologies to perform those wide ranging tasks.&lt;/p&gt;

&lt;p&gt;Does anyone in this Data Science forum has a problem to integrate your expertise into your product ? Or even doesn&amp;#39;t have any idea to breakdown your analysis or model creation into several part of tasks?&lt;/p&gt;

&lt;p&gt;Cause, I feel that we are a bunch of experts that having no strategical or tactical solution for managing project.&lt;/p&gt;

&lt;p&gt;Does anyone here has a same experience when serving data for product team or your company?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxkfwj,True,,fazz21,,0,True,all_ads,False,[],False,,/r/datascience/comments/nxkfwj/question_about_data_product/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxkfwj/question_about_data_product/,515406,1623429927.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"https://www.nature.com/articles/s41467-020-17280-8/figures/2

These graphs come from the following article on covid-19 and statistical models: https://www.nature.com/articles/s41467-020-17280-8

These graphs show the ""30 Day Risk Probabilities"" of ""critically ill patients"" vs ""other patients"". In these graphs, is a value of ""1"" supposed to indicate ""very high risk that a patient develops covid-19 symptoms""? So basically, these graphs are showing the day-by-day risk that (different groups of) patients develop covid-19 symptoms after their last hospital visit?

Thanks",t2_o4xj9,False,,0,False,Can someone please help me understand these graphs?,[],r/datascience,False,6,discussion,0,,,False,t3_nxk4vu,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623457937.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.nature.com/articles/s41467-020-17280-8/figures/2""&gt;https://www.nature.com/articles/s41467-020-17280-8/figures/2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These graphs come from the following article on covid-19 and statistical models: &lt;a href=""https://www.nature.com/articles/s41467-020-17280-8""&gt;https://www.nature.com/articles/s41467-020-17280-8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These graphs show the &amp;quot;30 Day Risk Probabilities&amp;quot; of &amp;quot;critically ill patients&amp;quot; vs &amp;quot;other patients&amp;quot;. In these graphs, is a value of &amp;quot;1&amp;quot; supposed to indicate &amp;quot;very high risk that a patient develops covid-19 symptoms&amp;quot;? So basically, these graphs are showing the day-by-day risk that (different groups of) patients develop covid-19 symptoms after their last hospital visit?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxk4vu,True,,blueest,,1,True,all_ads,False,[],False,,/r/datascience/comments/nxk4vu/can_someone_please_help_me_understand_these_graphs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxk4vu/can_someone_please_help_me_understand_these_graphs/,515406,1623429137.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello there I'm a data science student and I was looking for the Neoj4 book for graphs databases and machine learning, being a student I can't get the free copy.
Can someone pass it to me or suggest another book.

Thanks for the attention.",t2_4elw77qh,False,,0,False,Neoj4 books,[],r/datascience,False,6,education,0,,,False,t3_nxbx5g,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,,[],{},,True,,1623432887.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there I&amp;#39;m a data science student and I was looking for the Neoj4 book for graphs databases and machine learning, being a student I can&amp;#39;t get the free copy.
Can someone pass it to me or suggest another book.&lt;/p&gt;

&lt;p&gt;Thanks for the attention.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nxbx5g,True,,NitPo,,2,True,all_ads,False,[],False,,/r/datascience/comments/nxbx5g/neoj4_books/,all_ads,False,https://www.reddit.com/r/datascience/comments/nxbx5g/neoj4_books/,515406,1623404087.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi all,

I'm building my project portfolios, aiming for banks, advertisement firms..

I have a serious question: what's our competitive advantage as data scientists comparing to those software engineers/web developer?

This question has been wandering back of my head.

For SE, they can find jobs quite quickly even freelancing jobs. More demands for it in the job markets

For DS, you have to be really competitive across mathematics/statistics, software engineering, the domain knowledge. Fairly saying researchers could be data scientists. 

My daily frustration is there are more demands for senior DS rather than juniors...

Is there any freelancing jobs for DS?

Cheers!",t2_2xpkxipc,False,,0,False,What kind of freelancing jobs can data scientist do?,[],r/datascience,False,6,projects,0,,,False,t3_nwuzir,False,dark,0.9,,public,26,0,{},,,False,[],,False,False,,{},Projects,False,26,,False,False,self,False,,[],{},,True,,1623379103.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m building my project portfolios, aiming for banks, advertisement firms..&lt;/p&gt;

&lt;p&gt;I have a serious question: what&amp;#39;s our competitive advantage as data scientists comparing to those software engineers/web developer?&lt;/p&gt;

&lt;p&gt;This question has been wandering back of my head.&lt;/p&gt;

&lt;p&gt;For SE, they can find jobs quite quickly even freelancing jobs. More demands for it in the job markets&lt;/p&gt;

&lt;p&gt;For DS, you have to be really competitive across mathematics/statistics, software engineering, the domain knowledge. Fairly saying researchers could be data scientists. &lt;/p&gt;

&lt;p&gt;My daily frustration is there are more demands for senior DS rather than juniors...&lt;/p&gt;

&lt;p&gt;Is there any freelancing jobs for DS?&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwuzir,True,,homchange,,25,True,all_ads,False,[],False,,/r/datascience/comments/nwuzir/what_kind_of_freelancing_jobs_can_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwuzir/what_kind_of_freelancing_jobs_can_data_scientist/,515406,1623350303.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi!

Work from home has been wonderful ever since it has been implemented but I've found myself not working much on days like today. I just wasn't feeling like it. I'm not sure if it's a good thing or a bad thing about work from home. 

Do you guys have days like this too?

Not sure if it helps but I'm not missing out on any targets, deadlines. Manager is quite happy with what I'm delivering and I might even get promoted next year. 
But today I didn't have much to do and I just felt like relaxing and listening to a podcast instead of upskilling or working on left over small tasks at work.
Also, I'm a junior. Just finished my first year after grad school.

Thanks!",t2_bv171ji2,False,,0,False,Is it normal to feel guilty when you don't work much on a work day?,[],r/datascience,False,6,discussion,0,,,False,t3_nw9zkt,False,dark,0.95,,public,499,4,{},,,False,[],,False,False,,{},Discussion,False,499,,False,False,self,False,,[],{},,True,,1623312012.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Work from home has been wonderful ever since it has been implemented but I&amp;#39;ve found myself not working much on days like today. I just wasn&amp;#39;t feeling like it. I&amp;#39;m not sure if it&amp;#39;s a good thing or a bad thing about work from home. &lt;/p&gt;

&lt;p&gt;Do you guys have days like this too?&lt;/p&gt;

&lt;p&gt;Not sure if it helps but I&amp;#39;m not missing out on any targets, deadlines. Manager is quite happy with what I&amp;#39;m delivering and I might even get promoted next year. 
But today I didn&amp;#39;t have much to do and I just felt like relaxing and listening to a podcast instead of upskilling or working on left over small tasks at work.
Also, I&amp;#39;m a junior. Just finished my first year after grad school.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw9zkt,True,,quite--average,,147,True,all_ads,False,[],False,,/r/datascience/comments/nw9zkt/is_it_normal_to_feel_guilty_when_you_dont_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw9zkt/is_it_normal_to_feel_guilty_when_you_dont_work/,515406,1623283212.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi guys! So I'm enrolled in a university course called Legal Analytics for which I had to do a group research project on a dataset of case law metadata. So our group basically made dictionaries to map existing metadata values to a new label (i.e. apply label x if y exists in metadata).

In order to improve our dictionaries we repeated a certain exploratory exercise multiple times to discover additional values and add those to the dictionaries and after each update of the dictionaries we repeated the labelling proces as well. Ultimately, we visualized this process and the difference in labelled cases between each repetition of updating dictionaries and labelling cases in some line charts. In our draft research paper we referred to each repetition as an 'iteration' and to the process as a whole as 'classification'.

Now, this is where my question and confusion comes in. We've got some PhDs tutoring this course and grading our research projects. A part of their feedback on our draft research paper was about our use of terminology; they basically said that terms like iteration and classification are reserved to machine learning and we shouldn't use those terms in our paper.

This left me not entirely convinced, because while machine learning is all the fuzz these days these terms are surely not limited to the field of machine learning? So, are my tutors right? Are more simple solutions suddenly excluded from using this kind of terminology? I'll be looking forward to your responses!",t2_cugn2,False,,0,False,Analytical uni project: can't use certain terminology in research paper?,[],r/datascience,False,6,discussion,0,,,False,t3_nwv0ve,False,dark,0.73,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,False,,[],{},,True,,1623379195.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! So I&amp;#39;m enrolled in a university course called Legal Analytics for which I had to do a group research project on a dataset of case law metadata. So our group basically made dictionaries to map existing metadata values to a new label (i.e. apply label x if y exists in metadata).&lt;/p&gt;

&lt;p&gt;In order to improve our dictionaries we repeated a certain exploratory exercise multiple times to discover additional values and add those to the dictionaries and after each update of the dictionaries we repeated the labelling proces as well. Ultimately, we visualized this process and the difference in labelled cases between each repetition of updating dictionaries and labelling cases in some line charts. In our draft research paper we referred to each repetition as an &amp;#39;iteration&amp;#39; and to the process as a whole as &amp;#39;classification&amp;#39;.&lt;/p&gt;

&lt;p&gt;Now, this is where my question and confusion comes in. We&amp;#39;ve got some PhDs tutoring this course and grading our research projects. A part of their feedback on our draft research paper was about our use of terminology; they basically said that terms like iteration and classification are reserved to machine learning and we shouldn&amp;#39;t use those terms in our paper.&lt;/p&gt;

&lt;p&gt;This left me not entirely convinced, because while machine learning is all the fuzz these days these terms are surely not limited to the field of machine learning? So, are my tutors right? Are more simple solutions suddenly excluded from using this kind of terminology? I&amp;#39;ll be looking forward to your responses!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwv0ve,True,,Jansenzoon,,20,True,all_ads,False,[],False,,/r/datascience/comments/nwv0ve/analytical_uni_project_cant_use_certain/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwv0ve/analytical_uni_project_cant_use_certain/,515406,1623350395.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am still in academics and most of my activities and projects have had limited scope and it's nothing like what I may work like in industry. I would really appreciate if someone can share their experience of working as a data scientist in the industry.

How do you go about finding a problem/challenge/idea of what to work on? What steps does a project go through? Do you follow SDLC kind of methodologies for a DS project? Do you participate in model deployment? What tools do you use? 

I know this is not a structured list, but even though I am working through school it is really difficult for me to absorb anything without actually understanding industry practices",t2_a4yvnttd,False,,0,False,Project lifecycle,[],r/datascience,False,6,education,0,,,False,t3_nx3q3s,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623402778.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am still in academics and most of my activities and projects have had limited scope and it&amp;#39;s nothing like what I may work like in industry. I would really appreciate if someone can share their experience of working as a data scientist in the industry.&lt;/p&gt;

&lt;p&gt;How do you go about finding a problem/challenge/idea of what to work on? What steps does a project go through? Do you follow SDLC kind of methodologies for a DS project? Do you participate in model deployment? What tools do you use? &lt;/p&gt;

&lt;p&gt;I know this is not a structured list, but even though I am working through school it is really difficult for me to absorb anything without actually understanding industry practices&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx3q3s,True,,DietMediocre8993,,3,True,all_ads,False,[],False,,/r/datascience/comments/nx3q3s/project_lifecycle/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx3q3s/project_lifecycle/,515406,1623373978.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I head a BI department for a startup; well not much of a startup; We are almost a small enterprise now.

Currently, I run a suite of BI applications from my machine. My main applications that are run regularly are:

* MS Office
* Alteryx
* Database (PostGres)
* Python
* Power BI
* Tableau

Its quite capable for what it does however, we are looking to make the access for these tools organisational and hence, we are going to invest in a server where all these applications will reside.

Can the community please help me with the best server configurations for running the above applications seamlessly? 

Of course, we are looking to have Windows Server as an OS.",t2_kxy1kb7,False,,0,False,What server configuration should I get for Organisational data analytics applications?,[],r/datascience,False,6,discussion,0,,,False,t3_nx0v79,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623394184.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I head a BI department for a startup; well not much of a startup; We are almost a small enterprise now.&lt;/p&gt;

&lt;p&gt;Currently, I run a suite of BI applications from my machine. My main applications that are run regularly are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MS Office&lt;/li&gt;
&lt;li&gt;Alteryx&lt;/li&gt;
&lt;li&gt;Database (PostGres)&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Power BI&lt;/li&gt;
&lt;li&gt;Tableau&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Its quite capable for what it does however, we are looking to make the access for these tools organisational and hence, we are going to invest in a server where all these applications will reside.&lt;/p&gt;

&lt;p&gt;Can the community please help me with the best server configurations for running the above applications seamlessly? &lt;/p&gt;

&lt;p&gt;Of course, we are looking to have Windows Server as an OS.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx0v79,True,,card_chase,,2,True,all_ads,False,[],False,,/r/datascience/comments/nx0v79/what_server_configuration_should_i_get_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx0v79/what_server_configuration_should_i_get_for/,515406,1623365384.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have north of 100 products with a request to build more accurate forecast for them. For a 1-2 month turnaround, what are folks thought process on the approach: 1) go for each distinct product and determine best model and parameters, or 2) find a higher level hierarchy to reduce cardinality from 100 plus to let’s say 10-12 models which is more manageable. 

First time working on a ts project and would appreciate your thoughts. Ty!",t2_3hsrhmve,False,,0,False,Time Series for 125 distinct products: Suggestions,[],r/datascience,False,6,discussion,0,,,False,t3_nwrtvs,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623371236.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have north of 100 products with a request to build more accurate forecast for them. For a 1-2 month turnaround, what are folks thought process on the approach: 1) go for each distinct product and determine best model and parameters, or 2) find a higher level hierarchy to reduce cardinality from 100 plus to let’s say 10-12 models which is more manageable. &lt;/p&gt;

&lt;p&gt;First time working on a ts project and would appreciate your thoughts. Ty!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwrtvs,True,,Novel_Frosting_1977,,8,False,all_ads,False,[],False,,/r/datascience/comments/nwrtvs/time_series_for_125_distinct_products_suggestions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwrtvs/time_series_for_125_distinct_products_suggestions/,515406,1623342436.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"How do y’all go about explaining results to the business that aren’t what they expected to be? For example, I have a model that is using variable B as the most important for predictions. The business is very upset because they believe that variable A should be the most important for predictions. We’ve showed them numerous examples within the dataset that was used for training as to why variable A is not the most important but they’re still dead set on hard coding that into the code regardless if it doesn’t improve the results. Any pointers here??",t2_dxitb2k,False,,0,False,Explaining results to the business side,[],r/datascience,False,6,discussion,0,,,False,t3_nwrrb2,False,dark,0.79,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1623371052.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do y’all go about explaining results to the business that aren’t what they expected to be? For example, I have a model that is using variable B as the most important for predictions. The business is very upset because they believe that variable A should be the most important for predictions. We’ve showed them numerous examples within the dataset that was used for training as to why variable A is not the most important but they’re still dead set on hard coding that into the code regardless if it doesn’t improve the results. Any pointers here??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwrrb2,True,,WittyWillow3,,13,True,all_ads,False,[],False,,/r/datascience/comments/nwrrb2/explaining_results_to_the_business_side/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwrrb2/explaining_results_to_the_business_side/,515406,1623342252.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Been studying data science/machine learning for about a year now and struggling to remember all the terms and information like what each model does, validation methods, statistics etc. How do you remember all these terms, what they do and when to use them?",t2_1bbzzghq,False,,0,False,How do you remember all the data science/machine learning terms?,[],r/datascience,False,6,discussion,0,,,False,t3_nwj24j,False,dark,0.95,,public,18,0,{},,,False,[],,False,False,,{},Discussion,False,18,,False,False,self,False,,[],{},,True,,1623344757.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Been studying data science/machine learning for about a year now and struggling to remember all the terms and information like what each model does, validation methods, statistics etc. How do you remember all these terms, what they do and when to use them?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwj24j,True,,Barlton_Canks,,21,True,all_ads,False,[],False,,/r/datascience/comments/nwj24j/how_do_you_remember_all_the_data_sciencemachine/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwj24j/how_do_you_remember_all_the_data_sciencemachine/,515406,1623315957.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_34qgdyb6,False,,0,False,"I am looking for a personalization project, can you suggest what new things we can do with some open source data?",[],r/datascience,False,6,projects,0,,,False,t3_nx72ro,False,dark,0.38,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1623413807.0,text,6,,,text,self.datascience,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx72ro,True,,yaakarsh1011,,12,True,all_ads,False,[],False,,/r/datascience/comments/nx72ro/i_am_looking_for_a_personalization_project_can/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx72ro/i_am_looking_for_a_personalization_project_can/,515406,1623385007.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"&amp;#x200B;

Hi, I used a feed forward neural network on Keras to approximate a concave one dimensional function , I would like to find the argmax and the max of my neural network, what would be the easiest way to solve this ? Should I implement something myself or does Keras already have some buil in function for that ?

Thanks!",t2_64rkugj2,False,,0,False,neural network optimization,[],r/datascience,False,6,discussion,0,,,False,t3_nx0ygv,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623394452.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hi, I used a feed forward neural network on Keras to approximate a concave one dimensional function , I would like to find the argmax and the max of my neural network, what would be the easiest way to solve this ? Should I implement something myself or does Keras already have some buil in function for that ?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nx0ygv,True,,draleo183013,,33,True,all_ads,False,[],False,,/r/datascience/comments/nx0ygv/neural_network_optimization/,all_ads,False,https://www.reddit.com/r/datascience/comments/nx0ygv/neural_network_optimization/,515406,1623365652.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a problem statement of designing a recommender system for a pharmaceutical company which also produces healthcare products (vitamins, body care products, etc.). But they don't have an e-commerce website similar to Amazon, etc. Therefore, data collection is incomplete and dodgy to say the least.

Their current approach involves placing adverts on social media platforms like Facebook, Instagram, etc. and then getting/gathering user data from such platform by either using platform specific APIs and/or web scraping.

To design a recommender system for this company, can you suggest:

1. what data/features I might need?
2. which system to use- collaborative/content based filtering

I am new to recommender system domain and this will be my pilot project.

Thanks!",t2_2mmql89p,False,,0,False,Recommender System Advice,[],r/datascience,False,6,discussion,0,,,False,t3_nwnlc4,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1623360306.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a problem statement of designing a recommender system for a pharmaceutical company which also produces healthcare products (vitamins, body care products, etc.). But they don&amp;#39;t have an e-commerce website similar to Amazon, etc. Therefore, data collection is incomplete and dodgy to say the least.&lt;/p&gt;

&lt;p&gt;Their current approach involves placing adverts on social media platforms like Facebook, Instagram, etc. and then getting/gathering user data from such platform by either using platform specific APIs and/or web scraping.&lt;/p&gt;

&lt;p&gt;To design a recommender system for this company, can you suggest:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;what data/features I might need?&lt;/li&gt;
&lt;li&gt;which system to use- collaborative/content based filtering&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am new to recommender system domain and this will be my pilot project.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwnlc4,True,,grid_world,,2,True,all_ads,False,[],False,,/r/datascience/comments/nwnlc4/recommender_system_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwnlc4/recommender_system_advice/,515406,1623331506.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Outside of a job working for a company or freelancing online as an employee or contractor. 

What other options are possible for a software developer to make money using their programming/computer science skills/data science background? 

Any of you using your technology/data skills to make money outside of your regular job?",t2_a4a10m8w,False,,0,False,Ways to make money using tech/data skills outside of a regular job?,[],r/datascience,False,6,career,0,,,False,t3_nwfvo3,False,dark,0.8,,public,14,0,{},,,False,[],,False,False,,{},Career,False,14,,False,False,self,False,,[],{},,True,,1623331527.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Outside of a job working for a company or freelancing online as an employee or contractor. &lt;/p&gt;

&lt;p&gt;What other options are possible for a software developer to make money using their programming/computer science skills/data science background? &lt;/p&gt;

&lt;p&gt;Any of you using your technology/data skills to make money outside of your regular job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwfvo3,True,,jodster327,,29,True,all_ads,False,[],False,,/r/datascience/comments/nwfvo3/ways_to_make_money_using_techdata_skills_outside/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwfvo3/ways_to_make_money_using_techdata_skills_outside/,515406,1623302727.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I’m a newbie in data science and I haven’t worked on any real life projects till now. 
The projects I’ve completed have been part of some online course where the instructors either give away the solution/approach to the problem or give you a big enough hint so that you don’t think about the approach but just code whatever they say.

I’ve been trying to solve a couple of projects of my own and it’s been hard to build that ds intuition. 
Normally, how do you guys approach a new problem? What kind of preprocessing do you do to your data, which model to fit, which metrics to choose? How do you guys decide it? I’d really like if you could help me develop some intuition on this",t2_9ga7xr3c,False,,0,False,How do you guys approach a new data science project?,[],r/datascience,False,6,discussion,0,,,False,t3_nwuckk,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623377532.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a newbie in data science and I haven’t worked on any real life projects till now. 
The projects I’ve completed have been part of some online course where the instructors either give away the solution/approach to the problem or give you a big enough hint so that you don’t think about the approach but just code whatever they say.&lt;/p&gt;

&lt;p&gt;I’ve been trying to solve a couple of projects of my own and it’s been hard to build that ds intuition. 
Normally, how do you guys approach a new problem? What kind of preprocessing do you do to your data, which model to fit, which metrics to choose? How do you guys decide it? I’d really like if you could help me develop some intuition on this&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwuckk,True,,ElNinoo9,,1,True,all_ads,False,[],False,,/r/datascience/comments/nwuckk/how_do_you_guys_approach_a_new_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwuckk/how_do_you_guys_approach_a_new_data_science/,515405,1623348732.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If someone at work approaches you to consult on a type of analytical project that you’ve never worked on before, what’s the best way to respond?

A. Sorry, I haven’t done this before and can’t really help you.
B. While I’m not familiar with this kind of analysis, I’d be glad to take a look and provide my input. 
C. Send it my way and I’ll make magic happen.
D. ?",t2_qnn4s,False,,0,False,Data project you’ve never worked on before,[],r/datascience,False,6,discussion,0,,,False,t3_nvylxh,False,dark,0.93,,public,92,0,{},,,False,[],,False,False,,{},Discussion,False,92,,False,False,self,False,,[],{},,True,,1623281360.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If someone at work approaches you to consult on a type of analytical project that you’ve never worked on before, what’s the best way to respond?&lt;/p&gt;

&lt;p&gt;A. Sorry, I haven’t done this before and can’t really help you.
B. While I’m not familiar with this kind of analysis, I’d be glad to take a look and provide my input. 
C. Send it my way and I’ll make magic happen.
D. ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvylxh,True,,Aoiumi1234,,32,True,all_ads,False,[],False,,/r/datascience/comments/nvylxh/data_project_youve_never_worked_on_before/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvylxh/data_project_youve_never_worked_on_before/,515405,1623252560.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi guys, hope you can help me out with this one! 

I'm looking to do some research I've never done before; to track / visualize how many times a word has been mentioned on a website and on what dates they were mentioned. 

So for example, lets say I want to get data on how many times the word ""Covid-19"" has been mentioned on a specific news website on each date over the past 2 years. I don't have a clue on how to attach a date to each mention, how could I do this? 

Thanks in advance!",t2_yjkb6,False,,0,False,Need some advice on tracking specific words on a website over time,[],r/datascience,False,6,tooling,0,,,False,t3_nwlc2r,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1623353565.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, hope you can help me out with this one! &lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking to do some research I&amp;#39;ve never done before; to track / visualize how many times a word has been mentioned on a website and on what dates they were mentioned. &lt;/p&gt;

&lt;p&gt;So for example, lets say I want to get data on how many times the word &amp;quot;Covid-19&amp;quot; has been mentioned on a specific news website on each date over the past 2 years. I don&amp;#39;t have a clue on how to attach a date to each mention, how could I do this? &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwlc2r,True,,twitchy-y,,4,True,all_ads,False,[],False,,/r/datascience/comments/nwlc2r/need_some_advice_on_tracking_specific_words_on_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwlc2r/need_some_advice_on_tracking_specific_words_on_a/,515405,1623324765.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I hope this is allowed here because the interview I am about to have is Data Analyst instead of Data Science.

&amp;#x200B;

So I have been doing python self-learn. I even took IBM certificate on Data Science. Sadly enough I did not have much experience in doing it. My only IT experience is an Web Developer internship that I just recently ended. 

&amp;#x200B;

I am determined to give Data Science/Analyst a try. Now that I will be interviewed for one, my imposter syndrome just kicked in and feels like I might not qualified for the job. What are the good advice for the interview? I have python and SQL skill myself. The company I applied for listed required around 1 - 2 years of analyst experience, also needed work on Azure. This could be my very first data related job, and I wish to overcome my imposter syndrome...",t2_4e9t63m8,False,,0,False,Imposter Syndrome kicks in for next week interview (Data Analyst),[],r/datascience,False,6,career,0,,,False,t3_nwjmc1,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1623347129.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I hope this is allowed here because the interview I am about to have is Data Analyst instead of Data Science.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I have been doing python self-learn. I even took IBM certificate on Data Science. Sadly enough I did not have much experience in doing it. My only IT experience is an Web Developer internship that I just recently ended. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am determined to give Data Science/Analyst a try. Now that I will be interviewed for one, my imposter syndrome just kicked in and feels like I might not qualified for the job. What are the good advice for the interview? I have python and SQL skill myself. The company I applied for listed required around 1 - 2 years of analyst experience, also needed work on Azure. This could be my very first data related job, and I wish to overcome my imposter syndrome...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwjmc1,True,,xopherwwl,,6,True,all_ads,False,[],False,,/r/datascience/comments/nwjmc1/imposter_syndrome_kicks_in_for_next_week/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwjmc1/imposter_syndrome_kicks_in_for_next_week/,515405,1623318329.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi,

Now I’m sure I’m not the only one but - although causal models are the holy grail, due to time and cost and laziness constraints all my models are just association models, not necessarily causal but observational in nature.

Easy, so I then communicate the associative nature of model and no DO operator or intervention has been modelled at all.
E.g. recently and elasticity model i modelled observed price % change against qty sold %  change.
But then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e.g. promoting a product in a given week.

So my main question - how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model?

Obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs. But then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long.

Do we just accept the mental leap from association to assuming some causal relation at times (in that intervening now will somehow get similar results as merely observing did) and keep collecting our pay checks with a smile?
I’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives.",t2_64z1j9uv,False,,0,False,Association model used as causal models,[],r/datascience,False,6,education,0,,,False,t3_nw9slt,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1623311412.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Now I’m sure I’m not the only one but - although causal models are the holy grail, due to time and cost and laziness constraints all my models are just association models, not necessarily causal but observational in nature.&lt;/p&gt;

&lt;p&gt;Easy, so I then communicate the associative nature of model and no DO operator or intervention has been modelled at all.
E.g. recently and elasticity model i modelled observed price % change against qty sold %  change.
But then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e.g. promoting a product in a given week.&lt;/p&gt;

&lt;p&gt;So my main question - how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model?&lt;/p&gt;

&lt;p&gt;Obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs. But then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long.&lt;/p&gt;

&lt;p&gt;Do we just accept the mental leap from association to assuming some causal relation at times (in that intervening now will somehow get similar results as merely observing did) and keep collecting our pay checks with a smile?
I’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw9slt,True,,darter_analyst,,5,True,all_ads,False,[],False,,/r/datascience/comments/nw9slt/association_model_used_as_causal_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw9slt/association_model_used_as_causal_models/,515405,1623282612.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hello, I’m an undergrad student whose been working with my college team doing baseball analytics work. Current role with a few other students had been to look at their pitch sequencing data and to find insights. I had an idea today that I was presenting to them, and I made sure not to overwhelm them with technicals, provided insight into methodology, (didn’t state a value proposition, which is where I probably messed up), but as I’m going, midway through my 30-45 sec breakdown the professor cuts me off and starts talking about another students idea. Doesn’t even let me finish. I was pretty disappointed because I was pretty confident in my idea, but he didn’t want to continue listening.

My question is, does this happen a lot in industry when presenting to management? Do they sometimes dismiss your ideas like this? 

Also maybe give me some advice on how to pitch something too, I am planning on scheduling a meeting with those two professors again to give them a full run through of my idea and why it’s useful.",t2_5w4i5kd1,False,,0,False,"Professor cuts me off mid presentation and dismissed my idea, how common is this in industry with management?",[],r/datascience,False,6,discussion,0,,,False,t3_nwbzk3,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623318224.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I’m an undergrad student whose been working with my college team doing baseball analytics work. Current role with a few other students had been to look at their pitch sequencing data and to find insights. I had an idea today that I was presenting to them, and I made sure not to overwhelm them with technicals, provided insight into methodology, (didn’t state a value proposition, which is where I probably messed up), but as I’m going, midway through my 30-45 sec breakdown the professor cuts me off and starts talking about another students idea. Doesn’t even let me finish. I was pretty disappointed because I was pretty confident in my idea, but he didn’t want to continue listening.&lt;/p&gt;

&lt;p&gt;My question is, does this happen a lot in industry when presenting to management? Do they sometimes dismiss your ideas like this? &lt;/p&gt;

&lt;p&gt;Also maybe give me some advice on how to pitch something too, I am planning on scheduling a meeting with those two professors again to give them a full run through of my idea and why it’s useful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwbzk3,True,,veeeerain,,15,True,all_ads,False,[],False,,/r/datascience/comments/nwbzk3/professor_cuts_me_off_mid_presentation_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwbzk3/professor_cuts_me_off_mid_presentation_and/,515405,1623289424.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi there, I have a super basic/amateur question and apologies for it being elementary. But I figured that this group would be the right set of professionals to ask. (PS - did I use the right flair?) 

I have some basic grouping (sums) and filtering needs for a large CSV data set. (500K plus records). Excel on the Mac is abysmal for such tasks, even though a basic pivot table would accomplish what I need. 

Is there any tool out there (could be web-based or local) that fits the bill, and sits between Excel and a full-fledged database? (I’d rather not import to SQL and query it back out.) 

Any thoughts/suggestions would be appreciated!",t2_bui35ym,False,,0,False,Super Basic Question: Filter &amp; Grouping Tool for Mac?,[],r/datascience,False,6,tooling,0,,,False,t3_nwhghp,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623337894.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, I have a super basic/amateur question and apologies for it being elementary. But I figured that this group would be the right set of professionals to ask. (PS - did I use the right flair?) &lt;/p&gt;

&lt;p&gt;I have some basic grouping (sums) and filtering needs for a large CSV data set. (500K plus records). Excel on the Mac is abysmal for such tasks, even though a basic pivot table would accomplish what I need. &lt;/p&gt;

&lt;p&gt;Is there any tool out there (could be web-based or local) that fits the bill, and sits between Excel and a full-fledged database? (I’d rather not import to SQL and query it back out.) &lt;/p&gt;

&lt;p&gt;Any thoughts/suggestions would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwhghp,True,,wkeber,,3,True,all_ads,False,[],False,,/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/,515405,1623309094.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Real-time technologies are powerful but add significant complexity to your data architecture.

Find out how to reap the benefits of real-time processing with the least architectural changes and maintenance effort: https://dashbird.io/blog/real-time-processing-analytical/",t2_6eow2vnb,False,,0,False,Is real-time processing worth it for your analytical use cases?,[],r/datascience,False,6,discussion,0,,,False,t3_nwkfcf,False,dark,0.17,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623350404.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Real-time technologies are powerful but add significant complexity to your data architecture.&lt;/p&gt;

&lt;p&gt;Find out how to reap the benefits of real-time processing with the least architectural changes and maintenance effort: &lt;a href=""https://dashbird.io/blog/real-time-processing-analytical/""&gt;https://dashbird.io/blog/real-time-processing-analytical/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwkfcf,True,,Dashbird,,0,True,all_ads,False,[],False,,/r/datascience/comments/nwkfcf/is_realtime_processing_worth_it_for_your/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwkfcf/is_realtime_processing_worth_it_for_your/,515405,1623321604.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?auto=webp&amp;s=ac9f24fab2a9805f51a0a01e7f17ac448ce608c0', 'width': 1000, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9161938e7257e4c0569c80bcd5259d163f8b9bb1', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0228bffc89df0acf7a21624ecd32649e077b1604', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2c26774ec7861e88a167f2a00f6ba5ddd6a71b', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=051d8de17ee457c6a5746644cde39f5f07f6f821', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/ItGd1COkaCHvlJ-rHPQWe-9AVKzZrFyzOFpHKJW1HRI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9c6a5face24fd4028a2add16411b02491aafb3c', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'sVlk7Y9ZxBsh0S3J6tiSTcn04usQl3zDO62iNvc_Lj4'}], 'enabled': False}",,,,,
,datascience,"I am thinking about joining Kaggle competitions to try and win money prizes, however I am unsure about the time and money (cloud compute?) it would take to get a real shot at it.

Did you try to join a competition with the specific goal to win money? Any advice?",t2_cn40g2j,False,,0,False,Shooting for Kaggle Competition Prizes - is it worth it?,[],r/datascience,False,6,discussion,0,,,False,t3_nwf2wq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623328454.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am thinking about joining Kaggle competitions to try and win money prizes, however I am unsure about the time and money (cloud compute?) it would take to get a real shot at it.&lt;/p&gt;

&lt;p&gt;Did you try to join a competition with the specific goal to win money? Any advice?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nwf2wq,True,,EnricoT0,,5,True,all_ads,False,[],False,,/r/datascience/comments/nwf2wq/shooting_for_kaggle_competition_prizes_is_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nwf2wq/shooting_for_kaggle_competition_prizes_is_it/,515405,1623299654.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"**Use-case:** *A particular product has X remaining items in stocks. Predict when the product is sold out, given both time-invariant and time-varying covariates.*

Is a multivariate Cox regression model appropriate for this problem, where the ""event"" is the product being sold out? What about if the items can be restocked (i.e. by users issuing returns), effectively increasing the stock in the process?

Is there a more sensible approach? I tried to look up something like ARIMAX for non-negative + discrete forecasting to predict remaining stock, but it seems unnecessarily complicated.",t2_9aqvgklw,False,,0,False,Predicting when a product will be sold out - is survival analysis appropriate?,[],r/datascience,False,6,discussion,0,,,False,t3_nw5ptn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623299888.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;Use-case:&lt;/strong&gt; &lt;em&gt;A particular product has X remaining items in stocks. Predict when the product is sold out, given both time-invariant and time-varying covariates.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Is a multivariate Cox regression model appropriate for this problem, where the &amp;quot;event&amp;quot; is the product being sold out? What about if the items can be restocked (i.e. by users issuing returns), effectively increasing the stock in the process?&lt;/p&gt;

&lt;p&gt;Is there a more sensible approach? I tried to look up something like ARIMAX for non-negative + discrete forecasting to predict remaining stock, but it seems unnecessarily complicated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw5ptn,True,,populus27,,7,True,all_ads,False,[],False,,/r/datascience/comments/nw5ptn/predicting_when_a_product_will_be_sold_out_is/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw5ptn/predicting_when_a_product_will_be_sold_out_is/,515405,1623271088.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a survey where users rate a number of recommendations. The user does not know that half the recommendations are made by system A, and the other half are made by system B.

I initially thought this was called A/B testing, however, that does not seem to apply. What is the correct name for this ?",t2_r6jti,False,,0,False,What is the correct terminology for this type of survey?,[],r/datascience,False,6,discussion,0,,,False,t3_nvup3x,False,dark,0.93,,public,13,0,{},,,False,[],,False,False,,{},Discussion,False,13,,False,False,self,False,,[],{},,True,,1623270400.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a survey where users rate a number of recommendations. The user does not know that half the recommendations are made by system A, and the other half are made by system B.&lt;/p&gt;

&lt;p&gt;I initially thought this was called A/B testing, however, that does not seem to apply. What is the correct name for this ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvup3x,True,,thunderbirdsetup,,8,True,all_ads,False,[],False,,/r/datascience/comments/nvup3x/what_is_the_correct_terminology_for_this_type_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvup3x/what_is_the_correct_terminology_for_this_type_of/,515405,1623241600.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am retooling and revisiting STATA and came across my old notes on when to use each regression type.  

&amp;#x200B;

For when to use bivariate regression, my notes read: 

&amp;#x200B;

*""Don't. It's trash. Bivariate (one independent and one dependent) models don’t tell you much other than confirming if there is a strong correlation.""* 

&amp;#x200B;

I don't recall ever actually using it, and I started googling - but now I am puzzled.  

&amp;#x200B;

Is ""bivariate regression"" just a measure of correlation between independent variable X and dependent variable Y?? 

If so, when would it ever be useful?",t2_8azmn3,False,,0,False,"Question: when (if ever) is ""bivariate regression"" useful??",[],r/datascience,False,6,discussion,0,,,False,t3_nw4wxh,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623297721.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am retooling and revisiting STATA and came across my old notes on when to use each regression type.  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For when to use bivariate regression, my notes read: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;quot;Don&amp;#39;t. It&amp;#39;s trash. Bivariate (one independent and one dependent) models don’t tell you much other than confirming if there is a strong correlation.&amp;quot;&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t recall ever actually using it, and I started googling - but now I am puzzled.  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is &amp;quot;bivariate regression&amp;quot; just a measure of correlation between independent variable X and dependent variable Y?? &lt;/p&gt;

&lt;p&gt;If so, when would it ever be useful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nw4wxh,True,,TimboCA,,14,True,all_ads,False,[],False,,/r/datascience/comments/nw4wxh/question_when_if_ever_is_bivariate_regression/,all_ads,False,https://www.reddit.com/r/datascience/comments/nw4wxh/question_when_if_ever_is_bivariate_regression/,515405,1623268921.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, I'm trying to construct a model that takes in the first two/three input values from the user and based on those values, decides the best 4th independent variable to ask the user to optimize the time needed to bucket the user into one of the 5-6 different buckets (supervised clusters). The initial approaches that I was considering were bidirectional LSTM/RNN's.. but after reading up on these two, I'm now thinking there may be more suitable/clever approaches to tackles this. Thank you, the deep learning gods of Reddit xx",t2_4okx2lwr,False,,0,False,Dynamic model that predicts the best next input variable to ask based on the first two/three inputs,[],r/datascience,False,6,projects,0,,,False,t3_nvz9f7,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1623283105.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m trying to construct a model that takes in the first two/three input values from the user and based on those values, decides the best 4th independent variable to ask the user to optimize the time needed to bucket the user into one of the 5-6 different buckets (supervised clusters). The initial approaches that I was considering were bidirectional LSTM/RNN&amp;#39;s.. but after reading up on these two, I&amp;#39;m now thinking there may be more suitable/clever approaches to tackles this. Thank you, the deep learning gods of Reddit xx&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvz9f7,True,,lalopark,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvz9f7/dynamic_model_that_predicts_the_best_next_input/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvz9f7/dynamic_model_that_predicts_the_best_next_input/,515405,1623254305.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"How would you predict who someone may want to send a Snapchat or Gmail to?

I cant really think how to approach this interview question, can anyone help me get started with it?",t2_zqp1wbh,False,,0,False,Predicting who someone may want to send a message to,[],r/datascience,False,6,education,0,,,False,t3_nvrxp6,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Education,False,5,,False,False,self,False,,[],{},,True,,1623260996.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How would you predict who someone may want to send a Snapchat or Gmail to?&lt;/p&gt;

&lt;p&gt;I cant really think how to approach this interview question, can anyone help me get started with it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvrxp6,True,,SilurianWenlock,,7,True,all_ads,False,[],False,,/r/datascience/comments/nvrxp6/predicting_who_someone_may_want_to_send_a_message/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvrxp6/predicting_who_someone_may_want_to_send_a_message/,515405,1623232196.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"As the title suggests, there are a lot of good reviews on Datacamp, however, i've taken courses on edx before and they are amazing. There are a few from MIT and IBM etc. 

for a beginner, what would you recommend and why?",t2_13t60b,False,,0,False,"Datacamp vs edx, which would you recommend and why?",[],r/datascience,False,6,education,0,,,False,t3_nv8fwf,False,dark,0.96,,public,132,1,{},,,False,[],,False,False,,{},Education,False,132,,False,False,self,False,,[],{},,True,,1623199282.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title suggests, there are a lot of good reviews on Datacamp, however, i&amp;#39;ve taken courses on edx before and they are amazing. There are a few from MIT and IBM etc. &lt;/p&gt;

&lt;p&gt;for a beginner, what would you recommend and why?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv8fwf,True,,drugsarebadmky,,69,True,all_ads,False,[],False,,/r/datascience/comments/nv8fwf/datacamp_vs_edx_which_would_you_recommend_and_why/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv8fwf/datacamp_vs_edx_which_would_you_recommend_and_why/,515405,1623170482.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,,t2_6rd1h,False,,0,False,"Sell me on your physical input set up - keyboards, mice, accessories",[],r/datascience,False,6,discussion,0,,,False,t3_nvvid4,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623272811.0,text,6,,,text,self.datascience,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvvid4,True,,greenearrow,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvvid4/sell_me_on_your_physical_input_set_up_keyboards/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvvid4/sell_me_on_your_physical_input_set_up_keyboards/,515405,1623244011.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I am curious to know how everyone version controls their datasets in projects and research projects?,t2_i1a46n1,False,,0,False,[Q] How do you version control datasets?,[],r/datascience,False,6,discussion,0,,,False,t3_nvhd0j,False,dark,0.91,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1623222717.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am curious to know how everyone version controls their datasets in projects and research projects?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhd0j,True,,enzsio,,20,True,all_ads,False,[],False,,/r/datascience/comments/nvhd0j/q_how_do_you_version_control_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhd0j/q_how_do_you_version_control_datasets/,515405,1623193917.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, right now I'm struggling to pull data in order to make reports or any analytics. Everything is scattered around either in excel files or in a bunch of platforms. I just started so I don't have much working experience and I know, you guys, might be able to give some tips that could help me on this. Really appreciate it!",t2_117xez,False,,0,False,"[Q] currently working as DA, company doesn't have any cloud platform or servers",[],r/datascience,False,6,career,0,,,False,t3_nvm9qj,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1623238526.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, right now I&amp;#39;m struggling to pull data in order to make reports or any analytics. Everything is scattered around either in excel files or in a bunch of platforms. I just started so I don&amp;#39;t have much working experience and I know, you guys, might be able to give some tips that could help me on this. Really appreciate it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvm9qj,True,,diraceusse,,4,True,all_ads,False,[],False,,/r/datascience/comments/nvm9qj/q_currently_working_as_da_company_doesnt_have_any/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvm9qj/q_currently_working_as_da_company_doesnt_have_any/,515405,1623209726.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Good afternoon. I am facing a challenging situation on my career now and I would like some advice. I will provide a very brief summary of my experience.

First of all, I am based in Portugal, Europe. I have a a Mechanical Engineering Degree focused on Energy Systems  (With a bunch of optimisation, ML and numerical simulations mixed in, including the master thesis, as well as years in extra curricular activities revolving around Python and ML projects.). I have 2 years and a couple of months experience in Data Science in the following roles:

1- Data Science Consultant for 1y and 2m. Role included developing a genetic algorithm, time series analysis with LSTM, Prophet and Arima, as well as a Big Data project on Databricks and SQL. I started feeling like I wanted to be part of internal Data Science team, so that I could really own my projects and that is why I left for company 2:

2- Data Scientist for 7m in an Energy Related Company. Me and another person were brought in to start a Data Science team.  This company was not a technological company and, there was not much to do really. We were supposed to get Data to start working on some use cases, but after 7 months we did not have any data nor any indication that we would have in the future. During my time in there I explored outlier detection methods like Isolation Forests and LOF, did a local web page using Flask, Javascript and Bootstrap, and also worked on some data cleaning and process automation pipelines. I had an offer via a recruiter that contacted me on Linkedin that I felt would make my job much more meaningful, and at the time it was very hard for me to justify not taking it, so I took it.

3- 7m Fintech Startup. This was pretty much what I wanted in a Data Science Role. Build a model using H20 AI for client grading and well as lot of other analysis for managing client and portfolio risk. I felt like my job truly mattered and I was extremely satisfied with it and super excited for all the possibilities that I would have, but due to a very unexpected issue with financing we ended up closing after 7m on the role.

More and more I have been craving time to truly dedicate myself to learn and develop some personal projects, mostly related to crypto and financial markets. My question is, can I afford to have a gap on my CV, assuming I fill it with personal projects or do I need to start looking for jobs immediately? Also, since I left university  I wanted to start an international career and move to Germany, Sweden, Switzerland, Austria, UK... ( I have been learning German for 2 years). Would this be a good time to truly try to move? Do I even have a chance if I just send CV's from Portugal (I am wiling to reallocate) and get a job on any of this countries before I move or would I really have to move there before I start trying to get a job?

Thanks all!",t2_wik6z,False,,0,False,"Startup went bankrupt, need career advice",[],r/datascience,False,6,career,0,,,False,t3_nv6d5y,False,dark,0.85,,public,25,1,{},,,False,[],,False,False,,{},Career,False,25,,False,False,self,1623165828.0,,[],{},,True,,1623194407.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Good afternoon. I am facing a challenging situation on my career now and I would like some advice. I will provide a very brief summary of my experience.&lt;/p&gt;

&lt;p&gt;First of all, I am based in Portugal, Europe. I have a a Mechanical Engineering Degree focused on Energy Systems  (With a bunch of optimisation, ML and numerical simulations mixed in, including the master thesis, as well as years in extra curricular activities revolving around Python and ML projects.). I have 2 years and a couple of months experience in Data Science in the following roles:&lt;/p&gt;

&lt;p&gt;1- Data Science Consultant for 1y and 2m. Role included developing a genetic algorithm, time series analysis with LSTM, Prophet and Arima, as well as a Big Data project on Databricks and SQL. I started feeling like I wanted to be part of internal Data Science team, so that I could really own my projects and that is why I left for company 2:&lt;/p&gt;

&lt;p&gt;2- Data Scientist for 7m in an Energy Related Company. Me and another person were brought in to start a Data Science team.  This company was not a technological company and, there was not much to do really. We were supposed to get Data to start working on some use cases, but after 7 months we did not have any data nor any indication that we would have in the future. During my time in there I explored outlier detection methods like Isolation Forests and LOF, did a local web page using Flask, Javascript and Bootstrap, and also worked on some data cleaning and process automation pipelines. I had an offer via a recruiter that contacted me on Linkedin that I felt would make my job much more meaningful, and at the time it was very hard for me to justify not taking it, so I took it.&lt;/p&gt;

&lt;p&gt;3- 7m Fintech Startup. This was pretty much what I wanted in a Data Science Role. Build a model using H20 AI for client grading and well as lot of other analysis for managing client and portfolio risk. I felt like my job truly mattered and I was extremely satisfied with it and super excited for all the possibilities that I would have, but due to a very unexpected issue with financing we ended up closing after 7m on the role.&lt;/p&gt;

&lt;p&gt;More and more I have been craving time to truly dedicate myself to learn and develop some personal projects, mostly related to crypto and financial markets. My question is, can I afford to have a gap on my CV, assuming I fill it with personal projects or do I need to start looking for jobs immediately? Also, since I left university  I wanted to start an international career and move to Germany, Sweden, Switzerland, Austria, UK... ( I have been learning German for 2 years). Would this be a good time to truly try to move? Do I even have a chance if I just send CV&amp;#39;s from Portugal (I am wiling to reallocate) and get a job on any of this countries before I move or would I really have to move there before I start trying to get a job?&lt;/p&gt;

&lt;p&gt;Thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv6d5y,True,,Elbarro,,15,True,all_ads,False,[],False,,/r/datascience/comments/nv6d5y/startup_went_bankrupt_need_career_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv6d5y/startup_went_bankrupt_need_career_advice/,515405,1623165607.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Sometimes I fell 90% of my time is spent in data wrangling/munging is on data exploration - and most of that is spent trying to make sense of fields created by some long lost business logic or trying to discover how that dataset came to be.

As a consultant, I cry tears of join when I see a team that keeps a data dictionary or a well-organized catalog. Even if on Excel.

What is the best documentation practice you've seen?",t2_1sv1g0c4,False,,0,False,How do you document your datasets?,[],r/datascience,False,6,discussion,0,,,False,t3_nv9xlh,False,dark,0.86,,public,13,1,{},,,False,[],,False,False,,{},Discussion,False,13,,False,False,self,False,,[],{'gid_1': 1},,True,,1623203216.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sometimes I fell 90% of my time is spent in data wrangling/munging is on data exploration - and most of that is spent trying to make sense of fields created by some long lost business logic or trying to discover how that dataset came to be.&lt;/p&gt;

&lt;p&gt;As a consultant, I cry tears of join when I see a team that keeps a data dictionary or a well-organized catalog. Even if on Excel.&lt;/p&gt;

&lt;p&gt;What is the best documentation practice you&amp;#39;ve seen?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv9xlh,True,,FernandoCordeiro,,14,True,all_ads,False,[],False,,/r/datascience/comments/nv9xlh/how_do_you_document_your_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv9xlh/how_do_you_document_your_datasets/,515405,1623174416.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Dear Excel Geeks, we do have it ExcelTips group, but the mode is not active anymore. So I have created [another one](https://www.reddit.com/r/ExcelTips_ActiveGroup/).

If you are a pro-Excel user, [**you can share Excel Tips and Trick in this group**](https://www.reddit.com/r/ExcelTips_ActiveGroup/). If you starting with Excel you can join as well to follow the pro-users.

Reson I created because if it wasn't for the people who share their knowledge absolutely free, I wouldn't have survived and able to make a career as Data Analyst.

I still remember vividly, when I first got a job as Social Media Analyst in Shanghai, hardly had any knowledge about Excel, [Mike Girvin channel on YouTube name ExcelIsFun](https://www.youtube.com/user/ExcelIsFun/channels), absolutely saved my ass. If you are an absolute beginner in Microsoft Excel, I would highly recommend checking him.",t2_7v2c8olg,False,,0,False,Calling out Excel Pro Users,[],r/datascience,False,6,education,0,,,False,t3_nvs6nt,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,False,,[],{},,True,,1623261958.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear Excel Geeks, we do have it ExcelTips group, but the mode is not active anymore. So I have created &lt;a href=""https://www.reddit.com/r/ExcelTips_ActiveGroup/""&gt;another one&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are a pro-Excel user, &lt;a href=""https://www.reddit.com/r/ExcelTips_ActiveGroup/""&gt;&lt;strong&gt;you can share Excel Tips and Trick in this group&lt;/strong&gt;&lt;/a&gt;. If you starting with Excel you can join as well to follow the pro-users.&lt;/p&gt;

&lt;p&gt;Reson I created because if it wasn&amp;#39;t for the people who share their knowledge absolutely free, I wouldn&amp;#39;t have survived and able to make a career as Data Analyst.&lt;/p&gt;

&lt;p&gt;I still remember vividly, when I first got a job as Social Media Analyst in Shanghai, hardly had any knowledge about Excel, &lt;a href=""https://www.youtube.com/user/ExcelIsFun/channels""&gt;Mike Girvin channel on YouTube name ExcelIsFun&lt;/a&gt;, absolutely saved my ass. If you are an absolute beginner in Microsoft Excel, I would highly recommend checking him.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvs6nt,True,,Deepak__Deepu,,1,True,all_ads,False,[],False,,/r/datascience/comments/nvs6nt/calling_out_excel_pro_users/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvs6nt/calling_out_excel_pro_users/,515405,1623233158.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?auto=webp&amp;s=a3718084c207d04c3ac1364c4f1ad3748d1726fc', 'width': 900, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f726bbbe009c2a29a95363492fe1770916a35bd', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf036e7053befcf16715f695ab3cae00e0bc8d1c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95d6096ac728f22e903d93078befdbb69d77682e', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/XTlzUy4n3JLEpklo4MMv-giUnHoSKCsy1PP28nycAa8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4461d2d8a92b6c5204a76dafa2f96a1169b33f9', 'width': 640, 'height': 640}], 'variants': {}, 'id': 'rzPZzWEZghiyAIMmIgCSkEnf9kvLr1nYNU3QaswpW18'}], 'enabled': False}",,,,,
,datascience,"I'm currently looking around for a different data science job and I'm not exactly sure what I'm looking for but I could roughly classify the *types* of jobs I'm hearing about into a few broad categories. I'll list them and give my current thought but I was hoping you could all weigh in with your experience and help me make the right decision.

&amp;#x200B;

1) The BIG tech company. These are your FAANG+M's. I assume that they are the ones doing really cutting-edge stuff and I imagine the work is pretty fun. I also assume they pay really well. I'm not sure if I can get past one of those leetcode interviews though. This one is probably out of reach for me but I imagine I'd take it if I had the chance.

2) The small tech company. ""we just got XXXX Million in funding and just landed a contract with &lt;big company that you've heard of&gt;"". Yeah IDK this is the one that I'm most wary about. I think ""fast-paced"" and ""great for self-starters"" are just code language for ""shitty work-life balance"" and ""poorly organized"". But of course I could be wrong

3) The big retailer/non-technical company that has a whizzbang data science team. This sounds kinda fun but then other times I wonder if it's a little mundane. Anyone actually doing advanced statistical models and ML or are we just A/B testing all day long?

4) the huge non-technical company that doesn't yet have a data science team. ""Yeah we think we could find a lot of business value if we brought on a data scientist"". This is the one I'm most conflicted about. Some of the most fun I've ever had with data science is when I'm working with people who have absolutely no tech background and couldn't even imagine what was possible. I think in the right situation you could really be hot shit around there. The thing I'm most worried about is that you'd soon run out of things to do. With some of these places the work they describe sounds more like a 3-month project than a career.

&amp;#x200B;

What are some of your thoughts?",t2_1lmws42,False,,0,False,Advice on types of jobs,[],r/datascience,False,6,,0,,,False,t3_nviakd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Job Search,False,3,,False,False,self,False,,[],{},,True,,1623225465.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently looking around for a different data science job and I&amp;#39;m not exactly sure what I&amp;#39;m looking for but I could roughly classify the &lt;em&gt;types&lt;/em&gt; of jobs I&amp;#39;m hearing about into a few broad categories. I&amp;#39;ll list them and give my current thought but I was hoping you could all weigh in with your experience and help me make the right decision.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;1) The BIG tech company. These are your FAANG+M&amp;#39;s. I assume that they are the ones doing really cutting-edge stuff and I imagine the work is pretty fun. I also assume they pay really well. I&amp;#39;m not sure if I can get past one of those leetcode interviews though. This one is probably out of reach for me but I imagine I&amp;#39;d take it if I had the chance.&lt;/p&gt;

&lt;p&gt;2) The small tech company. &amp;quot;we just got XXXX Million in funding and just landed a contract with &amp;lt;big company that you&amp;#39;ve heard of&amp;gt;&amp;quot;. Yeah IDK this is the one that I&amp;#39;m most wary about. I think &amp;quot;fast-paced&amp;quot; and &amp;quot;great for self-starters&amp;quot; are just code language for &amp;quot;shitty work-life balance&amp;quot; and &amp;quot;poorly organized&amp;quot;. But of course I could be wrong&lt;/p&gt;

&lt;p&gt;3) The big retailer/non-technical company that has a whizzbang data science team. This sounds kinda fun but then other times I wonder if it&amp;#39;s a little mundane. Anyone actually doing advanced statistical models and ML or are we just A/B testing all day long?&lt;/p&gt;

&lt;p&gt;4) the huge non-technical company that doesn&amp;#39;t yet have a data science team. &amp;quot;Yeah we think we could find a lot of business value if we brought on a data scientist&amp;quot;. This is the one I&amp;#39;m most conflicted about. Some of the most fun I&amp;#39;ve ever had with data science is when I&amp;#39;m working with people who have absolutely no tech background and couldn&amp;#39;t even imagine what was possible. I think in the right situation you could really be hot shit around there. The thing I&amp;#39;m most worried about is that you&amp;#39;d soon run out of things to do. With some of these places the work they describe sounds more like a 3-month project than a career.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What are some of your thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nviakd,True,,old_mcfartigan,,11,True,all_ads,False,[],False,,/r/datascience/comments/nviakd/advice_on_types_of_jobs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nviakd/advice_on_types_of_jobs/,515405,1623196665.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I honestly, don't think people wanting to break into Data Science really know what all it entails. It just sounds good, and sounds like it will make them lots of money.

No one tells people what comes with the job. There are a lot of headaches that come with it, and you have to be a very patient person.

When any person starts out in IT, they learn some psychology. How to manage users and their expectations. You learn what to say and what not to say. You learn how to appear confident and reassuring even if you're getting up to speed in the moment. The good ones do anyway.

Data Science, BI, DA - you have to have those skills multiplied by ten. You have to be better than the rest at managing expectations. You have to learn how to avoid support drains, and be thinking ahead all of the time.

The data science people are the only people I respect as much as the people in Systems. Because other fields, you learn one thing and only one side of it, call yourself an engineer despite knowing one side. Sys Engineers have to know a little about everything and base knowledge in all kinds of things/ They are constantly growing. Data Science folks are similar because they have to know a wide assortment of things, and they have to know all of the tips and tricks at their disposal to get their desired result. Which means they will know Python, multiple types of SQL, Pandas, Jupyter, and so on. They'll pivot in Excel in a pinch if they need to.

But the main reason I respect them is just because of how patient they have to be to want to work in their field for 30+ years.

Our DA left in 2018 and one of my roles was a senior DBA, so they just put her job on top of mine. I learned a lot and I got very good at SQL and streamlining and reducing task turn around for reports and data tasks. But I obviously didn't have the time to dive ultra deep into the rabbit hole, and I didn't want to. Because I knew it wasn't for me.

We were acquired, and I transitioned all of that stuff onto the BI team of the new company. I have so much respect for those people. I am still answering questions and taking one off requests. This morning I was just hit in the face with how much I dislike actually doing he DS/DA side. A Sales Senior Manager needed something with some data. I asked a follow up question. I needed a key piece of info to ensure I did the right thing and didn't have to do re work later. They said they would get it to me later.

They emailed it to me at 7:11am this morning, then messaged me before my shift - ""Hey, I don't see the data task with the blah blah being done. We needed it 6/3."" And I am thinking - then why wait until 6/7 to give me the info. We got the request 6/4, and I asked you on 6/4, then you waited the weekend to get it to me.

And those individuals who just keep coming back telling you the data wasn't what they expected or wanted when it is what they asked for.. I'm so happy to be just a senior sys engineer again working on large scale infra.

It's not for everyone, and I think they need to talk about and teach managing expectations so you don't shoot yourself in the foot. Luckily the BI team of the new company are phenomenal, and now I am out of the game. 

But I am learning more Python at home in my spare time and things like Jupyter so I don't regress skill wise. Python is useful in what I do anyway. I've rewritten several PS automation scripts in it.",t2_mfhlt,False,,0,False,"Data Science and Data Analytics is becoming ultra glorified / romanticized, and I don't think people are really told what they are getting into.",[],r/datascience,False,6,discussion,0,,,False,t3_nue01q,False,dark,0.95,,public,983,8,{},,,False,[],,False,False,,{},Discussion,False,983,,False,False,self,1623076963.0,,[],{'gid_1': 2},,True,,1623105228.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I honestly, don&amp;#39;t think people wanting to break into Data Science really know what all it entails. It just sounds good, and sounds like it will make them lots of money.&lt;/p&gt;

&lt;p&gt;No one tells people what comes with the job. There are a lot of headaches that come with it, and you have to be a very patient person.&lt;/p&gt;

&lt;p&gt;When any person starts out in IT, they learn some psychology. How to manage users and their expectations. You learn what to say and what not to say. You learn how to appear confident and reassuring even if you&amp;#39;re getting up to speed in the moment. The good ones do anyway.&lt;/p&gt;

&lt;p&gt;Data Science, BI, DA - you have to have those skills multiplied by ten. You have to be better than the rest at managing expectations. You have to learn how to avoid support drains, and be thinking ahead all of the time.&lt;/p&gt;

&lt;p&gt;The data science people are the only people I respect as much as the people in Systems. Because other fields, you learn one thing and only one side of it, call yourself an engineer despite knowing one side. Sys Engineers have to know a little about everything and base knowledge in all kinds of things/ They are constantly growing. Data Science folks are similar because they have to know a wide assortment of things, and they have to know all of the tips and tricks at their disposal to get their desired result. Which means they will know Python, multiple types of SQL, Pandas, Jupyter, and so on. They&amp;#39;ll pivot in Excel in a pinch if they need to.&lt;/p&gt;

&lt;p&gt;But the main reason I respect them is just because of how patient they have to be to want to work in their field for 30+ years.&lt;/p&gt;

&lt;p&gt;Our DA left in 2018 and one of my roles was a senior DBA, so they just put her job on top of mine. I learned a lot and I got very good at SQL and streamlining and reducing task turn around for reports and data tasks. But I obviously didn&amp;#39;t have the time to dive ultra deep into the rabbit hole, and I didn&amp;#39;t want to. Because I knew it wasn&amp;#39;t for me.&lt;/p&gt;

&lt;p&gt;We were acquired, and I transitioned all of that stuff onto the BI team of the new company. I have so much respect for those people. I am still answering questions and taking one off requests. This morning I was just hit in the face with how much I dislike actually doing he DS/DA side. A Sales Senior Manager needed something with some data. I asked a follow up question. I needed a key piece of info to ensure I did the right thing and didn&amp;#39;t have to do re work later. They said they would get it to me later.&lt;/p&gt;

&lt;p&gt;They emailed it to me at 7:11am this morning, then messaged me before my shift - &amp;quot;Hey, I don&amp;#39;t see the data task with the blah blah being done. We needed it 6/3.&amp;quot; And I am thinking - then why wait until 6/7 to give me the info. We got the request 6/4, and I asked you on 6/4, then you waited the weekend to get it to me.&lt;/p&gt;

&lt;p&gt;And those individuals who just keep coming back telling you the data wasn&amp;#39;t what they expected or wanted when it is what they asked for.. I&amp;#39;m so happy to be just a senior sys engineer again working on large scale infra.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s not for everyone, and I think they need to talk about and teach managing expectations so you don&amp;#39;t shoot yourself in the foot. Luckily the BI team of the new company are phenomenal, and now I am out of the game. &lt;/p&gt;

&lt;p&gt;But I am learning more Python at home in my spare time and things like Jupyter so I don&amp;#39;t regress skill wise. Python is useful in what I do anyway. I&amp;#39;ve rewritten several PS automation scripts in it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nue01q,True,,PartTimeTulsa,,199,True,all_ads,False,[],False,,/r/datascience/comments/nue01q/data_science_and_data_analytics_is_becoming_ultra/,all_ads,False,https://www.reddit.com/r/datascience/comments/nue01q/data_science_and_data_analytics_is_becoming_ultra/,515405,1623076428.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases?

Keywords would be: Hadoop and MapReduce, Spark, parallel and distributed databases, Data warehousing

This is an advanced course on databases at our university so I wanted to know how important such knowledge is if some one is lets say working as an ML engineer/data scientist.",t2_8i9sqtxm,False,,0,False,How important is it to have a good grasp on scalable databases as a data scientist/ML engineer?,[],r/datascience,False,6,education,0,,,False,t3_nvhi9a,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1623223166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases?&lt;/p&gt;

&lt;p&gt;Keywords would be: Hadoop and MapReduce, Spark, parallel and distributed databases, Data warehousing&lt;/p&gt;

&lt;p&gt;This is an advanced course on databases at our university so I wanted to know how important such knowledge is if some one is lets say working as an ML engineer/data scientist.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhi9a,True,,Peter2448,,8,True,all_ads,False,[],False,,/r/datascience/comments/nvhi9a/how_important_is_it_to_have_a_good_grasp_on/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhi9a/how_important_is_it_to_have_a_good_grasp_on/,515405,1623194366.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So, I started my engineering journey with Electrical Engineering but somehow got interested in data science, I learned most of the things on my own, I studied about things required to be in the CS field but felt like I knew nothing and hence asked a few of my CS students to do a project with me so I can learn, they weren't that motivated so I did somethings myself. I was learning and going good doing my own project.

Though I didn't get any jobs in the field because I wasn't having a CS degree to be sitting in interviews plus due to pandemic, the company wants better at a lesser price, understandable to me. Somehow I got some research projects in the field of NLP and work independently suggesting my own ideas depending on the problem statement and Data peovided.

Now, I am in an MNC as a data science and analytics intern, with two other young team members, who are very excited, giving ideas and just TALKING, they don't know how to DO it, ultimately increasing my work because they don't need the job as they are just students and I need it, I don't want any bad impression from my side. On top of that, the project they gave us is of making a recommendation engine, they are not giving any idea, everything is on us.

I can make a recommendation engine like that, item-based, user-based anyone, and can try to build a hybrid one, depending on the data we have. They asked us to look for the domain yourself, it should have a unique feature too because, at the presentation, you have to tell that ""why the product is different"", I mean 8 weeks of time, no data nothing, need an MVP in 2 weeks, data has to searched and it should have a unique feature. Plus this one intern irritates that heck out of me........

Now I am thinking I was better working alone, but I have seen some great teammates, Please advise me anything on this. I don't know the feeling is right or not, or is it the lockdown eating my head. I am willing to correct my thought process but I need some advice from someone in the same field without being judged.

Update: Just got the problem system even broader, they need a personalization system not specifically a recommendation sytem.",t2_34qgdyb6,False,,0,False,"This is going to be a rant, so sit back",[],r/datascience,False,6,career,0,,,False,t3_nvn42p,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623214529.0,,[],{},,True,,1623241447.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I started my engineering journey with Electrical Engineering but somehow got interested in data science, I learned most of the things on my own, I studied about things required to be in the CS field but felt like I knew nothing and hence asked a few of my CS students to do a project with me so I can learn, they weren&amp;#39;t that motivated so I did somethings myself. I was learning and going good doing my own project.&lt;/p&gt;

&lt;p&gt;Though I didn&amp;#39;t get any jobs in the field because I wasn&amp;#39;t having a CS degree to be sitting in interviews plus due to pandemic, the company wants better at a lesser price, understandable to me. Somehow I got some research projects in the field of NLP and work independently suggesting my own ideas depending on the problem statement and Data peovided.&lt;/p&gt;

&lt;p&gt;Now, I am in an MNC as a data science and analytics intern, with two other young team members, who are very excited, giving ideas and just TALKING, they don&amp;#39;t know how to DO it, ultimately increasing my work because they don&amp;#39;t need the job as they are just students and I need it, I don&amp;#39;t want any bad impression from my side. On top of that, the project they gave us is of making a recommendation engine, they are not giving any idea, everything is on us.&lt;/p&gt;

&lt;p&gt;I can make a recommendation engine like that, item-based, user-based anyone, and can try to build a hybrid one, depending on the data we have. They asked us to look for the domain yourself, it should have a unique feature too because, at the presentation, you have to tell that &amp;quot;why the product is different&amp;quot;, I mean 8 weeks of time, no data nothing, need an MVP in 2 weeks, data has to searched and it should have a unique feature. Plus this one intern irritates that heck out of me........&lt;/p&gt;

&lt;p&gt;Now I am thinking I was better working alone, but I have seen some great teammates, Please advise me anything on this. I don&amp;#39;t know the feeling is right or not, or is it the lockdown eating my head. I am willing to correct my thought process but I need some advice from someone in the same field without being judged.&lt;/p&gt;

&lt;p&gt;Update: Just got the problem system even broader, they need a personalization system not specifically a recommendation sytem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvn42p,True,,yaakarsh1011,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvn42p/this_is_going_to_be_a_rant_so_sit_back/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvn42p/this_is_going_to_be_a_rant_so_sit_back/,515405,1623212647.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Ever thought to yourself that even though you are perfectly capable at your job, or maybe even the most knowledgable person, you are often underestimated in your role. Or worse, you find someone else (even though less capable technically) making the decisions and telling you how the project should be handled. You want, ask and expect a new opportunity or an exciting project from your leadership, but you simply watch them go to someone else in quiet despair.  


You feel under-appreciated, ignored, and mostly misunderstood. You probably have several great ideas for your company, but if only someone were to listen to you.  


I work with several people in Tech, IT, and Data Science, and this is such a common problem that I encounter.  


The reason for this is ‘Perception”. See, there’s a big difference between being an expert vs being perceived as one. Being an expert will help you excel in the opportunities you land in, but how people perceive you is what will land you in opportunities in the first place.  


Almost everyone is focused on honing their technical skills, which by the way is a great pursuit. But most ignore the art of basic persuasion and charm which keeps them from getting truly satisfying roles and opportunities.  


So how does one gets perceived the right way…?  


It has to do with the way you communicate. Experts have a way of talking that automatically demands compliance, respect, and conviction from others. Think of your last visit to a doctor. Did you argue with the doctor, or just wondered in your head that they probably know nothing, or simply dismissed what they asked you to do? Instead, you complied with whatever you said.  


Effective persuasion is a learnable skill. Once you make a few key changes to the way you communicate, the environment around you and the way people treat you changes dramatically. You will command respect and unquestionable trust that will get you elite opportunities in your industry. So you can later prove it through your capabilities.  


When you present something, people will just tune in to your reality as you speak. In fact, they literally will come to help you with your work even when you don’t ask them. All of this happens subconsciously when you know basic charisma and persuasion.",t2_4jjahzgd,False,,0,False,This one thing could be holding back your career in Data Science...,[],r/datascience,False,6,career,0,,,False,t3_nvhtly,False,dark,0.45,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1623224075.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ever thought to yourself that even though you are perfectly capable at your job, or maybe even the most knowledgable person, you are often underestimated in your role. Or worse, you find someone else (even though less capable technically) making the decisions and telling you how the project should be handled. You want, ask and expect a new opportunity or an exciting project from your leadership, but you simply watch them go to someone else in quiet despair.  &lt;/p&gt;

&lt;p&gt;You feel under-appreciated, ignored, and mostly misunderstood. You probably have several great ideas for your company, but if only someone were to listen to you.  &lt;/p&gt;

&lt;p&gt;I work with several people in Tech, IT, and Data Science, and this is such a common problem that I encounter.  &lt;/p&gt;

&lt;p&gt;The reason for this is ‘Perception”. See, there’s a big difference between being an expert vs being perceived as one. Being an expert will help you excel in the opportunities you land in, but how people perceive you is what will land you in opportunities in the first place.  &lt;/p&gt;

&lt;p&gt;Almost everyone is focused on honing their technical skills, which by the way is a great pursuit. But most ignore the art of basic persuasion and charm which keeps them from getting truly satisfying roles and opportunities.  &lt;/p&gt;

&lt;p&gt;So how does one gets perceived the right way…?  &lt;/p&gt;

&lt;p&gt;It has to do with the way you communicate. Experts have a way of talking that automatically demands compliance, respect, and conviction from others. Think of your last visit to a doctor. Did you argue with the doctor, or just wondered in your head that they probably know nothing, or simply dismissed what they asked you to do? Instead, you complied with whatever you said.  &lt;/p&gt;

&lt;p&gt;Effective persuasion is a learnable skill. Once you make a few key changes to the way you communicate, the environment around you and the way people treat you changes dramatically. You will command respect and unquestionable trust that will get you elite opportunities in your industry. So you can later prove it through your capabilities.  &lt;/p&gt;

&lt;p&gt;When you present something, people will just tune in to your reality as you speak. In fact, they literally will come to help you with your work even when you don’t ask them. All of this happens subconsciously when you know basic charisma and persuasion.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvhtly,True,,hummus_wolf,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvhtly/this_one_thing_could_be_holding_back_your_career/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvhtly/this_one_thing_could_be_holding_back_your_career/,515405,1623195275.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Just wanted to gauge how much data scientists like working with their colleagues? I've been interviewing for various positions and have been disappointed in the people interviewing me. Most of the time I'm not treated respectfully and subsequently haven't been able to see myself working with the interviewers. It's turning me away from joining the profession.,t2_3mao2qc7,False,,0,False,People you work with,[],r/datascience,False,6,discussion,0,,,False,t3_nuw45a,False,dark,0.92,,public,33,0,{},,,False,[],,False,False,,{},Discussion,False,33,,False,False,self,False,,[],{},,True,,1623154683.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just wanted to gauge how much data scientists like working with their colleagues? I&amp;#39;ve been interviewing for various positions and have been disappointed in the people interviewing me. Most of the time I&amp;#39;m not treated respectfully and subsequently haven&amp;#39;t been able to see myself working with the interviewers. It&amp;#39;s turning me away from joining the profession.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuw45a,True,,Professional_Owl_819,,16,True,all_ads,False,[],False,,/r/datascience/comments/nuw45a/people_you_work_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuw45a/people_you_work_with/,515405,1623125883.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"What I am looking to do:

1) Scrape data of SP 500 company names and prices at specific date and end date.

2) compile the data.

3) organize companies by return over a 10 year period.

my goal:

 see what % of all SP 500 companies end up with different returns.

For  example, what % of companies end up being 10x or being removed.

Thanks in advance. I haver zero skills related to this.",t2_ogpog,False,,0,False,knuckle Dragger seeking advice - On what courses to take so that I can analyze data on the SP500.,[],r/datascience,False,6,discussion,0,,,False,t3_nvgbn4,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623219770.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What I am looking to do:&lt;/p&gt;

&lt;p&gt;1) Scrape data of SP 500 company names and prices at specific date and end date.&lt;/p&gt;

&lt;p&gt;2) compile the data.&lt;/p&gt;

&lt;p&gt;3) organize companies by return over a 10 year period.&lt;/p&gt;

&lt;p&gt;my goal:&lt;/p&gt;

&lt;p&gt;see what % of all SP 500 companies end up with different returns.&lt;/p&gt;

&lt;p&gt;For  example, what % of companies end up being 10x or being removed.&lt;/p&gt;

&lt;p&gt;Thanks in advance. I haver zero skills related to this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvgbn4,True,,badtradesguy,,18,True,all_ads,False,[],False,,/r/datascience/comments/nvgbn4/knuckle_dragger_seeking_advice_on_what_courses_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvgbn4/knuckle_dragger_seeking_advice_on_what_courses_to/,515405,1623190970.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

I'm building a tool that asks personalized follow up questions in order to get detailed survey responses. It's powered in part by GPT-3 (created by OpenAI). If you're open to testing it, it takes a couple of minutes...and I welcome feedback.

It's imperfect, but that's why we're testing ;)  Please DM me for a link!

I'd also love to hear your experiences building standalone AI products like this.",t2_5ccn6,False,,0,False,Testing GPT-3 powered survey tool - feedback and thoughts?,[],r/datascience,False,6,projects,0,,,False,t3_nvfy81,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1623218797.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m building a tool that asks personalized follow up questions in order to get detailed survey responses. It&amp;#39;s powered in part by GPT-3 (created by OpenAI). If you&amp;#39;re open to testing it, it takes a couple of minutes...and I welcome feedback.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s imperfect, but that&amp;#39;s why we&amp;#39;re testing ;)  Please DM me for a link!&lt;/p&gt;

&lt;p&gt;I&amp;#39;d also love to hear your experiences building standalone AI products like this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvfy81,True,,gaga_loo,,2,True,all_ads,False,[],False,,/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/,515405,1623189997.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"
Hello all, I’m currently an undergraduate stats major who will be a junior in the fall. My goals is to apply to PhD programs in senior fall. If I wanted to look at opportunities for the summer prior to it, should I look into doing research within say the stats dept? Or should I be trying to look for an actual internship at a company? My research interests are within statistical learning, so I was thinking a research role would be better suited for me than some data analytics position at a company.

I feel that it will be hard for me to get on any papers or do research because I won’t have a ton of theory knowledge, but I’m hoping I can get on something more applied.

So what do you think? I feel like trying to get a research role would be better when applying then doing SQL all day at a company for a summer, chances are even if I expressed my case as I want to do some more data science and quantitative sort of internship it wouldn’t be as good as if I did research with a prof.

I will be applying to stats phd programs.",t2_5w4i5kd1,False,,0,False,Research or Internship in prep for phd applications?,[],r/datascience,False,6,discussion,0,,,False,t3_nv589x,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623191109.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I’m currently an undergraduate stats major who will be a junior in the fall. My goals is to apply to PhD programs in senior fall. If I wanted to look at opportunities for the summer prior to it, should I look into doing research within say the stats dept? Or should I be trying to look for an actual internship at a company? My research interests are within statistical learning, so I was thinking a research role would be better suited for me than some data analytics position at a company.&lt;/p&gt;

&lt;p&gt;I feel that it will be hard for me to get on any papers or do research because I won’t have a ton of theory knowledge, but I’m hoping I can get on something more applied.&lt;/p&gt;

&lt;p&gt;So what do you think? I feel like trying to get a research role would be better when applying then doing SQL all day at a company for a summer, chances are even if I expressed my case as I want to do some more data science and quantitative sort of internship it wouldn’t be as good as if I did research with a prof.&lt;/p&gt;

&lt;p&gt;I will be applying to stats phd programs.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv589x,True,,veeeerain,,7,True,all_ads,False,[],False,,/r/datascience/comments/nv589x/research_or_internship_in_prep_for_phd/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv589x/research_or_internship_in_prep_for_phd/,515405,1623162309.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"A friend of mine is going to make a presentation to an Indian state government on how it can better use data, and how data may be used to have more effective policy and decision making for government bureaucrats in the Indian state.

To achieve traction with the bureaucracy, the focus needs to be more on projects and topics that will lead to ""quick wins.""Show some quick results; make like for the bureaucrats/policy makers perform better; and enable a virtuous loop where they can see how data can help in an efficient manner…. rather than being long projects that rarely result in concrete measurable benefits.

What are some projects/ topics that you would suggest? Maybe you have worked on some for local govts in other countries or in India itself. Any links and suggestions will be helpful

Note that most of these bureaucrats/policy makers are not coders; and data they have access to is often not clean. Many data bases do not talk to each other and there are data inconsistencies as well.

Any ideas and suggestions are much appreciated!! Thanks in advance!!

&amp;#x200B;

EDIT:   Also: My friend's NGO will support with some data science volunteers…they will do the data cleaning and analysis… looking for some initial ideas that you think may lead to interest from bureaucrats/policy makers ",t2_3mcq6yhh,False,,0,False,Data Science projects for local governments,[],r/datascience,False,6,projects,0,,,False,t3_nvafx3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,1623179509.0,,[],{},,True,,1623204593.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A friend of mine is going to make a presentation to an Indian state government on how it can better use data, and how data may be used to have more effective policy and decision making for government bureaucrats in the Indian state.&lt;/p&gt;

&lt;p&gt;To achieve traction with the bureaucracy, the focus needs to be more on projects and topics that will lead to &amp;quot;quick wins.&amp;quot;Show some quick results; make like for the bureaucrats/policy makers perform better; and enable a virtuous loop where they can see how data can help in an efficient manner…. rather than being long projects that rarely result in concrete measurable benefits.&lt;/p&gt;

&lt;p&gt;What are some projects/ topics that you would suggest? Maybe you have worked on some for local govts in other countries or in India itself. Any links and suggestions will be helpful&lt;/p&gt;

&lt;p&gt;Note that most of these bureaucrats/policy makers are not coders; and data they have access to is often not clean. Many data bases do not talk to each other and there are data inconsistencies as well.&lt;/p&gt;

&lt;p&gt;Any ideas and suggestions are much appreciated!! Thanks in advance!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT:   Also: My friend&amp;#39;s NGO will support with some data science volunteers…they will do the data cleaning and analysis… looking for some initial ideas that you think may lead to interest from bureaucrats/policy makers &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvafx3,True,,kdas22,,11,True,all_ads,False,[],False,,/r/datascience/comments/nvafx3/data_science_projects_for_local_governments/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvafx3/data_science_projects_for_local_governments/,515405,1623175793.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,Need to analyse app data for work and showcase it on a dashboard. Don't have a lot of experience in it and would like to see how similar data has been analyzed before. Are there any open projects/dashboards that you are aware of that I could use as a point of reference? Any other resources on the same are also appreciated. Thanks in advance.,t2_47ledrng,False,,0,False,Open dashboards for app data analysis,[],r/datascience,False,6,discussion,0,,,False,t3_nva6p2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1623203906.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Need to analyse app data for work and showcase it on a dashboard. Don&amp;#39;t have a lot of experience in it and would like to see how similar data has been analyzed before. Are there any open projects/dashboards that you are aware of that I could use as a point of reference? Any other resources on the same are also appreciated. Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nva6p2,True,,humanmetric,,1,True,all_ads,False,[],False,,/r/datascience/comments/nva6p2/open_dashboards_for_app_data_analysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/nva6p2/open_dashboards_for_app_data_analysis/,515405,1623175106.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know Tableau very well and is my preferred tool. But, I noticed there's a cheaper option from Amazon. Any input from your experience is appreciated.",t2_862rknk8,False,,0,False,Tableau vs Amazon QuickSights,[],r/datascience,False,6,tooling,0,,,False,t3_nuwn27,False,dark,0.92,,public,11,1,{},,,False,[],,False,False,,{},Tooling,False,11,,False,False,self,False,,[],{},,True,,1623156596.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know Tableau very well and is my preferred tool. But, I noticed there&amp;#39;s a cheaper option from Amazon. Any input from your experience is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuwn27,True,,ricke813,,12,True,all_ads,False,[],False,,/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/,515405,1623127796.0,1,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all, not sure if this is allowed here or not, but I received a DS Case Study from a big pharma company in USA. I just wanted to see if there is anyone experienced in doing case studies and might want to help me brainstorm the problem and discuss possible methodologies and aligning the solution.",t2_6bdw2d0v,False,,0,False,Brainstorming a DS Case Study from a big pharma company,[],r/datascience,False,6,projects,0,,,False,t3_nvcn3w,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1623210495.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, not sure if this is allowed here or not, but I received a DS Case Study from a big pharma company in USA. I just wanted to see if there is anyone experienced in doing case studies and might want to help me brainstorm the problem and discuss possible methodologies and aligning the solution.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nvcn3w,True,,mboorlu,,9,True,all_ads,False,[],False,,/r/datascience/comments/nvcn3w/brainstorming_a_ds_case_study_from_a_big_pharma/,all_ads,False,https://www.reddit.com/r/datascience/comments/nvcn3w/brainstorming_a_ds_case_study_from_a_big_pharma/,515405,1623181695.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi everyone! I have been a lurker in this community and it has been super helpful in more ways than I can count. Recently, I spoke with a company for a DS position and they sent me a take home assignment a couple of days ago.

It involves building an full-fledged ML web app from scratch. The steps include:

1. Loading tables in a SQL database
2. Training a model that predicts an outcome, and
3. Building a REST API that would receive data and post predictions based on the model I trained above

**In addition they state that it should take only 3-4 hours to complete this. REALLY????**

I do not have any meaningful background in building web apps and servers. This is pretty clear from my resume. Also, the job description did not mention any such requirements or skills for this particular position. Although, the company has an interesting product, I feel I would be wasting my time working on this assignment given my lack of skills. I wonder if I should rather spend my time working on other applications/assignments/interviews rather than doing this.  I feel really uncomfortable and honestly a little angry that they've asked me to build an entire project from scratch.

Would love to hear if y'all have any recommendations and thoughts about what I should do. Thank you :)",t2_j6dye,False,,0,False,DS take home assignment requires building an entire project using skills I don't have,[],r/datascience,False,6,,0,,,False,t3_nurs3c,False,dark,0.83,,public,16,0,{},,,False,[],,False,False,,{},Job Search,False,16,,False,False,self,False,,[],{},,True,,1623140590.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I have been a lurker in this community and it has been super helpful in more ways than I can count. Recently, I spoke with a company for a DS position and they sent me a take home assignment a couple of days ago.&lt;/p&gt;

&lt;p&gt;It involves building an full-fledged ML web app from scratch. The steps include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Loading tables in a SQL database&lt;/li&gt;
&lt;li&gt;Training a model that predicts an outcome, and&lt;/li&gt;
&lt;li&gt;Building a REST API that would receive data and post predictions based on the model I trained above&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;In addition they state that it should take only 3-4 hours to complete this. REALLY????&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I do not have any meaningful background in building web apps and servers. This is pretty clear from my resume. Also, the job description did not mention any such requirements or skills for this particular position. Although, the company has an interesting product, I feel I would be wasting my time working on this assignment given my lack of skills. I wonder if I should rather spend my time working on other applications/assignments/interviews rather than doing this.  I feel really uncomfortable and honestly a little angry that they&amp;#39;ve asked me to build an entire project from scratch.&lt;/p&gt;

&lt;p&gt;Would love to hear if y&amp;#39;all have any recommendations and thoughts about what I should do. Thank you :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nurs3c,True,,restaremeredetails,,27,True,all_ads,False,[],False,,/r/datascience/comments/nurs3c/ds_take_home_assignment_requires_building_an/,all_ads,False,https://www.reddit.com/r/datascience/comments/nurs3c/ds_take_home_assignment_requires_building_an/,515405,1623111790.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"HI there all,

First time poster and so sorry in advance for misdemeanors. I work in a commodity trading company and have been tasked with building / sourcing a CSO option risk model, in this case the spread is a time spread between future contracts for Crude Oil.

I have a pretty good understanding on VBA and there are people at the company who can work with Python etc. We have a SQL database of future prices for the various contracts as well as the volatility / delta / price of each options contract, from outright contracts to the CSO options.

Is anyone able to give me advice  / help on how to go about building this? Seems to be a MC simulation, but my only experience with that is using Matlab (engineering degree) which I have been told is not an option due to licencing costs.

&amp;#x200B;

Thanks,

&amp;#x200B;

thpj20",t2_15gxf0,False,,0,False,CSO Option VAR model,[],r/datascience,False,6,projects,0,,,False,t3_nv5jk1,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1623191928.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;HI there all,&lt;/p&gt;

&lt;p&gt;First time poster and so sorry in advance for misdemeanors. I work in a commodity trading company and have been tasked with building / sourcing a CSO option risk model, in this case the spread is a time spread between future contracts for Crude Oil.&lt;/p&gt;

&lt;p&gt;I have a pretty good understanding on VBA and there are people at the company who can work with Python etc. We have a SQL database of future prices for the various contracts as well as the volatility / delta / price of each options contract, from outright contracts to the CSO options.&lt;/p&gt;

&lt;p&gt;Is anyone able to give me advice  / help on how to go about building this? Seems to be a MC simulation, but my only experience with that is using Matlab (engineering degree) which I have been told is not an option due to licencing costs.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thpj20&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv5jk1,True,,thpj20,,2,True,all_ads,False,[],False,,/r/datascience/comments/nv5jk1/cso_option_var_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv5jk1/cso_option_var_model/,515405,1623163128.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,We are running a kubernetes based development environment where data scientists are free to work on anything they deem appropriate. We need to ensure they are not introducing vulnerabilities. Does anyone have any tools they recommend for continuous scanning and reporting?,t2_9l51o08l,False,,0,False,Reccomendations on vulnerability scanners.,[],r/datascience,False,6,tooling,0,,,False,t3_nv4uuy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1623190120.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We are running a kubernetes based development environment where data scientists are free to work on anything they deem appropriate. We need to ensure they are not introducing vulnerabilities. Does anyone have any tools they recommend for continuous scanning and reporting?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nv4uuy,True,,Desperate-Walk1780,,2,True,all_ads,False,[],False,,/r/datascience/comments/nv4uuy/reccomendations_on_vulnerability_scanners/,all_ads,False,https://www.reddit.com/r/datascience/comments/nv4uuy/reccomendations_on_vulnerability_scanners/,515405,1623161320.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I see this on Reddit every single day: someone saying something to the effect of ""data science is over-saturated and not that interesting, I'm transitioning into data engineering where there are more jobs."" I don't blame people for being burnt out on data science and looking to go where the fields look greener, but we all see what's happening here, right? 

Combine this with Reddit's (and the tech community in general) tendency to have a massive hard-on for anything ""engineering"" and I think we're seeing the beginning of a trend we've all seen before. In a few years we'll all be back here (or maybe in /r/dataengineering ) saying ""data engineering is oversaturated - that's why I'm moving into...quantum skunk wrangling"" or something.",t2_ad5yokml,False,,0,False,"Calling it now: a few years from now, ""data engineering"" will be just as overhyped and saturated as ""data science"" is now.",[],r/datascience,False,6,discussion,0,,,False,t3_nukktk,False,dark,0.78,,public,26,0,{},,,False,[],,False,False,,{},Discussion,False,26,,False,False,self,False,,[],{},,True,,1623121553.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see this on Reddit every single day: someone saying something to the effect of &amp;quot;data science is over-saturated and not that interesting, I&amp;#39;m transitioning into data engineering where there are more jobs.&amp;quot; I don&amp;#39;t blame people for being burnt out on data science and looking to go where the fields look greener, but we all see what&amp;#39;s happening here, right? &lt;/p&gt;

&lt;p&gt;Combine this with Reddit&amp;#39;s (and the tech community in general) tendency to have a massive hard-on for anything &amp;quot;engineering&amp;quot; and I think we&amp;#39;re seeing the beginning of a trend we&amp;#39;ve all seen before. In a few years we&amp;#39;ll all be back here (or maybe in &lt;a href=""/r/dataengineering""&gt;/r/dataengineering&lt;/a&gt; ) saying &amp;quot;data engineering is oversaturated - that&amp;#39;s why I&amp;#39;m moving into...quantum skunk wrangling&amp;quot; or something.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nukktk,True,,antichain,,10,True,all_ads,False,[],False,,/r/datascience/comments/nukktk/calling_it_now_a_few_years_from_now_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nukktk/calling_it_now_a_few_years_from_now_data/,515405,1623092753.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello all,

Im working as a process engineer in a process equipment manufacturing company. I’m still in the 2-3 year experience category.

Most of the decision making for which equipment to select in my company is done through experience. They say it’s important to also develop a feeling for how the product runs and machine behaves. I feel like this decision making can be replicated by analyzing the past data.

So I’m interested to learn data science and statistical analysis, I have experience in programming in VBA, bash scripting and can implement algorithms in any programming language, thanks to stack overflow.

Does it help to have this skill in addition to my general process engineering background? What benefits can someone like me have with this addition.",t2_on8sl,False,,0,False,"What benefits can I have as a Mechanical/process engineer, if I have knowledge/skills in data science?",[],r/datascience,False,6,career,0,,,False,t3_nuxm2s,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,1623132445.0,,[],{},,True,,1623160260.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Im working as a process engineer in a process equipment manufacturing company. I’m still in the 2-3 year experience category.&lt;/p&gt;

&lt;p&gt;Most of the decision making for which equipment to select in my company is done through experience. They say it’s important to also develop a feeling for how the product runs and machine behaves. I feel like this decision making can be replicated by analyzing the past data.&lt;/p&gt;

&lt;p&gt;So I’m interested to learn data science and statistical analysis, I have experience in programming in VBA, bash scripting and can implement algorithms in any programming language, thanks to stack overflow.&lt;/p&gt;

&lt;p&gt;Does it help to have this skill in addition to my general process engineering background? What benefits can someone like me have with this addition.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nuxm2s,True,,new_clearProjekt,,11,True,all_ads,False,[],False,,/r/datascience/comments/nuxm2s/what_benefits_can_i_have_as_a_mechanicalprocess/,all_ads,False,https://www.reddit.com/r/datascience/comments/nuxm2s/what_benefits_can_i_have_as_a_mechanicalprocess/,515405,1623131460.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hello peepos,

I’ve started an internship with territorial bureau of statistics that has me doing broad statistical work. It is my intention to get into the environmental field with a strong statistical background when I’m done. I’m wondering if any of you beautiful folk work for Mother Earth and could offer some guidance. (Or any “green” company of sorts. :D) For now, I’ll be be honing my GIS skills, dashboarding, database management and intensive math, like survival rates and differential equations. Any big topics I should be studying as well? 

Ty",t2_2d0otfa7,False,,0,False,Environmental Work,[],r/datascience,False,6,career,0,,,False,t3_nus823,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1623142010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello peepos,&lt;/p&gt;

&lt;p&gt;I’ve started an internship with territorial bureau of statistics that has me doing broad statistical work. It is my intention to get into the environmental field with a strong statistical background when I’m done. I’m wondering if any of you beautiful folk work for Mother Earth and could offer some guidance. (Or any “green” company of sorts. :D) For now, I’ll be be honing my GIS skills, dashboarding, database management and intensive math, like survival rates and differential equations. Any big topics I should be studying as well? &lt;/p&gt;

&lt;p&gt;Ty&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nus823,True,,DeneHero,,4,True,all_ads,False,[],False,,/r/datascience/comments/nus823/environmental_work/,all_ads,False,https://www.reddit.com/r/datascience/comments/nus823/environmental_work/,515405,1623113210.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"So you’re reporting out weekly kpis... you want to report out WoW change. The metric went from 4.7% to 6.3%. How would you show the change?

[View Poll](https://www.reddit.com/poll/nujq7q)",t2_dzkzihh,False,,0,False,Reporting change of percentages.,[],r/datascience,False,6,meta,0,,,False,t3_nujq7q,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Meta,False,3,,False,False,self,False,,[],{},,True,,1623119408.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So you’re reporting out weekly kpis... you want to report out WoW change. The metric went from 4.7% to 6.3%. How would you show the change?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nujq7q""&gt;View Poll&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nujq7q,True,,jbt209,,5,True,all_ads,False,[],False,,/r/datascience/comments/nujq7q/reporting_change_of_percentages/,all_ads,False,https://www.reddit.com/r/datascience/comments/nujq7q/reporting_change_of_percentages/,515405,1623090608.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,"{'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1623349808569, 'options': [{'text': '+34%', 'vote_count': 35, 'id': '8424514'}, {'text': '+1.6%', 'vote_count': 37, 'id': '8424515'}, {'text': '+1.6pp', 'vote_count': 36, 'id': '8424516'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 108, 'total_stake_amount': None}"
,datascience,"Hi!

Interviews are not only a way for the company to test candidates, it is also an opportunity for the candidate to decide if the company can be a good fit or not.

I am going to have the first round of interviews next week with a medium-sized financial service company (they're opening a new medior/senior data scientist position).

Besides questions around the job content,  I am thinking about probing the following ""meta"" dimensions to detect potential orange (or red) flags:

\- size/seniority/background of the team

\- company culture

\- how mature is the organization in terms of data storage and management?

\- where does the data science team fit in the organigram / how close you are to key stakeholders?

\- Where does the company see the data science team in 5 years?

And you, what are the key dimensions that you are probing when you talk to a company about a new job?

&amp;#x200B;

EDIT: Thank you so much for all the valuable answers. They are incredibly helpful!",t2_t0451a,False,,0,False,Which dimensions are you probing when talking to a company about a new job?,[],r/datascience,False,6,career,0,,,False,t3_ntuj7e,False,dark,0.99,,public,165,0,{},,,False,[],,False,False,,{},Career,False,165,,False,False,self,1623100686.0,,[],{},,True,,1623039332.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Interviews are not only a way for the company to test candidates, it is also an opportunity for the candidate to decide if the company can be a good fit or not.&lt;/p&gt;

&lt;p&gt;I am going to have the first round of interviews next week with a medium-sized financial service company (they&amp;#39;re opening a new medior/senior data scientist position).&lt;/p&gt;

&lt;p&gt;Besides questions around the job content,  I am thinking about probing the following &amp;quot;meta&amp;quot; dimensions to detect potential orange (or red) flags:&lt;/p&gt;

&lt;p&gt;- size/seniority/background of the team&lt;/p&gt;

&lt;p&gt;- company culture&lt;/p&gt;

&lt;p&gt;- how mature is the organization in terms of data storage and management?&lt;/p&gt;

&lt;p&gt;- where does the data science team fit in the organigram / how close you are to key stakeholders?&lt;/p&gt;

&lt;p&gt;- Where does the company see the data science team in 5 years?&lt;/p&gt;

&lt;p&gt;And you, what are the key dimensions that you are probing when you talk to a company about a new job?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: Thank you so much for all the valuable answers. They are incredibly helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntuj7e,True,,gemag,,43,True,all_ads,False,[],False,,/r/datascience/comments/ntuj7e/which_dimensions_are_you_probing_when_talking_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntuj7e/which_dimensions_are_you_probing_when_talking_to/,515405,1623010532.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,,t2_vblkuw9,False,,0,False,"Apart from kaggle, where can I find data science challenges/projects?",[],r/datascience,False,6,discussion,0,,,False,t3_nu8eui,False,dark,0.93,,public,12,0,{},,,False,[],,False,False,,{},Discussion,False,12,,False,False,self,False,,[],{},,True,,1623087707.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu8eui,True,,b15985,,12,True,all_ads,False,[],False,,/r/datascience/comments/nu8eui/apart_from_kaggle_where_can_i_find_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu8eui/apart_from_kaggle_where_can_i_find_data_science/,515405,1623058907.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_1g9552cc,False,,0,False,Is it okay to forget a language if you haven’t used it in a while?,[],r/datascience,False,6,discussion,0,,,False,t3_nufkw3,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1623109222.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nufkw3,True,,MrDrPatrickStar,,8,True,all_ads,False,[],False,,/r/datascience/comments/nufkw3/is_it_okay_to_forget_a_language_if_you_havent/,all_ads,False,https://www.reddit.com/r/datascience/comments/nufkw3/is_it_okay_to_forget_a_language_if_you_havent/,515405,1623080422.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The shap package has been great when it works, but I would like an alternative package that has similar functionality. I mostly use gradient boosting, so any package that can use the tree-path methods (interventional is nice too, but not as important) would be a life saver.",t2_i8ujh,False,,0,False,Best alternatives to 'shap' package?,[],r/datascience,False,6,discussion,0,,,False,t3_nubt22,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1623099208.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The shap package has been great when it works, but I would like an alternative package that has similar functionality. I mostly use gradient boosting, so any package that can use the tree-path methods (interventional is nice too, but not as important) would be a life saver.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nubt22,True,,suspicious_gardener,,9,True,all_ads,False,[],False,,/r/datascience/comments/nubt22/best_alternatives_to_shap_package/,all_ads,False,https://www.reddit.com/r/datascience/comments/nubt22/best_alternatives_to_shap_package/,515405,1623070408.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I know there's a lot of different tasks people do, but I've generally worked on code bases that utilize many models, and our data has hundreds of variables. I've seen some guidance that if you've written any more than 10 lines of code it should be its own function, and that just seems insane to me.   


Obviously it's a balancing act and you shouldn't have functions with thousands of lines of code, but I've seen plenty of \~100-200 line functions that generally define either a clean group of variables (say defining the \~15 inputs to a specific model), or a business process that's a bit complex but one clear process as you'd explain in English. I've seen code that follows the paradigm of keeping functions short and I've ended up trying to search for how a variable is defined and literally following a path of 10+ functions to find the answer.   


I know it's a balancing act and you can say it always depends on details, but I didn't know if people could share their thoughts and whether in their actual day to day work they tried to follow the small function paradigm or whether the way I work is closer to how people handle their coding standards.

Edit:
Seen some posts about functions doing 1 thing and that's basically what I'm trying to figure out. Is ""prep data for model x"" considered one thing? Say you have 15 variables that all take 1-5 lines of code (and in my current project we have hundreds of models with ~15 variables each and not a ton of overlap). My opinion is that a function that defines those 15 variables is ""doing one thing"" and much easier to read/understand and maybe have to use the scroll wheel once or twice than to trace through different functions which are often factored into different code files. Do people tend to agree with this? If not how would you refactor such a function? Would you just have functions called prep_variable1, prep_variable2, etc some of which are one line long and called exactly once in your code base?",t2_541w9,False,,0,False,Data Science Coding Standards,[],r/datascience,False,6,discussion,0,,,False,t3_ntxx3j,False,dark,0.94,,public,37,0,{},,,False,[],,False,False,,{},Discussion,False,37,,False,False,self,1623021889.0,,[],{},,True,,1623048833.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know there&amp;#39;s a lot of different tasks people do, but I&amp;#39;ve generally worked on code bases that utilize many models, and our data has hundreds of variables. I&amp;#39;ve seen some guidance that if you&amp;#39;ve written any more than 10 lines of code it should be its own function, and that just seems insane to me.   &lt;/p&gt;

&lt;p&gt;Obviously it&amp;#39;s a balancing act and you shouldn&amp;#39;t have functions with thousands of lines of code, but I&amp;#39;ve seen plenty of ~100-200 line functions that generally define either a clean group of variables (say defining the ~15 inputs to a specific model), or a business process that&amp;#39;s a bit complex but one clear process as you&amp;#39;d explain in English. I&amp;#39;ve seen code that follows the paradigm of keeping functions short and I&amp;#39;ve ended up trying to search for how a variable is defined and literally following a path of 10+ functions to find the answer.   &lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s a balancing act and you can say it always depends on details, but I didn&amp;#39;t know if people could share their thoughts and whether in their actual day to day work they tried to follow the small function paradigm or whether the way I work is closer to how people handle their coding standards.&lt;/p&gt;

&lt;p&gt;Edit:
Seen some posts about functions doing 1 thing and that&amp;#39;s basically what I&amp;#39;m trying to figure out. Is &amp;quot;prep data for model x&amp;quot; considered one thing? Say you have 15 variables that all take 1-5 lines of code (and in my current project we have hundreds of models with ~15 variables each and not a ton of overlap). My opinion is that a function that defines those 15 variables is &amp;quot;doing one thing&amp;quot; and much easier to read/understand and maybe have to use the scroll wheel once or twice than to trace through different functions which are often factored into different code files. Do people tend to agree with this? If not how would you refactor such a function? Would you just have functions called prep_variable1, prep_variable2, etc some of which are one line long and called exactly once in your code base?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntxx3j,True,,zachvac,,35,True,all_ads,False,[],False,,/r/datascience/comments/ntxx3j/data_science_coding_standards/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntxx3j/data_science_coding_standards/,515405,1623020033.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm a first time hiring manager and I'm hiring a Data Analyst. I've reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.

Call me a purist, but I don't think it's possible to do good quantitative work if you don't understand how the models work from a basic mathematical perspective. For example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you're using? What does a p-value actually represent in mathematical terms? What are the assumptions around independence of your predictor variables for any given model you're using? What diagnostic tests should you run for any given model and what hypothesis are each of these testing? And more. If you don't understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.

The number of people I've talked to who have a MS in Business Analytics who say they're doing ML but then can't explain these basic underlying assumptions of the models they build is astounding and pitiful. It doesn't matter how prestigious the school they went to--I've talked to people with degrees like this from both Ivy League schools and from lesser known schools and it is all the same. I'm not trying to trick people with obscure trivia or anything; when I ask them to tell me about a project they worked on, they tell me ""I built \[insert model\] so I could \[solve whatever business problem\]"" and when I follow up with ""what are the underlying assumptions for \[insert model\]? What kinds of diagnostics did you run for that model?"", they get completely tongue tied, even for the basics like logistic and linear regression. The only things these people ever check for with diagnostics is ""sensitivity and specificity"" and ""area under the curve"", which barely scratches the surface in determining whether or not your model is good.

I've looked into the requirements on these programs (both when I was choosing a masters program a few years back and also more recently as I've been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like ""SQL for Analytics"", ""Tableau for Visualization"", and super tool specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.

So.... if you're thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a REAL ACADEMIC FIELD (ex: statistics, math, computer science, engineering) and not Some Buzzword That's Super Hot Right Now. No one will care about your bullshit buzzword degree 10 years from now. At the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into R or Python hoping that you'll be able to \~predict the future\~.",t2_7s1iybm6,False,,0,False,[rant from a hiring manager] MS in Buzzword programs are a waste of money,[],r/datascience,False,6,discussion,0,,,False,t3_nusz8e,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1623144392.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a first time hiring manager and I&amp;#39;m hiring a Data Analyst. I&amp;#39;ve reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.&lt;/p&gt;

&lt;p&gt;Call me a purist, but I don&amp;#39;t think it&amp;#39;s possible to do good quantitative work if you don&amp;#39;t understand how the models work from a basic mathematical perspective. For example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you&amp;#39;re using? What does a p-value actually represent in mathematical terms? What are the assumptions around independence of your predictor variables for any given model you&amp;#39;re using? What diagnostic tests should you run for any given model and what hypothesis are each of these testing? And more. If you don&amp;#39;t understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.&lt;/p&gt;

&lt;p&gt;The number of people I&amp;#39;ve talked to who have a MS in Business Analytics who say they&amp;#39;re doing ML but then can&amp;#39;t explain these basic underlying assumptions of the models they build is astounding and pitiful. It doesn&amp;#39;t matter how prestigious the school they went to--I&amp;#39;ve talked to people with degrees like this from both Ivy League schools and from lesser known schools and it is all the same. I&amp;#39;m not trying to trick people with obscure trivia or anything; when I ask them to tell me about a project they worked on, they tell me &amp;quot;I built [insert model] so I could [solve whatever business problem]&amp;quot; and when I follow up with &amp;quot;what are the underlying assumptions for [insert model]? What kinds of diagnostics did you run for that model?&amp;quot;, they get completely tongue tied, even for the basics like logistic and linear regression. The only things these people ever check for with diagnostics is &amp;quot;sensitivity and specificity&amp;quot; and &amp;quot;area under the curve&amp;quot;, which barely scratches the surface in determining whether or not your model is good.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve looked into the requirements on these programs (both when I was choosing a masters program a few years back and also more recently as I&amp;#39;ve been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like &amp;quot;SQL for Analytics&amp;quot;, &amp;quot;Tableau for Visualization&amp;quot;, and super tool specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.&lt;/p&gt;

&lt;p&gt;So.... if you&amp;#39;re thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a REAL ACADEMIC FIELD (ex: statistics, math, computer science, engineering) and not Some Buzzword That&amp;#39;s Super Hot Right Now. No one will care about your bullshit buzzword degree 10 years from now. At the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into R or Python hoping that you&amp;#39;ll be able to ~predict the future~.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nusz8e,True,,Apprehensive-Web-987,,46,True,all_ads,False,[],False,,/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/,all_ads,False,https://www.reddit.com/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/,515405,1623115592.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So suppose you have  different data sets that are not connected to each other via a ""primary ID, for example:

df 1:  Numerical data of medical records of patients (e.g BP, Sugar, etc)

df 2:  Objective and subjective records by the patient.

Now we have to match records from the df2 to that of df1. How can we match these two dfs without having any ID.

&amp;#x200B;

It can be data of anything, website data or exam data, etc. Let me know how would you approach this problem.",t2_34qgdyb6,False,,0,False,I was wondering about this problem and wanted to know if it makes any sense lol,[],r/datascience,False,6,discussion,0,,,False,t3_nu7uz0,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1623085243.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So suppose you have  different data sets that are not connected to each other via a &amp;quot;primary ID, for example:&lt;/p&gt;

&lt;p&gt;df 1:  Numerical data of medical records of patients (e.g BP, Sugar, etc)&lt;/p&gt;

&lt;p&gt;df 2:  Objective and subjective records by the patient.&lt;/p&gt;

&lt;p&gt;Now we have to match records from the df2 to that of df1. How can we match these two dfs without having any ID.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;It can be data of anything, website data or exam data, etc. Let me know how would you approach this problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu7uz0,True,,yaakarsh1011,,6,True,all_ads,False,[],False,,/r/datascience/comments/nu7uz0/i_was_wondering_about_this_problem_and_wanted_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu7uz0/i_was_wondering_about_this_problem_and_wanted_to/,515405,1623056443.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm graduating next year from a CS engineering school in France.

I was contacted by a recruiter on LinkedIn for a Business/Data Analyst role. I applied to it, had a few tests in SQL/Excel, and video interviews. Got the internship.

Then, I had my first call with my manager. He told me that we would be doing some descriptive analytics, but also predictive and prescriptive (basically ML) analytics if I was willing to. He then asked me about my expectations for the internship, and I told him that I would like to do a lot of DS/ML work, instead of BA/DA. Luckily, since he's the one responsible for DS in the team, he agreed and offered me to do a DS oriented internship instead of a BA/DA one.

&amp;#x200B;

And that's how I got an Amazon DS internship, without even completing DS-specific tests.

I'm starting my project on Monday, I feel kinda scared lol. I don't have much experience and have only implemented a few clustering algorithms and linear regressions, but I have some theoretical knowledge on more complex stuff. Since it's going to be my first 'real' internship, I don't know how it's going to work, but I guess I'll just Google anything I don't know when I need it.

&amp;#x200B;

I still can't realize the opportunity it is, and hope it's going to launch my career in Data Science ! All of my friends are mad jealous",t2_lblkf,False,,0,False,"I got my first internship as a Data Scientist, at Amazon !",[],r/datascience,False,6,career,0,,,False,t3_nt8cg8,False,dark,0.95,,public,1113,7,{},,,False,[],,False,False,,{},Career,False,1113,,False,False,self,False,,[],{'gid_1': 3},,True,,1622964698.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m graduating next year from a CS engineering school in France.&lt;/p&gt;

&lt;p&gt;I was contacted by a recruiter on LinkedIn for a Business/Data Analyst role. I applied to it, had a few tests in SQL/Excel, and video interviews. Got the internship.&lt;/p&gt;

&lt;p&gt;Then, I had my first call with my manager. He told me that we would be doing some descriptive analytics, but also predictive and prescriptive (basically ML) analytics if I was willing to. He then asked me about my expectations for the internship, and I told him that I would like to do a lot of DS/ML work, instead of BA/DA. Luckily, since he&amp;#39;s the one responsible for DS in the team, he agreed and offered me to do a DS oriented internship instead of a BA/DA one.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s how I got an Amazon DS internship, without even completing DS-specific tests.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m starting my project on Monday, I feel kinda scared lol. I don&amp;#39;t have much experience and have only implemented a few clustering algorithms and linear regressions, but I have some theoretical knowledge on more complex stuff. Since it&amp;#39;s going to be my first &amp;#39;real&amp;#39; internship, I don&amp;#39;t know how it&amp;#39;s going to work, but I guess I&amp;#39;ll just Google anything I don&amp;#39;t know when I need it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I still can&amp;#39;t realize the opportunity it is, and hope it&amp;#39;s going to launch my career in Data Science ! All of my friends are mad jealous&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt8cg8,True,,Picetash,,87,True,all_ads,False,[],False,,/r/datascience/comments/nt8cg8/i_got_my_first_internship_as_a_data_scientist_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt8cg8/i_got_my_first_internship_as_a_data_scientist_at/,515405,1622935898.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have some data, mostly in excel that I would like to build some dashboards for and break down into understandable pieces.  But it seems like most tools like Tableau don’t have a free/personal tier/option.  Are any tools approachable for someone without deep pockets?",t2_p7u2k,False,,0,False,What tools are available for personal use?,[],r/datascience,False,6,tooling,0,,,False,t3_nu52fh,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Tooling,False,3,,False,False,self,False,,[],{},,True,,1623073317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have some data, mostly in excel that I would like to build some dashboards for and break down into understandable pieces.  But it seems like most tools like Tableau don’t have a free/personal tier/option.  Are any tools approachable for someone without deep pockets?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu52fh,True,,mjulson,,16,True,all_ads,False,[],False,,/r/datascience/comments/nu52fh/what_tools_are_available_for_personal_use/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu52fh/what_tools_are_available_for_personal_use/,515405,1623044517.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi! Looking for a suggestion on a visual cleaning tool for time series (or any) data. 

I am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. 

Trying to avoid writing a bespoke thing in dash but I can ultimately do that if it comes to it. 

Not bound to a platform but python is preferred, or even something electron based. 

Thanks!",t2_ahu1o,False,,0,False,Visual Cleaning tool - Time Series,[],r/datascience,False,6,tooling,0,,,False,t3_nub8j8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,1623069562.0,,[],{},,True,,1623097506.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! Looking for a suggestion on a visual cleaning tool for time series (or any) data. &lt;/p&gt;

&lt;p&gt;I am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. &lt;/p&gt;

&lt;p&gt;Trying to avoid writing a bespoke thing in dash but I can ultimately do that if it comes to it. &lt;/p&gt;

&lt;p&gt;Not bound to a platform but python is preferred, or even something electron based. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nub8j8,True,,Namur007,,0,True,all_ads,False,[],False,,/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/,all_ads,False,https://www.reddit.com/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/,515405,1623068706.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi all, been learning Factor Analysis for the first time using datasets from Kaggle. I’ve been using Factor Analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with Parallel Analysis (other than Kaiser Criterion and Scree Plot).

There’s literally nothing I can find on Parallel Analysis (PA) in Python, so I read a paper called: ‘Parallel Analysis: a method for determining significant principal components’. It suggests generating a random matrix with the same number of variables and samples. After standardising my dataset, I randomly generated normally-distributed numbers with mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform Parallel Analysis. My end Scree Plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically I would be doing a glorified Kaiser Criterion comparison).

So have I done something wrong? Are there any resources on PA in Python?",t2_wsxt9,False,,0,False,Horn’s Parallel Analysis in Python: Am I doing it correctly?,[],r/datascience,False,6,projects,0,,,False,t3_nu4deh,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1623070743.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, been learning Factor Analysis for the first time using datasets from Kaggle. I’ve been using Factor Analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with Parallel Analysis (other than Kaiser Criterion and Scree Plot).&lt;/p&gt;

&lt;p&gt;There’s literally nothing I can find on Parallel Analysis (PA) in Python, so I read a paper called: ‘Parallel Analysis: a method for determining significant principal components’. It suggests generating a random matrix with the same number of variables and samples. After standardising my dataset, I randomly generated normally-distributed numbers with mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform Parallel Analysis. My end Scree Plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically I would be doing a glorified Kaiser Criterion comparison).&lt;/p&gt;

&lt;p&gt;So have I done something wrong? Are there any resources on PA in Python?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu4deh,True,,Myzziah,,0,True,all_ads,False,[],False,,/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/,515405,1623041943.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm planning to become a SWE but have been developing carpal tunnel syndrome symptoms, and I don't want to risk wasting my time learning software programming if I'll still always have recurring CTS in the end and can't code for hours a day. I'm also interested in DS but I understand that there is a coding aspect to this job as well.",t2_3rjdpsxd,False,,0,False,How much coding do data scientists do in a day?,[],r/datascience,False,6,meta,0,,,False,t3_nu1go7,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Meta,False,3,,False,False,self,False,,[],{},,True,,1623060477.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m planning to become a SWE but have been developing carpal tunnel syndrome symptoms, and I don&amp;#39;t want to risk wasting my time learning software programming if I&amp;#39;ll still always have recurring CTS in the end and can&amp;#39;t code for hours a day. I&amp;#39;m also interested in DS but I understand that there is a coding aspect to this job as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nu1go7,True,,Dudeguybrochingo,,8,True,all_ads,False,[],False,,/r/datascience/comments/nu1go7/how_much_coding_do_data_scientists_do_in_a_day/,all_ads,False,https://www.reddit.com/r/datascience/comments/nu1go7/how_much_coding_do_data_scientists_do_in_a_day/,515405,1623031677.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,
,datascience,"Are there any mainstream python libraries which, given data from some source, suggest normalization schemes for data (e.g., recommended table structures for a 3NF or star schema in an RDBMS), profile relationships between fields for cardinality (x% of the time this field has a 1:1 relationship with this other field, the remaining y% are missing data and 1:many relationships), or complete other data modeling-related tasks?

I could cobble together some of this functionality using builtins/pandas/numpy etc. but am looking for industry-standard tools that data scientists use for the kind of “lite” data modeling that comes up on the job. (People’s personal GitHub repos for these tasks are OK but not exactly what I am looking for.)

Here are some sample cases to clarify:
1. You received a huge, raw denormalized extract from somewhere and will continue to receive incremental files on a regular basis. You want to create a profile of the initial data, use said profile to make some tradeoffs to “tidy” the data into an RDBMS-suitable format (maybe force 1:1 relationships where they exist 99% of the time for instance or fix overlapping datespans), and then monitor subsequent incremental files to ensure the underlying data profile had not changed drastically.
2. You received access to a new database with no documentation and little support from DBAs or the business on structures. Assume there are not keys or constraints defined in the RDBMS itself to leverage (perhaps the database was created/maintained by a skilled business user without DBA-level skills). You would like to create an ERD or some other documentation on this database quickly.

The focus on RDBMS as the endgame is because the goal here is to support analysis by a BI team that is skilled in SQL but no other programming languages.",t2_enab6,False,,0,False,Standard Python Resources for Data Modeling,[],r/datascience,False,6,tooling,0,,,False,t3_ntptf0,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Tooling,False,6,,False,False,self,False,,[],{},,True,,1623026455.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any mainstream python libraries which, given data from some source, suggest normalization schemes for data (e.g., recommended table structures for a 3NF or star schema in an RDBMS), profile relationships between fields for cardinality (x% of the time this field has a 1:1 relationship with this other field, the remaining y% are missing data and 1:many relationships), or complete other data modeling-related tasks?&lt;/p&gt;

&lt;p&gt;I could cobble together some of this functionality using builtins/pandas/numpy etc. but am looking for industry-standard tools that data scientists use for the kind of “lite” data modeling that comes up on the job. (People’s personal GitHub repos for these tasks are OK but not exactly what I am looking for.)&lt;/p&gt;

&lt;p&gt;Here are some sample cases to clarify:
1. You received a huge, raw denormalized extract from somewhere and will continue to receive incremental files on a regular basis. You want to create a profile of the initial data, use said profile to make some tradeoffs to “tidy” the data into an RDBMS-suitable format (maybe force 1:1 relationships where they exist 99% of the time for instance or fix overlapping datespans), and then monitor subsequent incremental files to ensure the underlying data profile had not changed drastically.
2. You received access to a new database with no documentation and little support from DBAs or the business on structures. Assume there are not keys or constraints defined in the RDBMS itself to leverage (perhaps the database was created/maintained by a skilled business user without DBA-level skills). You would like to create an ERD or some other documentation on this database quickly.&lt;/p&gt;

&lt;p&gt;The focus on RDBMS as the endgame is because the goal here is to support analysis by a BI team that is skilled in SQL but no other programming languages.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntptf0,True,,AMereRedditor,,4,True,all_ads,False,[],False,,/r/datascience/comments/ntptf0/standard_python_resources_for_data_modeling/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntptf0/standard_python_resources_for_data_modeling/,515405,1622997655.0,2,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hello all! So recently, I started working at HP as a product management intern. I know, that’s marketing, not data science, but I’ve spent the past two weeks learning all about it, and I find it very interesting. 

So, for context, I’m a senior and this is my first PAID internship. I’ve done 5 unpaid internships over the course of 3 semesters before this in hopes it would land me a real one, and it actually did. I spoke to a HP recruiter which gave me a contact at another company. After 3 months of speaking to several people at the other company, just to result in my intern application being rejected, I returned to said HP recruiter. Their website had no internship that fit me, so I spoke to her directly, which led to two interviews and an offer. And after viewing my work profile, I saw that said internship was meant for an MBA candidate, not an undergrad. Therefore, I’m guessing my experience made up for what level of education they were looking for.

Thus, I signed on to be a marketing manager/product management intern for the Z by HP division. And Z makes computers for data scientists, like high gigs of ram, more core processors, and intel gold (and probably some other stuff, but my tech knowledge isn’t super expansive). And while working with them, I’ve interviewed a lot of DS professionals about their computer needs and what type they use. Also, about what SW stacks they like, and what language they code in. And there has been quite a consistency amongst their answers.

Therefore, if you are a DS undergrad/graduate looking for a job or internship, I have some advice for you:

1. Diversify your skill set. Most people I spoke to didn’t have a DS degree, rather they started their careers, something went wrong, and they made a career shift to DS. It was easy to switch to because from what I can tell, DS is growing, all companies are dependent on some sort of cloud, and python is their preferred code language. So with the influx of people entering DS, if you started in DS, learn for than one coding language, and make sure you can use a variety of SW stacks so that you can separate yourself from the competition.

2. Take an unpaid internship (only if it is worth it). Like I said, I’ve had 5, but I quit one early. The reason is if you aren’t going to pay me, I’m not going to perform work I believe a paid person should (which I know, is technically any work, but just watch where I’m going with this). I quit one because there was no way I was going to be at the stadium 3 hours before the game starts, run around setting every event table up and it’s accessories, miss the entire game because I have to help people the entire time, not have a break, get off at 11 pm, not have a parking pass, and on top of that only be fed a mini Jimmy John’s sandwhich. 

But, there were others where I liked what I was doing, and they only required 10 hours or less work a week. I’d never advise anyone to work 40 hours a week for free, I wouldn’t advise even 15 hours a week for free. Just a few internships to pad the Resumé, but most importantly, actually provide you with some knowledge that will be applicable to your next job.

3. Apply even if you are not qualified. This applies to both internships and scholarships because I’ve gotten my way with both. These things provide too much for you not to try. I almost didn’t apply to the scholarship that takes care of all my tuition for my last two years of college, but I did. I almost reach back out to the recruiter that led to my internship now, at a company that will pay off the few student loans I do have. Hell, I almost didn’t apply to transfer to the university in at now after my freshman year at one I didn’t like, but was the epitome of “safe”. So believe in yourself and pull the trigger. This saying be be shot to hell but it’s true: you only live once (unless you believe in reincarnation, but you still only live THIS life once).",t2_31rlco6l,False,,0,False,I got a DS related internship at HP!,[],r/datascience,False,6,career,0,,,False,t3_ntnan1,False,dark,0.63,,public,5,0,{},,,False,[],,False,False,,{},Career,False,5,,False,False,self,False,,[],{},,True,,1623019436.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all! So recently, I started working at HP as a product management intern. I know, that’s marketing, not data science, but I’ve spent the past two weeks learning all about it, and I find it very interesting. &lt;/p&gt;

&lt;p&gt;So, for context, I’m a senior and this is my first PAID internship. I’ve done 5 unpaid internships over the course of 3 semesters before this in hopes it would land me a real one, and it actually did. I spoke to a HP recruiter which gave me a contact at another company. After 3 months of speaking to several people at the other company, just to result in my intern application being rejected, I returned to said HP recruiter. Their website had no internship that fit me, so I spoke to her directly, which led to two interviews and an offer. And after viewing my work profile, I saw that said internship was meant for an MBA candidate, not an undergrad. Therefore, I’m guessing my experience made up for what level of education they were looking for.&lt;/p&gt;

&lt;p&gt;Thus, I signed on to be a marketing manager/product management intern for the Z by HP division. And Z makes computers for data scientists, like high gigs of ram, more core processors, and intel gold (and probably some other stuff, but my tech knowledge isn’t super expansive). And while working with them, I’ve interviewed a lot of DS professionals about their computer needs and what type they use. Also, about what SW stacks they like, and what language they code in. And there has been quite a consistency amongst their answers.&lt;/p&gt;

&lt;p&gt;Therefore, if you are a DS undergrad/graduate looking for a job or internship, I have some advice for you:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Diversify your skill set. Most people I spoke to didn’t have a DS degree, rather they started their careers, something went wrong, and they made a career shift to DS. It was easy to switch to because from what I can tell, DS is growing, all companies are dependent on some sort of cloud, and python is their preferred code language. So with the influx of people entering DS, if you started in DS, learn for than one coding language, and make sure you can use a variety of SW stacks so that you can separate yourself from the competition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take an unpaid internship (only if it is worth it). Like I said, I’ve had 5, but I quit one early. The reason is if you aren’t going to pay me, I’m not going to perform work I believe a paid person should (which I know, is technically any work, but just watch where I’m going with this). I quit one because there was no way I was going to be at the stadium 3 hours before the game starts, run around setting every event table up and it’s accessories, miss the entire game because I have to help people the entire time, not have a break, get off at 11 pm, not have a parking pass, and on top of that only be fed a mini Jimmy John’s sandwhich. &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But, there were others where I liked what I was doing, and they only required 10 hours or less work a week. I’d never advise anyone to work 40 hours a week for free, I wouldn’t advise even 15 hours a week for free. Just a few internships to pad the Resumé, but most importantly, actually provide you with some knowledge that will be applicable to your next job.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Apply even if you are not qualified. This applies to both internships and scholarships because I’ve gotten my way with both. These things provide too much for you not to try. I almost didn’t apply to the scholarship that takes care of all my tuition for my last two years of college, but I did. I almost reach back out to the recruiter that led to my internship now, at a company that will pay off the few student loans I do have. Hell, I almost didn’t apply to transfer to the university in at now after my freshman year at one I didn’t like, but was the epitome of “safe”. So believe in yourself and pull the trigger. This saying be be shot to hell but it’s true: you only live once (unless you believe in reincarnation, but you still only live THIS life once).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntnan1,True,,KVthegreatest,,0,True,all_ads,False,[],False,,/r/datascience/comments/ntnan1/i_got_a_ds_related_internship_at_hp/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntnan1/i_got_a_ds_related_internship_at_hp/,515405,1622990636.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 06 Jun 2021 - 13 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_ntk7dk,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1623009631.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,ntk7dk,True,,datascience-bot,,199,False,all_ads,False,[],False,dark,/r/datascience/comments/ntk7dk/weekly_entering_transitioning_thread_06_jun_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntk7dk/weekly_entering_transitioning_thread_06_jun_2021/,515405,1622980831.0,0,,False,,,,,,,,
,datascience,Edit: Thanks a lot for the people who have replied to this post. I now got a brief idea.,t2_1lhrbnal,False,,0,False,Is there any correlation between Supply Chain Management and Data Science ?,[],r/datascience,False,6,discussion,0,,,False,t3_nthz8k,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,1623006786.0,,[],{},,True,,1623000711.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: Thanks a lot for the people who have replied to this post. I now got a brief idea.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nthz8k,True,,akshithjay,,12,True,all_ads,False,[],False,,/r/datascience/comments/nthz8k/is_there_any_correlation_between_supply_chain/,all_ads,False,https://www.reddit.com/r/datascience/comments/nthz8k/is_there_any_correlation_between_supply_chain/,515405,1622971911.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"So far I've used only R and Python for my main projects, but I keep hearing about Julia as a much better solution (performance wise). Has anyone used it instead of Python in production. Do you think it could replace Python, (provided there is more support for libraries)?",t2_3s7gldef,False,,0,False,Thoughts on Julia Programming Language,[],r/datascience,False,6,tooling,0,,,False,t3_ntj7md,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Tooling,False,6,,False,False,self,False,,[],{},,True,,1623005858.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So far I&amp;#39;ve used only R and Python for my main projects, but I keep hearing about Julia as a much better solution (performance wise). Has anyone used it instead of Python in production. Do you think it could replace Python, (provided there is more support for libraries)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntj7md,True,,XhoniShollaj,,33,False,all_ads,False,[],False,,/r/datascience/comments/ntj7md/thoughts_on_julia_programming_language/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntj7md/thoughts_on_julia_programming_language/,515405,1622977058.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"My current work is very lax and although I was hired to do data engineering work, I have not done much of that. My position seem to be a placeholder and I am given random tasks (clean up data, check if some mathematics can be done with that data, image processing, extracting data from weird S3 buckets). And my work is not even very frequent, I can go on days without practically doing anything. 

I need to switch jobs immediately for family matters. Can I use these experiences in my resume and switch to a data science position? My coding skill is nothing to brag about but my understanding of statistics, Analytics and model building should hold up.

Edit: Thank you everyone for your responses. I had some good ideas.",t2_6pycc2fm,False,,0,False,How to explain lack of meaningful work in your last job?,[],r/datascience,False,6,,0,,,False,t3_nswxtw,False,dark,0.96,,public,170,0,{},,,False,[],,False,False,,{},Job Search,False,170,,False,False,self,1622925370.0,,[],{},,True,,1622932125.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My current work is very lax and although I was hired to do data engineering work, I have not done much of that. My position seem to be a placeholder and I am given random tasks (clean up data, check if some mathematics can be done with that data, image processing, extracting data from weird S3 buckets). And my work is not even very frequent, I can go on days without practically doing anything. &lt;/p&gt;

&lt;p&gt;I need to switch jobs immediately for family matters. Can I use these experiences in my resume and switch to a data science position? My coding skill is nothing to brag about but my understanding of statistics, Analytics and model building should hold up.&lt;/p&gt;

&lt;p&gt;Edit: Thank you everyone for your responses. I had some good ideas.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nswxtw,True,,not_y0ur_business,,53,True,all_ads,False,[],False,,/r/datascience/comments/nswxtw/how_to_explain_lack_of_meaningful_work_in_your/,all_ads,False,https://www.reddit.com/r/datascience/comments/nswxtw/how_to_explain_lack_of_meaningful_work_in_your/,515405,1622903325.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"What's the latest senior data scientist salary range in EU, especially countries like Germany, France and the Netherlands? In US tech companies based here, high end consulting such as BCG and also the average European company? Glassdoor is so outdated and anyway does not have sufficient data points for senior roles.",t2_6m8l9x87,False,,0,False,Senior data scientist salaries in Eurozone,[],r/datascience,False,6,career,0,,,False,t3_nsvihd,False,dark,0.88,,public,47,0,{},,,False,[],,False,False,,{},Career,False,47,,False,False,self,False,,[],{},,True,,1622927741.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What&amp;#39;s the latest senior data scientist salary range in EU, especially countries like Germany, France and the Netherlands? In US tech companies based here, high end consulting such as BCG and also the average European company? Glassdoor is so outdated and anyway does not have sufficient data points for senior roles.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsvihd,True,,darn321,,58,True,all_ads,False,[],False,,/r/datascience/comments/nsvihd/senior_data_scientist_salaries_in_eurozone/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsvihd/senior_data_scientist_salaries_in_eurozone/,515405,1622898941.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Looking for suggestions on either books, courses, websites etc. that cover things like:

* Survival analysis
* Reliability testing 
* Lifecycle testing 

Would be great if implementations were based on python/R.",t2_7m1zlf41,False,,0,False,"Lifetime, reliability and performance testing",[],r/datascience,False,6,discussion,0,,,False,t3_nt85ct,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622964073.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking for suggestions on either books, courses, websites etc. that cover things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Survival analysis&lt;/li&gt;
&lt;li&gt;Reliability testing &lt;/li&gt;
&lt;li&gt;Lifecycle testing &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Would be great if implementations were based on python/R.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt85ct,True,,Limp-Ad-7289,,2,True,all_ads,False,[],False,,/r/datascience/comments/nt85ct/lifetime_reliability_and_performance_testing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt85ct/lifetime_reliability_and_performance_testing/,515405,1622935273.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

Few months ago, I started working on a project that requires me to use R to fetch, clean data, do some feature engineering. I'm able to do whatever is required but I'm not sure if my code or rather code snippets are ""good"" or ""bad"". I'm not even sure what good or bad means but I've seen these words thrown around. 

Can more experienced people of this sub explain to me what qualifies as a good code?

Thanks!",t2_bv171ji2,False,,0,False,How do you know if you're writing a good code or a bad one?,[],r/datascience,False,6,discussion,0,,,False,t3_nt7t2p,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1622963003.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;Few months ago, I started working on a project that requires me to use R to fetch, clean data, do some feature engineering. I&amp;#39;m able to do whatever is required but I&amp;#39;m not sure if my code or rather code snippets are &amp;quot;good&amp;quot; or &amp;quot;bad&amp;quot;. I&amp;#39;m not even sure what good or bad means but I&amp;#39;ve seen these words thrown around. &lt;/p&gt;

&lt;p&gt;Can more experienced people of this sub explain to me what qualifies as a good code?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt7t2p,True,,quite--average,,15,True,all_ads,False,[],False,,/r/datascience/comments/nt7t2p/how_do_you_know_if_youre_writing_a_good_code_or_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt7t2p/how_do_you_know_if_youre_writing_a_good_code_or_a/,515405,1622934203.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

I recently found out that Carvana lets you use the internet while taking their technical test. They wrote something like this in the email invitation, ""We all know everybody googles the syntax on their job"". I'm sure there are many companies out there with similar mindset that I'm not aware of.

I found it interesting and was wondering what are your thoughts on this. Should more companies start allowing the use of internet in their coding tests?

Thanks!",t2_bv171ji2,False,,0,False,Carvana lets you google while taking a coding test. Do you think more companies need to do this?,[],r/datascience,False,6,discussion,0,,,False,t3_nsf633,False,dark,0.99,,public,377,1,{},,,False,[],,False,False,,{},Discussion,False,377,,False,False,self,False,,[],{},,True,,1622868622.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I recently found out that Carvana lets you use the internet while taking their technical test. They wrote something like this in the email invitation, &amp;quot;We all know everybody googles the syntax on their job&amp;quot;. I&amp;#39;m sure there are many companies out there with similar mindset that I&amp;#39;m not aware of.&lt;/p&gt;

&lt;p&gt;I found it interesting and was wondering what are your thoughts on this. Should more companies start allowing the use of internet in their coding tests?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsf633,True,,quite--average,,76,True,all_ads,False,[],False,,/r/datascience/comments/nsf633/carvana_lets_you_google_while_taking_a_coding/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsf633/carvana_lets_you_google_while_taking_a_coding/,515405,1622839822.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey, everyone! So I'm about to update my laptop that is long overdue to hit the trash, and I'm thinking about how, logistics wise, I do my schoolwork at work. The issue, though, is I can't use that laptop to connect to the internet. By go-to makeshift solution was to google or search stuff through the work computer and use my personal computer to do Jupyter Notebook stuff, which doesn't require the internet to work, but that doesn't work for things such as using StackOverflow, where they want screenshots or wanting to see the exact code used.

So my  question is, is there a way to bring Wi-Fi with you so that you can use your laptop on the go? Is there anything I can buy? Thanks!",t2_2wzu5n8,False,,0,False,Portable Laptop Wifi,[],r/datascience,False,6,tooling,0,,,False,t3_ntasnn,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1622972867.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, everyone! So I&amp;#39;m about to update my laptop that is long overdue to hit the trash, and I&amp;#39;m thinking about how, logistics wise, I do my schoolwork at work. The issue, though, is I can&amp;#39;t use that laptop to connect to the internet. By go-to makeshift solution was to google or search stuff through the work computer and use my personal computer to do Jupyter Notebook stuff, which doesn&amp;#39;t require the internet to work, but that doesn&amp;#39;t work for things such as using StackOverflow, where they want screenshots or wanting to see the exact code used.&lt;/p&gt;

&lt;p&gt;So my  question is, is there a way to bring Wi-Fi with you so that you can use your laptop on the go? Is there anything I can buy? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ntasnn,True,,KyronAWF,,5,True,all_ads,False,[],False,,/r/datascience/comments/ntasnn/portable_laptop_wifi/,all_ads,False,https://www.reddit.com/r/datascience/comments/ntasnn/portable_laptop_wifi/,515405,1622944067.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Disclaimer: not a data scientist but a policy and urban affairs researcher/consultant that uses DS/DA tools to do his job better. Thus, honest question, sort if it sounds stupid.
  The job I applied to required only Excel knowledge but right now I'm using SQL and Python (100% self taught) almost every day as my company is doing less traditional consultancy and more analytics stuff. Most of the times it's just data cleaning and wrangling and getting insights or designing the process when it involves geographic data so that the data engineering interns can do the proper ETL process. If I'm doing something more advanced (clustering, distance matrices, facility location...) I usually make do with out of the box solutions.
 I keep reading about data structures and algorithms in this sub and how important they are. However, I'm a bit mystified by the terminology and can't really see how it's useful. I know spatial indexing often uses trees and is used to make searching and performing operations faster (Union, intersection...). Other than that, I'm a at a loss and I'm a bit worried that whenever I need to do more advanced stuff or eventually interview for a more data-oriented role, I'll just make a fool of myself. 

Thanks so much!",t2_4xuiwc2u,False,,0,False,How useful is knowledge of data structures and algorithms and how to learn them best?,[],r/datascience,False,6,discussion,0,,,False,t3_nsqj14,False,dark,0.94,,public,26,0,{},,,False,[],,False,False,,{},Discussion,False,26,,False,False,self,False,,[],{},,True,,1622908072.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Disclaimer: not a data scientist but a policy and urban affairs researcher/consultant that uses DS/DA tools to do his job better. Thus, honest question, sort if it sounds stupid.
  The job I applied to required only Excel knowledge but right now I&amp;#39;m using SQL and Python (100% self taught) almost every day as my company is doing less traditional consultancy and more analytics stuff. Most of the times it&amp;#39;s just data cleaning and wrangling and getting insights or designing the process when it involves geographic data so that the data engineering interns can do the proper ETL process. If I&amp;#39;m doing something more advanced (clustering, distance matrices, facility location...) I usually make do with out of the box solutions.
 I keep reading about data structures and algorithms in this sub and how important they are. However, I&amp;#39;m a bit mystified by the terminology and can&amp;#39;t really see how it&amp;#39;s useful. I know spatial indexing often uses trees and is used to make searching and performing operations faster (Union, intersection...). Other than that, I&amp;#39;m a at a loss and I&amp;#39;m a bit worried that whenever I need to do more advanced stuff or eventually interview for a more data-oriented role, I&amp;#39;ll just make a fool of myself. &lt;/p&gt;

&lt;p&gt;Thanks so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsqj14,True,,tururut_tururut,,10,True,all_ads,False,[],False,,/r/datascience/comments/nsqj14/how_useful_is_knowledge_of_data_structures_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsqj14/how_useful_is_knowledge_of_data_structures_and/,515405,1622879272.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My methodology is just making lots of pivot tables. What do they want to see? 

More info: they gave me a list of a dozen questions to answer. All of which were easily answered by cleaning up the data and then making pivot tables. I submitted a report in excel and now I’ve been asked to present my findings. They said my presentation should include my methodology and process. I’m kind of stumped for how to include it in a PowerPoint presentation. 

Thanks for any insight.",t2_ai5p1w9o,False,,0,False,"I’m doing an excel project for a job interview, and I’m supposed to do a presentation that includes my “process and methodology.”",[],r/datascience,False,6,,0,,,False,t3_nt8i82,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622965217.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My methodology is just making lots of pivot tables. What do they want to see? &lt;/p&gt;

&lt;p&gt;More info: they gave me a list of a dozen questions to answer. All of which were easily answered by cleaning up the data and then making pivot tables. I submitted a report in excel and now I’ve been asked to present my findings. They said my presentation should include my methodology and process. I’m kind of stumped for how to include it in a PowerPoint presentation. &lt;/p&gt;

&lt;p&gt;Thanks for any insight.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nt8i82,True,,Enough_Blueberry_549,,9,True,all_ads,False,[],False,,/r/datascience/comments/nt8i82/im_doing_an_excel_project_for_a_job_interview_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt8i82/im_doing_an_excel_project_for_a_job_interview_and/,515405,1622936417.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"when you start a project with a problem and try to work towards a solution (which is what you should do to make sure your work is actually useful) then you arrive at this hurdle where you have the problem and an idea for the solution at hand, and they are your only lead to finding the specific data you need to train you models. Sometimes this data can be really hard to find using these search parameters. No matter how much I search, I don't find what I’m looking for

The data is probably out there and there is probably some search term that would make google put this data right at the top for you to see, but I've often found that the problem and prospective solution I have on hand is generally not it. Datasets online simply aren't indexed by their applications, they are probably most often indexed by their source. And that is something that I, in my experience, can’t really use to engineer a search term that gives good results (if the data even exists online).

I was wondering if you all had the same problem and whether you agreed with this idea. Is it the same case in your experience or am I just doing it wrong?",t2_7z7d7,False,,0,False,"For most of the problems I try to solve using data science, the biggest challenge surprisingly isn’t really the “science” part but the “data” part",[],r/datascience,False,6,discussion,0,,,False,t3_ns5lwu,False,dark,0.96,,public,316,0,{},,,False,[],,False,False,,{},Discussion,False,316,,False,False,self,False,,[],{},,True,,1622843477.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;when you start a project with a problem and try to work towards a solution (which is what you should do to make sure your work is actually useful) then you arrive at this hurdle where you have the problem and an idea for the solution at hand, and they are your only lead to finding the specific data you need to train you models. Sometimes this data can be really hard to find using these search parameters. No matter how much I search, I don&amp;#39;t find what I’m looking for&lt;/p&gt;

&lt;p&gt;The data is probably out there and there is probably some search term that would make google put this data right at the top for you to see, but I&amp;#39;ve often found that the problem and prospective solution I have on hand is generally not it. Datasets online simply aren&amp;#39;t indexed by their applications, they are probably most often indexed by their source. And that is something that I, in my experience, can’t really use to engineer a search term that gives good results (if the data even exists online).&lt;/p&gt;

&lt;p&gt;I was wondering if you all had the same problem and whether you agreed with this idea. Is it the same case in your experience or am I just doing it wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns5lwu,True,,samrus,,102,True,all_ads,False,[],False,,/r/datascience/comments/ns5lwu/for_most_of_the_problems_i_try_to_solve_using/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns5lwu/for_most_of_the_problems_i_try_to_solve_using/,515405,1622814677.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am new in Data Science so please excuse my limited knowledge. I am learning more about classification algorithms and how to appropriately selected algorithms for your task.

According to this website, [Classification Algorithms](https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/), the author says, ""When the classes are not linearly separable, a kernel trick can be used to map a non-linearly separable space into a higher dimension linearly separable space."" So do you just check if your class is linearly separable or the entire dataset? In any case, I could not find a proper source that would explain clearly how do you go about checking linearity of the data, with actual implementation, can you help how to achieve this?

Further, a lot of blogs and tutorials mention that when you are selecting a classification algorithm you have to consider dataset size, distribution, computation time, data type of attributes etc. but I came across a notebook which was authored by a university professor wherein he used almost all algorithms, like SVM, AdaBoost, DecisionTree, Random Forest, Bagging, Boosting etc for the same project. Does it make sense to use all of them straightaway? 

Is there like a resource that can help in making a decision, I did come across a few cheat sheets that have a flow chart but they don't seem to be in-depth.",t2_a4yvnttd,False,,0,False,Deciding a classification algorithm,[],r/datascience,False,6,discussion,0,,,False,t3_nt3g36,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622950336.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new in Data Science so please excuse my limited knowledge. I am learning more about classification algorithms and how to appropriately selected algorithms for your task.&lt;/p&gt;

&lt;p&gt;According to this website, &lt;a href=""https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/""&gt;Classification Algorithms&lt;/a&gt;, the author says, &amp;quot;When the classes are not linearly separable, a kernel trick can be used to map a non-linearly separable space into a higher dimension linearly separable space.&amp;quot; So do you just check if your class is linearly separable or the entire dataset? In any case, I could not find a proper source that would explain clearly how do you go about checking linearity of the data, with actual implementation, can you help how to achieve this?&lt;/p&gt;

&lt;p&gt;Further, a lot of blogs and tutorials mention that when you are selecting a classification algorithm you have to consider dataset size, distribution, computation time, data type of attributes etc. but I came across a notebook which was authored by a university professor wherein he used almost all algorithms, like SVM, AdaBoost, DecisionTree, Random Forest, Bagging, Boosting etc for the same project. Does it make sense to use all of them straightaway? &lt;/p&gt;

&lt;p&gt;Is there like a resource that can help in making a decision, I did come across a few cheat sheets that have a flow chart but they don&amp;#39;t seem to be in-depth.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nt3g36,True,,DietMediocre8993,,5,True,all_ads,False,[],False,,/r/datascience/comments/nt3g36/deciding_a_classification_algorithm/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt3g36/deciding_a_classification_algorithm/,515405,1622921536.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello everyone. I am currently self-learning Data Analysis and Data Science from scratch and I am really enjoying the process. I am still a begginer at the moment and still have a long way to go. I have just familliarised myself with python and the NumPy and Pandas Libraries. Just now diving deaper into data cleaning.

I was wondering if any freelancer out there would be willing to connect for the prospect of a future apprentice/assistant to help with freelance work. I could help by handling simple parts of projects at the start and escalate to harder ones as I go along, helping to speed up your work. I am not looking for any salary, just some mentorship and a reference in the future.

Anyone interested feel free to DM me so I can adress any questions.",t2_1k0s2zau,False,,0,False,Any freelancer looking for an appretice/assistant?,[],r/datascience,False,6,career,0,,,False,t3_nsnzqe,False,dark,0.77,,public,15,0,{},,,False,[],,False,False,,{},Career,False,15,,False,False,self,False,,[],{},,True,,1622897590.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone. I am currently self-learning Data Analysis and Data Science from scratch and I am really enjoying the process. I am still a begginer at the moment and still have a long way to go. I have just familliarised myself with python and the NumPy and Pandas Libraries. Just now diving deaper into data cleaning.&lt;/p&gt;

&lt;p&gt;I was wondering if any freelancer out there would be willing to connect for the prospect of a future apprentice/assistant to help with freelance work. I could help by handling simple parts of projects at the start and escalate to harder ones as I go along, helping to speed up your work. I am not looking for any salary, just some mentorship and a reference in the future.&lt;/p&gt;

&lt;p&gt;Anyone interested feel free to DM me so I can adress any questions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsnzqe,True,,Jeenghiskhan,,6,True,all_ads,False,[],False,,/r/datascience/comments/nsnzqe/any_freelancer_looking_for_an_appreticeassistant/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsnzqe/any_freelancer_looking_for_an_appreticeassistant/,515405,1622868790.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi all, I'm interested in salaries in Spain for the mid-career to most senior positions (including managing/head of data science kind of positions) in data science/machine learning engineer/etc., specifically for Spain. Any information is welcome (and yes, I know they are much lower than in the US, and the post is not about that).   
I'm asking specifically about mid-career positions and beyond because I get offers for more junior positions from time to time in Linkedin (so I have that salary range covered), but for more senior positions I don't have that information first hand, and I don't trust the numbers I see on Linkedin salaries or Glassdoor. They seem outdated and, more importantly, biased to people willing to answer. Kudos if you have information specifically about Barcelona and Madrid, where the salaries seem to be highest.  


Best",t2_nqspn,False,,0,False,Salaries in Spain (yet another post like this),[],r/datascience,False,6,,0,,,False,t3_nt1s22,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,False,,[],{},,True,,1622945629.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I&amp;#39;m interested in salaries in Spain for the mid-career to most senior positions (including managing/head of data science kind of positions) in data science/machine learning engineer/etc., specifically for Spain. Any information is welcome (and yes, I know they are much lower than in the US, and the post is not about that).&lt;br/&gt;
I&amp;#39;m asking specifically about mid-career positions and beyond because I get offers for more junior positions from time to time in Linkedin (so I have that salary range covered), but for more senior positions I don&amp;#39;t have that information first hand, and I don&amp;#39;t trust the numbers I see on Linkedin salaries or Glassdoor. They seem outdated and, more importantly, biased to people willing to answer. Kudos if you have information specifically about Barcelona and Madrid, where the salaries seem to be highest.  &lt;/p&gt;

&lt;p&gt;Best&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nt1s22,True,,polidrupa,,2,True,all_ads,False,[],False,,/r/datascience/comments/nt1s22/salaries_in_spain_yet_another_post_like_this/,all_ads,False,https://www.reddit.com/r/datascience/comments/nt1s22/salaries_in_spain_yet_another_post_like_this/,515405,1622916829.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I am working on a college project using streamlit for making a web app. Is it possible to get to this web app through some user authentication?
Moreover if possible to do so via a mobile application to take users credentials and verify it and then direct to this web app. Thank you in advance.",t2_71ommnq4,False,,0,False,Authorization in streamlit,[],r/datascience,False,6,projects,0,,,False,t3_nsr2l3,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1622910393.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a college project using streamlit for making a web app. Is it possible to get to this web app through some user authentication?
Moreover if possible to do so via a mobile application to take users credentials and verify it and then direct to this web app. Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsr2l3,True,,Entcune,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsr2l3/authorization_in_streamlit/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsr2l3/authorization_in_streamlit/,515405,1622881593.0,3,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I have the option to study two different courses, one is to become a Data Scientist Jr and the other one is the Google Project Management course. Right now I work as a
PM but I also know how to code, so I don’t know what path to pursue now, I mean I can improve my skills as a PM but in the other hand Data Science looks a very cool and interesting thing to study. 

I have some experience coding with Python and Django, so I think my profile match, however I’m still wondering what to do... Do you guys like being data scientists? What do you hate about the role? Is there professional growth?  Do you consider that the demand for your profile is increasing?",t2_5216olhf,False,,0,False,Do you like being a data scientist?,[],r/datascience,False,6,career,0,,,False,t3_nsda9k,False,dark,0.91,,public,43,0,{},,,False,[],,False,False,,{},Career,False,43,,False,False,self,False,,[],{},,True,,1622863678.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have the option to study two different courses, one is to become a Data Scientist Jr and the other one is the Google Project Management course. Right now I work as a
PM but I also know how to code, so I don’t know what path to pursue now, I mean I can improve my skills as a PM but in the other hand Data Science looks a very cool and interesting thing to study. &lt;/p&gt;

&lt;p&gt;I have some experience coding with Python and Django, so I think my profile match, however I’m still wondering what to do... Do you guys like being data scientists? What do you hate about the role? Is there professional growth?  Do you consider that the demand for your profile is increasing?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsda9k,True,,nachoaddict19,,49,True,all_ads,False,[],False,,/r/datascience/comments/nsda9k/do_you_like_being_a_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsda9k/do_you_like_being_a_data_scientist/,515405,1622834878.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,,t2_2iu6mxmf,False,,0,False,Doing open source science for a living,[],r/datascience,False,6,discussion,0,,,False,t3_nswaiu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,default,False,,[],{},,False,,1622930226.0,text,6,,,text,self.Researcher,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nswaiu,True,,Paranoid_Bot_42,,5,False,all_ads,False,[],False,,/r/datascience/comments/nswaiu/doing_open_source_science_for_a_living/,all_ads,False,/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/,515405,1622901426.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'Researcher', 'selftext': ""Hey science people, I'd like to consult you for career advice. So I am currently a physics undergrad, who likes programming and would like to be a data scientist in scientific applications. Right now, I am also working an programming job at a big company. One would say that I am at the right path but with one thing is messing me up. \n\nI don't like corporations and the thought of being a data scientist for one kinda makes me sad.  That's  beause as a scientist my goal is to create technologies or discover scientific insights that benefit society as a whole, rather than being a means for a corporation to make profits. What really makes me not want to be in such an environment is that my work will be patented and sold rather than being available for anyone to use it as they please. Other than that, I'd rather not solve business problems that don't directly benefit the general public. I don't really give a shit about finding optimal marketing strategies through data, for example. I wants things like contributing to solving climate change through data science (atmospheric physics) or analysing the brain and developing our understanding of it or creating a smart city that actually benefits the people. \n\nMy concern is that since we live in ever more corporate world, I won't find a true open source science job, neither in industry (because of the profit motive) neither in academia (due to job vacancies being so few). \n\nSo I am asking: are my concerns valid? how do I deal with them? how do I find a job that fits my goals? do you feel like this, too?\n\nSorry for the long post"", 'author_fullname': 't2_2iu6mxmf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Doing open-source science for a living', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/Researcher', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nsvvo7', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 26, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 26, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622928938.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.Researcher', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey science people, I&amp;#39;d like to consult you for career advice. So I am currently a physics undergrad, who likes programming and would like to be a data scientist in scientific applications. Right now, I am also working an programming job at a big company. One would say that I am at the right path but with one thing is messing me up. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t like corporations and the thought of being a data scientist for one kinda makes me sad.  That&amp;#39;s  beause as a scientist my goal is to create technologies or discover scientific insights that benefit society as a whole, rather than being a means for a corporation to make profits. What really makes me not want to be in such an environment is that my work will be patented and sold rather than being available for anyone to use it as they please. Other than that, I&amp;#39;d rather not solve business problems that don&amp;#39;t directly benefit the general public. I don&amp;#39;t really give a shit about finding optimal marketing strategies through data, for example. I wants things like contributing to solving climate change through data science (atmospheric physics) or analysing the brain and developing our understanding of it or creating a smart city that actually benefits the people. &lt;/p&gt;\n\n&lt;p&gt;My concern is that since we live in ever more corporate world, I won&amp;#39;t find a true open source science job, neither in industry (because of the profit motive) neither in academia (due to job vacancies being so few). &lt;/p&gt;\n\n&lt;p&gt;So I am asking: are my concerns valid? how do I deal with them? how do I find a job that fits my goals? do you feel like this, too?&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2rg08', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nsvvo7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Paranoid_Bot_42', 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/', 'subreddit_subscribers': 739, 'created_utc': 1622900138.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",/r/Researcher/comments/nsvvo7/doing_opensource_science_for_a_living/,t3_nsvvo7,
,datascience,"We can all agree that most of the job is boring data stuff, collecting, cleaning, designing, transforming and the list keeps going. This takes like 75% to 90% of our time. The rest of the time is powerpoint, excel, meetings, building models, ML and ML-dev ops.

So my question is why so many positions require Master or higher education? do you guys have this kind of education?

Do you guys do actual complex stuff? What do you do that you need to have a PhD to do it?

I got 3 years of experience and i've never needed to know anything that couldnt be learned in a basic blog post of some random dude.

I guess some positions do need very complex knowledge of NPL or DL but the rest?? no way",t2_bftt58l,False,,0,False,So what's the point of having a Masters or a PhD in this field?,[],r/datascience,False,6,discussion,0,,,False,t3_nshjld,False,dark,0.65,,public,13,0,{},,,False,[],,False,False,,{},Discussion,False,13,,False,False,self,False,,[],{},,True,,1622875383.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We can all agree that most of the job is boring data stuff, collecting, cleaning, designing, transforming and the list keeps going. This takes like 75% to 90% of our time. The rest of the time is powerpoint, excel, meetings, building models, ML and ML-dev ops.&lt;/p&gt;

&lt;p&gt;So my question is why so many positions require Master or higher education? do you guys have this kind of education?&lt;/p&gt;

&lt;p&gt;Do you guys do actual complex stuff? What do you do that you need to have a PhD to do it?&lt;/p&gt;

&lt;p&gt;I got 3 years of experience and i&amp;#39;ve never needed to know anything that couldnt be learned in a basic blog post of some random dude.&lt;/p&gt;

&lt;p&gt;I guess some positions do need very complex knowledge of NPL or DL but the rest?? no way&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nshjld,True,,felipecalderon1,,53,True,all_ads,False,[],False,,/r/datascience/comments/nshjld/so_whats_the_point_of_having_a_masters_or_a_phd/,all_ads,False,https://www.reddit.com/r/datascience/comments/nshjld/so_whats_the_point_of_having_a_masters_or_a_phd/,515405,1622846583.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am working on a dataset that has both categorical and numerical (continuous and discrete) features (26 columns, 30244 rows). Target is categorical (1, 2, 3) and I am performing EDA on this dataset.

My dataset is regarding hotel reservation status (Not cancelled(1), Cancelled(2), No show(3)) of customers in the span of 3 years (2015, 2016, 2017). Given data of the customer, my task is to predict if the customer will either cancel, not cancel or no-show for his reservation.

* The categorical features with numerical values (ex: gender has values 0 and 1) are also considered when taking the heatmap with seaborn. As per my knowledge, the heatmap is drawn to check the correlation between continuous numerical features right (correct me if I am wrong). Should I remove such features before taking the heatmap?
* The book-in date, expected check-in date, expected check-out date are given in the dataset. I extracted month and year for each feature separately. These month columns are also categorical right? (As they only have values between 1-12). I took screenshots of month distribution plots and uploaded them here.

[https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing](https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing)

* Should I do a test like the Chi-Square test on those features?",t2_4q1w55i3,False,,0,False,"(I am self learning data science. I asked this question on every platform I can think of, but still didn't get an answer. Please help me out if you know the answer) Should I remove features such as gender and birth month before drawing the heatmap because they are categorical?",[],r/datascience,False,6,education,0,,,False,t3_nsojd5,False,dark,0.56,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,1622913126.0,,[],{},,True,,1622899665.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a dataset that has both categorical and numerical (continuous and discrete) features (26 columns, 30244 rows). Target is categorical (1, 2, 3) and I am performing EDA on this dataset.&lt;/p&gt;

&lt;p&gt;My dataset is regarding hotel reservation status (Not cancelled(1), Cancelled(2), No show(3)) of customers in the span of 3 years (2015, 2016, 2017). Given data of the customer, my task is to predict if the customer will either cancel, not cancel or no-show for his reservation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The categorical features with numerical values (ex: gender has values 0 and 1) are also considered when taking the heatmap with seaborn. As per my knowledge, the heatmap is drawn to check the correlation between continuous numerical features right (correct me if I am wrong). Should I remove such features before taking the heatmap?&lt;/li&gt;
&lt;li&gt;The book-in date, expected check-in date, expected check-out date are given in the dataset. I extracted month and year for each feature separately. These month columns are also categorical right? (As they only have values between 1-12). I took screenshots of month distribution plots and uploaded them here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing""&gt;https://drive.google.com/drive/folders/1duayZBpuFrqLCEO9OYRgArtv1HVpVsBz?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Should I do a test like the Chi-Square test on those features?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsojd5,True,,hirushi_wijesinghe,,12,True,all_ads,False,[],False,,/r/datascience/comments/nsojd5/i_am_self_learning_data_science_i_asked_this/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsojd5/i_am_self_learning_data_science_i_asked_this/,515405,1622870865.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YeJZEzkcnDwlNU58RXraG9gbKZcGZvvTbcjCIwwXCYY.jpg?auto=webp&amp;s=3869a1565f12b3477e8cff17ec64e94a6fb115e9', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/YeJZEzkcnDwlNU58RXraG9gbKZcGZvvTbcjCIwwXCYY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4d3a80353a2c205ca5b617d6fd3cf7c31b4d872', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'duj8xNL5l81WpIkn0ToMaJAlR5YyBTeJcZYytO0KWDs'}], 'enabled': False}",,,,,
,datascience,"From your experience, is it worth tailoring resumes to job openings? Or is it a better idea to have a one-size-fits-all resume and cast a wide net?",t2_m4ekm0b,False,,0,False,On the quality vs quantity of job applications,[],r/datascience,False,6,career,0,,,False,t3_nseegw,False,dark,0.83,,public,11,0,{},,,False,[],,False,False,,{},Career,False,11,,False,False,self,False,,[],{},,True,,1622866559.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From your experience, is it worth tailoring resumes to job openings? Or is it a better idea to have a one-size-fits-all resume and cast a wide net?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nseegw,True,,fool126,,21,True,all_ads,False,[],False,,/r/datascience/comments/nseegw/on_the_quality_vs_quantity_of_job_applications/,all_ads,False,https://www.reddit.com/r/datascience/comments/nseegw/on_the_quality_vs_quantity_of_job_applications/,515405,1622837759.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I got a data science internship and I've just started, I've been in the role for about two weeks. It's a research and development role for the company's product. I'm honestly really enjoying it. The role is capturing meta data from data science tech stacks like Google, AWS, Microsoft etc. We get to build projects with the technology so there's free reign to do whatever I want to which is a lot of fun. I get to use a lot of the new or popular tools data scientists use but I was wondering if it's detrimental that I'm not actually creating models for production. 

I'll hopefully be very proficient with popular cloud services offerings but it's more of knowing how to create pipelines, setup services and a lot of the auxiliary things surrounding models. Like I previously said I do honestly enjoy the role. It wasn't what I was expecting but I get to learn a lot at a manageable pace which is nice.",t2_68lcg7iq,False,,0,False,Will My Internship Influence My Career Trajectory?,[],r/datascience,False,6,discussion,0,,,False,t3_nsq3k6,False,dark,0.55,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622906148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got a data science internship and I&amp;#39;ve just started, I&amp;#39;ve been in the role for about two weeks. It&amp;#39;s a research and development role for the company&amp;#39;s product. I&amp;#39;m honestly really enjoying it. The role is capturing meta data from data science tech stacks like Google, AWS, Microsoft etc. We get to build projects with the technology so there&amp;#39;s free reign to do whatever I want to which is a lot of fun. I get to use a lot of the new or popular tools data scientists use but I was wondering if it&amp;#39;s detrimental that I&amp;#39;m not actually creating models for production. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ll hopefully be very proficient with popular cloud services offerings but it&amp;#39;s more of knowing how to create pipelines, setup services and a lot of the auxiliary things surrounding models. Like I previously said I do honestly enjoy the role. It wasn&amp;#39;t what I was expecting but I get to learn a lot at a manageable pace which is nice.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsq3k6,True,,PowerTurtz,,8,True,all_ads,False,[],False,,/r/datascience/comments/nsq3k6/will_my_internship_influence_my_career_trajectory/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsq3k6/will_my_internship_influence_my_career_trajectory/,515405,1622877348.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"**The Approximately Correct Machine Intelligence (ACMI) Lab at Carnegie Mellon University** (CMU) has published a paper on **Randomly Assign, Train, and Track (RATT)**. RATT is an algorithm that uses noisy training data to put an upper bound on a deep-learning model’s actual error risk. Model developers can use **RATT to see how well a model generalizes to new input data**.

The researchers demonstrate mathematical proofs of RATT’s guarantees and conduct experiments on various datasets for computer vision (CV) and natural language processing (NLP) models in their publication. **When a trained model gets a high error rate on randomly labeled (or noisy) data but a low error rate on clean data, the model is assumed to have a low error rate on new data.**

Full Summary: [https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/](https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/) 

Paper: https://arxiv.org/pdf/2105.00303.pdf",t2_4wudjgid,False,,0,False,"CMU Researchers Propose RATT (Randomly Assign, Train and Track), A Method for Guaranteeing AI Model Generalization",[],r/datascience,False,6,discussion,0,,,False,t3_nsplyp,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622904016.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;The Approximately Correct Machine Intelligence (ACMI) Lab at Carnegie Mellon University&lt;/strong&gt; (CMU) has published a paper on &lt;strong&gt;Randomly Assign, Train, and Track (RATT)&lt;/strong&gt;. RATT is an algorithm that uses noisy training data to put an upper bound on a deep-learning model’s actual error risk. Model developers can use &lt;strong&gt;RATT to see how well a model generalizes to new input data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The researchers demonstrate mathematical proofs of RATT’s guarantees and conduct experiments on various datasets for computer vision (CV) and natural language processing (NLP) models in their publication. &lt;strong&gt;When a trained model gets a high error rate on randomly labeled (or noisy) data but a low error rate on clean data, the model is assumed to have a low error rate on new data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/""&gt;https://www.marktechpost.com/2021/06/04/cmu-researchers-propose-ratt-randomly-assign-train-and-track-a-method-for-guaranteeing-ai-model-generalization/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2105.00303.pdf""&gt;https://arxiv.org/pdf/2105.00303.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsplyp,True,,techsucker,,0,True,all_ads,False,[],False,,/r/datascience/comments/nsplyp/cmu_researchers_propose_ratt_randomly_assign/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsplyp/cmu_researchers_propose_ratt_randomly_assign/,515405,1622875216.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?auto=webp&amp;s=cca984f6c568e770cf55df1e6e1ace3fe886eb7a', 'width': 1374, 'height': 742}, 'resolutions': [{'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=84a8950ffe2808e8eea767fbf4ea02aa28d7b79e', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6afde8a34d3731c356a0c8e72585a3ad9d2bc3e2', 'width': 216, 'height': 116}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c2656c02948cf52d53dd73def5dd4b3f239bae', 'width': 320, 'height': 172}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a95044a19444ac21402b2e5e6313747341d7ec3', 'width': 640, 'height': 345}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8792baa88f2f45679005e6e80e180de5bc0b489', 'width': 960, 'height': 518}, {'url': 'https://external-preview.redd.it/dzTA6iZ34YP4HzJKO56Y_L3d1i_OiekCZjXsg894oAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f332720922825789e85c18411a698c87584cc41c', 'width': 1080, 'height': 583}], 'variants': {}, 'id': 'QjAMdGXZr6yeEFP_BInpH50tMe0fN8LXPut1apFYauA'}], 'enabled': False}",,,,,
,datascience,"I have a dataset for a bank where the objective is to predict whether a loan will end up as good where it's completely paid off, or a bad loan. The dataset has the target variable consisting of three labels: open (the loan is still ongoing), good loan and bad loan. The dataset is heavily skewed/imbalanced where majority of the rows/data points behind to good loan and the bad loans are a minority.

What I have done is filter the dataset for good and bad loans and trained different ML models on it which gives too good to be true performance. This makes it a binary classification. Accuracy, precision and recall are all 1.00 or 100% for the validation sets.

Then I want to use it to make predictions for the ""open"" rows/data points in the CSV file. But, the problem is that since these loans are still ongoing, we don't have the ground truth for them and therefore whatever predictions I get cannot be compared against the ground truth to get model metrics such as accuracy, precision, recall, etc.


Suggestions/help?

Thanks",t2_2mmql89p,False,,0,False,Unseen data prediction,[],r/datascience,False,6,discussion,0,,,False,t3_nspls7,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,1622946507.0,,[],{},,True,,1622903996.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset for a bank where the objective is to predict whether a loan will end up as good where it&amp;#39;s completely paid off, or a bad loan. The dataset has the target variable consisting of three labels: open (the loan is still ongoing), good loan and bad loan. The dataset is heavily skewed/imbalanced where majority of the rows/data points behind to good loan and the bad loans are a minority.&lt;/p&gt;

&lt;p&gt;What I have done is filter the dataset for good and bad loans and trained different ML models on it which gives too good to be true performance. This makes it a binary classification. Accuracy, precision and recall are all 1.00 or 100% for the validation sets.&lt;/p&gt;

&lt;p&gt;Then I want to use it to make predictions for the &amp;quot;open&amp;quot; rows/data points in the CSV file. But, the problem is that since these loans are still ongoing, we don&amp;#39;t have the ground truth for them and therefore whatever predictions I get cannot be compared against the ground truth to get model metrics such as accuracy, precision, recall, etc.&lt;/p&gt;

&lt;p&gt;Suggestions/help?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nspls7,True,,grid_world,,21,True,all_ads,False,[],False,,/r/datascience/comments/nspls7/unseen_data_prediction/,all_ads,False,https://www.reddit.com/r/datascience/comments/nspls7/unseen_data_prediction/,515405,1622875196.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey Forum. I wanted to get some help and potentially collaborate/hire someone to help me find / come up with a solution to the below problem and the correct methodology on how to go about it. Here is the background:

&amp;#x200B;

The use case is algorithmic trading. To make a simple example, I use indicators to trade forex/stocks / etc. I perform backtests on data (over 10 to 15 years) and my goal is to determine the best performing indicator input parameter combination. I do my backtesting using a process called ""walk forward analysis/walk forward optimization"" [https://en.wikipedia.org/wiki/Walk\_forward\_optimization](https://en.wikipedia.org/wiki/Walk_forward_optimization) . The goal is to determine how robust the trading algo is (the system) when it runs on ""out of sample data"". The goal is to select the best performing parameters (indicator parameters like stochastic, if that's what you are using), and carry that forward and use those input parameters on your out-of-sample data. so here is what a in sample results look like 

&amp;#x200B;

param1	param1	    CAGR/AvgDD

27	         140		10.661

27	        160		       10.236

29	         145	      9.633

31		 150	      12.927

33	        155	               3.952

35	       140	              3.214

37	        145	              5.977

&amp;#x200B;

CAGR/AVgDD is my performance metric. It can be anything really, Profit, or profit factor, etc. Basically, parameters 1, 2, X are inputs to the system. During an in-sample optimization run, for each input parameter that I want to optimize, i pick a start, end and a step. So if there are 3 parameters and each has a start value of 1, the end value of 4, and a step of 1, then you have 4\*4\*4 combinations/passes and each pass will generate a performance metric value. My in-sample runs have a statistically significant amount of data. For example, a run over 4 years (in-sample period) will have at least 5000 to 10000 trades) for each pass/parameter combination. 

&amp;#x200B;

So here is the problem that I want to solve. How do I know which single ""pass"" / ""parameter combination"" to select to run on my out-of-sample data set? How do I choose the best-performing input parameter combination? I know it won't always be the one with the highest performance metric value. For example, my highest value can be 10, and the one right below it can be 4.3. Clearly here 10 is an outlier. 

what algo or method should be used? 

some people have said KNN. Is that true? 

can this be done with more than just 2 parameter inputs? what if I am optimizing 3 or 4? is there a downside to optimizing 3, 4, 5, etc different parameters

&amp;#x200B;

I'm also looking for someone to help automate this for me. Looking for someone in math/data science background to help me with this.",t2_zqq2s,False,,0,False,choosing the best parameters in an optimization,[],r/datascience,False,6,discussion,0,,,False,t3_nspktt,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622903880.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Forum. I wanted to get some help and potentially collaborate/hire someone to help me find / come up with a solution to the below problem and the correct methodology on how to go about it. Here is the background:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The use case is algorithmic trading. To make a simple example, I use indicators to trade forex/stocks / etc. I perform backtests on data (over 10 to 15 years) and my goal is to determine the best performing indicator input parameter combination. I do my backtesting using a process called &amp;quot;walk forward analysis/walk forward optimization&amp;quot; &lt;a href=""https://en.wikipedia.org/wiki/Walk_forward_optimization""&gt;https://en.wikipedia.org/wiki/Walk_forward_optimization&lt;/a&gt; . The goal is to determine how robust the trading algo is (the system) when it runs on &amp;quot;out of sample data&amp;quot;. The goal is to select the best performing parameters (indicator parameters like stochastic, if that&amp;#39;s what you are using), and carry that forward and use those input parameters on your out-of-sample data. so here is what a in sample results look like &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;param1  param1      CAGR/AvgDD&lt;/p&gt;

&lt;p&gt;27           140        10.661&lt;/p&gt;

&lt;p&gt;27          160            10.236&lt;/p&gt;

&lt;p&gt;29           145          9.633&lt;/p&gt;

&lt;p&gt;31       150          12.927&lt;/p&gt;

&lt;p&gt;33          155                3.952&lt;/p&gt;

&lt;p&gt;35         140                3.214&lt;/p&gt;

&lt;p&gt;37          145               5.977&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;CAGR/AVgDD is my performance metric. It can be anything really, Profit, or profit factor, etc. Basically, parameters 1, 2, X are inputs to the system. During an in-sample optimization run, for each input parameter that I want to optimize, i pick a start, end and a step. So if there are 3 parameters and each has a start value of 1, the end value of 4, and a step of 1, then you have 4*4*4 combinations/passes and each pass will generate a performance metric value. My in-sample runs have a statistically significant amount of data. For example, a run over 4 years (in-sample period) will have at least 5000 to 10000 trades) for each pass/parameter combination. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So here is the problem that I want to solve. How do I know which single &amp;quot;pass&amp;quot; / &amp;quot;parameter combination&amp;quot; to select to run on my out-of-sample data set? How do I choose the best-performing input parameter combination? I know it won&amp;#39;t always be the one with the highest performance metric value. For example, my highest value can be 10, and the one right below it can be 4.3. Clearly here 10 is an outlier. &lt;/p&gt;

&lt;p&gt;what algo or method should be used? &lt;/p&gt;

&lt;p&gt;some people have said KNN. Is that true? &lt;/p&gt;

&lt;p&gt;can this be done with more than just 2 parameter inputs? what if I am optimizing 3 or 4? is there a downside to optimizing 3, 4, 5, etc different parameters&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m also looking for someone to help automate this for me. Looking for someone in math/data science background to help me with this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nspktt,True,,daproject85,,1,True,all_ads,False,[],False,,/r/datascience/comments/nspktt/choosing_the_best_parameters_in_an_optimization/,all_ads,False,https://www.reddit.com/r/datascience/comments/nspktt/choosing_the_best_parameters_in_an_optimization/,515405,1622875080.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/apkmo5zs_-YmFFDjmG62TyIS5jvdW18a790XKORnEu8.jpg?auto=webp&amp;s=d430e4efd42baf58b4bc71c9549944b7bdf40715', 'width': 50, 'height': 39}, 'resolutions': [], 'variants': {}, 'id': 'K8Lxcv5aHkjcbujO-2N7cVXtdXkwYe8BHCOouTCCjb8'}], 'enabled': False}",,,,,
,datascience,"Hello!

I am about to finish my first year of a data job after grad school. In the past year, I've found myself not being able to upskill much outside of new things I learnt at work. Reasons or excuses could be wanting time for my hobbies, not wanting to study on the weekend.

I know I need to put some hours every week to learn new stuff outside of my job but I am kind of struggling.

I was wondering how do you guys go about your upskilling and what are some tips would you like to give to someone like me.

Thank you!

P.S. I can't say my skill set is the same as it was a year ago but not doing much outside of work makes me a bit insecure.",t2_bv171ji2,False,,0,False,"In the early stage of your career, how many hours per week should be dedicated to upskilling?",[],r/datascience,False,6,discussion,0,,,False,t3_nsbf1f,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1622858694.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am about to finish my first year of a data job after grad school. In the past year, I&amp;#39;ve found myself not being able to upskill much outside of new things I learnt at work. Reasons or excuses could be wanting time for my hobbies, not wanting to study on the weekend.&lt;/p&gt;

&lt;p&gt;I know I need to put some hours every week to learn new stuff outside of my job but I am kind of struggling.&lt;/p&gt;

&lt;p&gt;I was wondering how do you guys go about your upskilling and what are some tips would you like to give to someone like me.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;P.S. I can&amp;#39;t say my skill set is the same as it was a year ago but not doing much outside of work makes me a bit insecure.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsbf1f,True,,quite--average,,11,True,all_ads,False,[],False,,/r/datascience/comments/nsbf1f/in_the_early_stage_of_your_career_how_many_hours/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsbf1f/in_the_early_stage_of_your_career_how_many_hours/,515405,1622829894.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"First DS job, working remotely, started about 3 months ago. It's a very small company in an industry that I was unfamiliar with, so there was a distinct learning curve in getting acquainted. There was also no onboarding, as is to be expected for very small companies.

I'm not sure they needed a data scientist and now I'm kind of scrambling to try and show value. Along with that, the part of the business that I'm working on does not generate any revenue, and has very little data (about 3000 sparse data points, maybe 1200 good ones, very few updates - maybe 5 per week, and very few insights that I am being asked to run on these actual data points).

With that, I'm being involved in the business development side of things (higher-ups do not know how this current 6-year old project should generate revenue), and every week my task changes. Usually it's about finding open-source datasets, but my boss has very little focus/patience, so each week is different. I struggle to maintain focus in my day-to-day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame (i.e. investigate a causal question that would be great for an entire econometric paper), and will likely not generate revenue anytime soon.

Are there any other DS folk who have been hired into small places with very little data, being the only DS? How did you handle the situation? How long does it take you to do open-source data collection? I don't mind the field, the work, nor wearing many hats, but I'm worried that I won't be generating enough value to justify staying on the team.",t2_5fbf7t,False,,0,False,Hired at a Small Company. My Job is... Shaky.,[],r/datascience,False,6,career,0,,,False,t3_nrp373,False,dark,0.97,,public,205,1,{},,,False,[],,False,False,,{},Career,False,205,,False,False,self,False,,[],{},,True,,1622786086.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;First DS job, working remotely, started about 3 months ago. It&amp;#39;s a very small company in an industry that I was unfamiliar with, so there was a distinct learning curve in getting acquainted. There was also no onboarding, as is to be expected for very small companies.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not sure they needed a data scientist and now I&amp;#39;m kind of scrambling to try and show value. Along with that, the part of the business that I&amp;#39;m working on does not generate any revenue, and has very little data (about 3000 sparse data points, maybe 1200 good ones, very few updates - maybe 5 per week, and very few insights that I am being asked to run on these actual data points).&lt;/p&gt;

&lt;p&gt;With that, I&amp;#39;m being involved in the business development side of things (higher-ups do not know how this current 6-year old project should generate revenue), and every week my task changes. Usually it&amp;#39;s about finding open-source datasets, but my boss has very little focus/patience, so each week is different. I struggle to maintain focus in my day-to-day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame (i.e. investigate a causal question that would be great for an entire econometric paper), and will likely not generate revenue anytime soon.&lt;/p&gt;

&lt;p&gt;Are there any other DS folk who have been hired into small places with very little data, being the only DS? How did you handle the situation? How long does it take you to do open-source data collection? I don&amp;#39;t mind the field, the work, nor wearing many hats, but I&amp;#39;m worried that I won&amp;#39;t be generating enough value to justify staying on the team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrp373,True,,DegenerateWaves,,56,False,all_ads,False,[],False,,/r/datascience/comments/nrp373/hired_at_a_small_company_my_job_is_shaky/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrp373/hired_at_a_small_company_my_job_is_shaky/,515405,1622757286.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm working on a dating app and was crunching some numbers and would love some feedback to see if there's any holes in my assumptions and calculations.

Thank you!

[https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing)",t2_xu50j,False,,0,False,Looks like the expected number of dating app matches one can get in Iceland is 187?,[],r/datascience,False,6,discussion,0,,,False,t3_nsiwd1,False,dark,0.56,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622879695.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a dating app and was crunching some numbers and would love some feedback to see if there&amp;#39;s any holes in my assumptions and calculations.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing""&gt;https://docs.google.com/spreadsheets/d/1yB36IMgetTkYxoMtDmYiKaCmMFemFerLMpq5E6LHhDs/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsiwd1,True,,broccolie,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsiwd1/looks_like_the_expected_number_of_dating_app/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsiwd1/looks_like_the_expected_number_of_dating_app/,515405,1622850895.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?auto=webp&amp;s=e1dc0ea8fe78cdb885562e5e0204bf72513bb78a', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3daacadd74ede240d61102a9ce86a8c8cfd4664', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5bb8e6ca5279aa5247f906cb60428a99f418f8d9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f74b085e7350ebfd6349e49e958678fecea3f12b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72fd61b4498b2b84a2faf01070a71abf0c4e5545', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c5fc9c42d0159a50dae732fada8b243db802e5', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/gTzKDrxa-cAfqomNaEs4P2dI3pAlicEjfmWLGhvxEM8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8801cb5e5f1ac15c206ae0b7850bac557aac2c81', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'cYhDIwEt8ylpsOspk4tAJprS3M-jj1txWKnXf_kG7RM'}], 'enabled': False}",,,,,
,datascience,"Hi, I am doing my masters in Data Science in Berlin, but my background is from automotive and have work experience as PMO for 2.9 years in Digital Transformation. Now to switch my career to Data Science, I took masters in Berlin. Now I have done 2 Company projects with the help of University and it was a part of University curriculum. Can I add this as experience in my resume or I need to add this only as a side project?

Will it be misleading the recruiter or the company I am giving Interviews. Kindly let me know.",t2_97m7yfi9,False,,0,False,Can I add University sponsored company Projects as experience in my resume?,[],r/datascience,False,6,,0,,,False,t3_nsi4ng,False,dark,0.62,,public,2,1,{},,,False,[],,False,False,,{},Job Search,False,2,,False,False,self,False,,[],{},,True,,1622877178.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am doing my masters in Data Science in Berlin, but my background is from automotive and have work experience as PMO for 2.9 years in Digital Transformation. Now to switch my career to Data Science, I took masters in Berlin. Now I have done 2 Company projects with the help of University and it was a part of University curriculum. Can I add this as experience in my resume or I need to add this only as a side project?&lt;/p&gt;

&lt;p&gt;Will it be misleading the recruiter or the company I am giving Interviews. Kindly let me know.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nsi4ng,True,,Vinothd19,,7,True,all_ads,False,[],False,,/r/datascience/comments/nsi4ng/can_i_add_university_sponsored_company_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsi4ng/can_i_add_university_sponsored_company_projects/,515405,1622848378.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience," I'm currently in a role where I've solely been working on NLP for the entire time. Recently I was offered an opportunity at a new company where I would be working with satellite and time series data with the goal of doing combining time series and CV. The thing is I have zero experience working with both time series and image data, the company are aware of this though, and they are fine with it.

My main reason for posting this is to try and gather some training resources so I don't go in with zero knowledge. Looking for any interesting projects/courses involving those types of data that would be beginner friendly and go through the processes that are generally required for this work. Appreciate any help you can provide",t2_18opmol6,False,,0,False,Transitioning from NLP to satellite and image based CV,[],r/datascience,False,6,career,0,,,False,t3_ns4sz8,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Career,False,6,,False,False,self,False,,[],{},,True,,1622841148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently in a role where I&amp;#39;ve solely been working on NLP for the entire time. Recently I was offered an opportunity at a new company where I would be working with satellite and time series data with the goal of doing combining time series and CV. The thing is I have zero experience working with both time series and image data, the company are aware of this though, and they are fine with it.&lt;/p&gt;

&lt;p&gt;My main reason for posting this is to try and gather some training resources so I don&amp;#39;t go in with zero knowledge. Looking for any interesting projects/courses involving those types of data that would be beginner friendly and go through the processes that are generally required for this work. Appreciate any help you can provide&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns4sz8,True,,sjh3192,,3,True,all_ads,False,[],False,,/r/datascience/comments/ns4sz8/transitioning_from_nlp_to_satellite_and_image/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns4sz8/transitioning_from_nlp_to_satellite_and_image/,515405,1622812348.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am new in Data Science so excuse my limited knowledge. I am studying an overview of the different stages that a DS project goes through.

Currently, I am learning more about outliers detection and removal. So, I have not found a resource that talks about real world application of outlier removal. Say you are working on a small project that has a data set that depends highly on other external factors. Let's assume the dataset that I will work on is economic growth in a particular state. In this case, economic growth depends on hundred different factors, growth in business, population, market, jobs etc. 

Given that in your project you are just limited to the economic data, my question is should you perform an outlier removal (get a boxplot of different attributes in the dataset and and remove all the values that fall out of the 1.5 times interquartile range) or let go of the removal so as to assume the closest real world possibility?

In practice, what would be the ideal solution to work in this scenario?",t2_a4yvnttd,False,,0,False,Should removing outlier be based on the specific data?,[],r/datascience,False,6,discussion,0,,,False,t3_ns7l7m,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622848705.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new in Data Science so excuse my limited knowledge. I am studying an overview of the different stages that a DS project goes through.&lt;/p&gt;

&lt;p&gt;Currently, I am learning more about outliers detection and removal. So, I have not found a resource that talks about real world application of outlier removal. Say you are working on a small project that has a data set that depends highly on other external factors. Let&amp;#39;s assume the dataset that I will work on is economic growth in a particular state. In this case, economic growth depends on hundred different factors, growth in business, population, market, jobs etc. &lt;/p&gt;

&lt;p&gt;Given that in your project you are just limited to the economic data, my question is should you perform an outlier removal (get a boxplot of different attributes in the dataset and and remove all the values that fall out of the 1.5 times interquartile range) or let go of the removal so as to assume the closest real world possibility?&lt;/p&gt;

&lt;p&gt;In practice, what would be the ideal solution to work in this scenario?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns7l7m,True,,DietMediocre8993,,9,True,all_ads,False,[],False,,/r/datascience/comments/ns7l7m/should_removing_outlier_be_based_on_the_specific/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns7l7m/should_removing_outlier_be_based_on_the_specific/,515405,1622819905.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, not sure if this is the right place for this, but just stressing cos I'm getting to the end of the sprint and I've got no results to show for my work. 

&amp;#x200B;

So basically, I started at my first DS job just over a month ago, and I'm currently working on developing a small object detection model to pick out various symbols in these complex noisy diagrams (crowded with lots of superfluous lines and text). 

&amp;#x200B;

The data set we have is pretty small, and I've had to label it myself. I've chosen to break the symbols into around 8 classes that are all pretty distinct from each other, and I've picked out \~600 data points of all symbols that we need, and \~400 cases of the general background cases. 

&amp;#x200B;

I've trained a CNN model to classify each case, and the confusion matrix looks pretty good, but when I run the model on the sliding windows it just performs terribly. 

&amp;#x200B;

Basically, it's OK at picking out these things in the small image sections that I've picked out but is terrible at picking them out in the random windows generated by the sliding windows technique that I'm applying. 

&amp;#x200B;

This is basically my first time building a NN model, and my first time labelling my own data. I think I have all of the implementations working fine, but I just don't know how to boost the performance.

&amp;#x200B;

I feel like this is mostly just a lack of experience on my part, but it just feels like I've hit a wall in terms of what I can do with this model... I just don't want to have to say to my team that the project I've been working on all sprint is a no-go... I'd also mean that I'd have to come up with an alternative strategy to identify these things cos we kind of need this in the long run...

&amp;#x200B;

Sorry for the rant.. Any advice or resources would be really appreciated!!

&amp;#x200B;

Thanks!",t2_711grept,False,,0,False,ML advice needed!,[],r/datascience,False,6,projects,0,,,False,t3_nrzvrn,False,dark,0.86,,public,10,0,{},,,False,[],,False,False,,{},Projects,False,10,,False,False,self,False,,[],{},,True,,1622823264.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, not sure if this is the right place for this, but just stressing cos I&amp;#39;m getting to the end of the sprint and I&amp;#39;ve got no results to show for my work. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So basically, I started at my first DS job just over a month ago, and I&amp;#39;m currently working on developing a small object detection model to pick out various symbols in these complex noisy diagrams (crowded with lots of superfluous lines and text). &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The data set we have is pretty small, and I&amp;#39;ve had to label it myself. I&amp;#39;ve chosen to break the symbols into around 8 classes that are all pretty distinct from each other, and I&amp;#39;ve picked out ~600 data points of all symbols that we need, and ~400 cases of the general background cases. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve trained a CNN model to classify each case, and the confusion matrix looks pretty good, but when I run the model on the sliding windows it just performs terribly. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Basically, it&amp;#39;s OK at picking out these things in the small image sections that I&amp;#39;ve picked out but is terrible at picking them out in the random windows generated by the sliding windows technique that I&amp;#39;m applying. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is basically my first time building a NN model, and my first time labelling my own data. I think I have all of the implementations working fine, but I just don&amp;#39;t know how to boost the performance.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I feel like this is mostly just a lack of experience on my part, but it just feels like I&amp;#39;ve hit a wall in terms of what I can do with this model... I just don&amp;#39;t want to have to say to my team that the project I&amp;#39;ve been working on all sprint is a no-go... I&amp;#39;d also mean that I&amp;#39;d have to come up with an alternative strategy to identify these things cos we kind of need this in the long run...&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Sorry for the rant.. Any advice or resources would be really appreciated!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrzvrn,True,,the_constant_reddit,,14,True,all_ads,False,[],False,,/r/datascience/comments/nrzvrn/ml_advice_needed/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrzvrn/ml_advice_needed/,515405,1622794464.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"This week I've had an interview with a company that manufactures wooden floors. They want to hire a data analyst / software engineer to digitilize  their manufacturing processses within their factory.

They told md which improvements they had in mine and some of them seemed completely okay, like installing sensors around the factory and gather data from those sensors yo atart building some big data bank.

The second is that they want to build applications for internal consumption for other workers to be able to better visualize what's going on in the factory.

However, their main focus was some other thing, they wanted to do was to build from zero a computer vision app in which you could take a picture of raw material and it wpuld predict its prone to failure or not.

The problem is that they don't hsve any data for that and I dont think they are well aware of the effort it would take, not to make the app itself (if possible) but to build a dataset large enough to be able to train the model.

The contract they are offering is just 6 months so I assume they need to see results within that time-frame.

I don't their degree of expertise in the topic, but to mind it seemed that they are not being realistic about this and the contractual relation wpuld end up rather bad.

What do you guys think?",t2_1fmeqg5h,False,,0,False,"Made an interview for a company, their project does not seem doable",[],r/datascience,False,6,career,0,,,False,t3_nrwt90,False,dark,0.95,,public,19,0,{},,,False,[],,False,False,,{},Career,False,19,,False,False,self,False,,[],{},,True,,1622810806.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This week I&amp;#39;ve had an interview with a company that manufactures wooden floors. They want to hire a data analyst / software engineer to digitilize  their manufacturing processses within their factory.&lt;/p&gt;

&lt;p&gt;They told md which improvements they had in mine and some of them seemed completely okay, like installing sensors around the factory and gather data from those sensors yo atart building some big data bank.&lt;/p&gt;

&lt;p&gt;The second is that they want to build applications for internal consumption for other workers to be able to better visualize what&amp;#39;s going on in the factory.&lt;/p&gt;

&lt;p&gt;However, their main focus was some other thing, they wanted to do was to build from zero a computer vision app in which you could take a picture of raw material and it wpuld predict its prone to failure or not.&lt;/p&gt;

&lt;p&gt;The problem is that they don&amp;#39;t hsve any data for that and I dont think they are well aware of the effort it would take, not to make the app itself (if possible) but to build a dataset large enough to be able to train the model.&lt;/p&gt;

&lt;p&gt;The contract they are offering is just 6 months so I assume they need to see results within that time-frame.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t their degree of expertise in the topic, but to mind it seemed that they are not being realistic about this and the contractual relation wpuld end up rather bad.&lt;/p&gt;

&lt;p&gt;What do you guys think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrwt90,True,,marmaduque_is_back,,18,True,all_ads,False,[],False,,/r/datascience/comments/nrwt90/made_an_interview_for_a_company_their_project/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrwt90/made_an_interview_for_a_company_their_project/,515405,1622782006.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm currently doing a SQL query (SELECT * FROM ... WHERE ...) using spark.sql() and then writing the results into a table using df.saveAsTable. However, this take upwards of an hour or more to run. The table I am querying from has ~1 billion rows of data and it is in Hive. Is this to be expected? Or is there something inherently wrong with my configurations of Spark that lead to the slow runtime?",t2_ql2r5,False,,0,False,Is upward of 1 hour in runtime a normal occurence when querying data from large tables using PySpark?,[],r/datascience,False,6,tooling,0,,,False,t3_nsd4m7,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1622863273.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently doing a SQL query (SELECT * FROM ... WHERE ...) using spark.sql() and then writing the results into a table using df.saveAsTable. However, this take upwards of an hour or more to run. The table I am querying from has ~1 billion rows of data and it is in Hive. Is this to be expected? Or is there something inherently wrong with my configurations of Spark that lead to the slow runtime?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsd4m7,True,,SaxxyBeast298,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsd4m7/is_upward_of_1_hour_in_runtime_a_normal_occurence/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsd4m7/is_upward_of_1_hour_in_runtime_a_normal_occurence/,515405,1622834473.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"My apologies if this isn't the proper group to ask the question - but any assistance would be greatly appreciated. 

My company does weekly surveys to our membership base - we have approximately 4 years worth of data/results that we were looking for someone to assist in interpreting the data and possibly tying it all together for a comprehensive report sort of speak. I am struggling to find a company that offers this service or even what this service would be called. 

Anyone here know of a company or consultant that could help?",t2_5d1vuffx,False,,0,False,Question on Survey Data Results and writing,[],r/datascience,False,6,network,0,,,False,t3_nsc65l,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Networking,False,0,,False,False,self,False,,[],{},,True,,1622860776.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My apologies if this isn&amp;#39;t the proper group to ask the question - but any assistance would be greatly appreciated. &lt;/p&gt;

&lt;p&gt;My company does weekly surveys to our membership base - we have approximately 4 years worth of data/results that we were looking for someone to assist in interpreting the data and possibly tying it all together for a comprehensive report sort of speak. I am struggling to find a company that offers this service or even what this service would be called. &lt;/p&gt;

&lt;p&gt;Anyone here know of a company or consultant that could help?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nsc65l,True,,Briand9595,,2,True,all_ads,False,[],False,,/r/datascience/comments/nsc65l/question_on_survey_data_results_and_writing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nsc65l/question_on_survey_data_results_and_writing/,515405,1622831976.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I’m currently researching different prosthetic heart valve replacements and how they affect patient outcomes in a certain region of California. I want to get my hands on county specific data that shows which type of valve was used and then follow up on the patients outcome. Is this possible? I have no idea where to acquire these data. 

Thanks for your help!",t2_o00jp,False,,0,False,What’s the best source for raw medical data?,[],r/datascience,False,6,education,0,,,False,t3_nryo30,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1622818137.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m currently researching different prosthetic heart valve replacements and how they affect patient outcomes in a certain region of California. I want to get my hands on county specific data that shows which type of valve was used and then follow up on the patients outcome. Is this possible? I have no idea where to acquire these data. &lt;/p&gt;

&lt;p&gt;Thanks for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nryo30,True,,BushDid9Eleven,,11,True,all_ads,False,[],False,,/r/datascience/comments/nryo30/whats_the_best_source_for_raw_medical_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nryo30/whats_the_best_source_for_raw_medical_data/,515405,1622789337.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"NB: this is NOT a rant post, I swear. I want to be proactive.

I'm writing here to ask some advice on how to tackle my next interview processes, I have a problem about this.

&amp;#x200B;

SOME CONTEXT, QUICKLY:

I am already a professional Data Scientist with almost 3 years of experience in a large company.

I have a PhD from a social science department. My main field of study has been application of statistical models. I spent four years studying (mostly) statistics and econometrics, and doing estimations. My final thesis was completely statistical in nature. Before that, I received good basics in CS.

I don't want to sound arrogant, but I think I'm good at my job. I have a good understanding of math, calculus, statistics, and algorithms. My colleagues with a background in STEM told me I'm good at Deep Learning. I am the reference guy in my company for the use of TensorFlow.

&amp;#x200B;

HERE'S THE PROBLEM:

I like my current job but I don't have faith in the future of my company. I have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon, that know nothing outside marketing. I'm sick of this and I want to leave.

However, every time I apply for a new job I feel that I'm not taken seriously because of my social science academic background. I can see how recruiters changed attitude when they found I come from a social science department. They believe I got there by mistake.

This is so frustrating. What can I do about this? How should I approach recruiters and companies when I apply for a new job?

&amp;#x200B;

Thank you people, love this sub.  


\-------  
EDIT:  
To make myself more clear, and give you an idea of why I wrote this post: I have JUST received an email (literally 1 minute ago!) by a company I applied for. They had cool DL projects, young data-savvy team, both interviews went great, we all liked each other. Now they just told me: listen, we liked you very much, but our company's policy is that no people with a social science background can be hired for this role. They literally told me that.

I hope you will now better understand the reason for this post, instead of calling my ""lack of humility"".

Again it's not a rant (partially now), but rather: tell me what to do to attenuate/bypass this problem.  
",t2_3fcw5pgq,False,,0,False,How to be taken seriously during a job interview when you don't have a STEM degree?,[],r/datascience,False,6,,0,,,False,t3_nr9b9x,False,dark,0.92,,public,252,0,{},,,False,[],,False,False,,{},Job Search,False,252,,False,False,self,1622822079.0,,[],{},,True,,1622741342.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;NB: this is NOT a rant post, I swear. I want to be proactive.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m writing here to ask some advice on how to tackle my next interview processes, I have a problem about this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;SOME CONTEXT, QUICKLY:&lt;/p&gt;

&lt;p&gt;I am already a professional Data Scientist with almost 3 years of experience in a large company.&lt;/p&gt;

&lt;p&gt;I have a PhD from a social science department. My main field of study has been application of statistical models. I spent four years studying (mostly) statistics and econometrics, and doing estimations. My final thesis was completely statistical in nature. Before that, I received good basics in CS.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t want to sound arrogant, but I think I&amp;#39;m good at my job. I have a good understanding of math, calculus, statistics, and algorithms. My colleagues with a background in STEM told me I&amp;#39;m good at Deep Learning. I am the reference guy in my company for the use of TensorFlow.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;HERE&amp;#39;S THE PROBLEM:&lt;/p&gt;

&lt;p&gt;I like my current job but I don&amp;#39;t have faith in the future of my company. I have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon, that know nothing outside marketing. I&amp;#39;m sick of this and I want to leave.&lt;/p&gt;

&lt;p&gt;However, every time I apply for a new job I feel that I&amp;#39;m not taken seriously because of my social science academic background. I can see how recruiters changed attitude when they found I come from a social science department. They believe I got there by mistake.&lt;/p&gt;

&lt;p&gt;This is so frustrating. What can I do about this? How should I approach recruiters and companies when I apply for a new job?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you people, love this sub.  &lt;/p&gt;

&lt;p&gt;-------&lt;br/&gt;
EDIT:&lt;br/&gt;
To make myself more clear, and give you an idea of why I wrote this post: I have JUST received an email (literally 1 minute ago!) by a company I applied for. They had cool DL projects, young data-savvy team, both interviews went great, we all liked each other. Now they just told me: listen, we liked you very much, but our company&amp;#39;s policy is that no people with a social science background can be hired for this role. They literally told me that.&lt;/p&gt;

&lt;p&gt;I hope you will now better understand the reason for this post, instead of calling my &amp;quot;lack of humility&amp;quot;.&lt;/p&gt;

&lt;p&gt;Again it&amp;#39;s not a rant (partially now), but rather: tell me what to do to attenuate/bypass this problem.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nr9b9x,True,,Le2vo,,85,True,all_ads,False,[],False,,/r/datascience/comments/nr9b9x/how_to_be_taken_seriously_during_a_job_interview/,all_ads,False,https://www.reddit.com/r/datascience/comments/nr9b9x/how_to_be_taken_seriously_during_a_job_interview/,515405,1622712542.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,Does anyone have experience implementing causal inference in data science? What exactly did you use it for and how effective was it? Did it actually provide some value?,t2_bb2sfju2,False,,0,False,Causal inference in Data Science,[],r/datascience,False,6,discussion,0,,,False,t3_nrzbg6,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622820934.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone have experience implementing causal inference in data science? What exactly did you use it for and how effective was it? Did it actually provide some value?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrzbg6,True,,mimeticaware,,2,True,all_ads,False,[],False,,/r/datascience/comments/nrzbg6/causal_inference_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrzbg6/causal_inference_in_data_science/,515405,1622792134.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone!

Many members of this subreddit want to brush up on data science or keep their skills sharp. Would anyone be interested in starting a community where we write each other challenge problems and get in the habit of solving problems daily? Think probability puzzles, coding problems, and questions about ML techniques. Research shows daily problem-solving can help you learn much quicker, boost recall, and prevent you from forgetting key concepts. Even with a small community of 20 members, writing 1 question means 20 questions to practice with every week.

Feel free to comment or DM me if you're interested!",t2_8epoa,False,,0,False,Interest in a Puzzle-Solving Community?,[],r/datascience,False,6,education,0,,,False,t3_nrgvaq,False,dark,0.93,,public,32,0,{},,,False,[],,False,False,,{},Education,False,32,,False,False,self,False,,[],{},,True,,1622764722.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;Many members of this subreddit want to brush up on data science or keep their skills sharp. Would anyone be interested in starting a community where we write each other challenge problems and get in the habit of solving problems daily? Think probability puzzles, coding problems, and questions about ML techniques. Research shows daily problem-solving can help you learn much quicker, boost recall, and prevent you from forgetting key concepts. Even with a small community of 20 members, writing 1 question means 20 questions to practice with every week.&lt;/p&gt;

&lt;p&gt;Feel free to comment or DM me if you&amp;#39;re interested!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrgvaq,True,,solanumtuberosum,,31,True,all_ads,False,[],False,,/r/datascience/comments/nrgvaq/interest_in_a_puzzlesolving_community/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrgvaq/interest_in_a_puzzlesolving_community/,515405,1622735922.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi  there, I am building a text classification model to match the name and  description of a customer's item (e.g. name: ""suction press nip"",  category: ""paper machine parts"") to a list of 10k basic items (name:  ""steel, unalloyed"", category: ""metals""). I have some initial matched  data to test and I will get more and more, hopefully.

I've build a sentiment analysis program in the past, this is a good example of what I used: [https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) (Spacy, Scikitlearn).

This  current problem is more complex though, it's 1 to 10k+ match and not  binary (or max 5, 6 values), the string for the item is short and  absolutely at the discretion of the source (client item log).

Which reads/tutorials/examples would you suggest to take a look at? (in Python please)",t2_10p8w7,False,,0,False,"Text classification for item matching, best setup?",[],r/datascience,False,6,projects,0,,,False,t3_ns1l29,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1622830110.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi  there, I am building a text classification model to match the name and  description of a customer&amp;#39;s item (e.g. name: &amp;quot;suction press nip&amp;quot;,  category: &amp;quot;paper machine parts&amp;quot;) to a list of 10k basic items (name:  &amp;quot;steel, unalloyed&amp;quot;, category: &amp;quot;metals&amp;quot;). I have some initial matched  data to test and I will get more and more, hopefully.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve build a sentiment analysis program in the past, this is a good example of what I used: &lt;a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/""&gt;https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/&lt;/a&gt; (Spacy, Scikitlearn).&lt;/p&gt;

&lt;p&gt;This  current problem is more complex though, it&amp;#39;s 1 to 10k+ match and not  binary (or max 5, 6 values), the string for the item is short and  absolutely at the discretion of the source (client item log).&lt;/p&gt;

&lt;p&gt;Which reads/tutorials/examples would you suggest to take a look at? (in Python please)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns1l29,True,,lele-canfora,,1,True,all_ads,False,[],False,,/r/datascience/comments/ns1l29/text_classification_for_item_matching_best_setup/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns1l29/text_classification_for_item_matching_best_setup/,515405,1622801310.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi All,

I am working on a problem for a client wherein they want to streamline sourcing of their products based on 3 factors, Cost of goods, Lead Time &amp; Risk Factor to fulfill the demand.

&amp;#x200B;

I tried approaching it in the conventional Linear Programming model specifying constraints and minimizing Cost , but this method has a drawback that it is optimizing on 1 function i.e Cost and doesn't take it into account Risk/Lead , so I will get lowest value but my risk/lead time  is high I am looking for an approach which gives  balanced answer .

&amp;#x200B;

I am thinking I should create a new objective function which is a mix of all the factors like 

Objective = Qntity \* Country1 + Qntity \* Country2 + Risk\*Country1 + Risk\* Country2 

But I cant understand if I should maximize or minimize this.

&amp;#x200B;

Any thoughts on how can I proceed ?",t2_6ys5mu5,False,,0,False,Optimizing Based on all multiple counteracting factors,[],r/datascience,False,6,discussion,0,,,False,t3_ns15kc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622828412.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;I am working on a problem for a client wherein they want to streamline sourcing of their products based on 3 factors, Cost of goods, Lead Time &amp;amp; Risk Factor to fulfill the demand.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I tried approaching it in the conventional Linear Programming model specifying constraints and minimizing Cost , but this method has a drawback that it is optimizing on 1 function i.e Cost and doesn&amp;#39;t take it into account Risk/Lead , so I will get lowest value but my risk/lead time  is high I am looking for an approach which gives  balanced answer .&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am thinking I should create a new objective function which is a mix of all the factors like &lt;/p&gt;

&lt;p&gt;Objective = Qntity * Country1 + Qntity * Country2 + Risk*Country1 + Risk* Country2 &lt;/p&gt;

&lt;p&gt;But I cant understand if I should maximize or minimize this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any thoughts on how can I proceed ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ns15kc,True,,user19911506,,4,True,all_ads,False,[],False,,/r/datascience/comments/ns15kc/optimizing_based_on_all_multiple_counteracting/,all_ads,False,https://www.reddit.com/r/datascience/comments/ns15kc/optimizing_based_on_all_multiple_counteracting/,515405,1622799612.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in my first real data science job at a F500 med device company. The team I am supporting is looking to implement smart features for a web application. The team is all software developers with zero experience/understanding of data science. The previous work/proof of concept for the work was a bunch of Juptyer notebooks using static log data as inputs, and we are working through which features to implement.

I'm working to frame the steps of using data science/ML in production to crawl/walk/run (i.e. start small and work up from there, considering there is currently zero infrastructure). Anyone been in a similar situation and have advice on how to frame the crawl/walk/run steps for a team with zero experience?",t2_bkhm1,False,,0,False,Team with no data science infrastructure/knowledge (crawl/walk/run),[],r/datascience,False,6,projects,0,,,False,t3_nrj0i1,False,dark,0.93,,public,12,0,{},,,False,[],,False,False,,{},Projects,False,12,,False,False,self,1622742336.0,,[],{},,True,,1622770287.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in my first real data science job at a F500 med device company. The team I am supporting is looking to implement smart features for a web application. The team is all software developers with zero experience/understanding of data science. The previous work/proof of concept for the work was a bunch of Juptyer notebooks using static log data as inputs, and we are working through which features to implement.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working to frame the steps of using data science/ML in production to crawl/walk/run (i.e. start small and work up from there, considering there is currently zero infrastructure). Anyone been in a similar situation and have advice on how to frame the crawl/walk/run steps for a team with zero experience?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrj0i1,True,,getbuckets41,,19,True,all_ads,False,[],False,,/r/datascience/comments/nrj0i1/team_with_no_data_science_infrastructureknowledge/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrj0i1/team_with_no_data_science_infrastructureknowledge/,515405,1622741487.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"https://imgur.com/a/wz1Zwv3

I drew this picture - a regression model is being used to estimate the y-value for some points, and error bars that look like mini normal distributions are shown for each point.

Can anyone please recommend a higher quality version of this image? Something from google images, etc? I spent some time looking, but I couldn't find anything that exactly matches what I am looking for.

Thanks",t2_xtuyc,False,,0,False,Can anyone recommend a higher quality version of this picture?,[],r/datascience,False,6,discussion,0,,,False,t3_nrxu53,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622814662.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://imgur.com/a/wz1Zwv3""&gt;https://imgur.com/a/wz1Zwv3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I drew this picture - a regression model is being used to estimate the y-value for some points, and error bars that look like mini normal distributions are shown for each point.&lt;/p&gt;

&lt;p&gt;Can anyone please recommend a higher quality version of this image? Something from google images, etc? I spent some time looking, but I couldn&amp;#39;t find anything that exactly matches what I am looking for.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrxu53,True,,ottawalanguages,,1,True,all_ads,False,[],False,,/r/datascience/comments/nrxu53/can_anyone_recommend_a_higher_quality_version_of/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrxu53/can_anyone_recommend_a_higher_quality_version_of/,515405,1622785862.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?auto=webp&amp;s=6e979e1e776c978d6ca5092399e1ee7404b7712a', 'width': 773, 'height': 592}, 'resolutions': [{'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dcd1a65491af55c10c641b0e324c0bab026a03', 'width': 108, 'height': 82}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5bac6171b8774da9d7075bc0892f2cba02aef261', 'width': 216, 'height': 165}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd18ba2690980e382713ad18b71fc58f8b67ab08', 'width': 320, 'height': 245}, {'url': 'https://external-preview.redd.it/bXPGWWZiKGnqFt92IRUQphgFkAIUO6JSXq6BdY--Bhk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8fab84b4c851411dc3b06825e5a1bcb7d672aa2', 'width': 640, 'height': 490}], 'variants': {}, 'id': 'QFKxKl03GqHcaGfbTpFNPTy1WANFEYTH3LBnHoSPBhI'}], 'enabled': False}",,,,,
,datascience,"Hello, this just merely an interesting thought I’ve had. I’ve noticed there’s an interesting niche within the field of Bayesian statistics that goes into probabilistic programming, building Bayesian models, Bayesian deep learning etc. This area seems like a big topic in research as well. My question is more so geared towards industry, but what is the trend recently when it comes to using Bayesian statistics and probabilistic programming in a company? My intuition tells me that  Bayesian methods are really interpretable to stats/DS/math folks, but to those outside of that in industry, say stakeholders or upper level management it may not be as interpretable. With most baseline statistics classes starting off at the frequentist perspective, it seems that these are the methods which are really interpretable to management in industry, and thus there is not much of a use case for probabilistic programming and Bayesian methods other than research.

Can anyone speak to this? I’m curious to see how much of an acceptance there is to probabilistic programming  in the industry and if it is really only limited to research?",t2_5w4i5kd1,False,,0,False,How much has probabilistic programming been adopted in industry?,[],r/datascience,False,6,discussion,0,,,False,t3_nridri,False,dark,0.83,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1622768658.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, this just merely an interesting thought I’ve had. I’ve noticed there’s an interesting niche within the field of Bayesian statistics that goes into probabilistic programming, building Bayesian models, Bayesian deep learning etc. This area seems like a big topic in research as well. My question is more so geared towards industry, but what is the trend recently when it comes to using Bayesian statistics and probabilistic programming in a company? My intuition tells me that  Bayesian methods are really interpretable to stats/DS/math folks, but to those outside of that in industry, say stakeholders or upper level management it may not be as interpretable. With most baseline statistics classes starting off at the frequentist perspective, it seems that these are the methods which are really interpretable to management in industry, and thus there is not much of a use case for probabilistic programming and Bayesian methods other than research.&lt;/p&gt;

&lt;p&gt;Can anyone speak to this? I’m curious to see how much of an acceptance there is to probabilistic programming  in the industry and if it is really only limited to research?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nridri,True,,veeeerain,,19,True,all_ads,False,[],False,,/r/datascience/comments/nridri/how_much_has_probabilistic_programming_been/,all_ads,False,https://www.reddit.com/r/datascience/comments/nridri/how_much_has_probabilistic_programming_been/,515405,1622739858.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello data scientists,

I am but a mere engineer, trying to pretend that I know what I’m doing. I have a small set of labelled greyscale images (2 classes), and a lot of unlabelled images. I want to understand what *qualitatively* differentiates the sets of labelled images.

My engineer brain understands how to train something to do a prediction, but absolutely fails to understand how to *analyze* the image data with the tools at my disposal. I would love to be able to tell someone, “*this* is the difference between these images”.

What’s the data science-y way to look at images?",t2_rsqm8,False,,0,False,Tools for analyzing images,[],r/datascience,False,6,tooling,0,,,False,t3_nrhzbw,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},Tooling,False,6,,False,False,self,False,,[],{},,True,,1622767619.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello data scientists,&lt;/p&gt;

&lt;p&gt;I am but a mere engineer, trying to pretend that I know what I’m doing. I have a small set of labelled greyscale images (2 classes), and a lot of unlabelled images. I want to understand what &lt;em&gt;qualitatively&lt;/em&gt; differentiates the sets of labelled images.&lt;/p&gt;

&lt;p&gt;My engineer brain understands how to train something to do a prediction, but absolutely fails to understand how to &lt;em&gt;analyze&lt;/em&gt; the image data with the tools at my disposal. I would love to be able to tell someone, “&lt;em&gt;this&lt;/em&gt; is the difference between these images”.&lt;/p&gt;

&lt;p&gt;What’s the data science-y way to look at images?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrhzbw,True,,Num1DeathEater,,3,True,all_ads,False,[],False,,/r/datascience/comments/nrhzbw/tools_for_analyzing_images/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrhzbw/tools_for_analyzing_images/,515405,1622738819.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,I have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool. Should I add web development to my data science toolbox?,t2_bnc9e5y2,False,,0,False,Is learning web development worth it?,[],r/datascience,False,6,discussion,0,,,False,t3_nrs4r6,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622794914.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool. Should I add web development to my data science toolbox?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrs4r6,True,,daytoniano,,5,True,all_ads,False,[],False,,/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/,515405,1622766114.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Unlimited PTO (paid-time-off). Some love it, others think it’s a scam.

But it’s worth exploring why this policy was implemented in the first place. And for that, we go back to the early days at Netflix.

It’s 2003. Netflix is galloping along in pursuit of Blockbuster. There’s a buzz around the office. The chase is on and an employee asks:

*""'We are all working online some weekends, responding to emails at odd hours, taking off an afternoon for personal time. We don't track hours worked per day or week. Why are we tracking days of vacation per year?""*

Reed Hastings, CEO of Netflix, doesn’t really have a great answer. After all, he’s always judged performance without looking at hours. Get the job done in 1 hour or 10 hours? Doesn’t matter as long as you're doing good work.

Hastings also realizes that some of the best ideas at work come after someone’s just taken vacation. They’ve got the mental bandwidth to think about their work in a fresh, creative manner. Something that’s not possible if you’re clocking in and out without any rest.

So Hastings decides to pull the trigger. He introduces Netflix’s *No Vacation Policy* which puts the onus on their employees to decide when and how much vacation they need to take.

In his book, *No Rules Rules*, Hastings describes getting nightmares when he first introduced this policy. In one of these nightmares, he’d drive to the office, park his car, and walk into a completely empty building.

Those nightmares, minus a few blips which we’ll get to in a bit, never really materialized. The policy was a success and soon other companies in the Valley started copying Netflix. Everybody wanted the best talent and implementing a no rules vacation policy seemed like a great differentiator.

Except that the same policy which worked so well for Netflix...wasn’t working for anyone else.

Other companies found that after implementing an unlimited PTO type policy, employees paradoxically started to take *less* vacation. They would worry that their co-workers would think they were slacking off or that they would get left behind come promotion time.

Hastings was surprised. After a bit of digging, he realized the reason behind why these policies had failed.

The leaders at these companies were not modelling big vacation taking.

Indeed, if the execs were only taking 10 days off, then the unlimited plan would deter other employees from taking anywhere near that amount or more than that.

As Hastings put it:

*“In the absence of a policy, the amount of vacation people take largely reflects what they see their boss and colleagues taking.”*

**Modelling others around you**

This concept of modelling others around us applies not only to vacation taking, but to all sorts of behaviors. As we continue to move towards a new distributed, remote-first workforce, there’s going to be a lot of ambiguity in the decisions that we need to make.

The companies that are able to best adapt to this changing environment will be the ones in which leaders model the right set of behaviors.

A big one will be written communication. As the ability to just randomly walk up to someone at the office and ask them a question subsides, we’ll need to document our practices much better and be able to communicate much more efficiently.

The more we see others, especially our leaders, invest in written communication and take the time to get better at it, the more we will do it.

And never mind us seeing them do this. Reed Hastings wants them to shout loud and clear just how much vacation they’re taking or just how much they’re investing in themselves, so as to encourage everyone else to do it.

An example of good modelling in practice is Evernote. The company, which also doesn’t limit employee vacation days, actually gives a $1,000 stipend to anyone who takes an entire week off in order to encourage vacation taking ([source](https://www.washingtonpost.com/news/on-leadership/wp/2013/08/13/the-catch-of-having-an-unlimited-vacation-policy/)).

**Other Things**

Okay, so there was one more thing that Reed Hastings found out. It wasn’t enough for leaders to just model the right behavior. They also had to set context and guidelines.

Reed realized this when it was the end of quarter and his accounting team was supposed to be closing up their financial books. But a member of the team, in an attempt to avoid the annual crunch period, took off the first two weeks of January. No bueno.

So Reed decided to put in place clear parameters and guidelines on what was acceptable within the context of taking time off. For example, it was imperative to mention things like how many people taking time off at the same time is acceptable and how managers must be notified well in advance of any such long vacations.

This would help prevent blows like the one above in the accounting department.

**Conclusion**

In the end, it seems like Unlimited PTO can work, but it also needs to be supported with strong management. Individuals need to model big vacation taking and put into place the right guidelines.

But I think the lessons here go beyond just vacation.

The behaviors we see and notice from those around us eventually have a strong impact on the type of people that we become. This is especially true at the managerial level, where the impact is 1 to N and can result in considerable [cultural debt](https://www.careerfair.io/reviews/cultural-debt).

So just like this question of unlimited vacation, the answer usually lies in its implementation. Context is king. But that does't always make for good headlines, now, does it. 

\--------

Hope that was useful.

*If you liked this post, you might like* [*my newsletter*](https://www.careerfair.io/subscribe)*. It's my best content delivered to your inbox once every two weeks. And if Twitter is more your thing, I would love it if you* [retweeted the thread](https://twitter.com/OGCareerFair/status/1400161823299604481)*!!*",t2_qr5uf,False,,0,False,I researched the origin of Unlimited PTO (at Netflix) and wrote up a case study :),[],r/datascience,False,6,career,0,,,False,t3_nqnrs6,False,dark,0.93,,public,377,2,{},,,False,[],,False,False,,{},Career,False,377,,False,True,self,1622659648.0,,[],{},,True,,1622674872.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Unlimited PTO (paid-time-off). Some love it, others think it’s a scam.&lt;/p&gt;

&lt;p&gt;But it’s worth exploring why this policy was implemented in the first place. And for that, we go back to the early days at Netflix.&lt;/p&gt;

&lt;p&gt;It’s 2003. Netflix is galloping along in pursuit of Blockbuster. There’s a buzz around the office. The chase is on and an employee asks:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;quot;&amp;#39;We are all working online some weekends, responding to emails at odd hours, taking off an afternoon for personal time. We don&amp;#39;t track hours worked per day or week. Why are we tracking days of vacation per year?&amp;quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Reed Hastings, CEO of Netflix, doesn’t really have a great answer. After all, he’s always judged performance without looking at hours. Get the job done in 1 hour or 10 hours? Doesn’t matter as long as you&amp;#39;re doing good work.&lt;/p&gt;

&lt;p&gt;Hastings also realizes that some of the best ideas at work come after someone’s just taken vacation. They’ve got the mental bandwidth to think about their work in a fresh, creative manner. Something that’s not possible if you’re clocking in and out without any rest.&lt;/p&gt;

&lt;p&gt;So Hastings decides to pull the trigger. He introduces Netflix’s &lt;em&gt;No Vacation Policy&lt;/em&gt; which puts the onus on their employees to decide when and how much vacation they need to take.&lt;/p&gt;

&lt;p&gt;In his book, &lt;em&gt;No Rules Rules&lt;/em&gt;, Hastings describes getting nightmares when he first introduced this policy. In one of these nightmares, he’d drive to the office, park his car, and walk into a completely empty building.&lt;/p&gt;

&lt;p&gt;Those nightmares, minus a few blips which we’ll get to in a bit, never really materialized. The policy was a success and soon other companies in the Valley started copying Netflix. Everybody wanted the best talent and implementing a no rules vacation policy seemed like a great differentiator.&lt;/p&gt;

&lt;p&gt;Except that the same policy which worked so well for Netflix...wasn’t working for anyone else.&lt;/p&gt;

&lt;p&gt;Other companies found that after implementing an unlimited PTO type policy, employees paradoxically started to take &lt;em&gt;less&lt;/em&gt; vacation. They would worry that their co-workers would think they were slacking off or that they would get left behind come promotion time.&lt;/p&gt;

&lt;p&gt;Hastings was surprised. After a bit of digging, he realized the reason behind why these policies had failed.&lt;/p&gt;

&lt;p&gt;The leaders at these companies were not modelling big vacation taking.&lt;/p&gt;

&lt;p&gt;Indeed, if the execs were only taking 10 days off, then the unlimited plan would deter other employees from taking anywhere near that amount or more than that.&lt;/p&gt;

&lt;p&gt;As Hastings put it:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In the absence of a policy, the amount of vacation people take largely reflects what they see their boss and colleagues taking.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modelling others around you&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This concept of modelling others around us applies not only to vacation taking, but to all sorts of behaviors. As we continue to move towards a new distributed, remote-first workforce, there’s going to be a lot of ambiguity in the decisions that we need to make.&lt;/p&gt;

&lt;p&gt;The companies that are able to best adapt to this changing environment will be the ones in which leaders model the right set of behaviors.&lt;/p&gt;

&lt;p&gt;A big one will be written communication. As the ability to just randomly walk up to someone at the office and ask them a question subsides, we’ll need to document our practices much better and be able to communicate much more efficiently.&lt;/p&gt;

&lt;p&gt;The more we see others, especially our leaders, invest in written communication and take the time to get better at it, the more we will do it.&lt;/p&gt;

&lt;p&gt;And never mind us seeing them do this. Reed Hastings wants them to shout loud and clear just how much vacation they’re taking or just how much they’re investing in themselves, so as to encourage everyone else to do it.&lt;/p&gt;

&lt;p&gt;An example of good modelling in practice is Evernote. The company, which also doesn’t limit employee vacation days, actually gives a $1,000 stipend to anyone who takes an entire week off in order to encourage vacation taking (&lt;a href=""https://www.washingtonpost.com/news/on-leadership/wp/2013/08/13/the-catch-of-having-an-unlimited-vacation-policy/""&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Things&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Okay, so there was one more thing that Reed Hastings found out. It wasn’t enough for leaders to just model the right behavior. They also had to set context and guidelines.&lt;/p&gt;

&lt;p&gt;Reed realized this when it was the end of quarter and his accounting team was supposed to be closing up their financial books. But a member of the team, in an attempt to avoid the annual crunch period, took off the first two weeks of January. No bueno.&lt;/p&gt;

&lt;p&gt;So Reed decided to put in place clear parameters and guidelines on what was acceptable within the context of taking time off. For example, it was imperative to mention things like how many people taking time off at the same time is acceptable and how managers must be notified well in advance of any such long vacations.&lt;/p&gt;

&lt;p&gt;This would help prevent blows like the one above in the accounting department.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the end, it seems like Unlimited PTO can work, but it also needs to be supported with strong management. Individuals need to model big vacation taking and put into place the right guidelines.&lt;/p&gt;

&lt;p&gt;But I think the lessons here go beyond just vacation.&lt;/p&gt;

&lt;p&gt;The behaviors we see and notice from those around us eventually have a strong impact on the type of people that we become. This is especially true at the managerial level, where the impact is 1 to N and can result in considerable &lt;a href=""https://www.careerfair.io/reviews/cultural-debt""&gt;cultural debt&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So just like this question of unlimited vacation, the answer usually lies in its implementation. Context is king. But that does&amp;#39;t always make for good headlines, now, does it. &lt;/p&gt;

&lt;p&gt;--------&lt;/p&gt;

&lt;p&gt;Hope that was useful.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, you might like&lt;/em&gt; &lt;a href=""https://www.careerfair.io/subscribe""&gt;&lt;em&gt;my newsletter&lt;/em&gt;&lt;/a&gt;&lt;em&gt;. It&amp;#39;s my best content delivered to your inbox once every two weeks. And if Twitter is more your thing, I would love it if you&lt;/em&gt; &lt;a href=""https://twitter.com/OGCareerFair/status/1400161823299604481""&gt;retweeted the thread&lt;/a&gt;&lt;em&gt;!!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqnrs6,True,,ibsurvivors,,92,True,all_ads,False,[],False,,/r/datascience/comments/nqnrs6/i_researched_the_origin_of_unlimited_pto_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqnrs6/i_researched_the_origin_of_unlimited_pto_at/,515405,1622646072.0,2,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?auto=webp&amp;s=e8ad4fb2df20393ca4d7f1227812e12c5a42670f', 'width': 1484, 'height': 779}, 'resolutions': [{'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1590fc33abbbaa194a00f3db7057fe9f78d642c6', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=899e1e217514a5db15d356a906d0cd5cea3ab8a1', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7034972a438497c9cb80db4b506b80b5b262e117', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc8c4b6068f0861a1df004c98412463f2007b9b6', 'width': 640, 'height': 335}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=82e19e786f176070f692b1ff243b50193bd31887', 'width': 960, 'height': 503}, {'url': 'https://external-preview.redd.it/7sGvvoVcaSKVifx-LUqL8w92jBoyQddFM1hnQME5NH4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4bc22529c62f391f36c2a0fc6f7cbe7c7421f75', 'width': 1080, 'height': 566}], 'variants': {}, 'id': 'LLVQ2Y4uZDDEJ_QONX_Uk5Se66vJWi-oOjaVm579-YU'}], 'enabled': False}",,,,,
,datascience,"I'm moving into a new role at my school next year--Data Strategist (yay!) and I will be responsible for managing the school's data. At the moment, data is not accessible and we have struggled as a result. 

To my question, what do you think the best tool is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. 

My first thought was google data studio since it's pretty straight forward, free, and easily works with sheets. However, I'm also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. 

Further information.. this is for a high school that has about 600 students and I need dashboards for the entire school, each grade level, and each content area. 

Thanks!",t2_969d1sbr,False,,0,False,Best platform for a school data dashboard?,[],r/datascience,False,6,career,0,,,False,t3_nrou0l,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1622785392.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m moving into a new role at my school next year--Data Strategist (yay!) and I will be responsible for managing the school&amp;#39;s data. At the moment, data is not accessible and we have struggled as a result. &lt;/p&gt;

&lt;p&gt;To my question, what do you think the best tool is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. &lt;/p&gt;

&lt;p&gt;My first thought was google data studio since it&amp;#39;s pretty straight forward, free, and easily works with sheets. However, I&amp;#39;m also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. &lt;/p&gt;

&lt;p&gt;Further information.. this is for a high school that has about 600 students and I need dashboards for the entire school, each grade level, and each content area. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrou0l,True,,buttchiquesybobs,,6,True,all_ads,False,[],False,,/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/,515405,1622756592.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I want to use the recently published VQGAN+CLIP implementation of transformers to generate images based on a text description, and then sell those images in any way I can, maybe through a website. I would use very specific seeds and text, so these images would be almost impossible for anybody to replicate with the same network. Is this legal? As I understand, [VQGAN](https://github.com/CompVis/taming-transformers) and [CLIP](https://github.com/openai/CLIP) are both open-source.

Thank you",t2_5d6zezk,False,,0,False,Is it legal to create images with an open-source NN implementation and sell them?,[],r/datascience,False,6,discussion,0,,,False,t3_nrnxka,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622782966.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to use the recently published VQGAN+CLIP implementation of transformers to generate images based on a text description, and then sell those images in any way I can, maybe through a website. I would use very specific seeds and text, so these images would be almost impossible for anybody to replicate with the same network. Is this legal? As I understand, &lt;a href=""https://github.com/CompVis/taming-transformers""&gt;VQGAN&lt;/a&gt; and &lt;a href=""https://github.com/openai/CLIP""&gt;CLIP&lt;/a&gt; are both open-source.&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrnxka,True,,dokasov,,3,True,all_ads,False,[],False,,/r/datascience/comments/nrnxka/is_it_legal_to_create_images_with_an_opensource/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrnxka/is_it_legal_to_create_images_with_an_opensource/,515405,1622754166.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?auto=webp&amp;s=162fa63b063c05585b3367a9deca69265fafef99', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e27f3bc325ee962cbbfed579f5e9a281ae894bd', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3a1784dece0168a30340af97b4e24c15e69d2d', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4be0a7838e5a5faf4215ea4d6f8ac1e7e12cbf5c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f755a6353b6feb1e26777181dd5c78f40d5f80b2', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=145706f614fff027dd1e4761b15d12d281be83c7', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/fYzEVfTz-BZ-FxnTh9HDXLbjkmrxp5wAr45qYUCW_-g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=760caa5c3ced375b8b513fa536257d8b446a25ae', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'SSo8Flx8Vqp1pRHNXHsKjDrKDSsGdGmeQ23z-_8rN1w'}], 'enabled': False}",,,,,
,datascience," Hi all. I was hoping for some insight. I haven't worked with Red Hat or OpenShift before, and so I was wondering if you could give me the Cliffs Notes version of how the platform differs to a traditional enterprise data architecture? Or is it meant to slot in to the gap to provide cloud services for the other tools being used?

Would appreciate a bit of insight there. Hope that isn't too silly a question.

I was also wondering if there is an open source Enterprise Data Architecture system (with all the components) that I could play with as I develop a set of tools for my company, to get a feel for what the various components do and how they fit together.",t2_vv6vm,False,,0,False,Explanation of Enterprise Data Architecture vs tools,[],r/datascience,False,6,discussion,0,,,False,t3_nrmhcn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622779225.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I was hoping for some insight. I haven&amp;#39;t worked with Red Hat or OpenShift before, and so I was wondering if you could give me the Cliffs Notes version of how the platform differs to a traditional enterprise data architecture? Or is it meant to slot in to the gap to provide cloud services for the other tools being used?&lt;/p&gt;

&lt;p&gt;Would appreciate a bit of insight there. Hope that isn&amp;#39;t too silly a question.&lt;/p&gt;

&lt;p&gt;I was also wondering if there is an open source Enterprise Data Architecture system (with all the components) that I could play with as I develop a set of tools for my company, to get a feel for what the various components do and how they fit together.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrmhcn,True,,ryanblumenow,,0,True,all_ads,False,[],False,,/r/datascience/comments/nrmhcn/explanation_of_enterprise_data_architecture_vs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrmhcn/explanation_of_enterprise_data_architecture_vs/,515405,1622750425.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"During this summer season, I hope to conduct a research project with my discord friend HonoredTarget. The goal of this project is to compile a giant database of resumes sent to FANG companies and compare those that were accepted/rejected to try and ""crack"" the screening process. In other words, we are essentially trying to figure out if there are certain keywords, phrases, or wordings that increase your chances of getting an interview. However, in order to do this, we need a large pool of resumes (then from this pool, we focus on the ""skills"" section of each resume).

Our initial plan was to create a form and post it on various subreddits but it seems that is not going well. I have spoken to various moderators of other CS-related reddits about permission to post the survey and they have flat out said no or just not responded, so it appears I am in a bit of a predicament. How do you guys think I should go about collecting a large amount of this sort of data? I have looked online and have been unable to find any public databases of resumes.

Let me know what you guys think I should do!",t2_83iz6ye1,False,,0,False,Advice On Resume Data Collection,[],r/datascience,False,6,projects,0,,,False,t3_nrpd22,False,dark,0.29,,public,0,0,{},,,False,[],,False,False,,{},Projects,False,0,,False,False,self,False,,[],{},,True,,1622786828.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;During this summer season, I hope to conduct a research project with my discord friend HonoredTarget. The goal of this project is to compile a giant database of resumes sent to FANG companies and compare those that were accepted/rejected to try and &amp;quot;crack&amp;quot; the screening process. In other words, we are essentially trying to figure out if there are certain keywords, phrases, or wordings that increase your chances of getting an interview. However, in order to do this, we need a large pool of resumes (then from this pool, we focus on the &amp;quot;skills&amp;quot; section of each resume).&lt;/p&gt;

&lt;p&gt;Our initial plan was to create a form and post it on various subreddits but it seems that is not going well. I have spoken to various moderators of other CS-related reddits about permission to post the survey and they have flat out said no or just not responded, so it appears I am in a bit of a predicament. How do you guys think I should go about collecting a large amount of this sort of data? I have looked online and have been unable to find any public databases of resumes.&lt;/p&gt;

&lt;p&gt;Let me know what you guys think I should do!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrpd22,True,,Nervous_Ad_9620,,4,True,all_ads,False,[],False,,/r/datascience/comments/nrpd22/advice_on_resume_data_collection/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrpd22/advice_on_resume_data_collection/,515405,1622758028.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I tried fitting a GLM style regression model to some data and it resulted in all the regression coefficients being estimated as 0 (i.e. model failed). Yet when I tried a random forest model on the same data, the model worked well and I was even able to get 70% accuracy on the test set.

My dataset has continuous and categorical variables, as well as a lot of ""naturally occurring zeros"".

I was just wondering: is this a common problem? I spent a whole day trying to tweak the regression model to work, but the random forest instantly outperformed it?

Thanks",t2_3f0i9m72,False,,0,False,Is this a common problem?,[],r/datascience,False,6,discussion,0,,,False,t3_nrecu2,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622758043.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I tried fitting a GLM style regression model to some data and it resulted in all the regression coefficients being estimated as 0 (i.e. model failed). Yet when I tried a random forest model on the same data, the model worked well and I was even able to get 70% accuracy on the test set.&lt;/p&gt;

&lt;p&gt;My dataset has continuous and categorical variables, as well as a lot of &amp;quot;naturally occurring zeros&amp;quot;.&lt;/p&gt;

&lt;p&gt;I was just wondering: is this a common problem? I spent a whole day trying to tweak the regression model to work, but the random forest instantly outperformed it?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nrecu2,True,,SQL_beginner,,13,True,all_ads,False,[],False,,/r/datascience/comments/nrecu2/is_this_a_common_problem/,all_ads,False,https://www.reddit.com/r/datascience/comments/nrecu2/is_this_a_common_problem/,515405,1622729243.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I was brought on to pretty large multinational organization as a Data Scientist thinking I would do Data Science work right?

Now let me preface with, this issue might just be because the client really has no idea what they’re doing and has horrible data management and data engineering. And there’s just a lot of Pre-work that needs to be done.

But every time my group encounters a Data Science problem, the first though is “Let’s just throw it in one of the AWS Services (Reckognize, Comprehend…etc) and close out the project”. I actually don’t think any of the other “Data Scientist” on the team are doing any Data Science. More of Data Engineering perhaps.

I’m all for fast and efficient solutions, but I’m not doing any Data Science work. I get it, like why train an NLP model when AWS has already done that for you, and you can make slight tweaks to it. My last job I was able to build models, pass them off to the Engineering team and move on to the next.

Has anyone encountered this? What are your thoughts and how would you respond?",t2_4hpiqt08,False,,0,False,"Hired as a Data Scientist, not doing Data Science work.",[],r/datascience,False,6,discussion,0,,,False,t3_nqlcp6,False,dark,0.93,,public,185,1,{},,,False,[],,False,False,,{},Discussion,False,185,,False,False,self,False,,[],{},,True,,1622667945.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was brought on to pretty large multinational organization as a Data Scientist thinking I would do Data Science work right?&lt;/p&gt;

&lt;p&gt;Now let me preface with, this issue might just be because the client really has no idea what they’re doing and has horrible data management and data engineering. And there’s just a lot of Pre-work that needs to be done.&lt;/p&gt;

&lt;p&gt;But every time my group encounters a Data Science problem, the first though is “Let’s just throw it in one of the AWS Services (Reckognize, Comprehend…etc) and close out the project”. I actually don’t think any of the other “Data Scientist” on the team are doing any Data Science. More of Data Engineering perhaps.&lt;/p&gt;

&lt;p&gt;I’m all for fast and efficient solutions, but I’m not doing any Data Science work. I get it, like why train an NLP model when AWS has already done that for you, and you can make slight tweaks to it. My last job I was able to build models, pass them off to the Engineering team and move on to the next.&lt;/p&gt;

&lt;p&gt;Has anyone encountered this? What are your thoughts and how would you respond?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqlcp6,True,,CmdrAstroNaughty,,79,True,all_ads,False,[],False,,/r/datascience/comments/nqlcp6/hired_as_a_data_scientist_not_doing_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqlcp6/hired_as_a_data_scientist_not_doing_data_science/,515405,1622639145.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[RANT]

Hey gang, stand back, it’s rant time. 

Analytics is a new field at my work, and I’m here to pioneer it. I work In corporate at a large medical devices company. 

I’ve had the luxury of an amazing boss, some amazing colleagues, and decent budget. 

But for the love of fucking god... I am so sick of being thrown responsibility or projects because good ol mary in sales watched a video on “gesture recognition”. The ideas are a great, and I have a framework for filtering them, but the fucking pressure, the initiation of projects with 0 data, no aim at data collection, no quality assurance or risk management and the icing on the cake, “we should roll out an MVP in 2 months”. What in gods name is that shit? 

I’m the asshole. I’m always the asshole. 
“Here are my requirements if we wish to complete this project in the given time frame.” 
“So... why can’t you develop it now?”
Bro... for starters, I’m not a full fledged software engineer / deep learning god. 

I ask for resources or a relaxed time, and I get 0. 


I don’t need advice. I know what I need to do. I just love this community and felt the need to rant.",t2_5e34w9d2,False,,0,False,I’m so sick of corporate morons,[],r/datascience,False,6,discussion,0,,,False,t3_npurud,False,dark,0.97,,public,945,4,{},,,False,[],,False,False,,{},Discussion,False,945,,False,False,self,False,,[],{},,True,,1622585023.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[RANT]&lt;/p&gt;

&lt;p&gt;Hey gang, stand back, it’s rant time. &lt;/p&gt;

&lt;p&gt;Analytics is a new field at my work, and I’m here to pioneer it. I work In corporate at a large medical devices company. &lt;/p&gt;

&lt;p&gt;I’ve had the luxury of an amazing boss, some amazing colleagues, and decent budget. &lt;/p&gt;

&lt;p&gt;But for the love of fucking god... I am so sick of being thrown responsibility or projects because good ol mary in sales watched a video on “gesture recognition”. The ideas are a great, and I have a framework for filtering them, but the fucking pressure, the initiation of projects with 0 data, no aim at data collection, no quality assurance or risk management and the icing on the cake, “we should roll out an MVP in 2 months”. What in gods name is that shit? &lt;/p&gt;

&lt;p&gt;I’m the asshole. I’m always the asshole. 
“Here are my requirements if we wish to complete this project in the given time frame.” 
“So... why can’t you develop it now?”
Bro... for starters, I’m not a full fledged software engineer / deep learning god. &lt;/p&gt;

&lt;p&gt;I ask for resources or a relaxed time, and I get 0. &lt;/p&gt;

&lt;p&gt;I don’t need advice. I know what I need to do. I just love this community and felt the need to rant.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npurud,True,,expatwithajetpack,,252,True,all_ads,False,[],False,,/r/datascience/comments/npurud/im_so_sick_of_corporate_morons/,all_ads,False,https://www.reddit.com/r/datascience/comments/npurud/im_so_sick_of_corporate_morons/,515405,1622556223.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

I'm trying to use a Jupyter Notebook and pandas with a large dataset, but it keeps crashing and freezing my computer. I've also tried Google Colab, and a friend's computer with double the RAM, to no avail.

Any recommendations of what to use when handling really large sets of data?

Thank you!",t2_57lqzcyf,False,,0,False,How do you handle large datasets?,[],r/datascience,False,6,tooling,0,,,False,t3_nqcl3k,False,dark,0.79,,public,13,0,{},,,False,[],,False,False,,{},Tooling,False,13,,False,False,self,False,,[],{},,True,,1622634365.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to use a Jupyter Notebook and pandas with a large dataset, but it keeps crashing and freezing my computer. I&amp;#39;ve also tried Google Colab, and a friend&amp;#39;s computer with double the RAM, to no avail.&lt;/p&gt;

&lt;p&gt;Any recommendations of what to use when handling really large sets of data?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqcl3k,True,,GirlyWorly,,31,True,all_ads,False,[],False,,/r/datascience/comments/nqcl3k/how_do_you_handle_large_datasets/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqcl3k/how_do_you_handle_large_datasets/,515405,1622605565.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,Hey all. I've been tasked with figuring out the best way to implement record linkage between multiple data sources at my job. My suspicion is that supervised approaches will increase accuracy over unsupervised approaches as long as we are willing to do a clerical review to create a training data set that is large enough to be representative. Admittedly this would be very time consuming but I think it might be worthwhile in the long run. After reviewing many papers/blogs etc I can't seem to find many comparisons of current supervised vs unsupervised algorithms. Has anyone seen any work on this? Any links or guidance is appreciated. Or just general record linkage insight is appreciated also. Thanks!,t2_1gii1xta,False,,0,False,Record Linkage - Supervised vs Unsupervised,[],r/datascience,False,6,discussion,0,,,False,t3_nqprvb,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622680017.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all. I&amp;#39;ve been tasked with figuring out the best way to implement record linkage between multiple data sources at my job. My suspicion is that supervised approaches will increase accuracy over unsupervised approaches as long as we are willing to do a clerical review to create a training data set that is large enough to be representative. Admittedly this would be very time consuming but I think it might be worthwhile in the long run. After reviewing many papers/blogs etc I can&amp;#39;t seem to find many comparisons of current supervised vs unsupervised algorithms. Has anyone seen any work on this? Any links or guidance is appreciated. Or just general record linkage insight is appreciated also. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqprvb,True,,drmantist123,,4,True,all_ads,False,[],False,,/r/datascience/comments/nqprvb/record_linkage_supervised_vs_unsupervised/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqprvb/record_linkage_supervised_vs_unsupervised/,515405,1622651217.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have maybe 1000 hours of audio recordings I want to convert to text with timestamps to match diarization timestamps. Or at the minimum, at least convert to text without diarization. The files are a few hours each and add up to maybe 200 5hr sessions. Quality isn't always great but a human can clearly understand what is being said. Approaches I have tried:

Mozilla freespeech: convoluted installed, no diarization 

Kaldi: also somewhat convoluted install, could revisit 

SpeechBrain with Huggingface pretrained: got working, but attention model may need 30 second or less inputs, worried about splitting 6 hour session into 30 seconds and the information loss.",t2_blel3,False,,0,False,What is the best package for combined speech recognition and diarization on long conversation audio files?,[],r/datascience,False,6,discussion,0,,,False,t3_nqenba,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622641781.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have maybe 1000 hours of audio recordings I want to convert to text with timestamps to match diarization timestamps. Or at the minimum, at least convert to text without diarization. The files are a few hours each and add up to maybe 200 5hr sessions. Quality isn&amp;#39;t always great but a human can clearly understand what is being said. Approaches I have tried:&lt;/p&gt;

&lt;p&gt;Mozilla freespeech: convoluted installed, no diarization &lt;/p&gt;

&lt;p&gt;Kaldi: also somewhat convoluted install, could revisit &lt;/p&gt;

&lt;p&gt;SpeechBrain with Huggingface pretrained: got working, but attention model may need 30 second or less inputs, worried about splitting 6 hour session into 30 seconds and the information loss.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqenba,True,,po-handz,,0,True,all_ads,False,[],False,,/r/datascience/comments/nqenba/what_is_the_best_package_for_combined_speech/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqenba/what_is_the_best_package_for_combined_speech/,515405,1622612981.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My company isn't too large but there is starting to be a disconnect between different groups on what data is being used to make decisions, how it is being used, who knows what data exists, etc. As a result I'm working on putting a business case together for getting a centralized data team consisting of data scientists and analyst. I was hoping you all may be able to provide more insight into the benefits and drawbacks of doing this based on your experience.",t2_69ab049y,False,,0,False,What pros or cons have you all seen by centralizing data science and data analysis operations across your organization?,[],r/datascience,False,6,discussion,0,,,False,t3_nq3kne,False,dark,0.93,,public,23,0,{},,,False,[],,False,False,,{},Discussion,False,23,,False,False,self,False,,[],{},,True,,1622607696.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My company isn&amp;#39;t too large but there is starting to be a disconnect between different groups on what data is being used to make decisions, how it is being used, who knows what data exists, etc. As a result I&amp;#39;m working on putting a business case together for getting a centralized data team consisting of data scientists and analyst. I was hoping you all may be able to provide more insight into the benefits and drawbacks of doing this based on your experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nq3kne,True,,MysteriousArmadillo5,,19,True,all_ads,False,[],False,,/r/datascience/comments/nq3kne/what_pros_or_cons_have_you_all_seen_by/,all_ads,False,https://www.reddit.com/r/datascience/comments/nq3kne/what_pros_or_cons_have_you_all_seen_by/,515405,1622578896.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I assume at least 70% will say Jupyter, probably mostly with Python, maybe a few with Julia. Another 20% will say R with RStudio, etc. A handful might say Spyder or something.

I just want to say that I've found nearly every stack, at least as a beginner looking to 'get started fast', is horribly convoluted. I say that as a software developer who writes JavaScript applications (a well-known convoluted platform).

I just want to find something as simple as basic markdown that also lets me execute code, and it doesn't require a special IDE or a command for the code to run (ie, it continuously runs, or it creates live updates on changes). 

- I really like the R Markdown format, but I don't love R itself, and I hate having to use R Studio. It also doesn't seem trivial to set up live updates while editing.
- I like Python more as a data science language, and it's cleanliness as a language also has a similar philosophy as markdown, but there doesn't seem to be a document format that I like. I don't want to write in cells (a la Jupyter) or use an IDE that controls where my cursor can go. I want to edit raw code.
- I even looked into writing code in JavaScript. Yes, Javascript math is UGLY, but I thought oh well, at least I can easily write documents and upload it to the internet, if that is my goal. I tried MDX engines, and you can't even write JS in MDX. At most, you can write a component in a separate file and import it into the MDX file, which is super bloaty.

So, what are your thoughts on this?

Is there any stack that has:

1. Markdown as a base document (not cells). I could be persuaded to something other than markdown, but it would have to be something that is easy to write books and author blogposts with. Markdown seems like the best game in town for that.
2. uses a decent high level language (I'm pretty flexible) that can be executed within the document and ideally create visuals too
3. live updates to the executed code (not just the markdown code) whenever the file is saved (or by some other similar mechanism)
4. no major requirements on IDE (I am a big time VS Code user, and I don't want to change my IDE just to write a little math)",t2_6pptpbhr,False,,0,False,What is your preferred workflow for working with documents and code at the same time?,[],r/datascience,False,6,tooling,0,,,False,t3_nqhdm6,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,1622624702.0,,[],{},,True,,1622653132.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I assume at least 70% will say Jupyter, probably mostly with Python, maybe a few with Julia. Another 20% will say R with RStudio, etc. A handful might say Spyder or something.&lt;/p&gt;

&lt;p&gt;I just want to say that I&amp;#39;ve found nearly every stack, at least as a beginner looking to &amp;#39;get started fast&amp;#39;, is horribly convoluted. I say that as a software developer who writes JavaScript applications (a well-known convoluted platform).&lt;/p&gt;

&lt;p&gt;I just want to find something as simple as basic markdown that also lets me execute code, and it doesn&amp;#39;t require a special IDE or a command for the code to run (ie, it continuously runs, or it creates live updates on changes). &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I really like the R Markdown format, but I don&amp;#39;t love R itself, and I hate having to use R Studio. It also doesn&amp;#39;t seem trivial to set up live updates while editing.&lt;/li&gt;
&lt;li&gt;I like Python more as a data science language, and it&amp;#39;s cleanliness as a language also has a similar philosophy as markdown, but there doesn&amp;#39;t seem to be a document format that I like. I don&amp;#39;t want to write in cells (a la Jupyter) or use an IDE that controls where my cursor can go. I want to edit raw code.&lt;/li&gt;
&lt;li&gt;I even looked into writing code in JavaScript. Yes, Javascript math is UGLY, but I thought oh well, at least I can easily write documents and upload it to the internet, if that is my goal. I tried MDX engines, and you can&amp;#39;t even write JS in MDX. At most, you can write a component in a separate file and import it into the MDX file, which is super bloaty.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, what are your thoughts on this?&lt;/p&gt;

&lt;p&gt;Is there any stack that has:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Markdown as a base document (not cells). I could be persuaded to something other than markdown, but it would have to be something that is easy to write books and author blogposts with. Markdown seems like the best game in town for that.&lt;/li&gt;
&lt;li&gt;uses a decent high level language (I&amp;#39;m pretty flexible) that can be executed within the document and ideally create visuals too&lt;/li&gt;
&lt;li&gt;live updates to the executed code (not just the markdown code) whenever the file is saved (or by some other similar mechanism)&lt;/li&gt;
&lt;li&gt;no major requirements on IDE (I am a big time VS Code user, and I don&amp;#39;t want to change my IDE just to write a little math)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqhdm6,True,,CallSignNovember,,21,True,all_ads,False,[],False,,/r/datascience/comments/nqhdm6/what_is_your_preferred_workflow_for_working_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqhdm6/what_is_your_preferred_workflow_for_working_with/,515405,1622624332.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys. Outside of one statistics class for stem majors that I took which involves python/jupyter notebooks and the likes, the rest of my statistics courses have been using R as the main programming language for our homework and projects. I'm a senior majoring in applied mathematics. I constantly see here that python is the ""now"" and R is being utilized less. 


Is this true? Should I just derp my way through the rest of these classes without much thought to learn it better so I can focus on getting better at python?",t2_20qhf3tk,False,,0,False,"Am I wasting my time learning ""R""",[],r/datascience,False,6,discussion,0,,,False,t3_npnf3s,False,dark,0.89,,public,133,0,{},,,False,[],,False,False,,{},Discussion,False,133,,False,False,self,False,,[],{},,True,,1622557443.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Outside of one statistics class for stem majors that I took which involves python/jupyter notebooks and the likes, the rest of my statistics courses have been using R as the main programming language for our homework and projects. I&amp;#39;m a senior majoring in applied mathematics. I constantly see here that python is the &amp;quot;now&amp;quot; and R is being utilized less. &lt;/p&gt;

&lt;p&gt;Is this true? Should I just derp my way through the rest of these classes without much thought to learn it better so I can focus on getting better at python?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npnf3s,True,,GeminiDavid,,132,True,all_ads,False,[],False,,/r/datascience/comments/npnf3s/am_i_wasting_my_time_learning_r/,all_ads,False,https://www.reddit.com/r/datascience/comments/npnf3s/am_i_wasting_my_time_learning_r/,515405,1622528643.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"If a choice is given to work as a Data Analyst at a rapidly growing startup which would have responsibilities of an analytics+engineering as well as future progression into becoming second data scientist in the company, or to work as a Senior Data Scientist at an analytics and measurement company, what would you think would be a good choice?

If compensation is a factor in deciding, the data analyst offer could fetch somewhere around 80k+ (no data as the company was recently founded) and the senior data scientist would be around $120k (as per published data)",t2_a4yvnttd,False,,0,False,How to finalize job offer?,[],r/datascience,False,6,career,0,,,False,t3_nqcfya,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Career,False,3,,False,False,self,False,,[],{},,True,,1622633879.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If a choice is given to work as a Data Analyst at a rapidly growing startup which would have responsibilities of an analytics+engineering as well as future progression into becoming second data scientist in the company, or to work as a Senior Data Scientist at an analytics and measurement company, what would you think would be a good choice?&lt;/p&gt;

&lt;p&gt;If compensation is a factor in deciding, the data analyst offer could fetch somewhere around 80k+ (no data as the company was recently founded) and the senior data scientist would be around $120k (as per published data)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqcfya,True,,DietMediocre8993,,15,True,all_ads,False,[],False,,/r/datascience/comments/nqcfya/how_to_finalize_job_offer/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqcfya/how_to_finalize_job_offer/,515405,1622605079.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm looking for a variety of opinions on the following:

&amp;#x200B;

I'm looking to build a website that brings together a variety of data sources and presents them as dashboards. The business model is paid subscription for access to the dashboards.

&amp;#x200B;

For the Dashboards I'm trying to decide whether to use Plotly/Dash or PowerBI.

&amp;#x200B;

I feel that Dash gives me more flexibility but the time taken to build fully interactive dashboards is higher compared with PowerBI. Also the user authorization side is more complex (sticking to the free open source version).

&amp;#x200B;

On the flip side, I don't necessarily want a business that is reliant on a product of another company like Microsoft.

&amp;#x200B;

Anyone have any thoughts on this?",t2_a3797,False,,0,False,Setup for a Dashboard-based business,[],r/datascience,False,6,tooling,0,,,False,t3_nqd3gh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1622636118.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a variety of opinions on the following:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking to build a website that brings together a variety of data sources and presents them as dashboards. The business model is paid subscription for access to the dashboards.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For the Dashboards I&amp;#39;m trying to decide whether to use Plotly/Dash or PowerBI.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I feel that Dash gives me more flexibility but the time taken to build fully interactive dashboards is higher compared with PowerBI. Also the user authorization side is more complex (sticking to the free open source version).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On the flip side, I don&amp;#39;t necessarily want a business that is reliant on a product of another company like Microsoft.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyone have any thoughts on this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nqd3gh,True,,yourfutureyesterday,,11,True,all_ads,False,[],False,,/r/datascience/comments/nqd3gh/setup_for_a_dashboardbased_business/,all_ads,False,https://www.reddit.com/r/datascience/comments/nqd3gh/setup_for_a_dashboardbased_business/,515405,1622607318.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"What do you do when you have finished most (all) your tasks for the day?

I have been wondering how much actual work data analysts do per day on their 9-5 job. I know that some days will be very busy, but do you frequently have calm workdays?

How should/do you feel about it when you're supposedly working but have nothing to do. Is that a bad thing or is it normal to most companies (average company, not a FAANG or something)?",t2_7mkrswyv,False,,0,False,How do you feel when you have nothing to do at work.,[],r/datascience,False,6,discussion,0,,,False,t3_npuger,False,dark,0.9,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1622584089.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you do when you have finished most (all) your tasks for the day?&lt;/p&gt;

&lt;p&gt;I have been wondering how much actual work data analysts do per day on their 9-5 job. I know that some days will be very busy, but do you frequently have calm workdays?&lt;/p&gt;

&lt;p&gt;How should/do you feel about it when you&amp;#39;re supposedly working but have nothing to do. Is that a bad thing or is it normal to most companies (average company, not a FAANG or something)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npuger,True,,Ecstatic_Tooth_1096,,20,True,all_ads,False,[],False,,/r/datascience/comments/npuger/how_do_you_feel_when_you_have_nothing_to_do_at/,all_ads,False,https://www.reddit.com/r/datascience/comments/npuger/how_do_you_feel_when_you_have_nothing_to_do_at/,515405,1622555289.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi, I am going crazy trying to figure out the relationship between these big data technologies. I understand what they do but I cannot find anything that tells me the relevance of a particular one to a field or task.

Are they technologies used in combination? Is one better for a particular type of data? Are there limitations that means the choice is budget dependent? Or is it just matter of preference?

Many thanks to anyone who can point me in the right direction!",t2_6inkacm6,False,,0,False,"Relationship between no NoSQL, Hadoop and Data lakes.",[],r/datascience,False,6,discussion,0,,,False,t3_nq1fj2,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622602220.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am going crazy trying to figure out the relationship between these big data technologies. I understand what they do but I cannot find anything that tells me the relevance of a particular one to a field or task.&lt;/p&gt;

&lt;p&gt;Are they technologies used in combination? Is one better for a particular type of data? Are there limitations that means the choice is budget dependent? Or is it just matter of preference?&lt;/p&gt;

&lt;p&gt;Many thanks to anyone who can point me in the right direction!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nq1fj2,True,,Bumblebee-Impressive,,1,True,all_ads,False,[],False,,/r/datascience/comments/nq1fj2/relationship_between_no_nosql_hadoop_and_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nq1fj2/relationship_between_no_nosql_hadoop_and_data/,515405,1622573420.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hey everyone,

after doing online courses I'm working my first guided project and I hope that I can transfer to unguided ones in Anaconda soon. My current project for practising is guided I find myself not remembering some solutions which I had in the courses eventhough I know I've solved a similar issue before.

I know it's common to have some kind of library for code for different problems. How do you store and manage these? I was thinking to maybe make a OneNote (Windows) while studying but I can imagine there are better ways? Especially when I switch to Linux I will need a new solution.",t2_zu1og,False,,0,False,How do you save and manage code for reuse?,[],r/datascience,False,6,discussion,0,,,False,t3_nptfed,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,1622552593.0,,[],{},,True,,1622581204.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;

&lt;p&gt;after doing online courses I&amp;#39;m working my first guided project and I hope that I can transfer to unguided ones in Anaconda soon. My current project for practising is guided I find myself not remembering some solutions which I had in the courses eventhough I know I&amp;#39;ve solved a similar issue before.&lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s common to have some kind of library for code for different problems. How do you store and manage these? I was thinking to maybe make a OneNote (Windows) while studying but I can imagine there are better ways? Especially when I switch to Linux I will need a new solution.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nptfed,True,,hugg3rs,,17,True,all_ads,False,[],False,,/r/datascience/comments/nptfed/how_do_you_save_and_manage_code_for_reuse/,all_ads,False,https://www.reddit.com/r/datascience/comments/nptfed/how_do_you_save_and_manage_code_for_reuse/,515405,1622552404.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[https://www.crosstab.io/articles/professionalizing-machine-learning](https://www.crosstab.io/articles/professionalizing-machine-learning)

I'm curious to hear what people think, especially about things I missed in my list. I'm sure there are some...",t2_arhctu9v,False,,0,False,A checklist for professionalizing machine learning models,[],r/datascience,False,6,discussion,0,,,False,t3_npyxks,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622595993.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.crosstab.io/articles/professionalizing-machine-learning""&gt;https://www.crosstab.io/articles/professionalizing-machine-learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious to hear what people think, especially about things I missed in my list. I&amp;#39;m sure there are some...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npyxks,True,,ctk_brian,,0,True,all_ads,False,[],False,,/r/datascience/comments/npyxks/a_checklist_for_professionalizing_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/npyxks/a_checklist_for_professionalizing_machine/,515405,1622567193.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?auto=webp&amp;s=49f8e795fd63a3d25b5e33174f01a405c1f73f08', 'width': 1160, 'height': 857}, 'resolutions': [{'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5c1aca4f9788a474b84ade5b600b930b984395f', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e6a9bf5b59cb97091af987cf5f2f821e264f96e', 'width': 216, 'height': 159}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12b3d78e03042038047e724746755fbe14d6d7f4', 'width': 320, 'height': 236}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbad8e01c9d5e74e5f9b7c5b806823da39d29eb2', 'width': 640, 'height': 472}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=81c84814d8f8fb06e841ecf106863c965f764300', 'width': 960, 'height': 709}, {'url': 'https://external-preview.redd.it/2i2JDs0h9S5aHHLFrRcEI_qxV6pnM8NRgKS3dva37rQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcdf632bd9ed3ca381f068b296b1c48c8048648f', 'width': 1080, 'height': 797}], 'variants': {}, 'id': 'DgIqH2aQ26rHZyiVgA8GCE9GS6FiEWAzhckDaNXuMu8'}], 'enabled': False}",,,,,
,datascience,"So I'm a data scientist with 2 years of experience, but I work only with traditional  ML, i.e., multiple regression, logistics regressions, regularized regression and clustering algorithms (kmeans and hierarchical for the most part.) I also don't have the opportunity to work with big data.

Since that is not going to change any time soon, I've decided to try to look for a better, less limited, DS opportunity. However, I'm struggling to find a role that doesn't require 5-10 years of experience using DL/AI and big data spec. I'm just wondering if anyone here has gone through a similar process, and if so, what advice would you give to someone in a similar position?

To add a bit to my background, I am currently completing my masters in computer science with a specialization in machine learning, have a lot of personal experience working with random forest (with boosting), SVMs, and some personal experience in DL. I am also currently completing the MLOps coursera course, which hopefully would give me some more experience in working in deploying ML models.",t2_1eoopt,False,,0,False,Any advice as to how to best transition into a more advanced data science role?,[],r/datascience,False,6,career,0,,,False,t3_npv75z,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},Career,False,4,,False,False,self,False,,[],{},,True,,1622586220.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m a data scientist with 2 years of experience, but I work only with traditional  ML, i.e., multiple regression, logistics regressions, regularized regression and clustering algorithms (kmeans and hierarchical for the most part.) I also don&amp;#39;t have the opportunity to work with big data.&lt;/p&gt;

&lt;p&gt;Since that is not going to change any time soon, I&amp;#39;ve decided to try to look for a better, less limited, DS opportunity. However, I&amp;#39;m struggling to find a role that doesn&amp;#39;t require 5-10 years of experience using DL/AI and big data spec. I&amp;#39;m just wondering if anyone here has gone through a similar process, and if so, what advice would you give to someone in a similar position?&lt;/p&gt;

&lt;p&gt;To add a bit to my background, I am currently completing my masters in computer science with a specialization in machine learning, have a lot of personal experience working with random forest (with boosting), SVMs, and some personal experience in DL. I am also currently completing the MLOps coursera course, which hopefully would give me some more experience in working in deploying ML models.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npv75z,True,,scun1995,,8,True,all_ads,False,[],False,,/r/datascience/comments/npv75z/any_advice_as_to_how_to_best_transition_into_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/npv75z/any_advice_as_to_how_to_best_transition_into_a/,515405,1622557420.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience," Cheer everyone,

I just moved from Python to SAS for 4 months due to new job requirements. I wonder how you think SAS compared with other languages, any future.

Mine:

\- SAS is not so complex. The only problem is we have to memorize weird syntax

\- Lots of problem can be solved with proc sql. Unfortunately, proc sql has some different characteristics compared with standard SQL (e.g. why row\_number is missing in proc sql????). I likely use SAS mainly for practicing SQL.

\- The syntax is unique and not transferrable. If you're in SAS industry for too long, then it's likely hard to move to other jobs with different tool. Unlike if you know MATLAB or Python, you can easily move to R, or even C/C++ (They're interconnected with each other very well, SAS is a standalone hero)

\- Company uses SAS likely for security purposes (need an organization who is responsible for the tool if anything bad happened)

\- Then SAS Visual Analytics is another story and if you program for Advanced Filter in SAS Viya, then again it's more or less different systax compared with SAS Guide.

What's your thought?",t2_5czjyjhi,False,,0,False,What is your thought on SAS as a tool for data science,[],r/datascience,False,6,tooling,0,,,False,t3_np8uqk,False,dark,0.91,,public,135,0,{},,,False,[],,False,False,,{},Tooling,False,135,,False,False,self,False,,[],{},,True,,1622511481.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Cheer everyone,&lt;/p&gt;

&lt;p&gt;I just moved from Python to SAS for 4 months due to new job requirements. I wonder how you think SAS compared with other languages, any future.&lt;/p&gt;

&lt;p&gt;Mine:&lt;/p&gt;

&lt;p&gt;- SAS is not so complex. The only problem is we have to memorize weird syntax&lt;/p&gt;

&lt;p&gt;- Lots of problem can be solved with proc sql. Unfortunately, proc sql has some different characteristics compared with standard SQL (e.g. why row_number is missing in proc sql????). I likely use SAS mainly for practicing SQL.&lt;/p&gt;

&lt;p&gt;- The syntax is unique and not transferrable. If you&amp;#39;re in SAS industry for too long, then it&amp;#39;s likely hard to move to other jobs with different tool. Unlike if you know MATLAB or Python, you can easily move to R, or even C/C++ (They&amp;#39;re interconnected with each other very well, SAS is a standalone hero)&lt;/p&gt;

&lt;p&gt;- Company uses SAS likely for security purposes (need an organization who is responsible for the tool if anything bad happened)&lt;/p&gt;

&lt;p&gt;- Then SAS Visual Analytics is another story and if you program for Advanced Filter in SAS Viya, then again it&amp;#39;s more or less different systax compared with SAS Guide.&lt;/p&gt;

&lt;p&gt;What&amp;#39;s your thought?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,np8uqk,True,,vietlinh12hoa,,141,True,all_ads,False,[],False,,/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/,515405,1622482681.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hi guys, 

I'm curious what websites you guys have used in the past for data science tutoring? What has been your experience with particular websites/services?

-Andrew",t2_1ns77nex,False,,0,False,Tutoring for Data Science,[],r/datascience,False,6,education,0,,,False,t3_npt96d,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},Education,False,3,,False,False,self,False,seniorflair,[],{},,True,,1622580655.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious what websites you guys have used in the past for data science tutoring? What has been your experience with particular websites/services?&lt;/p&gt;

&lt;p&gt;-Andrew&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,MBA/MS (Candidate) | Student,[],False,,,,t5_2sptq,,,,npt96d,True,,DJAlaskaAndrew,,2,True,all_ads,False,[],False,dark,/r/datascience/comments/npt96d/tutoring_for_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/npt96d/tutoring_for_data_science/,515405,1622551855.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hey all! I am trying to draw bounding boxes on arrays I have. For a 2-D set it's pretty simple (I follow a process detailed here: https://stackoverflow.com/a/67784869/9345615). However, my problem is now I may have much higher-dimension data. 

Is there an accepted method of drawing bounding boxes on N-dim data, that scales well to high N?

For example, if I had a 5D array, that was filled with 1's, but with 2 separate areas filled with 100's, how can I find and draw boundary boxes around them?

I'm really open to any idea. So far I'm using image filters for the 2D examples, but for the N-dim case I'm not sure how well this will apply? Or if those filters really make sense to be used on Dim&gt;2. It's hard for me to imagine as well an NN-esque approach (think Yolo), since I will only have a few arrays, so training would be an issue.",t2_dt0g2,False,,0,False,How to find and draw a high dimensional bounding box,[],r/datascience,False,6,discussion,0,,,False,t3_npuamu,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1622583600.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all! I am trying to draw bounding boxes on arrays I have. For a 2-D set it&amp;#39;s pretty simple (I follow a process detailed here: &lt;a href=""https://stackoverflow.com/a/67784869/9345615""&gt;https://stackoverflow.com/a/67784869/9345615&lt;/a&gt;). However, my problem is now I may have much higher-dimension data. &lt;/p&gt;

&lt;p&gt;Is there an accepted method of drawing bounding boxes on N-dim data, that scales well to high N?&lt;/p&gt;

&lt;p&gt;For example, if I had a 5D array, that was filled with 1&amp;#39;s, but with 2 separate areas filled with 100&amp;#39;s, how can I find and draw boundary boxes around them?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m really open to any idea. So far I&amp;#39;m using image filters for the 2D examples, but for the N-dim case I&amp;#39;m not sure how well this will apply? Or if those filters really make sense to be used on Dim&amp;gt;2. It&amp;#39;s hard for me to imagine as well an NN-esque approach (think Yolo), since I will only have a few arrays, so training would be an issue.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npuamu,True,,vaaalbara,,3,True,all_ads,False,[],False,,/r/datascience/comments/npuamu/how_to_find_and_draw_a_high_dimensional_bounding/,all_ads,False,https://www.reddit.com/r/datascience/comments/npuamu/how_to_find_and_draw_a_high_dimensional_bounding/,515405,1622554800.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",,,,,
,datascience,"I got this assessment today and just wanted to get an opinion on the difficulty level. Is it similar to LeetCode SQL Easy/Medium/Hard  or is it a totally different level. Trying to get a feel for what to expect in the 75 minute period.

TIA",t2_118kaw,False,,0,False,Have any of you taked the 'Qualified' SQL challenge as part of interview process for Zoom,[],r/datascience,False,6,,0,,,False,t3_npzaky,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622596904.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got this assessment today and just wanted to get an opinion on the difficulty level. Is it similar to LeetCode SQL Easy/Medium/Hard  or is it a totally different level. Trying to get a feel for what to expect in the 75 minute period.&lt;/p&gt;

&lt;p&gt;TIA&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,npzaky,True,,paglaindian,,0,True,all_ads,False,[],False,,/r/datascience/comments/npzaky/have_any_of_you_taked_the_qualified_sql_challenge/,all_ads,False,https://www.reddit.com/r/datascience/comments/npzaky/have_any_of_you_taked_the_qualified_sql_challenge/,515405,1622568104.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_q7fwt,False,,0,False,[Q] Bayesian statistics: how to introduce posterior-probability interpretation in the body an article?,[],r/datascience,False,6,discussion,0,,,False,t3_npxh8b,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,True,default,False,,[],{},,False,,1622592328.0,text,6,,,text,self.statistics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npxh8b,True,,vvvvalvalval,,0,True,all_ads,False,[],False,,/r/datascience/comments/npxh8b/q_bayesian_statistics_how_to_introduce/,all_ads,False,/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/,515405,1622563528.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'statistics', 'selftext': 'I\'m in the process of writing various Bayesian data analyses, and I feel it is required to **add a quick guide to Bayesian statistics within each article**, to help readers in their interpretation (especially if they have little or no statistical training).\n\nI was wondering if people here know good resources for that. It would be good to have some ready-to-use prose to include in each analysis.\n\n&amp;#x200B;\n\nIdeally, the content I\'d add to a data analysis would be something like this:\n\n**A- Below each major posterior-based result (e.g a 99% credible interval), a few sentences to help interpret the 99%.** For instance:\n\n&gt;***How to interpret:*** according to our model and prior assumptions, we have inferred that **with 99% credibility, the Effect Size lies between 1.8 and 7.8 points.** The Effect Size is the treatment\'s average reduction in perceived pain level compared to a placebo.  \n&gt;  \n&gt;**-** The phrase *\'with 99% credibility\'* may be understood as follows: given the results, a rational person who believes in the model and prior assumptions would be willing to bet 99-to-1 that the Effect Size lies in interval \\[1.8, 7.8\\]\n\n&amp;#x200B;\n\nB- As an appendix, a brief **motivation of the use of Bayesian statistics** rather than classical frequentist statistics, featuring:\n\n1. Evidence and explanation for frequentist results such as p-values and confidence intervals being commonly misinterpreted (in particular, being interpreted as Bayesian posterior probabilities).\n2. Brief explanation / references for grasping the philosophical implications of Bayesian probability (along the lines of: ""you have to be OK with using probability as a measure of subjective uncertainty and beliefs. As we saw above, most people do that intuitively.""), and the related tradeoffs and controversy.\n3. References for learning Bayesian statistics at various technical depths.\n\n&amp;#x200B;\n\nIf I\'m being facetious:\n\n*""Most people should interpret posterior probabilities like they usually interpret p-values. Except that this time they\'ll be correct.""*', 'author_fullname': 't2_q7fwt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Q] Bayesian statistics: how to introduce posterior-probability interpretation in the body an article?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/statistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_npu58g', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622583166.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.statistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of writing various Bayesian data analyses, and I feel it is required to &lt;strong&gt;add a quick guide to Bayesian statistics within each article&lt;/strong&gt;, to help readers in their interpretation (especially if they have little or no statistical training).&lt;/p&gt;\n\n&lt;p&gt;I was wondering if people here know good resources for that. It would be good to have some ready-to-use prose to include in each analysis.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ideally, the content I&amp;#39;d add to a data analysis would be something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A- Below each major posterior-based result (e.g a 99% credible interval), a few sentences to help interpret the 99%.&lt;/strong&gt; For instance:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How to interpret:&lt;/em&gt;&lt;/strong&gt; according to our model and prior assumptions, we have inferred that &lt;strong&gt;with 99% credibility, the Effect Size lies between 1.8 and 7.8 points.&lt;/strong&gt; The Effect Size is the treatment&amp;#39;s average reduction in perceived pain level compared to a placebo.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; The phrase &lt;em&gt;&amp;#39;with 99% credibility&amp;#39;&lt;/em&gt; may be understood as follows: given the results, a rational person who believes in the model and prior assumptions would be willing to bet 99-to-1 that the Effect Size lies in interval [1.8, 7.8]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;B- As an appendix, a brief &lt;strong&gt;motivation of the use of Bayesian statistics&lt;/strong&gt; rather than classical frequentist statistics, featuring:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Evidence and explanation for frequentist results such as p-values and confidence intervals being commonly misinterpreted (in particular, being interpreted as Bayesian posterior probabilities).&lt;/li&gt;\n&lt;li&gt;Brief explanation / references for grasping the philosophical implications of Bayesian probability (along the lines of: &amp;quot;you have to be OK with using probability as a measure of subjective uncertainty and beliefs. As we saw above, most people do that intuitively.&amp;quot;), and the related tradeoffs and controversy.&lt;/li&gt;\n&lt;li&gt;References for learning Bayesian statistics at various technical depths.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m being facetious:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;&amp;quot;Most people should interpret posterior probabilities like they usually interpret p-values. Except that this time they&amp;#39;ll be correct.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'npu58g', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'vvvvalvalval', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/', 'subreddit_subscribers': 310860, 'created_utc': 1622554366.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",/r/statistics/comments/npu58g/q_bayesian_statistics_how_to_introduce/,t3_npu58g,
,datascience,"Hey, I'm looking for a solution, open source docker deployable service.

What I need to do is stream data from several mysql db's to a central postgres db. Polling once every 30-min. So performance and query size should matter. 

Some of the tables in the mysql db do not have a date on them.

Now initially I planned to join the 6 tables in one of the db's to make a view of 130 columns, in order to get the date data in. Then SELECT * FROM table WHERE date=last 1 day. This should give me an option to limit the query by filtering it. 

One can quickly see how the massive view will lag the origin server, especially if I poll it every 30 mins. There are plenty of NULL values in there but that's beside the point. Its the best solution I can think of at the moment. Correct me if I'm wrong about the filtering performance and if its actually sensible. 

Alternatively I can stream individual tables but as mentioned above, there isn't any date column to filter, so it'd cause even more of a performance bottleneck to select all rows for each table at every update. 

I initially used airflow to achieve this, by writing the data to a csv as a staging area for loading on to pg.

I've also looked at airbyte but they seem to be pretty early stage, though correct me if I'm wrong.

Could be difficult to use a mysql slave db as the two db's are different types. I also don't think there's probably sufficient infrastructure currently to handle several additional slave db's. 

What would you do with the scenario above?",t2_xrizd,False,,0,False,Data streaming question,[],r/datascience,False,6,discussion,0,,,False,t3_npwyg0,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622590979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;m looking for a solution, open source docker deployable service.&lt;/p&gt;

&lt;p&gt;What I need to do is stream data from several mysql db&amp;#39;s to a central postgres db. Polling once every 30-min. So performance and query size should matter. &lt;/p&gt;

&lt;p&gt;Some of the tables in the mysql db do not have a date on them.&lt;/p&gt;

&lt;p&gt;Now initially I planned to join the 6 tables in one of the db&amp;#39;s to make a view of 130 columns, in order to get the date data in. Then SELECT * FROM table WHERE date=last 1 day. This should give me an option to limit the query by filtering it. &lt;/p&gt;

&lt;p&gt;One can quickly see how the massive view will lag the origin server, especially if I poll it every 30 mins. There are plenty of NULL values in there but that&amp;#39;s beside the point. Its the best solution I can think of at the moment. Correct me if I&amp;#39;m wrong about the filtering performance and if its actually sensible. &lt;/p&gt;

&lt;p&gt;Alternatively I can stream individual tables but as mentioned above, there isn&amp;#39;t any date column to filter, so it&amp;#39;d cause even more of a performance bottleneck to select all rows for each table at every update. &lt;/p&gt;

&lt;p&gt;I initially used airflow to achieve this, by writing the data to a csv as a staging area for loading on to pg.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also looked at airbyte but they seem to be pretty early stage, though correct me if I&amp;#39;m wrong.&lt;/p&gt;

&lt;p&gt;Could be difficult to use a mysql slave db as the two db&amp;#39;s are different types. I also don&amp;#39;t think there&amp;#39;s probably sufficient infrastructure currently to handle several additional slave db&amp;#39;s. &lt;/p&gt;

&lt;p&gt;What would you do with the scenario above?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npwyg0,True,,ApocalypseAce,,2,True,all_ads,False,[],False,,/r/datascience/comments/npwyg0/data_streaming_question/,all_ads,False,https://www.reddit.com/r/datascience/comments/npwyg0/data_streaming_question/,515405,1622562179.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let me start this off with a disclaimer that I'm still a beginner in data science, and I haven't been exposed to many projects. So far, the real life projects I've worked on concerned time-series data (energy demand and price forcasting + supply optimization, quantitative analysis for algorithmic trading), which I imagined to be heavy on stats before I began.

However, after I've worked on those projects, I feel like knowing or not knowing stats doesn't really affect my ability to complete the projects. I'm still able to analyze the data well, gather actionable insights and build models around those insights to optimize processes.

For example, linear regression is constrained by its assumptions, so what? We want stationary time-series data for ARIMA-based models so sample statistics are not time-dependent, so what? Bayesian methods like MCMC allow us to sample a distribution similar to the actual distribution due to the law of large numbers, so what?

Don't get me wrong, I really like learning about statistics as it fills me with a wondrous sense of appreciation every time I understand the underlying reasons behind why certain models work. But I'm just really curious as to why statistical knowledge is so valued in this field when (based on my experience so far) it doesn't really affect the quality of your work as long as you know WHEN and HOW to use statistical tools/models (even if you don't know WHY).",t2_l7hoj,False,,0,False,"How important is knowledge of statistics, really?",[],r/datascience,False,6,discussion,0,,,False,t3_npupwg,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1622557969.0,,[],{},,True,,1622584876.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let me start this off with a disclaimer that I&amp;#39;m still a beginner in data science, and I haven&amp;#39;t been exposed to many projects. So far, the real life projects I&amp;#39;ve worked on concerned time-series data (energy demand and price forcasting + supply optimization, quantitative analysis for algorithmic trading), which I imagined to be heavy on stats before I began.&lt;/p&gt;

&lt;p&gt;However, after I&amp;#39;ve worked on those projects, I feel like knowing or not knowing stats doesn&amp;#39;t really affect my ability to complete the projects. I&amp;#39;m still able to analyze the data well, gather actionable insights and build models around those insights to optimize processes.&lt;/p&gt;

&lt;p&gt;For example, linear regression is constrained by its assumptions, so what? We want stationary time-series data for ARIMA-based models so sample statistics are not time-dependent, so what? Bayesian methods like MCMC allow us to sample a distribution similar to the actual distribution due to the law of large numbers, so what?&lt;/p&gt;

&lt;p&gt;Don&amp;#39;t get me wrong, I really like learning about statistics as it fills me with a wondrous sense of appreciation every time I understand the underlying reasons behind why certain models work. But I&amp;#39;m just really curious as to why statistical knowledge is so valued in this field when (based on my experience so far) it doesn&amp;#39;t really affect the quality of your work as long as you know WHEN and HOW to use statistical tools/models (even if you don&amp;#39;t know WHY).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npupwg,True,,ReaperJr,,19,True,all_ads,False,[],False,,/r/datascience/comments/npupwg/how_important_is_knowledge_of_statistics_really/,all_ads,False,https://www.reddit.com/r/datascience/comments/npupwg/how_important_is_knowledge_of_statistics_really/,515405,1622556076.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been using R, python for data science for about 10 years, and I have a phd in industrial engineering. 

We all know this is a field requires constant learning. 

For example, I am starting to work on a time-series clusterings project, which is totally new to me. Concept like dynamic time warping is so great, however I found it difficult to dig into the algorithm. I  ended up just calling some package to solve the problem rather than trying to understand it inside out. 

Science is progressing fast and it's very difficult for me to stay on top of everything. What's your experience on keep yourself relevant on the latest and greatest things?",t2_12x43b,False,,0,False,continuous learning advice,[],r/datascience,False,6,discussion,0,,,False,t3_npa881,False,dark,0.57,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622515219.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been using R, python for data science for about 10 years, and I have a phd in industrial engineering. &lt;/p&gt;

&lt;p&gt;We all know this is a field requires constant learning. &lt;/p&gt;

&lt;p&gt;For example, I am starting to work on a time-series clusterings project, which is totally new to me. Concept like dynamic time warping is so great, however I found it difficult to dig into the algorithm. I  ended up just calling some package to solve the problem rather than trying to understand it inside out. &lt;/p&gt;

&lt;p&gt;Science is progressing fast and it&amp;#39;s very difficult for me to stay on top of everything. What&amp;#39;s your experience on keep yourself relevant on the latest and greatest things?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,npa881,True,,janicewa,,7,True,all_ads,False,[],False,,/r/datascience/comments/npa881/continuous_learning_advice/,all_ads,False,https://www.reddit.com/r/datascience/comments/npa881/continuous_learning_advice/,515405,1622486419.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Title basically says it all. I'm wrapping up a PhD in [computational biology field] and starting to think about what's next for me. I don't really want to stay in academia at this point: the odds of getting the fabled tenure track jobs are low and I'm pushing 30 so I haven less interest in bouncing around post-doc to post-doc until getting a TT or burning out. 

A lot of my friends who graduated before me went the Data Science route - they're making good money (much better then we made as graduate students or would make as Tenure Track Profs) but the work just seems so *boring.* Instead of wrangling with interesting data types and trying to solve interesting problems, a lot of it seems to be basically financial or behavioral user data, and the goal is to deliver ""actionable business insights"", which always seems to boil down to optimizing profit-to-cost ratio. Far less of the interesting questions about mathematics and inference that pulled me into computational modeling and a lot more focus on business, learning how to pitch ideas to managers, etc. 

I don't give a d*mn about that, and kind of chafe at the idea of using skills I spent 6 years developing at the cutting edge of scientific research to help make already-wealthy investors in a company richer. For context, my thesis research involves developing a very niche kind of computational model to explore distributed information processing in biological systems that I know has absolutely no relevance to anything in the world of business or finance.",t2_ad5yokml,False,,0,False,Wrapping up a data-intensive PhD but most industry data science seems really boring. Are there interesting jobs?,[],r/datascience,False,6,career,0,,,False,t3_nobqjn,False,dark,0.91,,public,268,1,{},,,False,[],,False,False,,{},Career,False,268,,False,False,self,False,,[],{},,True,,1622411878.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title basically says it all. I&amp;#39;m wrapping up a PhD in [computational biology field] and starting to think about what&amp;#39;s next for me. I don&amp;#39;t really want to stay in academia at this point: the odds of getting the fabled tenure track jobs are low and I&amp;#39;m pushing 30 so I haven less interest in bouncing around post-doc to post-doc until getting a TT or burning out. &lt;/p&gt;

&lt;p&gt;A lot of my friends who graduated before me went the Data Science route - they&amp;#39;re making good money (much better then we made as graduate students or would make as Tenure Track Profs) but the work just seems so &lt;em&gt;boring.&lt;/em&gt; Instead of wrangling with interesting data types and trying to solve interesting problems, a lot of it seems to be basically financial or behavioral user data, and the goal is to deliver &amp;quot;actionable business insights&amp;quot;, which always seems to boil down to optimizing profit-to-cost ratio. Far less of the interesting questions about mathematics and inference that pulled me into computational modeling and a lot more focus on business, learning how to pitch ideas to managers, etc. &lt;/p&gt;

&lt;p&gt;I don&amp;#39;t give a d*mn about that, and kind of chafe at the idea of using skills I spent 6 years developing at the cutting edge of scientific research to help make already-wealthy investors in a company richer. For context, my thesis research involves developing a very niche kind of computational model to explore distributed information processing in biological systems that I know has absolutely no relevance to anything in the world of business or finance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nobqjn,True,,antichain,,144,True,all_ads,False,[],False,,/r/datascience/comments/nobqjn/wrapping_up_a_dataintensive_phd_but_most_industry/,all_ads,False,https://www.reddit.com/r/datascience/comments/nobqjn/wrapping_up_a_dataintensive_phd_but_most_industry/,515405,1622383078.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Currently I'm working in civil engineering, and it has a pretty conservative culture. I'm a young queer lady and I had been closeted at work but I'm about to get married and I can imagine uncomfortable questions. 

I have the opportunity to get a masters in data science largely paid for. I'm wondering if a career pivot would increase the chances of having a queer friendly work place. What's the culture like in a data science career?

And yes I know this should be protected but with at will employment laws its harder to build a case. Also I think I'd enjoy working in an environment without racists, transphobes, climate change deniers and antivaxers.",t2_aycyec16,False,,0,False,What's the culture like in data science? Progressive?,[],r/datascience,False,6,career,0,,,False,t3_nompja,False,dark,0.6,,public,17,0,{},,,False,[],,False,False,,{},Career,False,17,,False,False,self,False,,[],{},,True,,1622443998.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I&amp;#39;m working in civil engineering, and it has a pretty conservative culture. I&amp;#39;m a young queer lady and I had been closeted at work but I&amp;#39;m about to get married and I can imagine uncomfortable questions. &lt;/p&gt;

&lt;p&gt;I have the opportunity to get a masters in data science largely paid for. I&amp;#39;m wondering if a career pivot would increase the chances of having a queer friendly work place. What&amp;#39;s the culture like in a data science career?&lt;/p&gt;

&lt;p&gt;And yes I know this should be protected but with at will employment laws its harder to build a case. Also I think I&amp;#39;d enjoy working in an environment without racists, transphobes, climate change deniers and antivaxers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nompja,True,,happybabylizard,,64,True,all_ads,False,[],False,,/r/datascience/comments/nompja/whats_the_culture_like_in_data_science_progressive/,all_ads,False,https://www.reddit.com/r/datascience/comments/nompja/whats_the_culture_like_in_data_science_progressive/,515405,1622415198.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I am interested to know how often do people participate in Kaggle competitions, and when do they get to work on the competition's code.

I have a full-time data science/research position so I am usually occupied during the weekdays. In the weekends I just like to take sometime off-the-screen and do outdoor activities (hiking, cycling, chilling out). I am keen on getting more engaged in Kaggle competitions so I can have a broader knowledge and more career opportunities in the future. However, I am concerned that I will burn out, having to stay coding behind the computer screen 7 days a week. My position is a priority for sure, and I don't want my work to be affected because I was not properly taking a time off when I am supposed to. How do other people handle such a situation?

Tl;dr: I am interested in becoming more active on Kaggle, but I am concerned that I will burn myself out. What are your tips?",t2_3vsv0v1p,False,,0,False,Kaggle and burnouts,[],r/datascience,False,6,discussion,0,,,False,t3_no7s9w,False,dark,0.96,,public,178,1,{},,,False,[],,False,False,,{},Discussion,False,178,,False,False,self,False,,[],{},,True,,1622396718.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am interested to know how often do people participate in Kaggle competitions, and when do they get to work on the competition&amp;#39;s code.&lt;/p&gt;

&lt;p&gt;I have a full-time data science/research position so I am usually occupied during the weekdays. In the weekends I just like to take sometime off-the-screen and do outdoor activities (hiking, cycling, chilling out). I am keen on getting more engaged in Kaggle competitions so I can have a broader knowledge and more career opportunities in the future. However, I am concerned that I will burn out, having to stay coding behind the computer screen 7 days a week. My position is a priority for sure, and I don&amp;#39;t want my work to be affected because I was not properly taking a time off when I am supposed to. How do other people handle such a situation?&lt;/p&gt;

&lt;p&gt;Tl;dr: I am interested in becoming more active on Kaggle, but I am concerned that I will burn myself out. What are your tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no7s9w,True,,straightbackward,,63,True,all_ads,False,[],False,,/r/datascience/comments/no7s9w/kaggle_and_burnouts/,all_ads,False,https://www.reddit.com/r/datascience/comments/no7s9w/kaggle_and_burnouts/,515405,1622367918.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,Just curious!,t2_5qn6m6wc,False,,0,False,"Did anyone here get their company to pay for their masters in a field of data science? If so, where did you work and what did you do?",[],r/datascience,False,6,education,0,,,False,t3_noe1fg,False,dark,0.92,,public,60,0,{},,,False,[],,False,False,,{},Education,False,60,,False,False,self,False,,[],{},,True,,1622418826.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just curious!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,noe1fg,True,,questions2067,,61,True,all_ads,False,[],False,,/r/datascience/comments/noe1fg/did_anyone_here_get_their_company_to_pay_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/noe1fg/did_anyone_here_get_their_company_to_pay_for/,515405,1622390026.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"So i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming. In fact my peers too seem like years ahead of me when it comes to knowledge and implementation. 

Curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something. Or if you still feel this way. I want to know your experience.",t2_7mksj1z1,False,,0,False,Data Science field is overwhelming,[],r/datascience,False,6,discussion,0,,,False,t3_no7t46,False,dark,0.8,,public,46,0,{},,,False,[],,False,False,,{},Discussion,False,46,,False,False,self,False,,[],{},,True,,1622396821.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming. In fact my peers too seem like years ahead of me when it comes to knowledge and implementation. &lt;/p&gt;

&lt;p&gt;Curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something. Or if you still feel this way. I want to know your experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no7t46,True,,HyunjinsPeach,,28,True,all_ads,False,[],False,,/r/datascience/comments/no7t46/data_science_field_is_overwhelming/,all_ads,False,https://www.reddit.com/r/datascience/comments/no7t46/data_science_field_is_overwhelming/,515405,1622368021.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_onq0,False,,0,False,ProteinBERT: A universal deep-learning model of protein sequence and function,[],r/datascience,False,6,projects,0,,,False,t3_nofgvu,False,dark,0.59,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,default,False,seniorflair,[],{},,False,,1622423034.0,text,6,,,text,self.bioinformatics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,MSC | Data Scientist | Bioinformatics &amp; AI,[],False,,,,t5_2sptq,,,,nofgvu,True,,ddofer,,0,True,all_ads,False,[],False,dark,/r/datascience/comments/nofgvu/proteinbert_a_universal_deeplearning_model_of/,all_ads,False,/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/,515405,1622394234.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?auto=webp&amp;s=a01d53e28ea2703748a93f24b9c36be28502973a', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9535b1ec3074f9be1d5ad488f32afcfa7fa4917f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11e566015051d306cd191c9c83b55533591d7c07', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'ZEtHsU9C8_lJyLY_pv_-HcDEP3BCZ770IN4DQxggJJs'}], 'enabled': False}",,"[{'approved_at_utc': None, 'subreddit': 'bioinformatics', 'selftext': ""# ProteinBERT: A universal deep-learning model of protein sequence and function\n\n&gt;Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal\n\nPaper: [https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1](https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1)\n\nTL;DR:\n\n&gt;Deep learning language models (like BERT in NLP) but for proteins!  \n&gt;  \n&gt;We trained a model on over 100 million proteins to predict their sequence and GO annotations (i.e their functions and properties). We show \\~SOTA performance on a wide range of benchmarks. Our model is much smaller and faster than comparable works (TAPE, ESM), and is quite interpretable thanks to our global attention. We provide the pretrained models and code, in a simple Keras/Tensorflow Python package.\n\nCode &amp; pretrained models:\n\n[https://github.com/nadavbra/protein\\_bert](https://github.com/nadavbra/protein_bert)\n\n&amp;#x200B;\n\nI'm one of the authors, AMA! :)"", 'author_fullname': 't2_onq0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ProteinBERT: A universal deep-learning model of protein sequence and function', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/bioinformatics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'academic', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_no76jp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 91, 'total_awards_received': 3, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'academic', 'can_mod_post': False, 'score': 91, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {'gid_1': 2}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622394001.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.bioinformatics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;ProteinBERT: A universal deep-learning model of protein sequence and function&lt;/h1&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Paper: &lt;a href=""https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1""&gt;https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Deep learning language models (like BERT in NLP) but for proteins!  &lt;/p&gt;\n\n&lt;p&gt;We trained a model on over 100 million proteins to predict their sequence and GO annotations (i.e their functions and properties). We show ~SOTA performance on a wide range of benchmarks. Our model is much smaller and faster than comparable works (TAPE, ESM), and is quite interpretable thanks to our global attention. We provide the pretrained models and code, in a simple Keras/Tensorflow Python package.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Code &amp;amp; pretrained models:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/nadavbra/protein_bert""&gt;https://github.com/nadavbra/protein_bert&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m one of the authors, AMA! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?auto=webp&amp;s=a01d53e28ea2703748a93f24b9c36be28502973a', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9535b1ec3074f9be1d5ad488f32afcfa7fa4917f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-89HKpc7k9F67kf9i93VXx_7fnQ638NWgo8kX3Z4DZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11e566015051d306cd191c9c83b55533591d7c07', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'ZEtHsU9C8_lJyLY_pv_-HcDEP3BCZ770IN4DQxggJJs'}], 'enabled': False}, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}], 'awarders': [], 'media_only': False, 'link_flair_template_id': '12168aa0-7f51-11e4-8866-22000b3396c4', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh0x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'no76jp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ddofer', 'discussion_type': None, 'num_comments': 36, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/', 'subreddit_subscribers': 65941, 'created_utc': 1622365201.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",/r/bioinformatics/comments/no76jp/proteinbert_a_universal_deeplearning_model_of/,t3_no76jp,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 30 May 2021 - 06 Jun 2021,[],r/datascience,False,6,,0,,,False,t3_no9q3m,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},Discussion,False,11,,False,False,self,False,,[],{},,True,,1622404831.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,no9q3m,True,,datascience-bot,,153,False,all_ads,False,[],False,dark,/r/datascience/comments/no9q3m/weekly_entering_transitioning_thread_30_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/no9q3m/weekly_entering_transitioning_thread_30_may_2021/,515405,1622376031.0,0,,False,,,,,,,,
,datascience,"I've been lurking around this subreddit since I started my final year project, a facial recognition project. I fell in love with data science overall and the stuff I was discovering every day. I lost interest in software engineering and embedded engineering a year ago, and I've been looking for the field that I would be happy to go into, and DS was the one. I've started applying for graduate jobs to become a data scientist, and I'm starting on a side project soon to boost my profile a bit more. I wondered how hard it would be when looking for jobs in DS, primarily when you haven't studied the course directly.",t2_nrpje,False,,0,False,Finally graduated from computer engineering,[],r/datascience,False,6,career,0,,,False,t3_nnqrta,False,dark,0.95,,public,231,1,{},,,False,[],,False,False,,{},Career,False,231,,False,False,self,False,,[],{},,True,,1622333848.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been lurking around this subreddit since I started my final year project, a facial recognition project. I fell in love with data science overall and the stuff I was discovering every day. I lost interest in software engineering and embedded engineering a year ago, and I&amp;#39;ve been looking for the field that I would be happy to go into, and DS was the one. I&amp;#39;ve started applying for graduate jobs to become a data scientist, and I&amp;#39;m starting on a side project soon to boost my profile a bit more. I wondered how hard it would be when looking for jobs in DS, primarily when you haven&amp;#39;t studied the course directly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnqrta,True,,JeffTheSpider,,34,True,all_ads,False,[],False,,/r/datascience/comments/nnqrta/finally_graduated_from_computer_engineering/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnqrta/finally_graduated_from_computer_engineering/,515405,1622305048.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Is there any open source work for someone who has no prior experience in making tools or any contributions? Someone who welcomes beginners and helps them or guides them to get better in contributing and improve collaboration skills.,t2_34qgdyb6,False,,0,False,Open-source work in data science for a newbie.,[],r/datascience,False,6,projects,0,,,False,t3_no77f5,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Projects,False,5,,False,False,self,False,,[],{},,True,,1622394085.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any open source work for someone who has no prior experience in making tools or any contributions? Someone who welcomes beginners and helps them or guides them to get better in contributing and improve collaboration skills.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no77f5,True,,yaakarsh1011,,8,True,all_ads,False,[],False,,/r/datascience/comments/no77f5/opensource_work_in_data_science_for_a_newbie/,all_ads,False,https://www.reddit.com/r/datascience/comments/no77f5/opensource_work_in_data_science_for_a_newbie/,515405,1622365285.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I've started working as a data analyst at a company where I used to work in a non-techical role. They didn't have an established data team. Instead myself and my manager (we both graduated last year). Great opportunity for me, but I'm being asked to write a pretty big app in Python and I'm really struggling with it from a technical ""know-how"" standpoint.   
I feel like maybe I'm not cut out for data work, but also understand that imposter syndrome is a thing. Just looking for advice from people who have spent a longer time in the field. Are there moments where you're completely over your head? Do you just try and work through it?   
Thanks team.",t2_7qw6e,False,,0,False,When is it imposter syndrome?,[],r/datascience,False,6,career,0,,,False,t3_no9tq4,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1622405200.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve started working as a data analyst at a company where I used to work in a non-techical role. They didn&amp;#39;t have an established data team. Instead myself and my manager (we both graduated last year). Great opportunity for me, but I&amp;#39;m being asked to write a pretty big app in Python and I&amp;#39;m really struggling with it from a technical &amp;quot;know-how&amp;quot; standpoint.&lt;br/&gt;
I feel like maybe I&amp;#39;m not cut out for data work, but also understand that imposter syndrome is a thing. Just looking for advice from people who have spent a longer time in the field. Are there moments where you&amp;#39;re completely over your head? Do you just try and work through it?&lt;br/&gt;
Thanks team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no9tq4,True,,Gusdafamon,,10,True,all_ads,False,[],False,,/r/datascience/comments/no9tq4/when_is_it_imposter_syndrome/,all_ads,False,https://www.reddit.com/r/datascience/comments/no9tq4/when_is_it_imposter_syndrome/,515405,1622376400.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi there.

I am building a website with a rating system and I was wondering if there are any highly recommendable books that could help with designing it.

I'm interested in anything ranging from details such as designing algorithms to deal with items with fewer ratings having higher average scores (as in, an item with a single 10 rating being ranked higher than an item with a thousand 9 votes) or the arguments behind different rating systems (10 stars, 5 stars, thumbs up/down, subcategory ratings, etc) to even more disperse topics such as adapting the math to the user psychology behind it all (such as how to adapt the system to  nostalgia votes or how to separate evaluating how impactful or revolutionary a product was when released from how good or recommendable it is today).

I am also really interested in the implications that those different rating system designs would have for a inter-user recommender/affinity system, but I understand that might be a tad specific \^\^

&amp;#x200B;

I am ok with technical reads and know some machine learning (I've done the Stanford course on Coursera), but I am not a data scientist, so I'd prefer it if the books/resources were reasonably accessible (though if a dense book is an absolute staple it would be good to know about it as well).

Cheers =)",t2_r3pn6,False,,0,False,Recommended resources for designing an item rating system?,[],r/datascience,False,6,education,0,,,False,t3_no33kc,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,1622352991.0,,[],{},,True,,1622376469.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there.&lt;/p&gt;

&lt;p&gt;I am building a website with a rating system and I was wondering if there are any highly recommendable books that could help with designing it.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in anything ranging from details such as designing algorithms to deal with items with fewer ratings having higher average scores (as in, an item with a single 10 rating being ranked higher than an item with a thousand 9 votes) or the arguments behind different rating systems (10 stars, 5 stars, thumbs up/down, subcategory ratings, etc) to even more disperse topics such as adapting the math to the user psychology behind it all (such as how to adapt the system to  nostalgia votes or how to separate evaluating how impactful or revolutionary a product was when released from how good or recommendable it is today).&lt;/p&gt;

&lt;p&gt;I am also really interested in the implications that those different rating system designs would have for a inter-user recommender/affinity system, but I understand that might be a tad specific ^^&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am ok with technical reads and know some machine learning (I&amp;#39;ve done the Stanford course on Coursera), but I am not a data scientist, so I&amp;#39;d prefer it if the books/resources were reasonably accessible (though if a dense book is an absolute staple it would be good to know about it as well).&lt;/p&gt;

&lt;p&gt;Cheers =)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,no33kc,True,,HigoChumbo,,4,True,all_ads,False,[],False,,/r/datascience/comments/no33kc/recommended_resources_for_designing_an_item/,all_ads,False,https://www.reddit.com/r/datascience/comments/no33kc/recommended_resources_for_designing_an_item/,515405,1622347669.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"These are two areas that broadly, I’m interested in applying ML and DS techniques and skills to. I was wondering if some people could shed some light on some of the pros and cons of working in a machine learning or data science capacity in either of these areas. Thanks!",t2_7ciqyc7g,False,,0,False,Applying ML/DS for cybersecurity vs finance,[],r/datascience,False,6,discussion,0,,,False,t3_nnwyn7,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1622353367.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;These are two areas that broadly, I’m interested in applying ML and DS techniques and skills to. I was wondering if some people could shed some light on some of the pros and cons of working in a machine learning or data science capacity in either of these areas. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnwyn7,True,,m4mancy,,2,True,all_ads,False,[],False,,/r/datascience/comments/nnwyn7/applying_mlds_for_cybersecurity_vs_finance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnwyn7/applying_mlds_for_cybersecurity_vs_finance/,515405,1622324567.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"As part of a bigger project, I'm looking to put some effort into open sourcing a data sourcing and data collaboration cli tool.  The utility of the tool has been great limited enterprise and research settings as a sort of a shadow IT tool that replaces ""emailing CSVs"" around"".

Please note, I'm not a data scientists or engineer so I'm looking to understand this use case further.  Thanks.

Edit: 
If possible, when commenting, could you include organztion context. For instance, enterprise, startup, tech, research or other.",t2_dnelb,False,,0,False,"What is your current process like to source, clean/prepare, and collaborate with datasets?",[],r/datascience,False,6,discussion,0,,,False,t3_nnlunt,False,dark,0.91,,public,22,0,{},,,False,[],,False,False,,{},Discussion,False,22,,False,False,self,1622293522.0,,[],{},,True,,1622317844.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As part of a bigger project, I&amp;#39;m looking to put some effort into open sourcing a data sourcing and data collaboration cli tool.  The utility of the tool has been great limited enterprise and research settings as a sort of a shadow IT tool that replaces &amp;quot;emailing CSVs&amp;quot; around&amp;quot;.&lt;/p&gt;

&lt;p&gt;Please note, I&amp;#39;m not a data scientists or engineer so I&amp;#39;m looking to understand this use case further.  Thanks.&lt;/p&gt;

&lt;p&gt;Edit: 
If possible, when commenting, could you include organztion context. For instance, enterprise, startup, tech, research or other.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nnlunt,True,,adgezaza87,,14,True,all_ads,False,[],False,,/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/,515405,1622289044.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Today, I got my first paycheck from my first internship and I am shocked about the entire situation. I come from a poor family, I am the first of my family to college (and grad-school) and the first to have a real professional work experience. I honestly feel blessed to be able to improve on my data science abilities and get paid for it! 

I have been working with the lead data scientist and have learned so much in these past two weeks. I enjoy coming to work and even more so now that I saw the paycheck. 

Sorry for the weird post, but I am just in a good mood right now. 

P.s. My boss asked me if I want to continue my internship for the Fall",t2_51tyvidz,False,,0,False,First two weeks of my first internship,[],r/datascience,False,6,career,0,,,False,t3_nmyg3i,False,dark,0.97,,public,900,8,{},,,False,[],,False,False,,{},Career,False,900,,False,False,self,False,,[],{'gid_1': 1},,True,,1622239176.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Today, I got my first paycheck from my first internship and I am shocked about the entire situation. I come from a poor family, I am the first of my family to college (and grad-school) and the first to have a real professional work experience. I honestly feel blessed to be able to improve on my data science abilities and get paid for it! &lt;/p&gt;

&lt;p&gt;I have been working with the lead data scientist and have learned so much in these past two weeks. I enjoy coming to work and even more so now that I saw the paycheck. &lt;/p&gt;

&lt;p&gt;Sorry for the weird post, but I am just in a good mood right now. &lt;/p&gt;

&lt;p&gt;P.s. My boss asked me if I want to continue my internship for the Fall&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 75, 'id': 'award_9663243a-e77f-44cf-abc6-850ead2cd18d', 'penny_donate': 0, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'For an especially amazing showing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Bravo Grande!', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=16&amp;height=16&amp;auto=webp&amp;s=3459bdf1d1777821a831c5bf9834f4365263fcff', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=32&amp;height=32&amp;auto=webp&amp;s=9181d68065ccfccf2b1074e499cd7c1103aa2ce8', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=48&amp;height=48&amp;auto=webp&amp;s=339b368d395219120abc50d54fb3e2cdcad8ca4f', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=64&amp;height=64&amp;auto=webp&amp;s=de4ebbe92f9019de05aaa77f88810d44adbe1e50', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=128&amp;height=128&amp;auto=webp&amp;s=ba6c1add5204ea43e5af010bd9622392a42140e3', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmyg3i,True,,royal-Brwn,,76,True,all_ads,False,[],False,,/r/datascience/comments/nmyg3i/first_two_weeks_of_my_first_internship/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmyg3i/first_two_weeks_of_my_first_internship/,515405,1622210376.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"In hope that this post won't get removed, I will take you through the steps of getting an internship at a Big4 in Belgium (Yes, mentioning the country is important because some things aren't the same in all the countries). The position was in data analytics\*.\* (the [full blog](https://dataanalystlife.blogspot.com/2021/06/working-at-big-4.html) if anyone is interested)

**The interview**

I had three interviews to pass.

* Technical (with one of the nicest recruiters/Data analyst/scientist)
* HR (also a very nice person)
* Manager (very serious guy, he scared the shit out of me, but once I started working with them he was super nice)

***Technical***

1. SQL = What would you write to do x, y, z. (Small question to test if I know how to use SQL or not at all)
2. Business question to test my business acumen
3. Statistics questions regarding outliers and robust preprocessing, median and mean in skewed data

Yes overall, the interview was not that hard, because I am a Civil Engineer + Master of AI. I truly think that having a good educational gives a nice push. The interviewer will think, if he made it that far, he won't be a dumb fuck (unless i cheated all the way). Yes I know, good grades does not mean intelligence. But good grades/good education means hard work and that's what most company want (hard workers) in my opinion.

***HR***

1. Testing my French skills (I am supposed to be bilingual but once you start talking English everyday you start losing the French vocab, ""le science de data"" I said it like 30 times (FFS)) mainly because in Belgium the languages are French, English, Dutch and German.
2. General storytelling, my cv, why i came to europe, why i chose my masters, what i did before coming
3. Motivation: on why I found the position interesting. BTW Deals Analytics is one of the most fun position if you check the YouTube videos.
4. \*\*Me saying some jokes to gain points :p\*\*

**Manager**

1. Same as HR but with a lot more pressure. I got scared to death no joke (I bet many people know what I am talking about)

**First Week as an Intern**

Right away, I had to learn the software they use, which is **Alteryx**. Since I have already done my Pandas and SQL on DataCamp, I did not require lots of time to get used to it. **VERY** fun no code software for data cleaning and prepping.

**First Project**

I directly started working on an RED financial data. The goal was to create insight from the financial data to direct the next investments in the right path (most lucrative).

**Second Project**

Geospatial analysis. Oh boiii, Oh BOIIIII ! This one was amazing. I had no idea how good is creating a map and understanding the effect of the surroundings on the business. This one was huge! Learned how to webscrape, how to deal with JSONS and much more.

**Third Project**

Computer Vision project (what the actual fuck? are we still in the same internship?). Haha. Matter of fact, I had learned a course on CV and I was like: ""guys i studied this a few months ago, I can solve it with CV"". Everyone liked it so much (even though i copy pasted the code from my old projects \*\*evil laugh\*\*)

**Fourth Project**

Geospatial Analysis again. But this was one very tough and stressing.

**Fifth Project**

At that point I had to work on my thesis because I had an intermediate presentation. However, the project was about traffic analysis (traffic as in cars). A dream coming true because i am a civil engineer specialized in traffic engineering/transportation. We did amazing stuff but they were very limited :(.

**The End**

One thing to add, for however gets an internship at a big4, I hope you get a supervisor similar to the one I had. I loved him. Always supportive and encouraging. He cared about me on the personal level.

**To the readers**

I hope you benefit from this. Either motivates you to apply or to learn something new.

hope you enjoyed it♥",t2_7mkrswyv,False,,0,False,Working as a Data Analyst at a Big4 (Europe),[],r/datascience,False,6,career,0,,,False,t3_nn8v5a,False,dark,0.77,,public,16,0,{},,,False,[],,False,False,,{},Career,False,16,,False,False,self,1622660382.0,,[],{},,True,,1622268152.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In hope that this post won&amp;#39;t get removed, I will take you through the steps of getting an internship at a Big4 in Belgium (Yes, mentioning the country is important because some things aren&amp;#39;t the same in all the countries). The position was in data analytics*.* (the &lt;a href=""https://dataanalystlife.blogspot.com/2021/06/working-at-big-4.html""&gt;full blog&lt;/a&gt; if anyone is interested)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The interview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I had three interviews to pass.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Technical (with one of the nicest recruiters/Data analyst/scientist)&lt;/li&gt;
&lt;li&gt;HR (also a very nice person)&lt;/li&gt;
&lt;li&gt;Manager (very serious guy, he scared the shit out of me, but once I started working with them he was super nice)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Technical&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;SQL = What would you write to do x, y, z. (Small question to test if I know how to use SQL or not at all)&lt;/li&gt;
&lt;li&gt;Business question to test my business acumen&lt;/li&gt;
&lt;li&gt;Statistics questions regarding outliers and robust preprocessing, median and mean in skewed data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Yes overall, the interview was not that hard, because I am a Civil Engineer + Master of AI. I truly think that having a good educational gives a nice push. The interviewer will think, if he made it that far, he won&amp;#39;t be a dumb fuck (unless i cheated all the way). Yes I know, good grades does not mean intelligence. But good grades/good education means hard work and that&amp;#39;s what most company want (hard workers) in my opinion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;HR&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Testing my French skills (I am supposed to be bilingual but once you start talking English everyday you start losing the French vocab, &amp;quot;le science de data&amp;quot; I said it like 30 times (FFS)) mainly because in Belgium the languages are French, English, Dutch and German.&lt;/li&gt;
&lt;li&gt;General storytelling, my cv, why i came to europe, why i chose my masters, what i did before coming&lt;/li&gt;
&lt;li&gt;Motivation: on why I found the position interesting. BTW Deals Analytics is one of the most fun position if you check the YouTube videos.&lt;/li&gt;
&lt;li&gt;**Me saying some jokes to gain points :p**&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Manager&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Same as HR but with a lot more pressure. I got scared to death no joke (I bet many people know what I am talking about)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;First Week as an Intern&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Right away, I had to learn the software they use, which is &lt;strong&gt;Alteryx&lt;/strong&gt;. Since I have already done my Pandas and SQL on DataCamp, I did not require lots of time to get used to it. &lt;strong&gt;VERY&lt;/strong&gt; fun no code software for data cleaning and prepping.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I directly started working on an RED financial data. The goal was to create insight from the financial data to direct the next investments in the right path (most lucrative).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Geospatial analysis. Oh boiii, Oh BOIIIII ! This one was amazing. I had no idea how good is creating a map and understanding the effect of the surroundings on the business. This one was huge! Learned how to webscrape, how to deal with JSONS and much more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Computer Vision project (what the actual fuck? are we still in the same internship?). Haha. Matter of fact, I had learned a course on CV and I was like: &amp;quot;guys i studied this a few months ago, I can solve it with CV&amp;quot;. Everyone liked it so much (even though i copy pasted the code from my old projects **evil laugh**)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Geospatial Analysis again. But this was one very tough and stressing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fifth Project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At that point I had to work on my thesis because I had an intermediate presentation. However, the project was about traffic analysis (traffic as in cars). A dream coming true because i am a civil engineer specialized in traffic engineering/transportation. We did amazing stuff but they were very limited :(.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The End&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One thing to add, for however gets an internship at a big4, I hope you get a supervisor similar to the one I had. I loved him. Always supportive and encouraging. He cared about me on the personal level.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To the readers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hope you benefit from this. Either motivates you to apply or to learn something new.&lt;/p&gt;

&lt;p&gt;hope you enjoyed it♥&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn8v5a,True,,Ecstatic_Tooth_1096,,17,True,all_ads,False,[],False,,/r/datascience/comments/nn8v5a/working_as_a_data_analyst_at_a_big4_europe/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn8v5a/working_as_a_data_analyst_at_a_big4_europe/,515405,1622239352.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?auto=webp&amp;s=8b97c0888f7fbb5d3636b1de1a3003d29752a279', 'width': 1100, 'height': 619}, 'resolutions': [{'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7712bda390a6d3e6ebaef44fff0553937c1f1f7b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b17b8c73ced12433e61e8e5870b3a5a4baf5b0e', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d0705e3f6e6c114b3b2051175dfe66bb13cb958', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f912cf9b331ae1037ac26c2e4eeda7f9c9b7488', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=025eb5700d54827954185f7d78b2703abce0be0b', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/ASdDUPfCg7Xvfm76EOVckdA2HMrIfAA6GOSi4wlLj9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2435b9f1684b896f6d480f0d2484f9aecb535e97', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'RfmbTLUIFPHuunMFk36ftpkiWcluXrN-0jeTfuGUHu4'}], 'enabled': False}",,,,,
,datascience,"Hi everyone,

I had an unusual situation happen in the past few days, and I'd like some advice.

A staffing agency in the Bay Area offered me the opportunity to interview for a DS role on a FAANG team that would directly impact a product that is popular worldwide (think 100m+ users). I like the role, but am hesitant about it being a contract position, considering I have a full-time job lined up post-MS in the Bay Area that is paying 135k (\~150k if you include benefits, 170k if equity options aren't worth crap) with a team I like, though at a much smaller scale (more relatively unknown) company with far fewer DS to learn from.

The staffing agency told me the team wants to bring me in for an additional 7 interviews, testing me on everything (statistics, ML, product sense, python, SQL, behavioral), but that the position would only be paying 120k. I told her that is ridiculous, since this is just a contract position, and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role, considering I have a full-time offer already. I was told today they would match the 180k.

Was I being extremely low-balled initially? The staffing agency is well known, and I've heard decent things about it. For context, this team has been looking for nearly a year for someone and I'm the only person to make it to the final stage (as far as I know). Do you think it is worthwhile to continue the interview process? Would you?

Any advice is appreciated. Thanks!

Edit #1: I'd be a W2 employee of the staffing agency. They'd have the contract with the FAANG.",t2_1100xc,False,,0,False,Lowballed for FAANG DS Contracting as New Grad? Advice Needed,[],r/datascience,False,6,,0,,,False,t3_nnfeb4,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,1622266646.0,,[],{},,True,,1622290979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I had an unusual situation happen in the past few days, and I&amp;#39;d like some advice.&lt;/p&gt;

&lt;p&gt;A staffing agency in the Bay Area offered me the opportunity to interview for a DS role on a FAANG team that would directly impact a product that is popular worldwide (think 100m+ users). I like the role, but am hesitant about it being a contract position, considering I have a full-time job lined up post-MS in the Bay Area that is paying 135k (~150k if you include benefits, 170k if equity options aren&amp;#39;t worth crap) with a team I like, though at a much smaller scale (more relatively unknown) company with far fewer DS to learn from.&lt;/p&gt;

&lt;p&gt;The staffing agency told me the team wants to bring me in for an additional 7 interviews, testing me on everything (statistics, ML, product sense, python, SQL, behavioral), but that the position would only be paying 120k. I told her that is ridiculous, since this is just a contract position, and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role, considering I have a full-time offer already. I was told today they would match the 180k.&lt;/p&gt;

&lt;p&gt;Was I being extremely low-balled initially? The staffing agency is well known, and I&amp;#39;ve heard decent things about it. For context, this team has been looking for nearly a year for someone and I&amp;#39;m the only person to make it to the final stage (as far as I know). Do you think it is worthwhile to continue the interview process? Would you?&lt;/p&gt;

&lt;p&gt;Any advice is appreciated. Thanks!&lt;/p&gt;

&lt;p&gt;Edit #1: I&amp;#39;d be a W2 employee of the staffing agency. They&amp;#39;d have the contract with the FAANG.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nnfeb4,True,,CustardEnigma,,30,True,all_ads,False,[],False,,/r/datascience/comments/nnfeb4/lowballed_for_faang_ds_contracting_as_new_grad/,all_ads,False,https://www.reddit.com/r/datascience/comments/nnfeb4/lowballed_for_faang_ds_contracting_as_new_grad/,515405,1622262179.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hello!

I am doing a project at work to predict On time delivery percentage in a manufacturing process.
I recently discovered Quantile Random Forest and I like the idea of it. I am thinking of using Quantile 0.5 as a point estimator and 0.1 and 0.9 quantile as prediction interval.

So far the results have been good but since I'm new to the real world project setting and new to quantile random forest, I was wondering is there something I should keep in mind while using this algorithm?

I read an article at Medium where they showed a use case of QRF at Instacart to predict On time delivery percentage but I was thinking why this algorithm is not so popular (maybe I just don't know about it).

What have your personal experience been using QRF?

Thank you!",t2_7ckcfm6o,False,,0,False,What are your thoughts on Quantile Random Forest?,[],r/datascience,False,6,discussion,0,,,False,t3_nn5z43,False,dark,0.92,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1622260096.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am doing a project at work to predict On time delivery percentage in a manufacturing process.
I recently discovered Quantile Random Forest and I like the idea of it. I am thinking of using Quantile 0.5 as a point estimator and 0.1 and 0.9 quantile as prediction interval.&lt;/p&gt;

&lt;p&gt;So far the results have been good but since I&amp;#39;m new to the real world project setting and new to quantile random forest, I was wondering is there something I should keep in mind while using this algorithm?&lt;/p&gt;

&lt;p&gt;I read an article at Medium where they showed a use case of QRF at Instacart to predict On time delivery percentage but I was thinking why this algorithm is not so popular (maybe I just don&amp;#39;t know about it).&lt;/p&gt;

&lt;p&gt;What have your personal experience been using QRF?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn5z43,True,,ysharm10,,15,True,all_ads,False,[],False,,/r/datascience/comments/nn5z43/what_are_your_thoughts_on_quantile_random_forest/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn5z43/what_are_your_thoughts_on_quantile_random_forest/,515405,1622231296.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"No disrespect to Ph'd's,  just an interesting analogy.

lots of internal validation and creds,  but poor performance in the wild.",t2_6hh47hwj,False,,0,False,A lot of people entering this field are like over-fitted models,[],r/datascience,False,6,discussion,0,,,False,t3_nmaguz,False,dark,0.91,,public,647,3,{},,,False,[],,False,False,,{},Discussion,False,647,,False,False,self,False,,[],{'gid_1': 1},,True,,1622159474.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;No disrespect to Ph&amp;#39;d&amp;#39;s,  just an interesting analogy.&lt;/p&gt;

&lt;p&gt;lots of internal validation and creds,  but poor performance in the wild.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmaguz,True,,redmoon_reddit,,161,True,all_ads,False,[],False,,/r/datascience/comments/nmaguz/a_lot_of_people_entering_this_field_are_like/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmaguz/a_lot_of_people_entering_this_field_are_like/,515405,1622130674.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"i'm a software developer first, so getting into AI it's hard to get used to handling so much binary data. how do people in this field do data and model versioning in general?

ninja edit: my first instinct is git LFS",t2_8hhmk,False,,0,False,How does version control work for ML stuff?,[],r/datascience,False,6,tooling,0,,,False,t3_nn53za,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},Tooling,False,6,,False,False,self,False,,[],{},,True,,1622257620.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i&amp;#39;m a software developer first, so getting into AI it&amp;#39;s hard to get used to handling so much binary data. how do people in this field do data and model versioning in general?&lt;/p&gt;

&lt;p&gt;ninja edit: my first instinct is git LFS&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn53za,True,,covercash2,,9,True,all_ads,False,[],False,,/r/datascience/comments/nn53za/how_does_version_control_work_for_ml_stuff/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn53za/how_does_version_control_work_for_ml_stuff/,515405,1622228820.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I'd really like to learn more about data science through practice and maybe build a portfolio, but I'm really uncreative and can't think of any projects to try. Does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience? Maybe something w a machine learning component.",t2_y46sv,False,,0,False,How do I think of data science projects?,[],r/datascience,False,6,projects,0,,,False,t3_nn4j8q,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Projects,False,4,,False,False,self,False,,[],{},,True,,1622256004.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;d really like to learn more about data science through practice and maybe build a portfolio, but I&amp;#39;m really uncreative and can&amp;#39;t think of any projects to try. Does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience? Maybe something w a machine learning component.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn4j8q,True,,JBizzle07,,2,True,all_ads,False,[],False,,/r/datascience/comments/nn4j8q/how_do_i_think_of_data_science_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn4j8q/how_do_i_think_of_data_science_projects/,515405,1622227204.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'll keep this short, the start up were impressed with me when I was a software developer for them. I have a mathematical background and my masters was on a data science working with AI PhD students. I have a lot of knowledge in algorithms, cloud and best CI/CD practises. And I was previously a PhD student in A.I (didn't finish for valid reasons).

&amp;#x200B;

The company has asked me to take the lead on projects despite my lack of experience. And due to the nature of startups, there's not much structure and mentorship involved, but they believe in me.  I'm extremely motivated to do well and constantly learn for me and my company. So I was wondering the data science journey might be more smooth if I make this post.

&amp;#x200B;

I know this is kind of vague so if you need more information about me...let me know!

&amp;#x200B;

Thanks a lot everyone.",t2_1tvv8r2v,False,,0,False,Experienced data scientists....what advice could you give to a Junior data scientist working at a start up?,[],r/datascience,False,6,discussion,0,,,False,t3_nn210e,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622249236.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ll keep this short, the start up were impressed with me when I was a software developer for them. I have a mathematical background and my masters was on a data science working with AI PhD students. I have a lot of knowledge in algorithms, cloud and best CI/CD practises. And I was previously a PhD student in A.I (didn&amp;#39;t finish for valid reasons).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The company has asked me to take the lead on projects despite my lack of experience. And due to the nature of startups, there&amp;#39;s not much structure and mentorship involved, but they believe in me.  I&amp;#39;m extremely motivated to do well and constantly learn for me and my company. So I was wondering the data science journey might be more smooth if I make this post.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I know this is kind of vague so if you need more information about me...let me know!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot everyone.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn210e,True,,onechamp27,,9,True,all_ads,False,[],False,,/r/datascience/comments/nn210e/experienced_data_scientistswhat_advice_could_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn210e/experienced_data_scientistswhat_advice_could_you/,515405,1622220436.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_21s9nfge,False,,0,False,"I held &gt;120 office hour sessions with aspiring data scientists, picked the best ones, and turned them into a free course on getting hired in DS",[],r/datascience,False,6,,0,,,False,t3_nmb3ff,False,dark,0.97,,public,262,5,{},,,False,[],,False,False,,{},Job Search,False,262,,False,False,default,False,,[],{'gid_1': 2},,False,,1622161147.0,text,6,,,text,sharpestminds.com,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmb3ff,True,,jeremie-harris,,35,True,all_ads,False,[],False,,/r/datascience/comments/nmb3ff/i_held_120_office_hour_sessions_with_aspiring/,all_ads,False,https://www.sharpestminds.com/landing-a-data-job-the-course,515405,1622132347.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,https://www.sharpestminds.com/landing-a-data-job-the-course,,
,datascience,"Let's say I have a binary classification model where one of the discrete variable has 3 values, for example, male, female, unidentified. Does it make sense to use different probability thresholds for each value?

It seems intuitive to me but I'm trying to think of potential downfalls.

To give more context, our model generates a lot of false positives but performs well on class 0 (TN/FN). Does it make sense to have one threshold optimizing for class 0 prediction, then accept class 1 prediction only if the probability score is high? 

Since high here means class 1 precision, if I want to fix this (at say 75%), each gender would have a different probability threshold. 

The rest of the predictions will have prob score above class 0 threshold but lower than precision@.75 threshold. These will trigger manual intervention and we won't rely on model prediction.",t2_qinw9,False,,0,False,Binary Classification with Pick-and-Choose Threshold?,[],r/datascience,False,6,discussion,0,,,False,t3_nn5wfs,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622259880.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I have a binary classification model where one of the discrete variable has 3 values, for example, male, female, unidentified. Does it make sense to use different probability thresholds for each value?&lt;/p&gt;

&lt;p&gt;It seems intuitive to me but I&amp;#39;m trying to think of potential downfalls.&lt;/p&gt;

&lt;p&gt;To give more context, our model generates a lot of false positives but performs well on class 0 (TN/FN). Does it make sense to have one threshold optimizing for class 0 prediction, then accept class 1 prediction only if the probability score is high? &lt;/p&gt;

&lt;p&gt;Since high here means class 1 precision, if I want to fix this (at say 75%), each gender would have a different probability threshold. &lt;/p&gt;

&lt;p&gt;The rest of the predictions will have prob score above class 0 threshold but lower than &lt;a href=""mailto:precision@.75""&gt;precision@.75&lt;/a&gt; threshold. These will trigger manual intervention and we won&amp;#39;t rely on model prediction.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn5wfs,True,,monkeyunited,,1,True,all_ads,False,[],False,,/r/datascience/comments/nn5wfs/binary_classification_with_pickandchoose_threshold/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn5wfs/binary_classification_with_pickandchoose_threshold/,515405,1622231080.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"[Source Code](https://github.com/tstewart161/Reddit_Sentiment_Trader) (mine)

[Article](https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664) (not mine but it's an amazing look into how this works)

**HOW I DID THIS**

Scraped WSB sentiment, got the top + most positively mentioned stocks on WSB (for the better part of this year, that's been $GME and $AMC, recently some $SPCE and $NVDA, and about 13 other stocks. I have the strategy rebalancing monthly.

Right now I'm up 60% YTD, compared to the SP500's 13% (the recent spikes in GME and AMC have helped tremendously)

**Some stats (and a** [**picture of a card**](https://preview.redd.it/62exai0wik171.png?width=620&amp;format=png&amp;auto=webp&amp;s=1f8ec63b65c84e0e28da6d9164edfbb618ff09d0) **I made giving more info about the strategy):**

\- The strategy is **backtested** only to the beginning of 2020, but I'm working on it. It's got an annualized return of 33% (compared to 16% for the SP500)

\- **Max drawdown of -8.7%** (thought this was pretty interesting - WSB would be a very cool hedge for financial markets at large. Rode COVID like a wave)

Happy to answer any more questions about the process/results. I think doing stuff like this is pretty cool as someone with a foot in algo trading and traditional financial markets",t2_9myqz8vx,False,,0,False,"I used VADER sentiment analysis to track and invest based on WallStreetBets stock sentiment -- I'm up 33% annually $16k) -- here's source code, process, result, and an article",[],r/datascience,False,6,projects,0,,,False,t3_nmiq15,False,dark,0.87,,public,50,0,{},,,False,[],,False,False,,{},Projects,False,50,,False,False,self,False,,[],{},,True,,1622181396.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/tstewart161/Reddit_Sentiment_Trader""&gt;Source Code&lt;/a&gt; (mine)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664""&gt;Article&lt;/a&gt; (not mine but it&amp;#39;s an amazing look into how this works)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HOW I DID THIS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Scraped WSB sentiment, got the top + most positively mentioned stocks on WSB (for the better part of this year, that&amp;#39;s been $GME and $AMC, recently some $SPCE and $NVDA, and about 13 other stocks. I have the strategy rebalancing monthly.&lt;/p&gt;

&lt;p&gt;Right now I&amp;#39;m up 60% YTD, compared to the SP500&amp;#39;s 13% (the recent spikes in GME and AMC have helped tremendously)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some stats (and a&lt;/strong&gt; &lt;a href=""https://preview.redd.it/62exai0wik171.png?width=620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f8ec63b65c84e0e28da6d9164edfbb618ff09d0""&gt;&lt;strong&gt;picture of a card&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;I made giving more info about the strategy):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;- The strategy is &lt;strong&gt;backtested&lt;/strong&gt; only to the beginning of 2020, but I&amp;#39;m working on it. It&amp;#39;s got an annualized return of 33% (compared to 16% for the SP500)&lt;/p&gt;

&lt;p&gt;- &lt;strong&gt;Max drawdown of -8.7%&lt;/strong&gt; (thought this was pretty interesting - WSB would be a very cool hedge for financial markets at large. Rode COVID like a wave)&lt;/p&gt;

&lt;p&gt;Happy to answer any more questions about the process/results. I think doing stuff like this is pretty cool as someone with a foot in algo trading and traditional financial markets&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmiq15,True,,notjimryan,,14,True,all_ads,False,[],False,,/r/datascience/comments/nmiq15/i_used_vader_sentiment_analysis_to_track_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmiq15/i_used_vader_sentiment_analysis_to_track_and/,515405,1622152596.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?auto=webp&amp;s=39a158f450dc1b27762cdfcaf0746d4206b517fe', 'width': 620, 'height': 1126}, 'resolutions': [{'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=487122150616a702dcf667e5e973f55727150c03', 'width': 108, 'height': 196}, {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=013f4c76756dc0deecd91287debe9358d39ed64c', 'width': 216, 'height': 392}, {'url': 'https://external-preview.redd.it/BpR_TFeMb9nRjb9JerPoDK5_KeRAERTO1G1qKZQv6u0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=668b911d284b536bf93a748c469d26adbe46b842', 'width': 320, 'height': 581}], 'variants': {}, 'id': 'I75ug1AxTprL5FUnkUsdNjsyRhowuFtELPEwsV_aEGA'}], 'enabled': False}",,,,,
,datascience,"Hey all. I'm working on a project to extract machine-generated text and table data from PDFs.

We got part of this using libraries such as PyPDF2, PyMuPDF, and extracting tables using Camelot. 

But you know PDFs can be a pain to read, sometimes have weird encoding or are scanned so we need an OCR solution. I've been researching and playing with OpenCV, but most resources involve neural networks on images. 

I need to learn how to read text via an OCR solution. Can you recommend any resources or methods? Do they require NNs? Any insights would be appreciated. Thank you!",t2_2o0q5m4h,False,,0,False,what does the path to computer vision/OpenCV for text look like?,[],r/datascience,False,6,discussion,0,,,False,t3_nn0f57,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622244800.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all. I&amp;#39;m working on a project to extract machine-generated text and table data from PDFs.&lt;/p&gt;

&lt;p&gt;We got part of this using libraries such as PyPDF2, PyMuPDF, and extracting tables using Camelot. &lt;/p&gt;

&lt;p&gt;But you know PDFs can be a pain to read, sometimes have weird encoding or are scanned so we need an OCR solution. I&amp;#39;ve been researching and playing with OpenCV, but most resources involve neural networks on images. &lt;/p&gt;

&lt;p&gt;I need to learn how to read text via an OCR solution. Can you recommend any resources or methods? Do they require NNs? Any insights would be appreciated. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nn0f57,True,,rotterdamn8,,1,True,all_ads,False,[],False,,/r/datascience/comments/nn0f57/what_does_the_path_to_computer_visionopencv_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nn0f57/what_does_the_path_to_computer_visionopencv_for/,515405,1622216000.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"We all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying, which are more related to software engineering than math/statistics. If one wants to be as well-rounded as possible, what are some of the best software engineering practices (things like version control) that are a must?",t2_3q66js10,False,,0,False,What are some good software engineering practices that all data scientists must know?,[],r/datascience,False,6,discussion,0,,,False,t3_nmgfgb,False,dark,0.95,,public,25,0,{},,,False,[],,False,False,,{},Discussion,False,25,,False,False,self,False,,[],{},,True,,1622175148.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying, which are more related to software engineering than math/statistics. If one wants to be as well-rounded as possible, what are some of the best software engineering practices (things like version control) that are a must?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmgfgb,True,,Pedro9870,,15,True,all_ads,False,[],False,,/r/datascience/comments/nmgfgb/what_are_some_good_software_engineering_practices/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmgfgb/what_are_some_good_software_engineering_practices/,515405,1622146348.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi , 

I am switching from PMO to a Data Science role and got one interview today for a part-time role.

The Manager asked me, are you working on any ML projects, I told them I am working on 2 projects one is with a company and another one is with a university.  

Manager asked me what models you use and I told Linear regression to predict the values and we are still in Data preprocessing like cleaning the data, imputing the values and stuff.

Then, later manager didn't ask any question and asked me whether I have any question for them

then the interview was done in 15 mins. now I feel I screwed up

What to do in future to prevent this scenario?

Please help me.",t2_97m7yfi9,False,,0,False,How to explain your projects?,[],r/datascience,False,6,,0,,,False,t3_nmt4pp,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1622219188.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi , &lt;/p&gt;

&lt;p&gt;I am switching from PMO to a Data Science role and got one interview today for a part-time role.&lt;/p&gt;

&lt;p&gt;The Manager asked me, are you working on any ML projects, I told them I am working on 2 projects one is with a company and another one is with a university.  &lt;/p&gt;

&lt;p&gt;Manager asked me what models you use and I told Linear regression to predict the values and we are still in Data preprocessing like cleaning the data, imputing the values and stuff.&lt;/p&gt;

&lt;p&gt;Then, later manager didn&amp;#39;t ask any question and asked me whether I have any question for them&lt;/p&gt;

&lt;p&gt;then the interview was done in 15 mins. now I feel I screwed up&lt;/p&gt;

&lt;p&gt;What to do in future to prevent this scenario?&lt;/p&gt;

&lt;p&gt;Please help me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmt4pp,True,,Vinothd19,,5,True,all_ads,False,[],False,,/r/datascience/comments/nmt4pp/how_to_explain_your_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmt4pp/how_to_explain_your_projects/,515405,1622190388.0,2,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Just had an interview task for a data engineer role where I had to filter by date, find some codes in different columns etc. I read after that I was not allowed to use pandas or any external library? Is this a ridiculous ask?  I feel like this doesn’t represent the working environment at all.",t2_5z43s,False,,0,False,"Technical interview timed task, no pandas?",[],r/datascience,False,6,,0,,,False,t3_nmkw02,False,dark,0.67,,public,5,0,{},,,False,[],,False,False,,{},Job Search,False,5,,False,False,self,False,,[],{},,True,,1622188086.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just had an interview task for a data engineer role where I had to filter by date, find some codes in different columns etc. I read after that I was not allowed to use pandas or any external library? Is this a ridiculous ask?  I feel like this doesn’t represent the working environment at all.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmkw02,True,,boboshoes,,24,True,all_ads,False,[],False,,/r/datascience/comments/nmkw02/technical_interview_timed_task_no_pandas/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmkw02/technical_interview_timed_task_no_pandas/,515405,1622159286.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"In my spare time, I do some freelance work as a side hustle on some of the bigger platforms out there. The amount of DA, DS, Stat and etc. students that are posting their university homework and projects is preposterous. It ranges from basic stuff like running simple models and cleaning datasets to big capstone projects. I  decline every interview from these types of students that really ramp me up and beat the whole purpose of studying. They don't know the basics and have gone into the field just because they've heard that you can make money there. Not saying that they can't learn it, but going into a field without a deep interest or passion for it, on average, breeds bad practitioners.

Have you run into people like these? What is your opinion on this matter?",t2_4o6wucq4,False,,0,False,"Paying for ""Personal"" Projects and Homework",[],r/datascience,False,6,discussion,0,,,False,t3_nmblj9,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1622162494.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my spare time, I do some freelance work as a side hustle on some of the bigger platforms out there. The amount of DA, DS, Stat and etc. students that are posting their university homework and projects is preposterous. It ranges from basic stuff like running simple models and cleaning datasets to big capstone projects. I  decline every interview from these types of students that really ramp me up and beat the whole purpose of studying. They don&amp;#39;t know the basics and have gone into the field just because they&amp;#39;ve heard that you can make money there. Not saying that they can&amp;#39;t learn it, but going into a field without a deep interest or passion for it, on average, breeds bad practitioners.&lt;/p&gt;

&lt;p&gt;Have you run into people like these? What is your opinion on this matter?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmblj9,True,,Ingvariuss,,5,True,all_ads,False,[],False,,/r/datascience/comments/nmblj9/paying_for_personal_projects_and_homework/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmblj9/paying_for_personal_projects_and_homework/,515405,1622133694.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Sorry if this is not allowed in this sub - 

Was wondering if anyone had experience working as a Data Scientist at a Big 4...What should I expect?",t2_7hcxrfsi,False,,0,False,Data Science at a Big 4,[],r/datascience,False,6,,0,,,False,t3_nmn22u,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,False,,[],{},,True,,1622195419.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this is not allowed in this sub - &lt;/p&gt;

&lt;p&gt;Was wondering if anyone had experience working as a Data Scientist at a Big 4...What should I expect?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nmn22u,True,,StringAbject7049,,10,True,all_ads,False,[],False,,/r/datascience/comments/nmn22u/data_science_at_a_big_4/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmn22u/data_science_at_a_big_4/,515405,1622166619.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"What’s everyone’s favorite data science related podcasts, YouTube channels and audiobooks?

I spend a lot of time in the car and would love to learn while I drive! Thanks.",t2_846wudmb,False,,0,False,Favorite podcasts and other audio resources for learning and staying up to date?,[],r/datascience,False,6,discussion,0,,,False,t3_nm8k0y,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622154114.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What’s everyone’s favorite data science related podcasts, YouTube channels and audiobooks?&lt;/p&gt;

&lt;p&gt;I spend a lot of time in the car and would love to learn while I drive! Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm8k0y,True,,Homura_A,,6,True,all_ads,False,[],False,,/r/datascience/comments/nm8k0y/favorite_podcasts_and_other_audio_resources_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm8k0y/favorite_podcasts_and_other_audio_resources_for/,515405,1622125314.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello!

Sorry this might be a very stupid question but I've seen many people saying, ""Never submit your task before the deadline"". What's the thought behind this statement?

Asking this because, I'm a junior at a company and my manager often overestimates the time required to do certain tasks he gives me. And right now I happened to be in a situation where he gave me a task with a deadline of a week and I finished it within a few hours.

In your experience, what's the best thing to do?

Thanks!",t2_bv171ji2,False,,0,False,"New to corporate, should you submit your task way before deadline?",[],r/datascience,False,6,discussion,0,,,False,t3_nmbicr,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622162261.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Sorry this might be a very stupid question but I&amp;#39;ve seen many people saying, &amp;quot;Never submit your task before the deadline&amp;quot;. What&amp;#39;s the thought behind this statement?&lt;/p&gt;

&lt;p&gt;Asking this because, I&amp;#39;m a junior at a company and my manager often overestimates the time required to do certain tasks he gives me. And right now I happened to be in a situation where he gave me a task with a deadline of a week and I finished it within a few hours.&lt;/p&gt;

&lt;p&gt;In your experience, what&amp;#39;s the best thing to do?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nmbicr,True,,quite--average,,20,True,all_ads,False,[],False,,/r/datascience/comments/nmbicr/new_to_corporate_should_you_submit_your_task_way/,all_ads,False,https://www.reddit.com/r/datascience/comments/nmbicr/new_to_corporate_should_you_submit_your_task_way/,515405,1622133461.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

 I work for a large wholesale company, and I am trying to create a way to forecast when specific items will go out of stock at a specific store. I have access to sales data by store-item for the past 5 years as well the past 5 years of out of stock data (also on a store-item level). 

How would you approach this? 

FYI I code in Python mainly, and I'm pretty familiar with most ML tools and models. 

Just trying to get a variety of ideas. 

Thank you.",t2_4r0mnaaz,False,,0,False,Forecasting Out of Stock items on a store-item level,[],r/datascience,False,6,projects,0,,,False,t3_nm9155,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1622155470.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I work for a large wholesale company, and I am trying to create a way to forecast when specific items will go out of stock at a specific store. I have access to sales data by store-item for the past 5 years as well the past 5 years of out of stock data (also on a store-item level). &lt;/p&gt;

&lt;p&gt;How would you approach this? &lt;/p&gt;

&lt;p&gt;FYI I code in Python mainly, and I&amp;#39;m pretty familiar with most ML tools and models. &lt;/p&gt;

&lt;p&gt;Just trying to get a variety of ideas. &lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm9155,True,,kking1122,,11,True,all_ads,False,[],False,,/r/datascience/comments/nm9155/forecasting_out_of_stock_items_on_a_storeitem/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm9155/forecasting_out_of_stock_items_on_a_storeitem/,515405,1622126670.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Ive been at this job for about three months now and I feel like I’m not being challenged. I get assigned work that takes me about four hours to complete then the rest of the day I’m not doing anything except maybe 3-4 meetings that last 20 mins and then Im done. 

I also spend most of my time just cleaning and pulling data in Excel. Which often makes me think why I’m being pay well when this stuff is so easy.

Is this the norm for a jr data analyst role or should i find another job?",t2_qpxlk,False,,0,False,How was your first job out of college?,[],r/datascience,False,6,career,0,,,False,t3_nlwviq,False,dark,0.94,,public,16,0,{},,,False,[],,False,False,,{},Career,False,16,,False,False,self,False,,[],{},,True,,1622108708.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ive been at this job for about three months now and I feel like I’m not being challenged. I get assigned work that takes me about four hours to complete then the rest of the day I’m not doing anything except maybe 3-4 meetings that last 20 mins and then Im done. &lt;/p&gt;

&lt;p&gt;I also spend most of my time just cleaning and pulling data in Excel. Which often makes me think why I’m being pay well when this stuff is so easy.&lt;/p&gt;

&lt;p&gt;Is this the norm for a jr data analyst role or should i find another job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlwviq,True,,ForeignMate,,10,True,all_ads,False,[],False,,/r/datascience/comments/nlwviq/how_was_your_first_job_out_of_college/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlwviq/how_was_your_first_job_out_of_college/,515405,1622079908.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I work at a data science consulting firm and there I was faced with the established procedure that, if the team presents some forecast to a client and the client thinks that the forecast is too high or too low, instead of changing hyperparameters or even changing models, the team basically changes the forecast values manually using ""market insights"" (they basically look for other specialists forecasts in the internet and subjectively choose a value close to those). This makes me incredibly uncomfortable. Is this a normal procedure to be used at data science consultancies? I never worked at another consultancy, so I don't know if this is normal or not.",t2_10ftnyis,False,,0,False,"Is it normal to manually ""adjust"" forecasts?",[],r/datascience,False,6,discussion,0,,,False,t3_nm00ol,False,dark,1.0,,public,7,1,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{'gid_1': 1},,True,,1622120537.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work at a data science consulting firm and there I was faced with the established procedure that, if the team presents some forecast to a client and the client thinks that the forecast is too high or too low, instead of changing hyperparameters or even changing models, the team basically changes the forecast values manually using &amp;quot;market insights&amp;quot; (they basically look for other specialists forecasts in the internet and subjectively choose a value close to those). This makes me incredibly uncomfortable. Is this a normal procedure to be used at data science consultancies? I never worked at another consultancy, so I don&amp;#39;t know if this is normal or not.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm00ol,True,,hi_fi_v,,16,True,all_ads,False,[],False,,/r/datascience/comments/nm00ol/is_it_normal_to_manually_adjust_forecasts/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm00ol/is_it_normal_to_manually_adjust_forecasts/,515405,1622091737.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have a dataset that is pre-computed into a distance matrix, fully connected between all points. This made sense as it was originally intended for clustering but now I am wondering if I could use it for classification, i.e. given unlabelled points and their distances to all labelled points, assign soft labels. Being able to do regression in the same manner would also be a plus.

I tried Googling this but I could not come up with the appropriate terms to search. The best I could find is graph-based semi-supervised techniques like Label Propogation in sklearn but I was wondering if anyone had any insight into additional models &amp; techniques I could look into. Thanks!",t2_8nyuj3,False,,0,False,Classification/Regression just from a distance matrix?,[],r/datascience,False,6,discussion,0,,,False,t3_nlj5j8,False,dark,0.94,,public,45,0,{},,,False,[],,False,False,,{},Discussion,False,45,,False,False,self,False,,[],{},,True,,1622070193.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset that is pre-computed into a distance matrix, fully connected between all points. This made sense as it was originally intended for clustering but now I am wondering if I could use it for classification, i.e. given unlabelled points and their distances to all labelled points, assign soft labels. Being able to do regression in the same manner would also be a plus.&lt;/p&gt;

&lt;p&gt;I tried Googling this but I could not come up with the appropriate terms to search. The best I could find is graph-based semi-supervised techniques like Label Propogation in sklearn but I was wondering if anyone had any insight into additional models &amp;amp; techniques I could look into. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlj5j8,True,,metsfan1025,,19,True,all_ads,False,[],False,,/r/datascience/comments/nlj5j8/classificationregression_just_from_a_distance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlj5j8/classificationregression_just_from_a_distance/,515405,1622041393.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been a data scientist for a bit over a year and a half. But I don't feel ""Senior"" at all.

If anything, I feel like what I assumed data scientists who got entry-level roles felt like.

I look at the interview questions for Uber, Amazon, MSFT and others and I don't really see myself learning them from my peers. I'll need to study ISLR or other materials.

But it's the other materials where I'm a bit stuck.

Wondering if folks who made it to Senior roles or FAANG roles (not counting analytics/product roles) could share tips on how to learn to be a better data scientist.",t2_a3t5z3gn,False,,0,False,Data scientists who moved to ML data science roles: How did you get senior level skills?,[],r/datascience,False,6,career,0,,,False,t3_nlwczv,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},Career,False,7,,False,False,self,False,,[],{},,True,,1622106883.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been a data scientist for a bit over a year and a half. But I don&amp;#39;t feel &amp;quot;Senior&amp;quot; at all.&lt;/p&gt;

&lt;p&gt;If anything, I feel like what I assumed data scientists who got entry-level roles felt like.&lt;/p&gt;

&lt;p&gt;I look at the interview questions for Uber, Amazon, MSFT and others and I don&amp;#39;t really see myself learning them from my peers. I&amp;#39;ll need to study ISLR or other materials.&lt;/p&gt;

&lt;p&gt;But it&amp;#39;s the other materials where I&amp;#39;m a bit stuck.&lt;/p&gt;

&lt;p&gt;Wondering if folks who made it to Senior roles or FAANG roles (not counting analytics/product roles) could share tips on how to learn to be a better data scientist.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlwczv,True,,latticeprep,,2,True,all_ads,False,[],False,,/r/datascience/comments/nlwczv/data_scientists_who_moved_to_ml_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlwczv/data_scientists_who_moved_to_ml_data_science/,515405,1622078083.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm interested in a side project that looks at grocery store shopping habits and health outcomes. For the shopper history, I was hoping for loyalty number-based transaction data that would give me a good picture on how ""healthy"" their trips look. I'm not as interested in price as much as types of food (i.e. fruits vs candy). Haven't come across how to buy these data, but this [tweet thread](https://twitter.com/RobertGReeve/status/1397034344833748992?s=20) made me think a data aggregator may have it. Has anyone purchased grocery shopper transaction data before?",t2_13dmdo,False,,0,False,Grocery store purchase records,[],r/datascience,False,6,projects,0,,,False,t3_nlmcr8,False,dark,0.77,,public,9,0,{},,,False,[],,False,False,,{},Projects,False,9,,False,False,self,False,,[],{},,True,,1622078475.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m interested in a side project that looks at grocery store shopping habits and health outcomes. For the shopper history, I was hoping for loyalty number-based transaction data that would give me a good picture on how &amp;quot;healthy&amp;quot; their trips look. I&amp;#39;m not as interested in price as much as types of food (i.e. fruits vs candy). Haven&amp;#39;t come across how to buy these data, but this &lt;a href=""https://twitter.com/RobertGReeve/status/1397034344833748992?s=20""&gt;tweet thread&lt;/a&gt; made me think a data aggregator may have it. Has anyone purchased grocery shopper transaction data before?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlmcr8,True,,capitolcustomer,,18,True,all_ads,False,[],False,,/r/datascience/comments/nlmcr8/grocery_store_purchase_records/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlmcr8/grocery_store_purchase_records/,515405,1622049675.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yeWln_oOQKCfHMVv-wDnoPLgmZsjdgW7gQlWuKsChz8.jpg?auto=webp&amp;s=dc5e44b524b4a5f204f4421083b51e300c808f68', 'width': 140, 'height': 140}, 'resolutions': [{'url': 'https://external-preview.redd.it/yeWln_oOQKCfHMVv-wDnoPLgmZsjdgW7gQlWuKsChz8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d66ec268c453746be133d7cdd842fb83853fcbac', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'gnPwJIajQwm335Rwf7irkj5bPUrc3fg0DECiwlg-xs8'}], 'enabled': False}",,,,,
,datascience,"I just got tossed an interesting problem by management.   There's a lot of a certain type of document laying out agreements between labor/mgmt at the local branches, and a poor understanding at the top over what topics occur regularly in these documents.  It's over a thousand .txt documents, up to about 15k characters in each, so there's a lot to look through, and therefore, text analytics to the rescue.

I just got finished reading them all into Hadoop and turning them into a big table of the form:

Row = LineOfText,OriginalDocumentName

&amp;#x200B;

Before I get too far into the modeling part, I'm wondering if this is the best form to have the data in, or whether I'd do better with a format of:

Row = EntireDocument, OriginalDocumentName

&amp;#x200B;

My tools will be SAS Viya's text analytics primarily because I like the concept modeling part, and some R package like tidytext and whatever I can find for concept formation (R is my real go-to language, but I'm a bit of a noob in text analytics, and don't fully understand those packages yet.)  I've done some with Python's nltk and similar packages, although I'm expert level in R, and struggle some in Python.

Any thoughts about what's the best general format to use -- one line at a time, or the entire document at a time?   By the way, the documents come from a variety of sources like Word, PDF, and OCR input, and are pretty low quality in terms of misread or mangled words, white space, and weird control characters.  I'm not sure there's an obvious way to skip the line feeds in a Hive import to get to the second document-as-a-row format, but I can cross that bridge later, and have a few databases whizzes on my team that could help me figure it out if needed.",t2_87whm2mh,False,,0,False,"Ideal Text Analytics Data Structure -- One Document at a time, or one Row at a Time?",[],r/datascience,False,6,projects,0,,,False,t3_nlh1wv,False,dark,0.95,,public,18,0,{},,,False,[],,False,False,,{},Projects,False,18,,False,False,self,False,,[],{},,True,,1622064533.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just got tossed an interesting problem by management.   There&amp;#39;s a lot of a certain type of document laying out agreements between labor/mgmt at the local branches, and a poor understanding at the top over what topics occur regularly in these documents.  It&amp;#39;s over a thousand .txt documents, up to about 15k characters in each, so there&amp;#39;s a lot to look through, and therefore, text analytics to the rescue.&lt;/p&gt;

&lt;p&gt;I just got finished reading them all into Hadoop and turning them into a big table of the form:&lt;/p&gt;

&lt;p&gt;Row = LineOfText,OriginalDocumentName&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Before I get too far into the modeling part, I&amp;#39;m wondering if this is the best form to have the data in, or whether I&amp;#39;d do better with a format of:&lt;/p&gt;

&lt;p&gt;Row = EntireDocument, OriginalDocumentName&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My tools will be SAS Viya&amp;#39;s text analytics primarily because I like the concept modeling part, and some R package like tidytext and whatever I can find for concept formation (R is my real go-to language, but I&amp;#39;m a bit of a noob in text analytics, and don&amp;#39;t fully understand those packages yet.)  I&amp;#39;ve done some with Python&amp;#39;s nltk and similar packages, although I&amp;#39;m expert level in R, and struggle some in Python.&lt;/p&gt;

&lt;p&gt;Any thoughts about what&amp;#39;s the best general format to use -- one line at a time, or the entire document at a time?   By the way, the documents come from a variety of sources like Word, PDF, and OCR input, and are pretty low quality in terms of misread or mangled words, white space, and weird control characters.  I&amp;#39;m not sure there&amp;#39;s an obvious way to skip the line feeds in a Hive import to get to the second document-as-a-row format, but I can cross that bridge later, and have a few databases whizzes on my team that could help me figure it out if needed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlh1wv,True,,BullCityPicker,,19,True,all_ads,False,[],False,,/r/datascience/comments/nlh1wv/ideal_text_analytics_data_structure_one_document/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlh1wv/ideal_text_analytics_data_structure_one_document/,515405,1622035733.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"I'm not sure if this is the right place to ask but here it goes.

I started my Etsy sales dataset in the daily format:
https://imgur.com/CXmWQuE

Then I grouped them by 'year-month': 
https://imgur.com/YFBODfA

I ended up with the following:
https://imgur.com/Lu0HlRa

For months where I don't have any sales for that specific listing_id, I want to have the month and zero next to it. 

For example:
listing #902533496 from the last screenshot, I want to add these rows: 

* 2021-01            0
* 2021-02            0
* 2021-05            0

Anybody has an idea on how to do this?

Thank you!",t2_7zm6p,False,,0,False,Working on a project analyzing my Etsy sales data,[],r/datascience,False,6,projects,0,,,False,t3_nlroo8,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1622092431.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m not sure if this is the right place to ask but here it goes.&lt;/p&gt;

&lt;p&gt;I started my Etsy sales dataset in the daily format:
&lt;a href=""https://imgur.com/CXmWQuE""&gt;https://imgur.com/CXmWQuE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then I grouped them by &amp;#39;year-month&amp;#39;: 
&lt;a href=""https://imgur.com/YFBODfA""&gt;https://imgur.com/YFBODfA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I ended up with the following:
&lt;a href=""https://imgur.com/Lu0HlRa""&gt;https://imgur.com/Lu0HlRa&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For months where I don&amp;#39;t have any sales for that specific listing_id, I want to have the month and zero next to it. &lt;/p&gt;

&lt;p&gt;For example:
listing #902533496 from the last screenshot, I want to add these rows: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2021-01            0&lt;/li&gt;
&lt;li&gt;2021-02            0&lt;/li&gt;
&lt;li&gt;2021-05            0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anybody has an idea on how to do this?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlroo8,True,,maxcaliburx,,5,True,all_ads,False,[],False,,/r/datascience/comments/nlroo8/working_on_a_project_analyzing_my_etsy_sales_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlroo8/working_on_a_project_analyzing_my_etsy_sales_data/,515405,1622063631.0,1,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?auto=webp&amp;s=eb0be617377d1035b74e223904049f299e6f0ccb', 'width': 708, 'height': 654}, 'resolutions': [{'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d811f8726df5091374ed527b1135bbba2cee885a', 'width': 108, 'height': 99}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b43e5b18443a2380de3d08b2b2bbce0ae3bd941', 'width': 216, 'height': 199}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=582e4989d8166e59dfe0579b75b9382ca9dfa42e', 'width': 320, 'height': 295}, {'url': 'https://external-preview.redd.it/3jRro3PC1MRXPiW080HnvoGUDulEwpFS47wCZzRYr5o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da68722d1c6172e7c824fc9a2b10ed6ae5ed6643', 'width': 640, 'height': 591}], 'variants': {}, 'id': 'eDCvs1jx9Yhw3OQOadWU8ScsnvqaDC7lATDXidwDFG4'}], 'enabled': False}",,,,,
,datascience,"Hello, 

I am looking for some suggestions or ideas surrounding the most onerous and irksome data formats you’ve had to work with. Here’s the skinny: a very large, and in my humble opinion, just straight up malevolent company X is conducting retaliatory data requests at small public offices and institutions in a US state. One of my friends happens to work there. Now, I am all for free and open public data, 100%, but this is a nefarious attempt to drain and waste public resources and to intimidate. They are making entirely useless requests. 
 
I am a senior data analyst with a few skills here and there and would like to help my friend stick it to evil corp while still operating well within the law and legal requirements. If you were tasked with handling hundreds of thousands of emails, meeting notes, minutes, powerpoints etc.. what would be the most onerous and cumbersome file formats or organization? Like, I was thinking of converting all text to binary and writing a little python program to generate folder hierarchies and compress every single email etc.. somehow scramble things up a bit while still ensuring the data has integrity etc.. any suggestions?",t2_17fdw9,False,,0,False,Most onerous data formats?,[],r/datascience,False,6,discussion,0,,,False,t3_nlnpcx,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,False,self,False,,[],{},,True,,1622081961.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, &lt;/p&gt;

&lt;p&gt;I am looking for some suggestions or ideas surrounding the most onerous and irksome data formats you’ve had to work with. Here’s the skinny: a very large, and in my humble opinion, just straight up malevolent company X is conducting retaliatory data requests at small public offices and institutions in a US state. One of my friends happens to work there. Now, I am all for free and open public data, 100%, but this is a nefarious attempt to drain and waste public resources and to intimidate. They are making entirely useless requests. &lt;/p&gt;

&lt;p&gt;I am a senior data analyst with a few skills here and there and would like to help my friend stick it to evil corp while still operating well within the law and legal requirements. If you were tasked with handling hundreds of thousands of emails, meeting notes, minutes, powerpoints etc.. what would be the most onerous and cumbersome file formats or organization? Like, I was thinking of converting all text to binary and writing a little python program to generate folder hierarchies and compress every single email etc.. somehow scramble things up a bit while still ensuring the data has integrity etc.. any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlnpcx,True,,pythagorasshat,,25,True,all_ads,False,[],False,,/r/datascience/comments/nlnpcx/most_onerous_data_formats/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlnpcx/most_onerous_data_formats/,515405,1622053161.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_bc606vut,False,,0,False,The Economist's excess deaths model,[],r/datascience,False,6,projects,0,70.0,,False,t3_nkz602,False,dark,0.97,,public,273,0,{},140.0,,False,[],,False,False,,{},Projects,False,273,,False,False,https://b.thumbs.redditmedia.com/iKrJJ8HNG7JckPGp-pF1WOw4vG5d6YE1OSA8Bp5xZdA.jpg,False,,[],{},,False,,1622003060.0,text,6,,,text,github.com,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkz602,True,,beleeee_dat,,49,True,all_ads,False,[],False,,/r/datascience/comments/nkz602/the_economists_excess_deaths_model/,all_ads,False,https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model,515405,1621974260.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?auto=webp&amp;s=717238cd6a5f5bd38e6d1f4af2dc19afe13a4b10', 'width': 2400, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3266601f149b7853aa79b9471c064805d61ed750', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=812e850b90195dfe1a1620e3635fff178022630e', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d40fbc7a742a8568c10b2f612678d0fe8dad6b9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=741a630cbb893fa0f06e0d63ace37f73bc2da231', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1259206dc74b91d5ef844fd4fefe356520b8800', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/W-Do4TpGuAawWm3ywgAWiuzjh36ztg5uQtAOToiG1Fw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1801bcf4a2ee87e4dbdf6688d29d6125abe1cb1', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'eLGUN6fbVeJpOidBYzQIkX1RdE1KmMiaCOnXvdBkyBY'}], 'enabled': False}",,,https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model,,
,datascience,,t2_cc4pi6l7,False,,0,False,"Join Chief/ Heads of Data &amp; Analytics from Grab; Pfizer; Reckitt; Unistar Credit &amp; Finance; TymeGlobal. Talking AI, Big Data, Data Ethics",[],r/datascience,False,6,career,0,73.0,,False,t3_nltp3t,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Career,False,1,,False,False,https://a.thumbs.redditmedia.com/JkH_0qOdibB758RrS4n8caqAqE7eBjKu36-sIwtf4l4.jpg,False,,[],{},,False,,1622098324.0,text,6,,,text,cdao-asia.coriniumintelligence.com,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nltp3t,True,,Corinium_APAC,,0,True,all_ads,False,[],False,,/r/datascience/comments/nltp3t/join_chief_heads_of_data_analytics_from_grab/,all_ads,False,https://cdao-asia.coriniumintelligence.com/?utm_source=Linkedin%20Conv&amp;utm_medium=Linkedin%20Conv&amp;utm_campaign=0659%20CDAO%20ASEAN#CDAOREG,515405,1622069524.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?auto=webp&amp;s=883d06ad0202e142f3260ef55e6ea583f3fb1fd5', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fefb6c9a51af3391d0a5bd2b7bfcd618fb0441d5', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=09df3b86b117d6bb513a3062d1f00b871313e46d', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cbc189b38a9836883f326594278d043e77712f7', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=567456f0d359b33f70cab852e146790ad8b85ba6', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcd9d3d5bfd68946d63a542297bd93e314ee8763', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/xl2p2wDCLvMC-1bSfwe_HNIgYiCKsjiVNSgrDqJkY9A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa88640a5572eed141b7a62e23009fee06ebbd54', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'G4c7GSDz80CFFMKvgq_y-d4c6yzJ_Z6eS3POUeNN6SI'}], 'enabled': False}",,,https://cdao-asia.coriniumintelligence.com/?utm_source=Linkedin%20Conv&amp;utm_medium=Linkedin%20Conv&amp;utm_campaign=0659%20CDAO%20ASEAN#CDAOREG,,
,datascience,"Hi all,

I posted a question yesterday asking if anyone had any thoughts on how I could predict the rate at which employees in my would the company. 

Thank you so much for the help! I've manage to get a working model that does it based on my current employee base, but when new employee's join, I haven't quite got it working there.

What my analysis showed is that, on average, 5% of all new employees we get will leave in the first week. Due to this, we should onboard an additional 5%. Here is where the circular reference starts. We therefore onboard 5% more and so our attrition rate increases, meaning we need to onboard more.

Has anyone come across an issue like this before?  


What assumptions can I take to stop the constant circle?

Any thoughts/advice would be really appreciated all!",t2_n8vyt,False,,0,False,Forecasting employee turnover with circular reference,[],r/datascience,False,6,discussion,0,,,False,t3_nlryy8,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622093289.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I posted a question yesterday asking if anyone had any thoughts on how I could predict the rate at which employees in my would the company. &lt;/p&gt;

&lt;p&gt;Thank you so much for the help! I&amp;#39;ve manage to get a working model that does it based on my current employee base, but when new employee&amp;#39;s join, I haven&amp;#39;t quite got it working there.&lt;/p&gt;

&lt;p&gt;What my analysis showed is that, on average, 5% of all new employees we get will leave in the first week. Due to this, we should onboard an additional 5%. Here is where the circular reference starts. We therefore onboard 5% more and so our attrition rate increases, meaning we need to onboard more.&lt;/p&gt;

&lt;p&gt;Has anyone come across an issue like this before?  &lt;/p&gt;

&lt;p&gt;What assumptions can I take to stop the constant circle?&lt;/p&gt;

&lt;p&gt;Any thoughts/advice would be really appreciated all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlryy8,True,,claret_n_blue,,3,True,all_ads,False,[],False,,/r/datascience/comments/nlryy8/forecasting_employee_turnover_with_circular/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlryy8/forecasting_employee_turnover_with_circular/,515405,1622064489.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"A recruiter has sent me a paper to review including “The six primary data quality assessment” before an online assessment I will go through with him, in which he will send me a dataset to find data quality issues using Python/Pandas within an hour.

- Where to find datasets to practice on?

- What do I expect to be doing in this hour?

- How to practice and be ready in the next couple of days?

Link to the paper: https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf",t2_ryrcq,False,,0,False,How can I practice finding data quality issues before an interview assessment?,[],r/datascience,False,6,,0,,,False,t3_nlql25,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,False,self,1622063095.0,,[],{},,True,,1622089461.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A recruiter has sent me a paper to review including “The six primary data quality assessment” before an online assessment I will go through with him, in which he will send me a dataset to find data quality issues using Python/Pandas within an hour.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where to find datasets to practice on?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What do I expect to be doing in this hour?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to practice and be ready in the next couple of days?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Link to the paper: &lt;a href=""https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf""&gt;https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nlql25,True,,Abdullah_super,,1,True,all_ads,False,[],False,,/r/datascience/comments/nlql25/how_can_i_practice_finding_data_quality_issues/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlql25/how_can_i_practice_finding_data_quality_issues/,515405,1622060661.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I've been reading a lot about Palantir lately and how they are creating software that analyzes data and creates models for you. So naturally, i've been concerned that this new technology will eventually replace data analysts/scientists. I know right now they are only working with the government but from what i've read, they hope to eventually move into the private sector as well. Anyone have any thoughts on this and how this might affect the future of data science jobs? I'm worried the job will be made redundant by this tech and the more I read the more worried I get- it just makes me sad tbh",t2_6adzpsau,False,,0,False,Will Palantir replace data science teams?,[],r/datascience,False,6,career,0,,,False,t3_nm0lab,False,dark,0.27,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1622122861.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been reading a lot about Palantir lately and how they are creating software that analyzes data and creates models for you. So naturally, i&amp;#39;ve been concerned that this new technology will eventually replace data analysts/scientists. I know right now they are only working with the government but from what i&amp;#39;ve read, they hope to eventually move into the private sector as well. Anyone have any thoughts on this and how this might affect the future of data science jobs? I&amp;#39;m worried the job will be made redundant by this tech and the more I read the more worried I get- it just makes me sad tbh&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nm0lab,True,,Lanky_Seaworthiness8,,8,True,all_ads,False,[],False,,/r/datascience/comments/nm0lab/will_palantir_replace_data_science_teams/,all_ads,False,https://www.reddit.com/r/datascience/comments/nm0lab/will_palantir_replace_data_science_teams/,515405,1622094061.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm seeing this quite a lot, is this normal or are they just throwing in some buzz words they've seen for data science skills?

I've always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist's preference.",t2_4mk1e,False,,0,False,Data science job postings asking for both Python and R?,[],r/datascience,False,6,discussion,0,,,False,t3_nle5jq,False,dark,0.58,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622055545.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m seeing this quite a lot, is this normal or are they just throwing in some buzz words they&amp;#39;ve seen for data science skills?&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist&amp;#39;s preference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nle5jq,True,,Jimbobmij,,27,True,all_ads,False,[],False,,/r/datascience/comments/nle5jq/data_science_job_postings_asking_for_both_python/,all_ads,False,https://www.reddit.com/r/datascience/comments/nle5jq/data_science_job_postings_asking_for_both_python/,515405,1622026745.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm currently an engineering student, and I started my data science journey about a year ago when I discovered my passion for data. Since then, I have been self-taught all the way from MOOCs and books. I tried to apply what I learnt through personal projects and internships, but the entire time I feel like I've only been searching up, copying code and modifying it for my own needs. 

Most of the time I don't write any original code except for some simple functions or if I'm working on a platform with custom syntax. I am mostly familiar with the libraries I need, but only to the extent where I know which specific library can help me with a particular task. 

I know it's still pretty early in my DS journey, but I can't help but feel incompetent sometimes. Does anyone else have the same problem? If not, how do I overcome this and become better at writing original code?",t2_l7hoj,False,,0,False,Does anyone else feel like an incompetent programmer?,[],r/datascience,False,6,discussion,0,,,False,t3_nknmvd,False,dark,0.91,,public,144,2,{},,,False,[],,False,False,,{},Discussion,False,144,,False,False,self,False,,[],{},,True,,1621971504.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently an engineering student, and I started my data science journey about a year ago when I discovered my passion for data. Since then, I have been self-taught all the way from MOOCs and books. I tried to apply what I learnt through personal projects and internships, but the entire time I feel like I&amp;#39;ve only been searching up, copying code and modifying it for my own needs. &lt;/p&gt;

&lt;p&gt;Most of the time I don&amp;#39;t write any original code except for some simple functions or if I&amp;#39;m working on a platform with custom syntax. I am mostly familiar with the libraries I need, but only to the extent where I know which specific library can help me with a particular task. &lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s still pretty early in my DS journey, but I can&amp;#39;t help but feel incompetent sometimes. Does anyone else have the same problem? If not, how do I overcome this and become better at writing original code?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nknmvd,True,,ReaperJr,,52,True,all_ads,False,[],False,,/r/datascience/comments/nknmvd/does_anyone_else_feel_like_an_incompetent/,all_ads,False,https://www.reddit.com/r/datascience/comments/nknmvd/does_anyone_else_feel_like_an_incompetent/,515405,1621942704.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"&amp;#x200B;

So I have been working overall for around 6 months as a data analyst. The first 4 months were a full-time internship at a big4 and the last 2 months have been a full-time position at a 6 years old startup. The experience at both firms is amazing and enjoyable. However, the tasks themselves can sometimes be frustrating. 

**Problem**

The frustration comes mainly from hard to clean datasets (data inconsistency) or tasks that are not at my level of experience yet (aka challenges). We also receive in some cases datasets that are slightly modified (for no reason) which causes the script to give an error...

**Context**

We receive generally sales data from retailers, we clean them and push them to the database, so we can later on visualize, analyze, create predictive models.

**Discussion**

I would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them (stackoverflow, asked your supervisor ...). 

I wrote a [small blog here](https://dataanalystlife.blogspot.com/2021/05/data-inconsistency-in-real-life.html) about my thoughts  and stories so far (a 2 mins read). However, I would be much more interested in the real stories in the comment section than the traffic to my blog, because it will help me deal with the situations in a better way and with less frustrations on the long run.",t2_7mkrswyv,False,,0,False,Job frustrations (Data science/analysis),[],r/datascience,False,6,discussion,0,,,False,t3_nky5ei,False,dark,0.87,,public,31,0,{},,,False,[],,False,False,,{},Discussion,False,31,,False,False,self,False,,[],{},,True,,1622000320.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I have been working overall for around 6 months as a data analyst. The first 4 months were a full-time internship at a big4 and the last 2 months have been a full-time position at a 6 years old startup. The experience at both firms is amazing and enjoyable. However, the tasks themselves can sometimes be frustrating. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The frustration comes mainly from hard to clean datasets (data inconsistency) or tasks that are not at my level of experience yet (aka challenges). We also receive in some cases datasets that are slightly modified (for no reason) which causes the script to give an error...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We receive generally sales data from retailers, we clean them and push them to the database, so we can later on visualize, analyze, create predictive models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them (stackoverflow, asked your supervisor ...). &lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=""https://dataanalystlife.blogspot.com/2021/05/data-inconsistency-in-real-life.html""&gt;small blog here&lt;/a&gt; about my thoughts  and stories so far (a 2 mins read). However, I would be much more interested in the real stories in the comment section than the traffic to my blog, because it will help me deal with the situations in a better way and with less frustrations on the long run.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nky5ei,True,,Ecstatic_Tooth_1096,,18,True,all_ads,False,[],False,,/r/datascience/comments/nky5ei/job_frustrations_data_scienceanalysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/nky5ei/job_frustrations_data_scienceanalysis/,515405,1621971520.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm head of sales for a relatively small company (~20m annual sales) and do all of the sales data analysis via Power BI and I'm learning Python.  Just curious how many other people in similar positions and how you're dealing with it.  Any insight or guidance is appreciated.

I love working with data but have 20 years of sales specific experience.  Previously wrote programs/whatever you'd like to call it in Excel VBA.  Currently pulling from SQL into Power BI realizing I can't do what I want without Python/Seaborn etc.",t2_1f2mdynj,False,,0,False,How many of you are in a Sales position but doing data science for your company?,[],r/datascience,False,6,discussion,0,,,False,t3_nl8wmt,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1622034386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m head of sales for a relatively small company (~20m annual sales) and do all of the sales data analysis via Power BI and I&amp;#39;m learning Python.  Just curious how many other people in similar positions and how you&amp;#39;re dealing with it.  Any insight or guidance is appreciated.&lt;/p&gt;

&lt;p&gt;I love working with data but have 20 years of sales specific experience.  Previously wrote programs/whatever you&amp;#39;d like to call it in Excel VBA.  Currently pulling from SQL into Power BI realizing I can&amp;#39;t do what I want without Python/Seaborn etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl8wmt,True,,neededtowrite,,4,True,all_ads,False,[],False,,/r/datascience/comments/nl8wmt/how_many_of_you_are_in_a_sales_position_but_doing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl8wmt/how_many_of_you_are_in_a_sales_position_but_doing/,515405,1622005586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I find it demeaning,t2_gw1hd,False,,0,False,I'm offended by having to scale my data,[],r/datascience,False,6,fun,0,,,False,t3_nkbqx6,False,dark,0.94,,public,608,4,{},,,False,[],,False,False,,{},Fun/Trivia,False,608,,False,False,self,False,,[],{'gid_1': 3},,True,,1621928644.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I find it demeaning&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkbqx6,True,,beepbloopbloop,,40,True,all_ads,False,[],False,,/r/datascience/comments/nkbqx6/im_offended_by_having_to_scale_my_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkbqx6/im_offended_by_having_to_scale_my_data/,515405,1621899844.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_gafbm9o,False,,0,False,"I find sometimes that the most interesting or unique analysis comes from ""amateurs"". What cool projects you have seen or worked on?",[],r/datascience,False,6,discussion,0,,,False,t3_nlhan4,False,dark,0.42,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,default,False,,[],{},,False,,1622065205.0,text,6,,,text,self.TheAnalystEconomy,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlhan4,True,,Kobedoggg,,1,True,all_ads,False,[],False,,/r/datascience/comments/nlhan4/i_find_sometimes_that_the_most_interesting_or/,all_ads,False,/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/,515405,1622036405.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'TheAnalystEconomy', 'selftext': 'By ""amateur"", I do not mean inexperienced, less skilled or bad necessarily. More that the analysis came from an area of passion, rather than a professional environment.\n\nThe origin of the word ""amateur"" is French and means ""one who loves"". Amateur analysts, to me, are those who spend time on problems for their own sake because they are intrinsically motivating, beautiful or fascinating.\n\nI would love to hear about any cool projects people have worked on in their spare time, regardless of the topic. Comment below.', 'author_fullname': 't2_gafbm9o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I find sometimes that the most interesting or unique analysis comes from ""amateurs"". What cool projects you have seen or worked on?', 'link_flair_richtext': [{'e': 'text', 't': 'General discussion'}], 'subreddit_name_prefixed': 'r/TheAnalystEconomy', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nlf6ih', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'General discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622059011.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.TheAnalystEconomy', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;By &amp;quot;amateur&amp;quot;, I do not mean inexperienced, less skilled or bad necessarily. More that the analysis came from an area of passion, rather than a professional environment.&lt;/p&gt;\n\n&lt;p&gt;The origin of the word &amp;quot;amateur&amp;quot; is French and means &amp;quot;one who loves&amp;quot;. Amateur analysts, to me, are those who spend time on problems for their own sake because they are intrinsically motivating, beautiful or fascinating.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear about any cool projects people have worked on in their spare time, regardless of the topic. Comment below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dcd9e614-ad4a-11eb-9428-0e37c2f586d7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_4bo841', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#dadada', 'id': 'nlf6ih', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Kobedoggg', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/', 'subreddit_subscribers': 1661, 'created_utc': 1622030211.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",/r/TheAnalystEconomy/comments/nlf6ih/i_find_sometimes_that_the_most_interesting_or/,t3_nlf6ih,
,datascience,"TLDR: As a data scientist how important are the assumptions for the statistical methods of analysis?

Currently I am working on my MS in data science, and I've been thinking a lot about how my data science CS courses practically ignore basic statistical practices that I was taught in undergrad as a stats major. For example: the last two semesters I have taken two courses offered through the college of computer science, taught by the same professor, one: data preparation and analysis, and two: data mining. Having taken a data analysis class in undergad (a different university and offered under the college of mathematics as a statistics course) I was very well equipped for this course but I noticed that the professor overlooked a lot of things that I was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met. I didn't think much of it, this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods (though not all of the methods in this course are addressed in the prerequisite courses).

The next semester I take Data Mining, an undergraduate level course. Like I said, same professor, and a CS course. I understand that data mining might not be heavily as heavily based in stats as CS with its basis in machine learning and AI, but the stats is a piece. During our final project I was discussing with some friends the trouble I was having meeting assumptions with the dataset given (same project and dataset for everyone) and asked them how they were handling it. My friends, no stats majors, could not understand what my issues were. When I was explaining to them the assumptions for a principle component analysis (PCA) (a large part of the project) they said that I was making it a bigger deal than it needs to be, and I should just run the PCA with no check on the assumptions and move on like the example the prof provided us. Unable to get any help on my problem I did just that, turned in my project and to my surprise I got a 100%. I couldn't believe that I got no points deducted for not checking the assumptions. The previous semester I had a project too, my partner dropped the class in the middle of the semester so I did the project, assumption checks and all, on my own with little problems so there wasn't an issue.

As a statistician running analyses without checking assumptions raises huge ethical flags. How can I know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met? I went on Kaggle to look at other people's code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed. It made me wonder if in data science it was enough to run the analysis and predict, as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data.",t2_4oa8ltdj,False,,0,False,How much of a role should assumptions for statistical methods of analysis play in data science?,[],r/datascience,False,6,discussion,0,,,False,t3_nkwlgu,False,dark,0.79,,public,17,0,{},,,False,[],,False,False,,{},Discussion,False,17,,False,False,self,False,,[],{},,True,,1621996159.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;TLDR: As a data scientist how important are the assumptions for the statistical methods of analysis?&lt;/p&gt;

&lt;p&gt;Currently I am working on my MS in data science, and I&amp;#39;ve been thinking a lot about how my data science CS courses practically ignore basic statistical practices that I was taught in undergrad as a stats major. For example: the last two semesters I have taken two courses offered through the college of computer science, taught by the same professor, one: data preparation and analysis, and two: data mining. Having taken a data analysis class in undergad (a different university and offered under the college of mathematics as a statistics course) I was very well equipped for this course but I noticed that the professor overlooked a lot of things that I was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met. I didn&amp;#39;t think much of it, this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods (though not all of the methods in this course are addressed in the prerequisite courses).&lt;/p&gt;

&lt;p&gt;The next semester I take Data Mining, an undergraduate level course. Like I said, same professor, and a CS course. I understand that data mining might not be heavily as heavily based in stats as CS with its basis in machine learning and AI, but the stats is a piece. During our final project I was discussing with some friends the trouble I was having meeting assumptions with the dataset given (same project and dataset for everyone) and asked them how they were handling it. My friends, no stats majors, could not understand what my issues were. When I was explaining to them the assumptions for a principle component analysis (PCA) (a large part of the project) they said that I was making it a bigger deal than it needs to be, and I should just run the PCA with no check on the assumptions and move on like the example the prof provided us. Unable to get any help on my problem I did just that, turned in my project and to my surprise I got a 100%. I couldn&amp;#39;t believe that I got no points deducted for not checking the assumptions. The previous semester I had a project too, my partner dropped the class in the middle of the semester so I did the project, assumption checks and all, on my own with little problems so there wasn&amp;#39;t an issue.&lt;/p&gt;

&lt;p&gt;As a statistician running analyses without checking assumptions raises huge ethical flags. How can I know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met? I went on Kaggle to look at other people&amp;#39;s code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed. It made me wonder if in data science it was enough to run the analysis and predict, as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkwlgu,True,,rProgs,,34,True,all_ads,False,[],False,,/r/datascience/comments/nkwlgu/how_much_of_a_role_should_assumptions_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkwlgu/how_much_of_a_role_should_assumptions_for/,515405,1621967359.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been wracking my brains on trying to do this efficiently.  I want to learn a probability distribution of a bunch of values and sample new data points from that curve

Given a numpy array/matrix of integers/floats, I want to be able to learn the probability density function of those values(per array in matrix) and sample or generate new values or columns for the matrix as per the probability distribution of that row

If possible, group similar rows in the matrix or pick a subset and then do a signal to noise kind of thing  to generate more data points as features or columns. 

Is there any prebuilt package in numpy, sklearn or scipy that allows me to achieve this or apply it to a whole matrix at a time to scale quickly?  Even if some part of this is achievable through a package that would be a huge help",t2_69fak7f,False,,0,False,Data Augmentation idea help,[],r/datascience,False,6,discussion,0,,,False,t3_nl595z,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,True,self,False,,[],{},,True,,1622021640.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been wracking my brains on trying to do this efficiently.  I want to learn a probability distribution of a bunch of values and sample new data points from that curve&lt;/p&gt;

&lt;p&gt;Given a numpy array/matrix of integers/floats, I want to be able to learn the probability density function of those values(per array in matrix) and sample or generate new values or columns for the matrix as per the probability distribution of that row&lt;/p&gt;

&lt;p&gt;If possible, group similar rows in the matrix or pick a subset and then do a signal to noise kind of thing  to generate more data points as features or columns. &lt;/p&gt;

&lt;p&gt;Is there any prebuilt package in numpy, sklearn or scipy that allows me to achieve this or apply it to a whole matrix at a time to scale quickly?  Even if some part of this is achievable through a package that would be a huge help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl595z,True,,Nike_Zoldyck,,7,True,all_ads,False,[],False,,/r/datascience/comments/nl595z/data_augmentation_idea_help/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl595z/data_augmentation_idea_help/,515405,1621992840.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I read an interesting article on Medium that claims the Data Scientist title will go the way of the dodo in a decade. 

The TLDR is: data science tools will become as ubiquitous as MS Office products, and everyone will be expected to be skilled in using them. Titles will transition back to reflecting domain knowledge rather than skills will data.

What are your thoughts? Do you agree? Will this impact how you approach your career path?

It reminds me of Chandler making a lot of money for simple data entry. I never understood why something so easy would pay so well, but maybe the next generation will say the same of us...

 https://link.medium.com/hiXbXHrDzgb",t2_2ncqmqa0,False,,0,False,Data Scientists extinct within 10 years?,[],r/datascience,False,6,discussion,0,,,False,t3_nlgnd6,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1622063386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read an interesting article on Medium that claims the Data Scientist title will go the way of the dodo in a decade. &lt;/p&gt;

&lt;p&gt;The TLDR is: data science tools will become as ubiquitous as MS Office products, and everyone will be expected to be skilled in using them. Titles will transition back to reflecting domain knowledge rather than skills will data.&lt;/p&gt;

&lt;p&gt;What are your thoughts? Do you agree? Will this impact how you approach your career path?&lt;/p&gt;

&lt;p&gt;It reminds me of Chandler making a lot of money for simple data entry. I never understood why something so easy would pay so well, but maybe the next generation will say the same of us...&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/hiXbXHrDzgb""&gt;https://link.medium.com/hiXbXHrDzgb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nlgnd6,True,,kdawgovich,,50,True,all_ads,False,[],False,,/r/datascience/comments/nlgnd6/data_scientists_extinct_within_10_years/,all_ads,False,https://www.reddit.com/r/datascience/comments/nlgnd6/data_scientists_extinct_within_10_years/,515405,1622034586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This is a little confusing, so I'll add all the details that I know so far. We have Facebook Ads running for a product that is displayed on Amazon, but my manager wants me to create a website for this product and then analyze which Ad people are coming from.

At the same time, there's Influencer Marketing for this product as well. Unfortunately, Adobe Analytics would be an easy way to track where people are coming from, but my company doesn't use that. There are a lot of clicks on the Ads, but no one is buying the product, so I'm trying to see exactly where on the website they stop and how to optimize the site layout. My manager really wants me to create a website in order to track all of the information instead of using Amazon, and I'm not so sure if that step is needed at all.

The product started out as really successful, but now almost no one is buying it, so it seems like a weird project. 

The main issue is that I'm not sure how to even start collecting data. After I'm able to collect the data, I can then analyze it and make a model, but I don't have a background in marketing or marketing analytics, so I'm pretty lost. ",t2_7lgf0u9e,False,,0,False,Marketing Analytics - how do I track where customers are coming from and how far on the website they go to?,[],r/datascience,False,6,projects,0,,,False,t3_nl0fz7,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1622006690.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a little confusing, so I&amp;#39;ll add all the details that I know so far. We have Facebook Ads running for a product that is displayed on Amazon, but my manager wants me to create a website for this product and then analyze which Ad people are coming from.&lt;/p&gt;

&lt;p&gt;At the same time, there&amp;#39;s Influencer Marketing for this product as well. Unfortunately, Adobe Analytics would be an easy way to track where people are coming from, but my company doesn&amp;#39;t use that. There are a lot of clicks on the Ads, but no one is buying the product, so I&amp;#39;m trying to see exactly where on the website they stop and how to optimize the site layout. My manager really wants me to create a website in order to track all of the information instead of using Amazon, and I&amp;#39;m not so sure if that step is needed at all.&lt;/p&gt;

&lt;p&gt;The product started out as really successful, but now almost no one is buying it, so it seems like a weird project. &lt;/p&gt;

&lt;p&gt;The main issue is that I&amp;#39;m not sure how to even start collecting data. After I&amp;#39;m able to collect the data, I can then analyze it and make a model, but I don&amp;#39;t have a background in marketing or marketing analytics, so I&amp;#39;m pretty lost. &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl0fz7,True,,SnooWalruses4775,,3,True,all_ads,False,[],False,,/r/datascience/comments/nl0fz7/marketing_analytics_how_do_i_track_where/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl0fz7/marketing_analytics_how_do_i_track_where/,515405,1621977890.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hey there! Long time lurker, first time poster. I love this sub, I learned quite a lot and I am very thankful for all the interactions. 
About my post, I work as a DS in the risk area of a bank, nonetheless, lately my boss has been taking away my projects and giving them to someone else, also cancelling my meetings and engaging less with me. I fear he is trying to make me quit, management has used this tactic to force other employees to quit before.
I decided to look for other jobs, no luck so far, and I read a bunch of articles suggesting me to do freelance work. My github portfolio is wanting and I am asking for any and all advice from the community:
What is your experience doing freelance? I am setting my account on upwork atm.
How may I get noticed? Any experience you can give a n00b like me? Can I get a living wage from freelance?
Much appreciated!",t2_4oymsu8g,False,,0,False,Freelance experience as a DS you may share,[],r/datascience,False,6,,0,,,False,t3_nky62p,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1622000370.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there! Long time lurker, first time poster. I love this sub, I learned quite a lot and I am very thankful for all the interactions. 
About my post, I work as a DS in the risk area of a bank, nonetheless, lately my boss has been taking away my projects and giving them to someone else, also cancelling my meetings and engaging less with me. I fear he is trying to make me quit, management has used this tactic to force other employees to quit before.
I decided to look for other jobs, no luck so far, and I read a bunch of articles suggesting me to do freelance work. My github portfolio is wanting and I am asking for any and all advice from the community:
What is your experience doing freelance? I am setting my account on upwork atm.
How may I get noticed? Any experience you can give a n00b like me? Can I get a living wage from freelance?
Much appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nky62p,True,,another_dissapoint,,2,True,all_ads,False,[],False,,/r/datascience/comments/nky62p/freelance_experience_as_a_ds_you_may_share/,all_ads,False,https://www.reddit.com/r/datascience/comments/nky62p/freelance_experience_as_a_ds_you_may_share/,515405,1621971570.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I just started a lead role with my company and have was curious if anyone else has done something similar. Basically, I will be leading (and helping develop) efforts towards curating a use case list, choosing, of that list, what we'll be working on, based on business value, and executing those solutions. This effort started a couple of weeks before I began the new role, so I am catching up. Our executive leader has some passion around developing and using some tools we do not already have, so there is a bit of focus on graph analytics right now, but other ML and AI solutions are not out of the question, I just need work with my team to decide on the use cases and create a document for our plan. 

I've done this for other projects that were not DS related, but was curious if anyone has any sort of template or guidance for a format that worked really well for them. Maybe even an outline of what should be included and how it should be organized. I don't have a lot of direction, since I am being asked to run with this, so I thought this community might have some good suggestions. Thanks so much!",t2_v7o60,False,,0,False,Roadmap / Plan Documentation for DS Projects,[],r/datascience,False,6,discussion,0,,,False,t3_nkzz3e,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1622005439.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just started a lead role with my company and have was curious if anyone else has done something similar. Basically, I will be leading (and helping develop) efforts towards curating a use case list, choosing, of that list, what we&amp;#39;ll be working on, based on business value, and executing those solutions. This effort started a couple of weeks before I began the new role, so I am catching up. Our executive leader has some passion around developing and using some tools we do not already have, so there is a bit of focus on graph analytics right now, but other ML and AI solutions are not out of the question, I just need work with my team to decide on the use cases and create a document for our plan. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve done this for other projects that were not DS related, but was curious if anyone has any sort of template or guidance for a format that worked really well for them. Maybe even an outline of what should be included and how it should be organized. I don&amp;#39;t have a lot of direction, since I am being asked to run with this, so I thought this community might have some good suggestions. Thanks so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkzz3e,True,,azdatasci,,2,True,all_ads,False,[],False,,/r/datascience/comments/nkzz3e/roadmap_plan_documentation_for_ds_projects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkzz3e/roadmap_plan_documentation_for_ds_projects/,515405,1621976639.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

At work, I am looking to predict how many employees will leave the company, on a weekly basis.

We contract a lot of our staff, so every week, we can have new joiners and those that leave (either due to their contracts ending, poor performance, etc).

Depending how work is going, we may be looking to hire 10 new contractors every week, with around 3/4 leaving. However in times when we get particularly busy, I want to be able to predict that more employees will leave and so we can tell HR to hire more contractors.

The data I have available (but can try and get more) is:

\- Total number of employees that have left every week

\- Total number of employees we hire every week

\- Current end of contracts/rating of contractors (those with particularly poor ratings, we will look to terminate).

&amp;#x200B;

Can someone help me understand which direction I need to go in to do my forecasting? Just to confirm, I'm asking for you to point me in the direction, as I have never done this before!  


I've read a few articles on Python that try and predict which of your current crop of employees will leave, but as there is a chance that our contractors can change, I need this model to be future proof.

&amp;#x200B;

My initial thoughts were to create a basic model in Excel, where I calculate a baseline using the past 4 weeks actuals, then apply an uplift in weeks with a high end of contract, and when we see high number of low rated contractors. 

This is quite manual though, so I was wondering if there was a more clever and robust way to do  this.

&amp;#x200B;

Any advice/information would hugely appreciated.

&amp;#x200B;

Thanks all",t2_n8vyt,False,,0,False,Predicting number of employees that will leave for a high turnover workforce,[],r/datascience,False,6,discussion,0,,,False,t3_nkurf1,False,dark,0.65,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1621991350.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;At work, I am looking to predict how many employees will leave the company, on a weekly basis.&lt;/p&gt;

&lt;p&gt;We contract a lot of our staff, so every week, we can have new joiners and those that leave (either due to their contracts ending, poor performance, etc).&lt;/p&gt;

&lt;p&gt;Depending how work is going, we may be looking to hire 10 new contractors every week, with around 3/4 leaving. However in times when we get particularly busy, I want to be able to predict that more employees will leave and so we can tell HR to hire more contractors.&lt;/p&gt;

&lt;p&gt;The data I have available (but can try and get more) is:&lt;/p&gt;

&lt;p&gt;- Total number of employees that have left every week&lt;/p&gt;

&lt;p&gt;- Total number of employees we hire every week&lt;/p&gt;

&lt;p&gt;- Current end of contracts/rating of contractors (those with particularly poor ratings, we will look to terminate).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can someone help me understand which direction I need to go in to do my forecasting? Just to confirm, I&amp;#39;m asking for you to point me in the direction, as I have never done this before!  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read a few articles on Python that try and predict which of your current crop of employees will leave, but as there is a chance that our contractors can change, I need this model to be future proof.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My initial thoughts were to create a basic model in Excel, where I calculate a baseline using the past 4 weeks actuals, then apply an uplift in weeks with a high end of contract, and when we see high number of low rated contractors. &lt;/p&gt;

&lt;p&gt;This is quite manual though, so I was wondering if there was a more clever and robust way to do  this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any advice/information would hugely appreciated.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks all&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkurf1,True,,claret_n_blue,,16,True,all_ads,False,[],False,,/r/datascience/comments/nkurf1/predicting_number_of_employees_that_will_leave/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkurf1/predicting_number_of_employees_that_will_leave/,515405,1621962550.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"These last couple of years, I've spent a lot of time writing SQL. I put together some lessons learned to use it effectively for Data Science projects.

Small things like using CTEs, auto-formatting, and jinja have made a huge difference for me.

What other recommendations you have to master SQL for Data Science?

[https://ploomber.io/posts/sql/](https://ploomber.io/posts/sql/)",t2_5r54hksr,False,,0,False,Effective SQL for Data Science,[],r/datascience,False,6,discussion,0,,,False,t3_nka0nu,False,dark,0.94,,public,119,0,{},,,False,[],,False,False,,{},Discussion,False,119,,False,True,self,False,,[],{},,True,,1621923595.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;These last couple of years, I&amp;#39;ve spent a lot of time writing SQL. I put together some lessons learned to use it effectively for Data Science projects.&lt;/p&gt;

&lt;p&gt;Small things like using CTEs, auto-formatting, and jinja have made a huge difference for me.&lt;/p&gt;

&lt;p&gt;What other recommendations you have to master SQL for Data Science?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://ploomber.io/posts/sql/""&gt;https://ploomber.io/posts/sql/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nka0nu,True,,ploomber-io,,55,True,all_ads,False,[],False,,/r/datascience/comments/nka0nu/effective_sql_for_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nka0nu/effective_sql_for_data_science/,515405,1621894795.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am using programs like node2vec, graph2vec, doc2vec etc to build a knowledge graph. The output of all of them comes in 128 length vectors.

Is it feasible to try and compress the length to 2 or 3 dimensions so that I might visualize what any of the above methods have accomplished? If it it feasible than can I get pointed in the right direction for python libraries, packages, etc? Does not need to be perfect... just needs to give more than a csv file lol.

Thank you!",t2_wwwre7,False,,0,False,Visualizing Multidimensional Data... 128 Dimensions ---&gt; 2D or 3D,[],r/datascience,False,6,tooling,0,,,False,t3_nl3j1f,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1622015961.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using programs like node2vec, graph2vec, doc2vec etc to build a knowledge graph. The output of all of them comes in 128 length vectors.&lt;/p&gt;

&lt;p&gt;Is it feasible to try and compress the length to 2 or 3 dimensions so that I might visualize what any of the above methods have accomplished? If it it feasible than can I get pointed in the right direction for python libraries, packages, etc? Does not need to be perfect... just needs to give more than a csv file lol.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nl3j1f,True,,smok1naces,,5,True,all_ads,False,[],False,,/r/datascience/comments/nl3j1f/visualizing_multidimensional_data_128_dimensions/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl3j1f/visualizing_multidimensional_data_128_dimensions/,515405,1621987161.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys,

I recently got a position of a PO of a data science team. Previously I always worked with classic engineering teams. Needless to say, I quickly come to realize that 1) data science team != engineering team in terms of way of working and approach to work 2) I probably could use some development of how to be a better PO for such a team.

&amp;#x200B;

What I would like to ask you guys about:

1. Could you recommend a good book / training / materials to become better in working with a data science team?
2. Could you recommend a good blog / web-site where I, as a PO can read updates in data science community that might be relevant for me? (by now I typically find resources which are more focused on tech issues / updates which would be more relevant for the ML engineers, data scientists) 

Thank you!",t2_45py034y,False,,0,False,Advice needed - PO for data science team,[],r/datascience,False,6,education,0,,,False,t3_nkqyll,False,dark,0.75,,public,6,0,{},,,False,[],,False,False,,{},Education,False,6,,False,False,self,False,,[],{},,True,,1621981461.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;I recently got a position of a PO of a data science team. Previously I always worked with classic engineering teams. Needless to say, I quickly come to realize that 1) data science team != engineering team in terms of way of working and approach to work 2) I probably could use some development of how to be a better PO for such a team.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I would like to ask you guys about:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Could you recommend a good book / training / materials to become better in working with a data science team?&lt;/li&gt;
&lt;li&gt;Could you recommend a good blog / web-site where I, as a PO can read updates in data science community that might be relevant for me? (by now I typically find resources which are more focused on tech issues / updates which would be more relevant for the ML engineers, data scientists) &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkqyll,True,,ohshouldi,,8,False,all_ads,False,[],False,,/r/datascience/comments/nkqyll/advice_needed_po_for_data_science_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkqyll/advice_needed_po_for_data_science_team/,515405,1621952661.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Hi datascience,
My team at work has been asked to share some of our work at an upcoming event. from a best-practices perspective, how much of our methodology should we share? 

My concern is that much of our data is publicly available and the rest can be purchased easily. Our models are largely off the shelf or require minimal tuning. Our innovations are really elegant and not hard to reproduce. Should I be at all worried that a larger, more experienced DS team will piggyback off of our efforts and compete for our lucrative contracts? What parts of data + model + configuration should be kept obscure?",t2_4hepb,False,,0,False,How much to share in a public presentation,[],r/datascience,False,6,discussion,0,,,False,t3_nkyiwc,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1622001312.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi datascience,
My team at work has been asked to share some of our work at an upcoming event. from a best-practices perspective, how much of our methodology should we share? &lt;/p&gt;

&lt;p&gt;My concern is that much of our data is publicly available and the rest can be purchased easily. Our models are largely off the shelf or require minimal tuning. Our innovations are really elegant and not hard to reproduce. Should I be at all worried that a larger, more experienced DS team will piggyback off of our efforts and compete for our lucrative contracts? What parts of data + model + configuration should be kept obscure?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkyiwc,True,,theinexplicablefuzz,,2,True,all_ads,False,[],False,,/r/datascience/comments/nkyiwc/how_much_to_share_in_a_public_presentation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkyiwc/how_much_to_share_in_a_public_presentation/,515405,1621972512.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Edit: Disclaimer, not 'stress' burnout, more like, feels like a total dead end role now, can't go on much longer, like Peter Gibbons from Office Space, that's how I feel.

Ok, context in a nutshell, been at this tech company for about 4yrs in a mixture of roles from product to market research, but been working as a data analyst for the last 2yrs, nearly 2.5

My goal had been (and agreed with HR) to progress towards a data scientist role when I originally signed up to it. But this has not materialised.

Fast forward 2.5yrs, I've had absolutely nobody in the business to learn from(never had), everything has been self-taught, my day to day work has been much more BI related (dashboards, product stats etc) as opposed to core data science tasks. I really have to go out of my way if I want to write and code, even SQL (data is all maintained by gatekeepers, mostly inefficient ones, so I have to request it mostly, then it's just an out the box plug in to PowerBI/Tableau or similar)

I've absolutely maxed out my patience with this company and their talk the talk grand plans. My technical skills have totally stagnated, and if I was to compare it to when I left uni, my skills regressed in areas such as advanced mathematical/statistical techniques. My day to day tasks typically involve adhoc requests solved with Tableau (with absolutely horrendous data quality). There are some back end things involving SQL/Mongo/AWS but these are probably 10% overall if even that. When it comes to addressing the data, I'm more like a project manager, because I'm not allowed to fix or implement things myself. So I end up making requests for things and gathering requirements like a PO/BA/ProjMan hybrid role, but even that is a waste of space, e.g. simple request for consumer product usage? Dead in the water for over a year despite weekly meetings because the development team won't implement changes needed to track them (new POs can be seen mentally questioning life choices when the answer to 'so where can I see the usage stats' is, ""they don't exist"")

The only thing it has that appeals is a degree of autonomy with what I do once I finish the adhoc requests. I easily have 50% of my working hours to address anything I like (in my more motivated, enthusiastic days this was spent addressing the data quality issues, data warehouse etc, but the way these ideas and suggestions were valued/met just killed my soul; options and recommendations closed due to 'time', 'cost', 'effort' etc

I'm thinking of just self teaching the core data science content I wanted to initially learn (I'm already doing this, just not applying it to anything at work), and finding things to apply this to at work. 

The problem is, I'm all alone. Nobody in this company has a data science background, so I can't learn from anyone here. I'll have no idea if what I am doing is good practice or even straight up wrong. Even if I did, the data is such bad quality and held in rigid, closed off systems (I am always refused direct access to it) that I'm not convinced there's a lot of real insight to get from it

Given that I have no real world data science experience, I'll be competing for entry roles in a competitive job market. So I'm really not sure if I should stick at this place and do my own thing on the job (flying blind, probably with low output) or just quit, spend summer selfteaching and practicing on projects I actually care about personally, while applying for jobs (have some master's degree offers in September if shit hits fan)",t2_1w1o79i7,False,,0,False,"Completely and totally 'burnt out' at work, unsure if I should quit or just go rouge and do my own core data science based projects?",[],r/datascience,False,6,career,0,,,False,t3_njyxnj,False,dark,0.94,,public,191,0,{},,,False,[],,False,False,,{},Career,False,191,,False,False,self,1621867899.0,,[],{},,True,,1621894438.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: Disclaimer, not &amp;#39;stress&amp;#39; burnout, more like, feels like a total dead end role now, can&amp;#39;t go on much longer, like Peter Gibbons from Office Space, that&amp;#39;s how I feel.&lt;/p&gt;

&lt;p&gt;Ok, context in a nutshell, been at this tech company for about 4yrs in a mixture of roles from product to market research, but been working as a data analyst for the last 2yrs, nearly 2.5&lt;/p&gt;

&lt;p&gt;My goal had been (and agreed with HR) to progress towards a data scientist role when I originally signed up to it. But this has not materialised.&lt;/p&gt;

&lt;p&gt;Fast forward 2.5yrs, I&amp;#39;ve had absolutely nobody in the business to learn from(never had), everything has been self-taught, my day to day work has been much more BI related (dashboards, product stats etc) as opposed to core data science tasks. I really have to go out of my way if I want to write and code, even SQL (data is all maintained by gatekeepers, mostly inefficient ones, so I have to request it mostly, then it&amp;#39;s just an out the box plug in to PowerBI/Tableau or similar)&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve absolutely maxed out my patience with this company and their talk the talk grand plans. My technical skills have totally stagnated, and if I was to compare it to when I left uni, my skills regressed in areas such as advanced mathematical/statistical techniques. My day to day tasks typically involve adhoc requests solved with Tableau (with absolutely horrendous data quality). There are some back end things involving SQL/Mongo/AWS but these are probably 10% overall if even that. When it comes to addressing the data, I&amp;#39;m more like a project manager, because I&amp;#39;m not allowed to fix or implement things myself. So I end up making requests for things and gathering requirements like a PO/BA/ProjMan hybrid role, but even that is a waste of space, e.g. simple request for consumer product usage? Dead in the water for over a year despite weekly meetings because the development team won&amp;#39;t implement changes needed to track them (new POs can be seen mentally questioning life choices when the answer to &amp;#39;so where can I see the usage stats&amp;#39; is, &amp;quot;they don&amp;#39;t exist&amp;quot;)&lt;/p&gt;

&lt;p&gt;The only thing it has that appeals is a degree of autonomy with what I do once I finish the adhoc requests. I easily have 50% of my working hours to address anything I like (in my more motivated, enthusiastic days this was spent addressing the data quality issues, data warehouse etc, but the way these ideas and suggestions were valued/met just killed my soul; options and recommendations closed due to &amp;#39;time&amp;#39;, &amp;#39;cost&amp;#39;, &amp;#39;effort&amp;#39; etc&lt;/p&gt;

&lt;p&gt;I&amp;#39;m thinking of just self teaching the core data science content I wanted to initially learn (I&amp;#39;m already doing this, just not applying it to anything at work), and finding things to apply this to at work. &lt;/p&gt;

&lt;p&gt;The problem is, I&amp;#39;m all alone. Nobody in this company has a data science background, so I can&amp;#39;t learn from anyone here. I&amp;#39;ll have no idea if what I am doing is good practice or even straight up wrong. Even if I did, the data is such bad quality and held in rigid, closed off systems (I am always refused direct access to it) that I&amp;#39;m not convinced there&amp;#39;s a lot of real insight to get from it&lt;/p&gt;

&lt;p&gt;Given that I have no real world data science experience, I&amp;#39;ll be competing for entry roles in a competitive job market. So I&amp;#39;m really not sure if I should stick at this place and do my own thing on the job (flying blind, probably with low output) or just quit, spend summer selfteaching and practicing on projects I actually care about personally, while applying for jobs (have some master&amp;#39;s degree offers in September if shit hits fan)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njyxnj,True,,tea_horse,,49,True,all_ads,False,[],False,,/r/datascience/comments/njyxnj/completely_and_totally_burnt_out_at_work_unsure/,all_ads,False,https://www.reddit.com/r/datascience/comments/njyxnj/completely_and_totally_burnt_out_at_work_unsure/,515405,1621865638.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Hi,

I'm working on a project where I need to automatically create a large number of decision tree, and to prune them.

The only way to do that is a function like that :

    def do_best_tree(Xtrain, ytrain, Xtest, ytest):
        clf = DecisionTreeClassifier()
        clf.fit(Xtrain, ytrain)
        path = clf.cost_complexity_pruning_path(Xtrain, ytrain)
        ccp_alphas = path.ccp_alphas
        clfs = []
        if len(ccp_alphas) &gt; 100:
            nb = 2
        else:
            nb=1
        for ccp_alpha in tqdm(ccp_alphas[::nb]):
            clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
            clf.fit(Xtrain, ytrain)
            clfs.append(clf)
        return max(clfs, key=lambda x:x.score(Xtest, ytest))
    

So it take a huge amount of time, as it fit a lot of trees. See the

       if len(ccp_alphas) &gt; 100:
            nb = 2

I added in order to divide by two the number of trees, but it still very slow.

Isn't there another way to do that ? For example by really pruning a single tree ? I'm not limited to scikit-learn, but didn't find anything that is doing that....

Thank you in advance !",t2_1lbcpzsa,False,,0,False,Efficient Decision Tree Pruning in Python,[],r/datascience,False,6,tooling,0,,,False,t3_nkpidl,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1621977344.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a project where I need to automatically create a large number of decision tree, and to prune them.&lt;/p&gt;

&lt;p&gt;The only way to do that is a function like that :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def do_best_tree(Xtrain, ytrain, Xtest, ytest):
    clf = DecisionTreeClassifier()
    clf.fit(Xtrain, ytrain)
    path = clf.cost_complexity_pruning_path(Xtrain, ytrain)
    ccp_alphas = path.ccp_alphas
    clfs = []
    if len(ccp_alphas) &amp;gt; 100:
        nb = 2
    else:
        nb=1
    for ccp_alpha in tqdm(ccp_alphas[::nb]):
        clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
        clf.fit(Xtrain, ytrain)
        clfs.append(clf)
    return max(clfs, key=lambda x:x.score(Xtest, ytest))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So it take a huge amount of time, as it fit a lot of trees. See the&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   if len(ccp_alphas) &amp;gt; 100:
        nb = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I added in order to divide by two the number of trees, but it still very slow.&lt;/p&gt;

&lt;p&gt;Isn&amp;#39;t there another way to do that ? For example by really pruning a single tree ? I&amp;#39;m not limited to scikit-learn, but didn&amp;#39;t find anything that is doing that....&lt;/p&gt;

&lt;p&gt;Thank you in advance !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkpidl,True,,ez613,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/,515405,1621948544.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I graduated undergrad in late 2000’s, grad school in early 2010’s. I’m a statistician focused mostly in the social science, medical, health insurance, and public health spheres. 

Around 2010-2012, all I heard was “you gotta learn R” so I did and I’m pretty proficient with it now. Then I heard you gotta learn SQL, and now I’m an intermediate SQL user, I mostly just use it in SAS commands.  It’s great with large datasets. 

Then in the mid 2010’s you gotta learn Tableau, check easy enough. Then the tidyverse package,... which has a lot of overlap with SQL skills anyways. Then other packages then a few job ads wanted SPSS-Amos and/or Mplus. 

Now almost every job posting for statistical / data analytics type roles I’ve come across wants me to know python. I’m sure I’ll pick it up eventually. But I can do most of what I need to do in R (and other tools). Hiring committees can’t explain why they want me to know python. I think they just stick it on there because it sounds cool. Wow snake programming language?

What’s up with these committees and job ads and their affinity towards python, yet they don’t know the value or applicability or necessity of it in these roles?",t2_5dpllzer,False,,0,False,What’s the deal with python becoming more popular in job ads?,[],r/datascience,False,6,,0,,,False,t3_nl02h9,False,dark,0.35,,public,0,0,{},,,False,[],,False,False,,{},Job Search,False,0,,False,True,self,1622030721.0,,[],{},,True,,1622005676.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I graduated undergrad in late 2000’s, grad school in early 2010’s. I’m a statistician focused mostly in the social science, medical, health insurance, and public health spheres. &lt;/p&gt;

&lt;p&gt;Around 2010-2012, all I heard was “you gotta learn R” so I did and I’m pretty proficient with it now. Then I heard you gotta learn SQL, and now I’m an intermediate SQL user, I mostly just use it in SAS commands.  It’s great with large datasets. &lt;/p&gt;

&lt;p&gt;Then in the mid 2010’s you gotta learn Tableau, check easy enough. Then the tidyverse package,... which has a lot of overlap with SQL skills anyways. Then other packages then a few job ads wanted SPSS-Amos and/or Mplus. &lt;/p&gt;

&lt;p&gt;Now almost every job posting for statistical / data analytics type roles I’ve come across wants me to know python. I’m sure I’ll pick it up eventually. But I can do most of what I need to do in R (and other tools). Hiring committees can’t explain why they want me to know python. I think they just stick it on there because it sounds cool. Wow snake programming language?&lt;/p&gt;

&lt;p&gt;What’s up with these committees and job ads and their affinity towards python, yet they don’t know the value or applicability or necessity of it in these roles?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nl02h9,True,,Weird_Surname,,25,True,all_ads,False,[],False,,/r/datascience/comments/nl02h9/whats_the_deal_with_python_becoming_more_popular/,all_ads,False,https://www.reddit.com/r/datascience/comments/nl02h9/whats_the_deal_with_python_becoming_more_popular/,515405,1621976876.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I’m wondering if I should stay a generalist (there’s always stuff to learn) or if I should hone in one niche area (NLP, computer vision, causal inference or bayesian stats)

It seems like the ideal data scientist is an expert in all of those areas, even though in reality it’s quite difficult. We all sorta learn what we need depending on the problem we’re tryna solve

Update: seems like most of y’all are recommending be a generalist unless I’m passionate about a specific area",t2_7nbpziqx,False,,0,False,Specialize in one area or be a generalist?,[],r/datascience,False,6,career,0,,,False,t3_nkg3d9,False,dark,0.84,,public,9,1,{},,,False,[],,False,False,,{},Career,False,9,,False,False,self,1621954676.0,,[],{},,True,,1621942284.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m wondering if I should stay a generalist (there’s always stuff to learn) or if I should hone in one niche area (NLP, computer vision, causal inference or bayesian stats)&lt;/p&gt;

&lt;p&gt;It seems like the ideal data scientist is an expert in all of those areas, even though in reality it’s quite difficult. We all sorta learn what we need depending on the problem we’re tryna solve&lt;/p&gt;

&lt;p&gt;Update: seems like most of y’all are recommending be a generalist unless I’m passionate about a specific area&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 20, 'id': 'award_abb865cf-620b-4219-8777-3658cf9091fb', 'penny_donate': 0, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Starstruck_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Can't stop seeing stars"", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Starstruck', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=16&amp;height=16&amp;auto=webp&amp;s=800ea0775a3f25602bfab03058d64d25352c04d2', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=32&amp;height=32&amp;auto=webp&amp;s=1d4be9117f8e389c54e0a7e23918355d7d2df185', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=48&amp;height=48&amp;auto=webp&amp;s=45443e65acd1cf76585f7c9d904e1484f89db521', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=64&amp;height=64&amp;auto=webp&amp;s=f4164f96ab1df25de2024b8b65e9ce91d3424c86', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=128&amp;height=128&amp;auto=webp&amp;s=b09819f49e26e5ad518dbdf1aa69f0916d514c6e', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkg3d9,True,,Affectionate_Shine55,,10,True,all_ads,False,[],False,,/r/datascience/comments/nkg3d9/specialize_in_one_area_or_be_a_generalist/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkg3d9/specialize_in_one_area_or_be_a_generalist/,515405,1621913484.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I’m a data science n00b and was recently asked to look into graph2vec... I ran the ex. program and it outputs a csv file with 128 dimensions... is their anyway to visualize this so that one can roughly see what is happening?

Thanks!",t2_wwwre7,False,,0,False,Visualizing graph2vec,[],r/datascience,False,6,tooling,0,,,False,t3_nkptwy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,,[],{},,True,,1621978282.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a data science n00b and was recently asked to look into graph2vec... I ran the ex. program and it outputs a csv file with 128 dimensions... is their anyway to visualize this so that one can roughly see what is happening?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkptwy,True,,smok1naces,,1,True,all_ads,False,[],False,,/r/datascience/comments/nkptwy/visualizing_graph2vec/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkptwy/visualizing_graph2vec/,515405,1621949482.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"In statistics, we are always warned about the ""bias-variance tradeoff"":  simple statistical models are reliable, but are generally unable to sufficiently capture the complexity within the data (i.e. high bias, low variance) ; complex statistical models are able to capture complexity within the data, but are generally not as reliable when generalizing to new data (i.e. high variance, low bias).

This leads me to my questions:

1) Are successful statistical models able to defy the ""bias-variance tradeoff""? As a simple example, consider the famous ""iris dataset"".  Kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data. Are these statistical models defying the ""bias-variance tradeoff""? Now, let's imagine a far more complicated problem and dataset - but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data : are we again defying the ""bias-variance tradeoff""?

2) I have seen proofs that show how the MSE (mean squared error) can be decomposed into a ""bias term"" and a ""variance term"". Thus, for a given statistical model : for a fixed value of this model's MSE, if the variance is high then the bias must be low in order to compensate (and vice versa).

My question relates to the following : when people discuss the variance in the ""bias-variance tradeoff"", they are generally interested in the variance of a statistical model's performance when dealing with unseen data. Since this unseen data might not even exist at the moment, how is the ""bias-variance tradeoff"" able to make claims about unseen data? Is the ""bias-variance tradeoff"" a general idea with some theoretical foundations? Or is it mainly empirical?

3) Finally, how does the ""bias-variance tradeoff"" apply to real world models such as the ""self driving car"" , ""alpha go"" and computers playing tetris? Or in the case of reinforcement learning models, the ""bias-variance tradeoff"" does not apply the same way it does in supervised learning models?

Thanks",t2_o4xj9,False,,0,False,"Do successful models defy the ""bias-variance tradeoff""?",[],r/datascience,False,6,discussion,0,,,False,t3_nkfa84,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1621939663.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In statistics, we are always warned about the &amp;quot;bias-variance tradeoff&amp;quot;:  simple statistical models are reliable, but are generally unable to sufficiently capture the complexity within the data (i.e. high bias, low variance) ; complex statistical models are able to capture complexity within the data, but are generally not as reliable when generalizing to new data (i.e. high variance, low bias).&lt;/p&gt;

&lt;p&gt;This leads me to my questions:&lt;/p&gt;

&lt;p&gt;1) Are successful statistical models able to defy the &amp;quot;bias-variance tradeoff&amp;quot;? As a simple example, consider the famous &amp;quot;iris dataset&amp;quot;.  Kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data. Are these statistical models defying the &amp;quot;bias-variance tradeoff&amp;quot;? Now, let&amp;#39;s imagine a far more complicated problem and dataset - but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data : are we again defying the &amp;quot;bias-variance tradeoff&amp;quot;?&lt;/p&gt;

&lt;p&gt;2) I have seen proofs that show how the MSE (mean squared error) can be decomposed into a &amp;quot;bias term&amp;quot; and a &amp;quot;variance term&amp;quot;. Thus, for a given statistical model : for a fixed value of this model&amp;#39;s MSE, if the variance is high then the bias must be low in order to compensate (and vice versa).&lt;/p&gt;

&lt;p&gt;My question relates to the following : when people discuss the variance in the &amp;quot;bias-variance tradeoff&amp;quot;, they are generally interested in the variance of a statistical model&amp;#39;s performance when dealing with unseen data. Since this unseen data might not even exist at the moment, how is the &amp;quot;bias-variance tradeoff&amp;quot; able to make claims about unseen data? Is the &amp;quot;bias-variance tradeoff&amp;quot; a general idea with some theoretical foundations? Or is it mainly empirical?&lt;/p&gt;

&lt;p&gt;3) Finally, how does the &amp;quot;bias-variance tradeoff&amp;quot; apply to real world models such as the &amp;quot;self driving car&amp;quot; , &amp;quot;alpha go&amp;quot; and computers playing tetris? Or in the case of reinforcement learning models, the &amp;quot;bias-variance tradeoff&amp;quot; does not apply the same way it does in supervised learning models?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkfa84,True,,blueest,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkfa84/do_successful_models_defy_the_biasvariance/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkfa84/do_successful_models_defy_the_biasvariance/,515405,1621910863.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Prior Post for context: https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/

Sadly I think it was an air raid for the most part... and I don't think I did very well.

I didn't really get many rapid fire trivia questions, although I got some, and I had to answer a couple with ""I don't know."" For example I was asked what is the mathematical equation to calculate p-values... I answered with ""I don't remember? I would have to consult my college notes from 11 years ago"". In my head I was thinking, why does it matter when scikit learn or R does that all under the hood? There was another trivia question I got which I answered I don't know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.

I got some data engineering questions(not my strong suit) and I don't believe I gave good answers. Everywhere I have been I have worked along side with data engineering teams but have never actually done the data engineering work. When I got to the point of the interview where I could ask questions I asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so I have no idea why I was even asked these questions.

I think my strong suit was using my past work experiences to outline how I got things done, but even then I am not sure if my responses were good enough because I felt like I was on defense from the get go. I do better in interviews when I am ""on offense"", so to speak. For example I was asked for an example of how I segmented customers in the past and used that to drive revenue growth. I said I would look at various segmentation techniques and choose the best one based on model performance against test data, and how I determined that to be K-Means clustering after using PCA to reduce the number of features and using a scree plot to determine the number of segments. And how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. I don't think I was precise or deep enough in this answer and was too vague and I didn't project enough confidence. But, in an hour long verbal interview, how deep am I supposed to go? I struggled with finding the right balance of depth and respecting the time constraints.

I'll be surprised if I don't get rejected... but I feel like this format was tough to succeed in. I believe I am a strong data scientist, but I am at my best when I know exactly what scenario I am facing. Questions like ""Here is what we want to accomplish, how would you do it?"" are what I do best, I didn't get any of those questions. It was more open ended broad questions such as ""What is your go to machine learning model"" and I literally could have given anything. It depends on the task at hand. I said Random Forest to go with the question and how I like the built in OOB functionality in R. Although if I could answer the question again I'd go with Gradient Boosting. 

Another question I hated was ""How do you stay on top of data science industry trends"". I didn't really know how to answer that one. I said I stay engaged with publications and gave a personal example of what I do in my free time(that I won't say since I don't want to out myself incase those who interviewed lurk here) but I don't like this because I believe you stick with what works and you don't always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you'd probably know about it without having to be proactive to learn about it.

Another question was ""Do you follow up on your own models performance against real world data?"". I didn't have a problem with this question because of course I do. Who doesn't? I said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. Therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. Then they followed up with ""What if you gained more responsibilities on your plate and could no longer follow up on all of your models?"", which I think was a silly follow up, because I feel that is putting the cart before the horse. I said in a hypothetical scenario where my bandwidth was overextended to where I could not do such a thing, as I hope it would not get to that point, I would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.

I got asked ""How do you perform feature selection"" and I simply explained various ways to do it based on the model at hand(i.e. p-Values for linear regression, feature score for Random Forest, etc.). I didn't like this question because its something you would ask an entry level candidate, not someone interviewing for director. I answered it anyway but with only an hour, I feel like better questions could have been asked.

The interview was solely to gauge my technical abilities, not my leadership or management abilities, so I understand that the interview was aimed to do that. With that being said I don't think I answered the questions given to me with enough confidence or precision. I am good at gauging body language, facial expressions and tone of voice of those interviewing me and I don't think they were impressed. If its up to these 2 guys I don't have a chance. The only way I have a chance is if all I needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. 

Comparing this interview to the 3 bad ones I had before I think this one was definitely the worst of the bunch in terms of how I gauged my performance. I am starting to lose confidence in my interviewing skills, as I believe I have a hard time translating my actual 10 year track record into verbal competency. I know how to do things, I just have a hard time coherently explaining how those things are done.

Fortunately... I have another interview starting in 10 minutes... and 5 others lined up this week. But this was the job I really wanted out of all of them.",t2_y5gxz,False,,0,False,"UPDATE: Just had my interview, was it an ""air raid"" style of interview that I was afraid it was going to be?",[],r/datascience,False,6,,0,,,False,t3_nk5dny,False,dark,0.73,,public,10,0,{},,,False,[],,False,False,,{},Job Search,False,10,,False,False,self,False,,[],{},,True,,1621911457.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Prior Post for context: &lt;a href=""https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/""&gt;https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Sadly I think it was an air raid for the most part... and I don&amp;#39;t think I did very well.&lt;/p&gt;

&lt;p&gt;I didn&amp;#39;t really get many rapid fire trivia questions, although I got some, and I had to answer a couple with &amp;quot;I don&amp;#39;t know.&amp;quot; For example I was asked what is the mathematical equation to calculate p-values... I answered with &amp;quot;I don&amp;#39;t remember? I would have to consult my college notes from 11 years ago&amp;quot;. In my head I was thinking, why does it matter when scikit learn or R does that all under the hood? There was another trivia question I got which I answered I don&amp;#39;t know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.&lt;/p&gt;

&lt;p&gt;I got some data engineering questions(not my strong suit) and I don&amp;#39;t believe I gave good answers. Everywhere I have been I have worked along side with data engineering teams but have never actually done the data engineering work. When I got to the point of the interview where I could ask questions I asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so I have no idea why I was even asked these questions.&lt;/p&gt;

&lt;p&gt;I think my strong suit was using my past work experiences to outline how I got things done, but even then I am not sure if my responses were good enough because I felt like I was on defense from the get go. I do better in interviews when I am &amp;quot;on offense&amp;quot;, so to speak. For example I was asked for an example of how I segmented customers in the past and used that to drive revenue growth. I said I would look at various segmentation techniques and choose the best one based on model performance against test data, and how I determined that to be K-Means clustering after using PCA to reduce the number of features and using a scree plot to determine the number of segments. And how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. I don&amp;#39;t think I was precise or deep enough in this answer and was too vague and I didn&amp;#39;t project enough confidence. But, in an hour long verbal interview, how deep am I supposed to go? I struggled with finding the right balance of depth and respecting the time constraints.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be surprised if I don&amp;#39;t get rejected... but I feel like this format was tough to succeed in. I believe I am a strong data scientist, but I am at my best when I know exactly what scenario I am facing. Questions like &amp;quot;Here is what we want to accomplish, how would you do it?&amp;quot; are what I do best, I didn&amp;#39;t get any of those questions. It was more open ended broad questions such as &amp;quot;What is your go to machine learning model&amp;quot; and I literally could have given anything. It depends on the task at hand. I said Random Forest to go with the question and how I like the built in OOB functionality in R. Although if I could answer the question again I&amp;#39;d go with Gradient Boosting. &lt;/p&gt;

&lt;p&gt;Another question I hated was &amp;quot;How do you stay on top of data science industry trends&amp;quot;. I didn&amp;#39;t really know how to answer that one. I said I stay engaged with publications and gave a personal example of what I do in my free time(that I won&amp;#39;t say since I don&amp;#39;t want to out myself incase those who interviewed lurk here) but I don&amp;#39;t like this because I believe you stick with what works and you don&amp;#39;t always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you&amp;#39;d probably know about it without having to be proactive to learn about it.&lt;/p&gt;

&lt;p&gt;Another question was &amp;quot;Do you follow up on your own models performance against real world data?&amp;quot;. I didn&amp;#39;t have a problem with this question because of course I do. Who doesn&amp;#39;t? I said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. Therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. Then they followed up with &amp;quot;What if you gained more responsibilities on your plate and could no longer follow up on all of your models?&amp;quot;, which I think was a silly follow up, because I feel that is putting the cart before the horse. I said in a hypothetical scenario where my bandwidth was overextended to where I could not do such a thing, as I hope it would not get to that point, I would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.&lt;/p&gt;

&lt;p&gt;I got asked &amp;quot;How do you perform feature selection&amp;quot; and I simply explained various ways to do it based on the model at hand(i.e. p-Values for linear regression, feature score for Random Forest, etc.). I didn&amp;#39;t like this question because its something you would ask an entry level candidate, not someone interviewing for director. I answered it anyway but with only an hour, I feel like better questions could have been asked.&lt;/p&gt;

&lt;p&gt;The interview was solely to gauge my technical abilities, not my leadership or management abilities, so I understand that the interview was aimed to do that. With that being said I don&amp;#39;t think I answered the questions given to me with enough confidence or precision. I am good at gauging body language, facial expressions and tone of voice of those interviewing me and I don&amp;#39;t think they were impressed. If its up to these 2 guys I don&amp;#39;t have a chance. The only way I have a chance is if all I needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. &lt;/p&gt;

&lt;p&gt;Comparing this interview to the 3 bad ones I had before I think this one was definitely the worst of the bunch in terms of how I gauged my performance. I am starting to lose confidence in my interviewing skills, as I believe I have a hard time translating my actual 10 year track record into verbal competency. I know how to do things, I just have a hard time coherently explaining how those things are done.&lt;/p&gt;

&lt;p&gt;Fortunately... I have another interview starting in 10 minutes... and 5 others lined up this week. But this was the job I really wanted out of all of them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nk5dny,True,,dothingsright_,,35,True,all_ads,False,[],False,,/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/,515405,1621882657.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience," Hi  Guys, I have a problem statement where there is a need for fire  detection which is usually handled by Computer Vision Object Detection  models - YOLO, Faster R-CNN, etc. However, I was thinking about using  Multimodal DL for this to take inputs from heat/thermal sensor, etc.  apart from video feeds.

Any practical blog/tutorial you can point me to?

Thanks!",t2_2mmql89p,False,,0,False,Multimodal Deep Learning,[],r/datascience,False,6,discussion,0,,,False,t3_nkgeop,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621943380.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi  Guys, I have a problem statement where there is a need for fire  detection which is usually handled by Computer Vision Object Detection  models - YOLO, Faster R-CNN, etc. However, I was thinking about using  Multimodal DL for this to take inputs from heat/thermal sensor, etc.  apart from video feeds.&lt;/p&gt;

&lt;p&gt;Any practical blog/tutorial you can point me to?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkgeop,True,,grid_world,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkgeop/multimodal_deep_learning/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkgeop/multimodal_deep_learning/,515405,1621914580.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_c4iedttd,False,,0,False,Solving Data Reproducibility,[],r/datascience,False,6,education,0,93.0,,False,t3_njxxzw,False,dark,0.68,,public,9,0,{},140.0,,False,[],,False,False,,{},Education,False,9,,False,False,https://b.thumbs.redditmedia.com/kQjLYt3j4bPKUiWntV_75P7wTm0H3VgISa0L3s6svxU.jpg,False,,[],{},,False,,1621891607.0,text,6,,,text,lakefs.io,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njxxzw,True,,MagicianPutrid5245,,4,True,all_ads,False,[],False,,/r/datascience/comments/njxxzw/solving_data_reproducibility/,all_ads,False,https://lakefs.io/solving-data-reproducibility/,515405,1621862807.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,link,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?auto=webp&amp;s=160916fedf27f4f97c389fc4741035317f00640d', 'width': 2560, 'height': 1707}, 'resolutions': [{'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e443e72eb3104417395966097d6607b657e35c8', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50dba762021fa1f52248195ea9dbee85d3027b2d', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2756ba7f45530b436539d7b6246b16dcb16477e7', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c523a73e78e25b72fc7b43b929403c7f628a443', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f019a1459aa3fcff13e328a1581fb303b2b818c1', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/DdMDRA2A6Wmzzx0Jug0FanBGiP4NGgcPLfxIvKKM338.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a38b68951b8fc6c1424888acbc7895b106cefef8', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'rp0Ipk9UJAk0VSXiYJVzIGk2P5crNNykj8vSL74E2-M'}], 'enabled': False}",,,https://lakefs.io/solving-data-reproducibility/,,
,datascience,"Hi , Looking at [metaflow](https://docs.metaflow.org/) tutorials and trying to see how different/ better it is from the setup below that can work in a SWE based pipeline; And if any reason to switch to metaflow , or all its features listed below are covered adequately by this setup:

&amp;#x200B;

1. **DataWarehouse** \- Source of data (pumped in from dbt pipeline)
2. **Compute resources** \- Covered by autoscaling in aws;
3. **Job Scheduler**:

* Quartz/Spring application reads configuration from a job database with a UI for entering schedules &amp; jobs;
* Schedules are decoupled from jobs; can evolve independently without touching code.
* Most jobs are to send a message to a kafka queue to kick off spring batch or python ML code.

**4. Architecture**:

* Orchestration handled by Java Spring cloud  stream / data flows plus kafka topics / producers/consumers.
* ML jobs are python classes /in-house python libraries so mostly in the ML pipeline only parameters are read from db and applied to the appropriate ML class;
* Emphasis on avoiding one-off ML python procedural scripts
* Data transfer between steps done using dbt (sql is understood by more folks). Only ML steps done in python

&amp;#x200B;

5.     **Versioning/Inspecting Results &amp; Organizing Results:**

* dbt  &amp; python ML jobs  results histories  based on batch/run\_id are stored and can be queried in sql
* Micrometer&amp;Prometheus metrics used to monitor success/ failed stages of all jobs.
* Sleuth distributed log tracing also used.
* Namespaces for development artifacts are simply git branches of each developer
* Namespaces isolation when inspecting results is based on run\_ids/user\_ids embedded in micrometer metrics and intermediate results of dbt &amp; python ML job histories.

**6. Loading and Storing Data:**

* Intermediate steps data is stored in database/s3

**7.  Loading and Storing Data:**

* Again no need to reinvent the wheel ; gradle for java code , pip aand requirements.txt for python

In all these steps using a mature framework like spring boot &amp; spring cloud stream makes life a lot easier

This setup will allow use common workflows that are already well defined in SWE rather than redefine new workflows in the existing infrastructure",t2_1j1zx80q,False,,0,False,Explain the need for non SWE ML pipeline vs existing SWE frameworks,[],r/datascience,False,6,tooling,0,,,False,t3_nkcehu,False,dark,1.0,,public,1,0,{},,,True,[],,False,False,,{},Tooling,False,1,,False,False,self,1621902065.0,,[],{},,True,,1621930670.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi , Looking at &lt;a href=""https://docs.metaflow.org/""&gt;metaflow&lt;/a&gt; tutorials and trying to see how different/ better it is from the setup below that can work in a SWE based pipeline; And if any reason to switch to metaflow , or all its features listed below are covered adequately by this setup:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;DataWarehouse&lt;/strong&gt; - Source of data (pumped in from dbt pipeline)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute resources&lt;/strong&gt; - Covered by autoscaling in aws;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Job Scheduler&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Quartz/Spring application reads configuration from a job database with a UI for entering schedules &amp;amp; jobs;&lt;/li&gt;
&lt;li&gt;Schedules are decoupled from jobs; can evolve independently without touching code.&lt;/li&gt;
&lt;li&gt;Most jobs are to send a message to a kafka queue to kick off spring batch or python ML code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Architecture&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Orchestration handled by Java Spring cloud  stream / data flows plus kafka topics / producers/consumers.&lt;/li&gt;
&lt;li&gt;ML jobs are python classes /in-house python libraries so mostly in the ML pipeline only parameters are read from db and applied to the appropriate ML class;&lt;/li&gt;
&lt;li&gt;Emphasis on avoiding one-off ML python procedural scripts&lt;/li&gt;
&lt;li&gt;Data transfer between steps done using dbt (sql is understood by more folks). Only ML steps done in python&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;    &lt;strong&gt;Versioning/Inspecting Results &amp;amp; Organizing Results:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;dbt  &amp;amp; python ML jobs  results histories  based on batch/run_id are stored and can be queried in sql&lt;/li&gt;
&lt;li&gt;Micrometer&amp;amp;Prometheus metrics used to monitor success/ failed stages of all jobs.&lt;/li&gt;
&lt;li&gt;Sleuth distributed log tracing also used.&lt;/li&gt;
&lt;li&gt;Namespaces for development artifacts are simply git branches of each developer&lt;/li&gt;
&lt;li&gt;Namespaces isolation when inspecting results is based on run_ids/user_ids embedded in micrometer metrics and intermediate results of dbt &amp;amp; python ML job histories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;6. Loading and Storing Data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Intermediate steps data is stored in database/s3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;7.  Loading and Storing Data:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Again no need to reinvent the wheel ; gradle for java code , pip aand requirements.txt for python&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In all these steps using a mature framework like spring boot &amp;amp; spring cloud stream makes life a lot easier&lt;/p&gt;

&lt;p&gt;This setup will allow use common workflows that are already well defined in SWE rather than redefine new workflows in the existing infrastructure&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkcehu,True,,dev-1773,,1,True,all_ads,False,[],False,,/r/datascience/comments/nkcehu/explain_the_need_for_non_swe_ml_pipeline_vs/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkcehu/explain_the_need_for_non_swe_ml_pipeline_vs/,515405,1621901870.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?auto=webp&amp;s=4ebad4b6de1c70b5aee591b6b982b03dff86935a', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce3cc02960dc84452f80dfbec0265763836ff49', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fdc21aef33f177a9577a165595113c249702379f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a9ac6e65b2724a701a5068591e3e8bb16670497', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f6b83391a3e86f843e0d2a1c89ac3353fbbda72', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b34e9ae2037f47b501ddf1b35223c5bdc3f8ece0', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/nAlqsIyQipuO-zZzihsI8x-aOplA64J9y-Exo-GdIfg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69bda3cac43d547ca62eec68f643502de1a64d95', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'vhUSJAG4C7ac0HmwHNqVfs5T8E-4KHs5MskFwWHQXjw'}], 'enabled': False}",,,,,
,datascience,"I see this as an example interview question all the time.

But I never see great answers.

So far as I can tell, a good answer might be..

\- L2 norm is continuously differentiable  
\- L1 has closed-form solution  


\-L0.5 or L3 are possible   
\-but L3 norm will tend to overregularize and bring coefficients towards each other.  
\-L0.5 norm would induce some irregular sparsity; not even sure how to clarify this one.

&amp;#x200B;

What else would people mention? I know there are answers involving Hilbert spaces with the L2 norm but I don't really understand that.",t2_a3t5z3gn,False,,0,False,What is a good answer for: Why use l1/l2 norms but not L0.5 or L3?,[],r/datascience,False,6,discussion,0,,,False,t3_nk7hs9,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621916860.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see this as an example interview question all the time.&lt;/p&gt;

&lt;p&gt;But I never see great answers.&lt;/p&gt;

&lt;p&gt;So far as I can tell, a good answer might be..&lt;/p&gt;

&lt;p&gt;- L2 norm is continuously differentiable&lt;br/&gt;
- L1 has closed-form solution  &lt;/p&gt;

&lt;p&gt;-L0.5 or L3 are possible&lt;br/&gt;
-but L3 norm will tend to overregularize and bring coefficients towards each other.&lt;br/&gt;
-L0.5 norm would induce some irregular sparsity; not even sure how to clarify this one.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What else would people mention? I know there are answers involving Hilbert spaces with the L2 norm but I don&amp;#39;t really understand that.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk7hs9,True,,latticeprep,,2,True,all_ads,False,[],False,,/r/datascience/comments/nk7hs9/what_is_a_good_answer_for_why_use_l1l2_norms_but/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk7hs9/what_is_a_good_answer_for_why_use_l1l2_norms_but/,515405,1621888060.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"LinkedIn recently opened-sourced [Greykite](https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library), a Python library originally built for LinkedIn’s forecasting needs. Greykite’s main algorithm is Silverkite, which delivers automated forecasting, which LinkedIn uses for resource planning, performance management, optimization, and ecosystem insight.

While using predictive models to estimate consumer behavior, data drift has proven to be a great challenge during the pandemic in 2020. In such a situation, predicting future expectations is challenging as well as necessarily helpful to any business. Automation, which allows for repeatability, can increase accuracy and can be used by algorithms to make decisions further down the line. According to LinkedIn, Silverkite has improved revenue forecasts for ‘1-day ahead’ and ‘7-day ahead’ and Weekly Active User forecasts for 2-week ahead.

Full Summary: [https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/](https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/?_ga=2.74959442.1924646600.1621739878-488125022.1618729090)

GitHub: [https://github.com/linkedin/greykite](https://github.com/linkedin/greykite)

PyPI: [https://pypi.org/project/greykite/](https://pypi.org/project/greykite/)

Paper: http://arxiv.org/abs/2105.01098",t2_4wudjgid,False,,0,False,"LinkedIn Open-Sources ‘Greykite’, A Time Series Forecasting Library",[],r/datascience,False,6,discussion,0,,,False,t3_nj9s57,False,dark,0.98,,public,316,1,{},,,False,[],,False,False,,{},Discussion,False,316,,False,False,self,False,,[],{},,True,,1621811802.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;LinkedIn recently opened-sourced &lt;a href=""https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library""&gt;Greykite&lt;/a&gt;, a Python library originally built for LinkedIn’s forecasting needs. Greykite’s main algorithm is Silverkite, which delivers automated forecasting, which LinkedIn uses for resource planning, performance management, optimization, and ecosystem insight.&lt;/p&gt;

&lt;p&gt;While using predictive models to estimate consumer behavior, data drift has proven to be a great challenge during the pandemic in 2020. In such a situation, predicting future expectations is challenging as well as necessarily helpful to any business. Automation, which allows for repeatability, can increase accuracy and can be used by algorithms to make decisions further down the line. According to LinkedIn, Silverkite has improved revenue forecasts for ‘1-day ahead’ and ‘7-day ahead’ and Weekly Active User forecasts for 2-week ahead.&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/?_ga=2.74959442.1924646600.1621739878-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/23/linkedin-open-sources-greykite-a-time-series-forecasting-library/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/linkedin/greykite""&gt;https://github.com/linkedin/greykite&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyPI: &lt;a href=""https://pypi.org/project/greykite/""&gt;https://pypi.org/project/greykite/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""http://arxiv.org/abs/2105.01098""&gt;http://arxiv.org/abs/2105.01098&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nj9s57,True,,techsucker,,30,True,all_ads,False,[],False,,/r/datascience/comments/nj9s57/linkedin_opensources_greykite_a_time_series/,all_ads,False,https://www.reddit.com/r/datascience/comments/nj9s57/linkedin_opensources_greykite_a_time_series/,515405,1621783002.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?auto=webp&amp;s=7f878bb82e1d42a4f047eb9ea0ab2a672174f972', 'width': 700, 'height': 429}, 'resolutions': [{'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dd66d32bdeef9af4ebbe5b1cc5d795726d77c36', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37dbcb228d193c9c884cb7b66b8326ff5d206b70', 'width': 216, 'height': 132}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20a255678eac392685a8f7c5e5faca0bf19669d', 'width': 320, 'height': 196}, {'url': 'https://external-preview.redd.it/72XS_4upxAN1-lJniY8l6_eBf6Alp5WIfg24zF2vBsU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=220f2947bf099b6a546fba0b558a93cb51caaa83', 'width': 640, 'height': 392}], 'variants': {}, 'id': 'hmkDzPyA5ng1AHnJpwon-OFBITQk95iQy6nblLoR2Ag'}], 'enabled': False}",,,,,
,datascience,"I have a fairly large portfolio of houses (think thousands) that I want to cluster based on - proximity to neighboring houses and some house types (fuel source, detached / apartment ...).

The goal is to create clusters based on the distance to other houses and the types (e.g., cluster of 5 houses max. 50 meters from each other which are all on the same fuel source and are detached). Luckily, in my dataset it is most likely that houses next to each other will also be of the same type.

Do you have any tips on algorithms / approaches for this job? I am proficient in Python / R.

Thank you.",t2_14qnk0,False,,0,False,How to geo-cluster houses in a real estate dataset?,[],r/datascience,False,6,discussion,0,,,False,t3_njxwij,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621891483.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a fairly large portfolio of houses (think thousands) that I want to cluster based on - proximity to neighboring houses and some house types (fuel source, detached / apartment ...).&lt;/p&gt;

&lt;p&gt;The goal is to create clusters based on the distance to other houses and the types (e.g., cluster of 5 houses max. 50 meters from each other which are all on the same fuel source and are detached). Luckily, in my dataset it is most likely that houses next to each other will also be of the same type.&lt;/p&gt;

&lt;p&gt;Do you have any tips on algorithms / approaches for this job? I am proficient in Python / R.&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njxwij,True,,Mieleki,,14,True,all_ads,False,[],False,,/r/datascience/comments/njxwij/how_to_geocluster_houses_in_a_real_estate_dataset/,all_ads,False,https://www.reddit.com/r/datascience/comments/njxwij/how_to_geocluster_houses_in_a_real_estate_dataset/,515405,1621862683.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Happy Sunday!

I was hoping we'd have a light discussion before jumping in the week tomorrow. 

I'm 26 right now, when I was in my undergrad, I remember Android development or some kind of website development were pretty popular among the college students. Today I was wondering that apart from money, what makes a job ""hot""?

Would love to hear from you guys, especially who have been in the market before Data Science even existed.

Thank you!",t2_bv171ji2,False,,0,False,"Why do you think Data Science became this ""sexy"" job and what job was hot before this?",[],r/datascience,False,6,discussion,0,,,False,t3_njihmm,False,dark,0.89,,public,80,0,{},,,False,[],,False,False,,{},Discussion,False,80,,False,False,self,False,,[],{},,True,,1621836176.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Happy Sunday!&lt;/p&gt;

&lt;p&gt;I was hoping we&amp;#39;d have a light discussion before jumping in the week tomorrow. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m 26 right now, when I was in my undergrad, I remember Android development or some kind of website development were pretty popular among the college students. Today I was wondering that apart from money, what makes a job &amp;quot;hot&amp;quot;?&lt;/p&gt;

&lt;p&gt;Would love to hear from you guys, especially who have been in the market before Data Science even existed.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njihmm,True,,quite--average,,82,True,all_ads,False,[],False,,/r/datascience/comments/njihmm/why_do_you_think_data_science_became_this_sexy/,all_ads,False,https://www.reddit.com/r/datascience/comments/njihmm/why_do_you_think_data_science_became_this_sexy/,515405,1621807376.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The situation is like this:  We receive deliveries of data electronically, and let's say they are expected to arrive at 8 AM daily. But historically they can arrive at say 5-10 AM. So it's periodic but not really. The data is gathered only hourly at the top of the hour. 

The data we're gathering now is basically ""time since last arrival""  which is zero when something has arrived in the last hour, and ticks up hour by hour until the next delivery.  What I've been doing so far is fitting this with a sawtooth wave and whenever a delivery is late, the ""time since last"" goes above the threshold and alerts us.  I researched time series data and such but it's not really what I'm looking for. 

Here's the issue:  we have dozens of different ones of these per customer and hundreds of customers,  and my fitting works well most of the time, but there are still a large number of false alerts. Also, some of these are 7 days a week, others are M-F,  others are more than once per day.  All very different kinds of sawtooths, and a lot of manual adjusting. 

What I've started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day, this gives me just a set of times. I did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable. (Is this a Poisson process?) I have enough history to look back 6 weeks. This gives only 6 data points for e.g. a Tuesday, but for 7-day-per-week systems there is no problem aggregating all the days and looking just at the hour of the day (42 total data points)  But knowing when to combine 7 days, vs 5/2 split etc is a manual decision. I would prefer it it could fit each arrival event separately.

(I'm actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week, so I see the scatter plots overlayed and can guess at the amount of variation when I do these, so for example 8 AM Tuesday would be at T=56 hour and constructing the threshold sawtooth function on that axis.)

What I am looking for (perhaps) is a way to give say, based on the real history, give me the time of day that if the packet hasn't arrived by, there's a high % chance that it's late and set an alarm. 

Since the history can and does contain some that were late (or absent) I know those should factor in with less weight. I don't know what kind of distribution these should fit or how to output a cutoff value for a given % chance it's out of compliance.  And the reason I'm trying is so I can script this to analyze hundreds of such data sets and set thresholds automatically. 

Any suggestions where to look?    Online article?  videos?  
I would be interested in suggestions for textbooks as well. 

Thanks",t2_4hrqs,False,,0,False,"Need pointers on where to start on how to model this data that is essentially ""arrival times"" or ""arrival delays""",[],r/datascience,False,6,education,0,,,False,t3_nk8t4x,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Education,False,0,,False,False,self,1621909656.0,,[],{},,True,,1621920228.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The situation is like this:  We receive deliveries of data electronically, and let&amp;#39;s say they are expected to arrive at 8 AM daily. But historically they can arrive at say 5-10 AM. So it&amp;#39;s periodic but not really. The data is gathered only hourly at the top of the hour. &lt;/p&gt;

&lt;p&gt;The data we&amp;#39;re gathering now is basically &amp;quot;time since last arrival&amp;quot;  which is zero when something has arrived in the last hour, and ticks up hour by hour until the next delivery.  What I&amp;#39;ve been doing so far is fitting this with a sawtooth wave and whenever a delivery is late, the &amp;quot;time since last&amp;quot; goes above the threshold and alerts us.  I researched time series data and such but it&amp;#39;s not really what I&amp;#39;m looking for. &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the issue:  we have dozens of different ones of these per customer and hundreds of customers,  and my fitting works well most of the time, but there are still a large number of false alerts. Also, some of these are 7 days a week, others are M-F,  others are more than once per day.  All very different kinds of sawtooths, and a lot of manual adjusting. &lt;/p&gt;

&lt;p&gt;What I&amp;#39;ve started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day, this gives me just a set of times. I did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable. (Is this a Poisson process?) I have enough history to look back 6 weeks. This gives only 6 data points for e.g. a Tuesday, but for 7-day-per-week systems there is no problem aggregating all the days and looking just at the hour of the day (42 total data points)  But knowing when to combine 7 days, vs 5/2 split etc is a manual decision. I would prefer it it could fit each arrival event separately.&lt;/p&gt;

&lt;p&gt;(I&amp;#39;m actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week, so I see the scatter plots overlayed and can guess at the amount of variation when I do these, so for example 8 AM Tuesday would be at T=56 hour and constructing the threshold sawtooth function on that axis.)&lt;/p&gt;

&lt;p&gt;What I am looking for (perhaps) is a way to give say, based on the real history, give me the time of day that if the packet hasn&amp;#39;t arrived by, there&amp;#39;s a high % chance that it&amp;#39;s late and set an alarm. &lt;/p&gt;

&lt;p&gt;Since the history can and does contain some that were late (or absent) I know those should factor in with less weight. I don&amp;#39;t know what kind of distribution these should fit or how to output a cutoff value for a given % chance it&amp;#39;s out of compliance.  And the reason I&amp;#39;m trying is so I can script this to analyze hundreds of such data sets and set thresholds automatically. &lt;/p&gt;

&lt;p&gt;Any suggestions where to look?    Online article?  videos?&lt;br/&gt;
I would be interested in suggestions for textbooks as well. &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk8t4x,True,,ggrieves,,2,True,all_ads,False,[],False,,/r/datascience/comments/nk8t4x/need_pointers_on_where_to_start_on_how_to_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk8t4x/need_pointers_on_where_to_start_on_how_to_model/,515405,1621891428.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Next year I will start my MBA and after that I am starting a MSc in Data Science. I am 31 years old and have worked as Marketing Manager and eCommerce Manager. All of these positions involve lots of data analysis and data visualization but none of data science (prediction models, machine learning, etc). 

I have experience using Tableau, PowerBi, Google Data Studio, Analytics, Ads... I have worked mostly on Analysis and Visualization. My biggest weakness would be the ""math"" side of data science.

My ultimate goal is to work at Google, it has always been my dream company. Other options are big companies such as Amazon, Facebook, Netflix, DiDi, Uber, or any other ""big"" company.

I don't know if I am the right thing by looking for a Data Science mentor, or if I should be looking for a different person, like a general career counselor? Any tips?",t2_618gmrpn,False,,0,False,Looking yo hire a Data Science mentor for career advice,[],r/datascience,False,6,career,0,,,False,t3_nk6noh,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1621914740.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Next year I will start my MBA and after that I am starting a MSc in Data Science. I am 31 years old and have worked as Marketing Manager and eCommerce Manager. All of these positions involve lots of data analysis and data visualization but none of data science (prediction models, machine learning, etc). &lt;/p&gt;

&lt;p&gt;I have experience using Tableau, PowerBi, Google Data Studio, Analytics, Ads... I have worked mostly on Analysis and Visualization. My biggest weakness would be the &amp;quot;math&amp;quot; side of data science.&lt;/p&gt;

&lt;p&gt;My ultimate goal is to work at Google, it has always been my dream company. Other options are big companies such as Amazon, Facebook, Netflix, DiDi, Uber, or any other &amp;quot;big&amp;quot; company.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if I am the right thing by looking for a Data Science mentor, or if I should be looking for a different person, like a general career counselor? Any tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nk6noh,True,,diegoarmando50,,8,True,all_ads,False,[],False,,/r/datascience/comments/nk6noh/looking_yo_hire_a_data_science_mentor_for_career/,all_ads,False,https://www.reddit.com/r/datascience/comments/nk6noh/looking_yo_hire_a_data_science_mentor_for_career/,515405,1621885940.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"Some context, I'm gonna be speaker for a meetup and I want to touch the fundamentals for people who wants to start their career as data scientists.

Rather than talk about the usual stuff (develop math, biz and coding skills) **I want to people reflect on the mindset they need to build so they can become successful data scientists**.

Right now I have for things to share with them:

1. _Curiosity_: Data scientist are people with lot of curiosity and they are willing to research. If your stay curious you will have the mindset for ask questions, look for data, formulate hypothesis and test them.  


2. _Perseverance_: Usually, Data Science is a rough path. There is no magic bootcamp or 6 month course to become a good scientific. You need to stay focus and practice your skills pretty often (even by just listening a podcast or reading a paper).  


3. _Emphaty_: You have put yourself in other scenarios rather than just thinking inside the Data Science side. Often you need to collaborate with other disciplines and understand the context of unknown situations. You might analyze social events, business metrics, human behavior, etcetera.  


4. _Participation_: As data scientists, we need to get our hands dirty. No better way to learn (imo) than building real applications, solve real problems.  


**What other characteristics do you perceive in a successful data scientist?**",t2_c9ezux7k,False,,0,False,What kind of mindset a person should shape in order to bocome a good data scientist?,[],r/datascience,False,6,discussion,0,,,False,t3_nkbfil,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1621908574.0,,[],{},,True,,1621927685.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Some context, I&amp;#39;m gonna be speaker for a meetup and I want to touch the fundamentals for people who wants to start their career as data scientists.&lt;/p&gt;

&lt;p&gt;Rather than talk about the usual stuff (develop math, biz and coding skills) &lt;strong&gt;I want to people reflect on the mindset they need to build so they can become successful data scientists&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Right now I have for things to share with them:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Curiosity&lt;/em&gt;: Data scientist are people with lot of curiosity and they are willing to research. If your stay curious you will have the mindset for ask questions, look for data, formulate hypothesis and test them.  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Perseverance&lt;/em&gt;: Usually, Data Science is a rough path. There is no magic bootcamp or 6 month course to become a good scientific. You need to stay focus and practice your skills pretty often (even by just listening a podcast or reading a paper).  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Emphaty&lt;/em&gt;: You have put yourself in other scenarios rather than just thinking inside the Data Science side. Often you need to collaborate with other disciplines and understand the context of unknown situations. You might analyze social events, business metrics, human behavior, etcetera.  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Participation&lt;/em&gt;: As data scientists, we need to get our hands dirty. No better way to learn (imo) than building real applications, solve real problems.  &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;What other characteristics do you perceive in a successful data scientist?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nkbfil,True,,donelianc,,3,True,all_ads,False,[],False,,/r/datascience/comments/nkbfil/what_kind_of_mindset_a_person_should_shape_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/nkbfil/what_kind_of_mindset_a_person_should_shape_in/,515405,1621898885.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Are there any tools to make it easier to debug the model performance and evaluate the model errors? For example, I want to be able to able to dissect how my model is performing for certain classes etc",t2_1o26bmw3,False,,0,False,Tools for error Analysis,[],r/datascience,False,6,tooling,0,,,False,t3_njqqpw,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,False,seniorflair,[],{},,True,,1621864024.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any tools to make it easier to debug the model performance and evaluate the model errors? For example, I want to be able to able to dissect how my model is performing for certain classes etc&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,MS | Student,[],False,,,,t5_2sptq,,,,njqqpw,True,,da_chosen1,,1,True,all_ads,False,[],False,dark,/r/datascience/comments/njqqpw/tools_for_error_analysis/,all_ads,False,https://www.reddit.com/r/datascience/comments/njqqpw/tools_for_error_analysis/,515405,1621835224.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I have noticed this in practice like in Kaggle comps. Sometimes I have done no hyperparameter tuning by just setting whatever seems reasonable ish in a ballpark or using defaults and it ends up performing better than doing computationally intensive and tuning every little hyperparameter. 

In the real world I also wonder whether cross validation for hyperparams can result in being more sensitive to things like data and concept drift. Because well if the future data doesn’t look like your validation set then the CV would have resulted in you overfitting the hyperparameters themselves.",t2_wmwkc,False,,0,False,Is cross-validation hyperparameter tuning sometimes not better than just setting reasonable values?,[],r/datascience,False,6,discussion,0,,,False,t3_njbini,False,dark,0.87,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,1621795785.0,,[],{},,True,,1621816890.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have noticed this in practice like in Kaggle comps. Sometimes I have done no hyperparameter tuning by just setting whatever seems reasonable ish in a ballpark or using defaults and it ends up performing better than doing computationally intensive and tuning every little hyperparameter. &lt;/p&gt;

&lt;p&gt;In the real world I also wonder whether cross validation for hyperparams can result in being more sensitive to things like data and concept drift. Because well if the future data doesn’t look like your validation set then the CV would have resulted in you overfitting the hyperparameters themselves.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njbini,True,,ice_shadow,,8,True,all_ads,False,[],False,,/r/datascience/comments/njbini/is_crossvalidation_hyperparameter_tuning/,all_ads,False,https://www.reddit.com/r/datascience/comments/njbini/is_crossvalidation_hyperparameter_tuning/,515405,1621788090.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Let’s take Covid-19 global number of positive cases.

For each day, the global daily number is known by aggregating each country number. And each country cases can be further grouped into different variables like by virus variant, by country state, etc.

If someone looks at the global time series, it’s hard to know which variables are contributing to the number of cases without cutting and slicing the data.

Are there methods that allow us to surface the variable values that contribute to the number or cases for a given time interval?

This sounds somewhat easy, like I can calculate the contribution percentage of each variable value, and sort them.

Though I may also be interested in the contributing trend as well which can be positively contributing to the global number of cases and negatively contributing to the global number of cases.

From my cursory reading, I found topics such as aberration detection. Or generally anomaly detection. But these topics almost never explain what are the variable values that contribute to the anomaly. It max explain anomalous points due to trends and seasonalities but not so much on what are the explanatory variables given that we have aggregated time series.",t2_4bix301o,False,,0,False,What are methods that explain aggregated time series anomalies? For example what feature values contributes to number of Covid-19 cases at a given time interval?,[],r/datascience,False,6,discussion,0,,,False,t3_nja1gq,False,dark,0.85,,public,12,0,{},,,False,[],,False,False,,{},Discussion,False,12,,False,False,self,False,,[],{},,True,,1621812585.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let’s take Covid-19 global number of positive cases.&lt;/p&gt;

&lt;p&gt;For each day, the global daily number is known by aggregating each country number. And each country cases can be further grouped into different variables like by virus variant, by country state, etc.&lt;/p&gt;

&lt;p&gt;If someone looks at the global time series, it’s hard to know which variables are contributing to the number of cases without cutting and slicing the data.&lt;/p&gt;

&lt;p&gt;Are there methods that allow us to surface the variable values that contribute to the number or cases for a given time interval?&lt;/p&gt;

&lt;p&gt;This sounds somewhat easy, like I can calculate the contribution percentage of each variable value, and sort them.&lt;/p&gt;

&lt;p&gt;Though I may also be interested in the contributing trend as well which can be positively contributing to the global number of cases and negatively contributing to the global number of cases.&lt;/p&gt;

&lt;p&gt;From my cursory reading, I found topics such as aberration detection. Or generally anomaly detection. But these topics almost never explain what are the variable values that contribute to the anomaly. It max explain anomalous points due to trends and seasonalities but not so much on what are the explanatory variables given that we have aggregated time series.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nja1gq,True,,levenshteinn,,6,True,all_ads,False,[],False,,/r/datascience/comments/nja1gq/what_are_methods_that_explain_aggregated_time/,all_ads,False,https://www.reddit.com/r/datascience/comments/nja1gq/what_are_methods_that_explain_aggregated_time/,515405,1621783785.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello!

I an looking for a book that explains all the distributions, probability, Anova, p value, confidence and prediction interval and maybe linear regression too. 

Is there a book you like that explains this well?

Thank you!",t2_7ckcfm6o,False,,0,False,"Need to go back to the basics, what's your favorite Stats 101 book?",[],r/datascience,False,6,education,0,,,False,t3_nino7x,False,dark,0.99,,public,373,1,{},,,False,[],,False,False,,{},Education,False,373,,False,False,self,1621705520.0,,[],{},,True,,1621733528.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I an looking for a book that explains all the distributions, probability, Anova, p value, confidence and prediction interval and maybe linear regression too. &lt;/p&gt;

&lt;p&gt;Is there a book you like that explains this well?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nino7x,True,,ysharm10,,88,True,all_ads,False,[],False,,/r/datascience/comments/nino7x/need_to_go_back_to_the_basics_whats_your_favorite/,all_ads,False,https://www.reddit.com/r/datascience/comments/nino7x/need_to_go_back_to_the_basics_whats_your_favorite/,515405,1621704728.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I am giving a technical interview later this week.  My background is in statistics and I will be a question with coding on it.  But I also want to ask a machine learning question. 

Our company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  My own understanding is just that decision tree has a decent interpretability but less accurate than random forest or Xgboost.  I don’t think it is fair for me to ask the candidate their algorithms in details since I am not an expert myself.  

Does anyone have any suggestions?  Thanks.",t2_y7l57,False,,0,False,What is a good technical interview machine learning question?,[],r/datascience,False,6,discussion,0,,,False,t3_njd0ft,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621821094.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am giving a technical interview later this week.  My background is in statistics and I will be a question with coding on it.  But I also want to ask a machine learning question. &lt;/p&gt;

&lt;p&gt;Our company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  My own understanding is just that decision tree has a decent interpretability but less accurate than random forest or Xgboost.  I don’t think it is fair for me to ask the candidate their algorithms in details since I am not an expert myself.  &lt;/p&gt;

&lt;p&gt;Does anyone have any suggestions?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njd0ft,True,,sonicking12,,11,True,all_ads,False,[],False,,/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/,all_ads,False,https://www.reddit.com/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/,515405,1621792294.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm trying to find resources to learn more about a new subfield in Machine Learning called online learning.

The idea is beautiful and powerful: your model in production trains itself with new latest data to react to changes faster. However the classic ways to build the MLOps infrastructure and algorithms' maths won't do the job here, so I'm eager to learn more:

* I've found [this post](https://huyenchip.com/2020/12/27/real-time-machine-learning.html) by Standford's ML lecturer Chip Huyen to be a great introduction to the concept of Online Learning.
* I've found [river](https://github.com/online-ml/river) to be a promising python library for online learning.

Apart from that, I don't know many resources out there, do you? Any blogs to follow? Any ""Titanic"" equivalents (a simple problem to get going)?",t2_c4rvshhm,False,,0,False,Online machine learning (or how to automatically update your model in production),[],r/datascience,False,6,discussion,0,,,False,t3_njen7v,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,1621800204.0,,[],{},,True,,1621825605.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to find resources to learn more about a new subfield in Machine Learning called online learning.&lt;/p&gt;

&lt;p&gt;The idea is beautiful and powerful: your model in production trains itself with new latest data to react to changes faster. However the classic ways to build the MLOps infrastructure and algorithms&amp;#39; maths won&amp;#39;t do the job here, so I&amp;#39;m eager to learn more:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I&amp;#39;ve found &lt;a href=""https://huyenchip.com/2020/12/27/real-time-machine-learning.html""&gt;this post&lt;/a&gt; by Standford&amp;#39;s ML lecturer Chip Huyen to be a great introduction to the concept of Online Learning.&lt;/li&gt;
&lt;li&gt;I&amp;#39;ve found &lt;a href=""https://github.com/online-ml/river""&gt;river&lt;/a&gt; to be a promising python library for online learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apart from that, I don&amp;#39;t know many resources out there, do you? Any blogs to follow? Any &amp;quot;Titanic&amp;quot; equivalents (a simple problem to get going)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njen7v,True,,JB__Quix,,5,True,all_ads,False,[],False,,/r/datascience/comments/njen7v/online_machine_learning_or_how_to_automatically/,all_ads,False,https://www.reddit.com/r/datascience/comments/njen7v/online_machine_learning_or_how_to_automatically/,515405,1621796805.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?auto=webp&amp;s=afc490b7ded3f15b261d944c6fada22a812f2b58', 'width': 1999, 'height': 1293}, 'resolutions': [{'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8aabc02673c1d710295046ce6abdc8a4ae70b99', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1502d89e7e95c23c9c9d44c88b1d8c286fa9638f', 'width': 216, 'height': 139}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f537ef5f39159f95f7e6f5e3b33ded65da91a20', 'width': 320, 'height': 206}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee994ae648a0e8a28cae01e81aa9e84187ca6ee9', 'width': 640, 'height': 413}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=41dce60ae2abd99411174cf179a6a3988658aa30', 'width': 960, 'height': 620}, {'url': 'https://external-preview.redd.it/AvsFWp4Ku17gObQ-xyaNkjUBU06HurvkpWzRAdGbZaY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59104f8f3e4f7fdcbb133574095d313209923a5b', 'width': 1080, 'height': 698}], 'variants': {}, 'id': 'W3GknR_Tnzbar4S08sfFzXqOVZNSULnboBhRnnDiRxc'}], 'enabled': False}",,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,1,False,Weekly Entering &amp; Transitioning Thread | 23 May 2021 - 30 May 2021,[],r/datascience,False,6,,0,,,False,t3_nj6cc2,False,dark,0.8,,public,6,1,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{'gid_2': 1},,True,,1621800030.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Gold', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png'}]",[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,nj6cc2,True,,datascience-bot,,172,False,all_ads,False,[],False,dark,/r/datascience/comments/nj6cc2/weekly_entering_transitioning_thread_23_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/nj6cc2/weekly_entering_transitioning_thread_23_may_2021/,515405,1621771230.0,0,,False,,,,,,,,
,datascience,"Hi All,

So I recently joined a firm with massive operations in the logistics &amp; delivery business. We are currently building a system that can automatically flag anomalous events based on the time-series nature of business KPIs across multiple cities &amp; zones. We are facing multiple roadblocks &amp; I feel completely stuck in a loop without any progress. Below are a few problems we're facing:

1. Prior to this system business never tried detecting anomalies manually, so they don't know what an anomaly is. This is my biggest concern as it makes the problem statement quite open-ended. Also, even after detecting an anomaly through time series models, we don't have any mechanism to evaluate them
2. While building a time series model, should we include the underlying variables as the predictors? Or just treat the target variable as a univariate variable and perform Root cause analysis on the detected anomaly events?

Please share any resources or case studies of a similar system that is implemented on a large scale. I know this problem is very business-specific but I assume the underlying techniques will be similar for such systems.",t2_dlxtbst,False,,0,False,Time Series Anomaly detection system,[],r/datascience,False,6,discussion,0,,,False,t3_njcmm2,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621820037.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;So I recently joined a firm with massive operations in the logistics &amp;amp; delivery business. We are currently building a system that can automatically flag anomalous events based on the time-series nature of business KPIs across multiple cities &amp;amp; zones. We are facing multiple roadblocks &amp;amp; I feel completely stuck in a loop without any progress. Below are a few problems we&amp;#39;re facing:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Prior to this system business never tried detecting anomalies manually, so they don&amp;#39;t know what an anomaly is. This is my biggest concern as it makes the problem statement quite open-ended. Also, even after detecting an anomaly through time series models, we don&amp;#39;t have any mechanism to evaluate them&lt;/li&gt;
&lt;li&gt;While building a time series model, should we include the underlying variables as the predictors? Or just treat the target variable as a univariate variable and perform Root cause analysis on the detected anomaly events?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Please share any resources or case studies of a similar system that is implemented on a large scale. I know this problem is very business-specific but I assume the underlying techniques will be similar for such systems.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,njcmm2,True,,jkashish1818,,3,True,all_ads,False,[],False,,/r/datascience/comments/njcmm2/time_series_anomaly_detection_system/,all_ads,False,https://www.reddit.com/r/datascience/comments/njcmm2/time_series_anomaly_detection_system/,515405,1621791237.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi everyone,

I was scrolling feeds of the group and did a quick search for Knime. It actually surprises me how unpopular as a platform is considering that the last post was a year ago. 

I have started to learn more about Knime (required for job) and wanted to see your thoughts on the platform based on the experience you had.

Is there any substitute that does a better job than Knime and this is the reason why it is not very popular.

Any opinion is helpful.",t2_7gvgv,False,,0,False,Your experience with Knime,[],r/datascience,False,6,tooling,0,,,False,t3_niieqi,False,dark,0.95,,public,57,0,{},,,False,[],,False,False,,{},Tooling,False,57,,False,False,self,False,,[],{},,True,,1621718131.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I was scrolling feeds of the group and did a quick search for Knime. It actually surprises me how unpopular as a platform is considering that the last post was a year ago. &lt;/p&gt;

&lt;p&gt;I have started to learn more about Knime (required for job) and wanted to see your thoughts on the platform based on the experience you had.&lt;/p&gt;

&lt;p&gt;Is there any substitute that does a better job than Knime and this is the reason why it is not very popular.&lt;/p&gt;

&lt;p&gt;Any opinion is helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niieqi,True,,salihveseli,,37,True,all_ads,False,[],False,,/r/datascience/comments/niieqi/your_experience_with_knime/,all_ads,False,https://www.reddit.com/r/datascience/comments/niieqi/your_experience_with_knime/,515405,1621689331.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I am currently a data scientist/analyst. My current job has a lot of analyses and code that could be functionalized and used as an internal Python package. I am interested in working in a role that is focused on creating packages, tools, and other processes to help streamline and create efficiency for an entire DS team... and less so a role that is actually DS itself. Curious if anyone knows if there is a certain job title that fits this description?",t2_yy1onqs,False,,0,False,What type of job would allow me to create useful tools for a data science/data analyst team?,[],r/datascience,False,6,career,0,,,False,t3_niuauk,False,dark,0.71,,public,9,0,{},,,False,[],,False,False,,{},Career,False,9,,False,False,self,False,,[],{},,True,,1621753022.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am currently a data scientist/analyst. My current job has a lot of analyses and code that could be functionalized and used as an internal Python package. I am interested in working in a role that is focused on creating packages, tools, and other processes to help streamline and create efficiency for an entire DS team... and less so a role that is actually DS itself. Curious if anyone knows if there is a certain job title that fits this description?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niuauk,True,,createAnAccount13,,10,True,all_ads,False,[],False,,/r/datascience/comments/niuauk/what_type_of_job_would_allow_me_to_create_useful/,all_ads,False,https://www.reddit.com/r/datascience/comments/niuauk/what_type_of_job_would_allow_me_to_create_useful/,515405,1621724222.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have 8 years of experience in data science, and was reached out to by a recruiter in regards to a Director of Marketing Analytics position for a company that is not a no-name company(many of you have probably heard of them). I have gone through the interview process and have advanced far in the process, so I would imagine I am coming to the end of the line.

The problem is I keep reading the job description and am wondering how qualified I truly am for this role. The last 5 years I have managed teams, with my most recent role managing a team of 8 as a senior data science manager(my team was laid off in December). I consider myself to be very well versed in data science and everything that comes with it.

HOWEVER... the job description(in terms of duties) has a lot of data engineering lingo in it. Something I really don't have experience in. Everywhere I have been, we have had a separate data engineering team that handles all of that, which I work parallel with. In all my roles I have been strictly data science, modeling, machine learning, deep learning, analytics and insights, client and executive presentations, and nothing to do with data warehousing and data pipelining. But, in the job requirements section, I meet all of the qualification criteria. 

I can send the exact job description, qualifications and responsibilities in private message if anyone asks. But, I feel like my data engineering skills are lacking, and you think the recruiters and those I have interviewed with so far would have been able to tell that, but I keep advancing. In my interviews, I talk up my technical skills, but most importantly I talk up my people skills. As I have advanced in this profession, the one thing I have learned is the higher you go, the less important your techinical skills become, and the more important your people skills become. Those I have spoke with so far are really glad to hear someone say that...

But, I don't want to set myself up to fail. Should someone with my level of experience in this industry be expected to have data engineering experience as well? I've always viewed the two as two separate discplines, and I believe a position marrying the two would be overkill. I believe in specialization in that regard. I believe in terms of pure data science and analytics I am a home run for this role, but all of the data engineering mumbo jumbo in the job description is scaring the bejeezus out of me. Surely in my next interview this week I could ask if there is a separate data engineering team or if its something I would have to handle. Or am I worrying over nothing? Surely a company this size would have enough resources to split the two disciplines out?",t2_260w6lk0,False,,0,False,Questioning my qualifications for an Analytics Director position I have been interviewing for(and have advanced far in the process for),[],r/datascience,False,6,,0,,,False,t3_nir1qm,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Job Search,False,4,,False,False,self,False,,[],{},,True,,1621743270.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 8 years of experience in data science, and was reached out to by a recruiter in regards to a Director of Marketing Analytics position for a company that is not a no-name company(many of you have probably heard of them). I have gone through the interview process and have advanced far in the process, so I would imagine I am coming to the end of the line.&lt;/p&gt;

&lt;p&gt;The problem is I keep reading the job description and am wondering how qualified I truly am for this role. The last 5 years I have managed teams, with my most recent role managing a team of 8 as a senior data science manager(my team was laid off in December). I consider myself to be very well versed in data science and everything that comes with it.&lt;/p&gt;

&lt;p&gt;HOWEVER... the job description(in terms of duties) has a lot of data engineering lingo in it. Something I really don&amp;#39;t have experience in. Everywhere I have been, we have had a separate data engineering team that handles all of that, which I work parallel with. In all my roles I have been strictly data science, modeling, machine learning, deep learning, analytics and insights, client and executive presentations, and nothing to do with data warehousing and data pipelining. But, in the job requirements section, I meet all of the qualification criteria. &lt;/p&gt;

&lt;p&gt;I can send the exact job description, qualifications and responsibilities in private message if anyone asks. But, I feel like my data engineering skills are lacking, and you think the recruiters and those I have interviewed with so far would have been able to tell that, but I keep advancing. In my interviews, I talk up my technical skills, but most importantly I talk up my people skills. As I have advanced in this profession, the one thing I have learned is the higher you go, the less important your techinical skills become, and the more important your people skills become. Those I have spoke with so far are really glad to hear someone say that...&lt;/p&gt;

&lt;p&gt;But, I don&amp;#39;t want to set myself up to fail. Should someone with my level of experience in this industry be expected to have data engineering experience as well? I&amp;#39;ve always viewed the two as two separate discplines, and I believe a position marrying the two would be overkill. I believe in specialization in that regard. I believe in terms of pure data science and analytics I am a home run for this role, but all of the data engineering mumbo jumbo in the job description is scaring the bejeezus out of me. Surely in my next interview this week I could ask if there is a separate data engineering team or if its something I would have to handle. Or am I worrying over nothing? Surely a company this size would have enough resources to split the two disciplines out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nir1qm,True,,Beer_Makes_You_Fat,,17,True,all_ads,False,[],False,,/r/datascience/comments/nir1qm/questioning_my_qualifications_for_an_analytics/,all_ads,False,https://www.reddit.com/r/datascience/comments/nir1qm/questioning_my_qualifications_for_an_analytics/,515405,1621714470.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_jokwd,False,,0,False,"Currently a Data Scientist... Want to increase my skillset to expand into Data Engineering... Any great resources, courses etc that you guys can recommend. Thanks",[],r/datascience,False,6,education,0,,,False,t3_ni0b8j,False,dark,0.97,,public,269,1,{},,,False,[],,False,False,,{},Education,False,269,,False,False,self,False,,[],{'gid_1': 1},,True,,1621652985.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni0b8j,True,,Bosser7,,38,True,all_ads,False,[],False,,/r/datascience/comments/ni0b8j/currently_a_data_scientist_want_to_increase_my/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni0b8j/currently_a_data_scientist_want_to_increase_my/,515405,1621624185.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Does anyone know of any good tools for knowledge sharing post fact?

We've tried this but found it came up a bit short.

https://github.com/airbnb/knowledge-repo",t2_ur9jhrn,False,,0,False,How does everyone share their models etc. across teams for re-use effectively?,[],r/datascience,False,6,tooling,0,,,False,t3_niq3eb,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Tooling,False,2,,False,False,self,False,,[],{},,True,,1621740450.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know of any good tools for knowledge sharing post fact?&lt;/p&gt;

&lt;p&gt;We&amp;#39;ve tried this but found it came up a bit short.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/airbnb/knowledge-repo""&gt;https://github.com/airbnb/knowledge-repo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niq3eb,True,,SimpleEnthusiasm,,1,True,all_ads,False,[],False,,/r/datascience/comments/niq3eb/how_does_everyone_share_their_models_etc_across/,all_ads,False,https://www.reddit.com/r/datascience/comments/niq3eb/how_does_everyone_share_their_models_etc_across/,515405,1621711650.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?auto=webp&amp;s=72f747c55187d86115126cf085f85ddafa20b95e', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f322bba42b689af77aa1a9465d6d2b8559d1620', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=417361eb84b420b2e14ec73e71f53c1fe2d478bc', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb2d707e4a56e2382dc9e65be394076814105d2', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db5b918a07af15c657a173593e5ea67fdba27f04', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=453be7f6124a043bd4e88fbb64ae2cee30b76187', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/egbvQQqjJjVOCVsh2J-TFzT4pOKInvH75vdxPOeBFyA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4402785456011f4ff77bb947faa96940e5957054', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'GFCwLI821B9LV6Lussb5HWKwu5cg_cho_mHtdCSct1c'}], 'enabled': False}",,,,,
,datascience,"I have a Master's of Public Policy and I generally would do data cleaning / ETL in Excel, and then import it into STATA for multivariate regressions. 

It was nothing fancy but it was insightful and I enjoyed it; now I want to expand my knowledge into SQL, but I am wondering if realistically so few people/places use STATA that I should start learning Python, or SPSS instead?",t2_8azmn3,False,,0,False,Does anyone use STATA?,[],r/datascience,False,6,education,0,,,False,t3_niljmw,False,dark,0.58,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1621727511.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a Master&amp;#39;s of Public Policy and I generally would do data cleaning / ETL in Excel, and then import it into STATA for multivariate regressions. &lt;/p&gt;

&lt;p&gt;It was nothing fancy but it was insightful and I enjoyed it; now I want to expand my knowledge into SQL, but I am wondering if realistically so few people/places use STATA that I should start learning Python, or SPSS instead?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,niljmw,True,,TimboCA,,21,True,all_ads,False,[],False,,/r/datascience/comments/niljmw/does_anyone_use_stata/,all_ads,False,https://www.reddit.com/r/datascience/comments/niljmw/does_anyone_use_stata/,515405,1621698711.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"I've been back on the job market for the last month since being laid off and I have what may be a final interview on Monday for a job I really want, its kind of a dream job for me, Director of Data/Analytics for a company I've always wanted to work for. I didn't even apply, a recruiter sought me out personally. I have the experience and qualifications. I've been in this business since I graduated in 2011, I have management experience, and a successful track record. 

I've already had multiple calls with hiring managers, and those have all been great, hence why I have advanced so far. However my next interview they said they are bringing in 2 outside guys from a data firm they contract to ""gauge"" my technical experience.

This immediately brought in fear because I've been interviewing for many jobs over the past month and I've already had 3 interviews that I considered to be ""air raid"" interviews where it seemed like I was under attack from the panel rather than being interviewed by the panel.

The first ""air raid"" interview I was facing a panel of 3 in a final interview setting and all of a sudden we get to a part where it was rapid fire trivia style questions. I felt like I was on the fast money round of Family Feud because I was only given 30 seconds to answer each question. The thing is it was pure Python trivia, a lot of questions over stuff I'll automate anyway that I didn't know off the top of my head and would have to reference my work. The guy leading the interview, the entire time during this, had a smug, skeptical look on his face like that blonde dude from Napoleon Dynamite who is always making fun of Napoelon. Afterward the 2 other interviewers were trying to apologize without apologizing as you could tell they werent a fan of that part of the interview. I did fine on everything else, but that was a train wreck and I got the rejection email the next week.

Then about a month ago I had another panel style interview, going up against 3 guys who I felt were just trying to knock me down from the get go. Asking a lot of ""booby trapped"" questions that I was clever enough to ""figure out"" but there were a couple that got by me. Questions obviously designed to trip people up. It didn't help that the microphones one of the interviewers had was horrible and I kept having to ask him to repeat questions, but these guys were very smug and hostile and played off of each other as if it were some kind of game to ruin the dreams of anyone applying. I got rejected by these people too, and actually got a customized rejection email saying that my skillset was below their standards. No shit, if they are judging my skillset based on that interview they won't find anyone that meets their standards. Which is the case because I still get emails from LinkedIn telling me to apply for that job as they are still actively recruiting for it. Good luck!

Finally I had a take home task that took me about 5 hours and I had to present it to a panel, and they were really unimpressed, not because of my conclusions/findings/work, but because it wasn't ""visually appealing"" enough. They actually wanted me to present my findings in PowerPoint(I used R Markdown) with fancy infographics and such. I didn't realize I was being hired for a graphic design job. I did what I was tasked to do and infographics were not in the task instructions. It's not like my R Markdown graphs and charts were ugly or off, they told the story perfectly and accurately, but its clear they were looking for someone who wanted to be an artist as well. 

Anyway, I feel like these 3 experiences have prepared me for what I may be facing, and I suppose there is a chance that these 2 guys they are bringing in to ""gauge"" my technical skills won't bombard me and will be totally cool, but I am not getting my hopes up. These two have already reached out to me and asked for my GitHub account, something I was not prepared for since I was never asked about it prior nor was it in the job description. I sent it over anyway but a lot of my code was written for a one man audience(me) and I wasn't able to save all of my work from my past job as they blocked access to my work OneDrive as soon as I was laid off so I could only save my work samples that I had on my own machine. Therefore I don't feel like my GitHub is truly putting my best foot forward. 

I had a good panel interview experience 3 weeks ago, I made it to the final 2 but was not selected, but it was still a great experience, and I let the recruiters/hiring managers know it. They didn't try to pepper you with trivia or kaggle or leetcode, and instead just probed you about your conceptual understanding of concepts. 

When I was a hiring manager and was hiring people for my own teams, I always hired on potential, not pure ability. I didn't hire them based on what they could do now, but what I felt like they could ultimately become. I feel like a lot of these interviews are structured around finding candidates with high floors, rather than high ceilings. 

Anyway, just kind of ranting as I mentally prepare for what could be another air raid interview. Anyone have any tips in case it gets to that point?",t2_y5gxz,False,,0,False,"Has anyone here had the experience of what I call an ""air raid"" style of interview, where the panel of interviewers seem to be constantly trying to attack and poke holes in you?",[],r/datascience,False,6,,0,,,False,t3_nhi11p,False,dark,0.98,,public,368,3,{},,,False,[],,False,False,,{},Job Search,False,368,,False,False,self,False,,[],{},,True,,1621593266.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been back on the job market for the last month since being laid off and I have what may be a final interview on Monday for a job I really want, its kind of a dream job for me, Director of Data/Analytics for a company I&amp;#39;ve always wanted to work for. I didn&amp;#39;t even apply, a recruiter sought me out personally. I have the experience and qualifications. I&amp;#39;ve been in this business since I graduated in 2011, I have management experience, and a successful track record. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve already had multiple calls with hiring managers, and those have all been great, hence why I have advanced so far. However my next interview they said they are bringing in 2 outside guys from a data firm they contract to &amp;quot;gauge&amp;quot; my technical experience.&lt;/p&gt;

&lt;p&gt;This immediately brought in fear because I&amp;#39;ve been interviewing for many jobs over the past month and I&amp;#39;ve already had 3 interviews that I considered to be &amp;quot;air raid&amp;quot; interviews where it seemed like I was under attack from the panel rather than being interviewed by the panel.&lt;/p&gt;

&lt;p&gt;The first &amp;quot;air raid&amp;quot; interview I was facing a panel of 3 in a final interview setting and all of a sudden we get to a part where it was rapid fire trivia style questions. I felt like I was on the fast money round of Family Feud because I was only given 30 seconds to answer each question. The thing is it was pure Python trivia, a lot of questions over stuff I&amp;#39;ll automate anyway that I didn&amp;#39;t know off the top of my head and would have to reference my work. The guy leading the interview, the entire time during this, had a smug, skeptical look on his face like that blonde dude from Napoleon Dynamite who is always making fun of Napoelon. Afterward the 2 other interviewers were trying to apologize without apologizing as you could tell they werent a fan of that part of the interview. I did fine on everything else, but that was a train wreck and I got the rejection email the next week.&lt;/p&gt;

&lt;p&gt;Then about a month ago I had another panel style interview, going up against 3 guys who I felt were just trying to knock me down from the get go. Asking a lot of &amp;quot;booby trapped&amp;quot; questions that I was clever enough to &amp;quot;figure out&amp;quot; but there were a couple that got by me. Questions obviously designed to trip people up. It didn&amp;#39;t help that the microphones one of the interviewers had was horrible and I kept having to ask him to repeat questions, but these guys were very smug and hostile and played off of each other as if it were some kind of game to ruin the dreams of anyone applying. I got rejected by these people too, and actually got a customized rejection email saying that my skillset was below their standards. No shit, if they are judging my skillset based on that interview they won&amp;#39;t find anyone that meets their standards. Which is the case because I still get emails from LinkedIn telling me to apply for that job as they are still actively recruiting for it. Good luck!&lt;/p&gt;

&lt;p&gt;Finally I had a take home task that took me about 5 hours and I had to present it to a panel, and they were really unimpressed, not because of my conclusions/findings/work, but because it wasn&amp;#39;t &amp;quot;visually appealing&amp;quot; enough. They actually wanted me to present my findings in PowerPoint(I used R Markdown) with fancy infographics and such. I didn&amp;#39;t realize I was being hired for a graphic design job. I did what I was tasked to do and infographics were not in the task instructions. It&amp;#39;s not like my R Markdown graphs and charts were ugly or off, they told the story perfectly and accurately, but its clear they were looking for someone who wanted to be an artist as well. &lt;/p&gt;

&lt;p&gt;Anyway, I feel like these 3 experiences have prepared me for what I may be facing, and I suppose there is a chance that these 2 guys they are bringing in to &amp;quot;gauge&amp;quot; my technical skills won&amp;#39;t bombard me and will be totally cool, but I am not getting my hopes up. These two have already reached out to me and asked for my GitHub account, something I was not prepared for since I was never asked about it prior nor was it in the job description. I sent it over anyway but a lot of my code was written for a one man audience(me) and I wasn&amp;#39;t able to save all of my work from my past job as they blocked access to my work OneDrive as soon as I was laid off so I could only save my work samples that I had on my own machine. Therefore I don&amp;#39;t feel like my GitHub is truly putting my best foot forward. &lt;/p&gt;

&lt;p&gt;I had a good panel interview experience 3 weeks ago, I made it to the final 2 but was not selected, but it was still a great experience, and I let the recruiters/hiring managers know it. They didn&amp;#39;t try to pepper you with trivia or kaggle or leetcode, and instead just probed you about your conceptual understanding of concepts. &lt;/p&gt;

&lt;p&gt;When I was a hiring manager and was hiring people for my own teams, I always hired on potential, not pure ability. I didn&amp;#39;t hire them based on what they could do now, but what I felt like they could ultimately become. I feel like a lot of these interviews are structured around finding candidates with high floors, rather than high ceilings. &lt;/p&gt;

&lt;p&gt;Anyway, just kind of ranting as I mentally prepare for what could be another air raid interview. Anyone have any tips in case it gets to that point?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nhi11p,True,,dothingsright_,,170,True,all_ads,False,[],False,,/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/,515405,1621564466.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I've always pronounced it as ""eeee-pock"" which is how my Comp Sci professor who first taught me neural nets said it. But I hear people say ""epic"" or ""eh-pock"" all the time and it really irritates me for some reason.

How do you think it's supposed to be pronounced in a data science context?

Edit: I've learned from some commenters that the American pronunciation is supposed to be ""eh-puk"" (like epic) and the British pronunciation is supposed to be ""e-pock"". But I swear I hear some people sort of meet in the middle and use ""eh-pock"" as well.",t2_b2g0m82o,False,,0,False,"How do YOU pronounce ""epoch""?",[],r/datascience,False,6,discussion,0,,,False,t3_nhxqrn,False,dark,0.8,,public,14,0,{},,,False,[],,False,False,,{},Discussion,False,14,,False,False,self,1621625515.0,,[],{},,True,,1621646129.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve always pronounced it as &amp;quot;eeee-pock&amp;quot; which is how my Comp Sci professor who first taught me neural nets said it. But I hear people say &amp;quot;epic&amp;quot; or &amp;quot;eh-pock&amp;quot; all the time and it really irritates me for some reason.&lt;/p&gt;

&lt;p&gt;How do you think it&amp;#39;s supposed to be pronounced in a data science context?&lt;/p&gt;

&lt;p&gt;Edit: I&amp;#39;ve learned from some commenters that the American pronunciation is supposed to be &amp;quot;eh-puk&amp;quot; (like epic) and the British pronunciation is supposed to be &amp;quot;e-pock&amp;quot;. But I swear I hear some people sort of meet in the middle and use &amp;quot;eh-pock&amp;quot; as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhxqrn,True,,horsewithmanynames,,46,True,all_ads,False,[],False,,/r/datascience/comments/nhxqrn/how_do_you_pronounce_epoch/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhxqrn/how_do_you_pronounce_epoch/,515405,1621617329.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I'm in the unique and fortunate situation of having the option of choosing between a data scientist (promotion in current company) or senior product analyst (offer from a new company) position for my next role.

Both would be significant upgrades for me, but the product analyst position at a new place will pay significantly more (I don't know by how much yet as I'm waiting for a formal offer from my current company). I have wanted to break into DS and get the title for a while, but I'm not sure if staying at my current company with lower pay is worth the title.

I know titles and actual work in our field really aren't well defined. I have some indication of what the DS work if I stayed would look like- some cool ML models, recommendation algorithms, testing, as well as more typical data analyst work with SQL and no modeling. My preference would be to do modeling and more DS long term, but I'm also kind of ready for a reset and new company. 

There's certainly some factors I'm leaving out,, but which opportunity would you jump at? A DS role with the title and maybe more interesting work, but lower pay and company frustrations - OR a senior product analyst role at a new, exciting company with significantly more pay and more typical product analytics work.

Maybe ""which would you pick"" is too subjective- but what considerations would you make?

Any advice welcome, thanks!",t2_ot5g6,False,,0,False,Data Scientist vs Senior Product Analyst,[],r/datascience,False,6,,0,,,False,t3_ni7cix,False,dark,0.6,,public,2,0,{},,,False,[],,False,False,,{},Job Search,False,2,,False,False,self,False,,[],{},,True,,1621674141.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the unique and fortunate situation of having the option of choosing between a data scientist (promotion in current company) or senior product analyst (offer from a new company) position for my next role.&lt;/p&gt;

&lt;p&gt;Both would be significant upgrades for me, but the product analyst position at a new place will pay significantly more (I don&amp;#39;t know by how much yet as I&amp;#39;m waiting for a formal offer from my current company). I have wanted to break into DS and get the title for a while, but I&amp;#39;m not sure if staying at my current company with lower pay is worth the title.&lt;/p&gt;

&lt;p&gt;I know titles and actual work in our field really aren&amp;#39;t well defined. I have some indication of what the DS work if I stayed would look like- some cool ML models, recommendation algorithms, testing, as well as more typical data analyst work with SQL and no modeling. My preference would be to do modeling and more DS long term, but I&amp;#39;m also kind of ready for a reset and new company. &lt;/p&gt;

&lt;p&gt;There&amp;#39;s certainly some factors I&amp;#39;m leaving out,, but which opportunity would you jump at? A DS role with the title and maybe more interesting work, but lower pay and company frustrations - OR a senior product analyst role at a new, exciting company with significantly more pay and more typical product analytics work.&lt;/p&gt;

&lt;p&gt;Maybe &amp;quot;which would you pick&amp;quot; is too subjective- but what considerations would you make?&lt;/p&gt;

&lt;p&gt;Any advice welcome, thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,ni7cix,True,,robo_capybara,,8,True,all_ads,False,[],False,,/r/datascience/comments/ni7cix/data_scientist_vs_senior_product_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni7cix/data_scientist_vs_senior_product_analyst/,515405,1621645341.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Confused as to when to use [StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer) vs StringIndexer+[OneHotEncoder](https://spark.apache.org/docs/latest/ml-features#onehotencoder).

The OneHotEncoder docs say

&gt;For string type input data, it is common to encode categorical features using StringIndexer first.

In what situations would I want to take the extra step of transforming StringIndex'ed output to one-hot encoded features? I can find a lot of resources on how to use which, but not in which cases the OneHotEncoder would be better.",t2_4rwmx54d,False,,0,False,"spark ml StringIndexer vs OneHotEncoder, when to use which?",[],r/datascience,False,6,discussion,0,,,False,t3_ni5ptp,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621668381.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Confused as to when to use &lt;a href=""https://spark.apache.org/docs/latest/ml-features#stringindexer""&gt;StringIndexer&lt;/a&gt; vs StringIndexer+&lt;a href=""https://spark.apache.org/docs/latest/ml-features#onehotencoder""&gt;OneHotEncoder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The OneHotEncoder docs say&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For string type input data, it is common to encode categorical features using StringIndexer first.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In what situations would I want to take the extra step of transforming StringIndex&amp;#39;ed output to one-hot encoded features? I can find a lot of resources on how to use which, but not in which cases the OneHotEncoder would be better.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni5ptp,True,,Anxious_Reporter,,3,True,all_ads,False,[],False,,/r/datascience/comments/ni5ptp/spark_ml_stringindexer_vs_onehotencoder_when_to/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni5ptp/spark_ml_stringindexer_vs_onehotencoder_when_to/,515405,1621639581.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,,t2_kkj7t,False,,0,False,The best data science newsletters that you subscribe for?,[],r/datascience,False,6,education,0,,,False,t3_ni0xcu,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Education,False,5,,False,False,self,False,,[],{},,True,,1621654613.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni0xcu,True,,so_phrasing,,11,True,all_ads,False,[],False,,/r/datascience/comments/ni0xcu/the_best_data_science_newsletters_that_you/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni0xcu/the_best_data_science_newsletters_that_you/,515405,1621625813.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"[https://www.crosstab.io/articles/statistical-rethinking-review](https://www.crosstab.io/articles/statistical-rethinking-review)

I noticed a lot of folks recommending this book, so I followed the crowd and got a copy. It's oriented toward researchers in natural and social sciences, so I wrote up my thoughts about the book from the perspective of an industry data scientist",t2_arhctu9v,False,,0,False,"Review: Statistical Rethinking, by Richard McElreath",[],r/datascience,False,6,discussion,0,,,False,t3_nhv5w3,False,dark,0.81,,public,9,0,{},,,False,[],,False,False,,{},Discussion,False,9,,False,False,self,False,,[],{},,True,,1621639528.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.crosstab.io/articles/statistical-rethinking-review""&gt;https://www.crosstab.io/articles/statistical-rethinking-review&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I noticed a lot of folks recommending this book, so I followed the crowd and got a copy. It&amp;#39;s oriented toward researchers in natural and social sciences, so I wrote up my thoughts about the book from the perspective of an industry data scientist&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhv5w3,True,,ctk_brian,,7,True,all_ads,False,[],False,,/r/datascience/comments/nhv5w3/review_statistical_rethinking_by_richard_mcelreath/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhv5w3/review_statistical_rethinking_by_richard_mcelreath/,515405,1621610728.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?auto=webp&amp;s=73e934a08f4dc1f41bf5a5c432f72216ca329ece', 'width': 508, 'height': 815}, 'resolutions': [{'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5624eb6861009cbd5a5357c36b161a292ac00b71', 'width': 108, 'height': 173}, {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e877d6591d7578affca157f982befa4165bf2f3', 'width': 216, 'height': 346}, {'url': 'https://external-preview.redd.it/0YWH1ZiyfgUC7fy552BgbgvM8dyb-FY0ajqq66hkL-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50868d99e36a0c55ed989823a21a8ae6a3b2083d', 'width': 320, 'height': 513}], 'variants': {}, 'id': 'Y-GLK6h0320mqstot6PY-jNOxe5YOduEhhxJaHHh9Zs'}], 'enabled': False}",,,,,
,datascience,"I am working on a Data Science project at work and my manager gave me permission to use Python. However, he also said that my code should be clean and readable and I must explain it to my coworkers, since none of them have extensive experience with Python before. This is because I am an intern, so when I leave, my coworkers must be able to understand my code and maintain it. Because of this, do you think I should use Python? Or use something else like Excel to be safe cause everyone know Excel?",t2_4j10zx4j,False,,0,False,Should I use Python at work if my coworkers don't have experience with it?,[],r/datascience,False,6,tooling,0,,,False,t3_nhtcug,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Tooling,False,3,,False,False,self,False,,[],{},,True,,1621634790.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a Data Science project at work and my manager gave me permission to use Python. However, he also said that my code should be clean and readable and I must explain it to my coworkers, since none of them have extensive experience with Python before. This is because I am an intern, so when I leave, my coworkers must be able to understand my code and maintain it. Because of this, do you think I should use Python? Or use something else like Excel to be safe cause everyone know Excel?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhtcug,True,,---Imperator---,,7,True,all_ads,False,[],False,,/r/datascience/comments/nhtcug/should_i_use_python_at_work_if_my_coworkers_dont/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhtcug/should_i_use_python_at_work_if_my_coworkers_dont/,515405,1621605990.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"I was recently reading some articles on the importance of ""interpertability"" when dealing with ""blackbox"" models. ""Blackbox"" models like neural networks are said to have a very low level of interpertability, because they don't allow the analyst to understand why the model is making a certain predictions for an individual observation.

On the other hand, models like decision trees and regression models are said to have much higher levels of interpertability. In a general sense, I can understand why models like decision trees are interpretable, because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation. 

If you look at a regression model, 

e.g. salary = 5.3 * height  + 2 * weight  - 15.8 * age 

A regression model can allow the analyst to understand how much each variable contributes to the prediction (e.g. in this example, age contributes more to the prediction by a factor of almost 8 times), and you can also find out how statistically significant each variable is (e.g. indivudal p-value of each regression coefficient). 

Is this what is meant by the ""interpertability of a regression model""?

Thanks",t2_xtuyc,False,,0,False,"how ""interpertable"" are regression models?",[],r/datascience,False,6,discussion,0,,,False,t3_ni2x2q,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621660025.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was recently reading some articles on the importance of &amp;quot;interpertability&amp;quot; when dealing with &amp;quot;blackbox&amp;quot; models. &amp;quot;Blackbox&amp;quot; models like neural networks are said to have a very low level of interpertability, because they don&amp;#39;t allow the analyst to understand why the model is making a certain predictions for an individual observation.&lt;/p&gt;

&lt;p&gt;On the other hand, models like decision trees and regression models are said to have much higher levels of interpertability. In a general sense, I can understand why models like decision trees are interpretable, because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation. &lt;/p&gt;

&lt;p&gt;If you look at a regression model, &lt;/p&gt;

&lt;p&gt;e.g. salary = 5.3 * height  + 2 * weight  - 15.8 * age &lt;/p&gt;

&lt;p&gt;A regression model can allow the analyst to understand how much each variable contributes to the prediction (e.g. in this example, age contributes more to the prediction by a factor of almost 8 times), and you can also find out how statistically significant each variable is (e.g. indivudal p-value of each regression coefficient). &lt;/p&gt;

&lt;p&gt;Is this what is meant by the &amp;quot;interpertability of a regression model&amp;quot;?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ni2x2q,True,,ottawalanguages,,11,True,all_ads,False,[],False,,/r/datascience/comments/ni2x2q/how_interpertable_are_regression_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/ni2x2q/how_interpertable_are_regression_models/,515405,1621631225.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Example - at the company I work for, they had been trying to hire a analyst for quite some time. It was originally called ""technical analyst"", and the response was...lukewarm. 20-25 applicants, and some even withdrew their applications underway. 

Then HR renamed the job to ""Data Scientist"", included that in the tittle of the listing, and slapped on some buzzwords on the new tools we use.  

Result? Almost 300 applications. The shortlist included people with experience from big name tech and banking companies, prestigious schools, etc.",t2_klsal,False,,0,False,"It's crazy how effective it's to include ""Data Scientist"" in your job listing.",[],r/datascience,False,6,,0,,,False,t3_nguua9,False,dark,0.98,,public,489,0,{},,,False,[],,False,False,,{},Job Search,False,489,,False,False,self,False,,[],{},,True,,1621528881.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Example - at the company I work for, they had been trying to hire a analyst for quite some time. It was originally called &amp;quot;technical analyst&amp;quot;, and the response was...lukewarm. 20-25 applicants, and some even withdrew their applications underway. &lt;/p&gt;

&lt;p&gt;Then HR renamed the job to &amp;quot;Data Scientist&amp;quot;, included that in the tittle of the listing, and slapped on some buzzwords on the new tools we use.  &lt;/p&gt;

&lt;p&gt;Result? Almost 300 applications. The shortlist included people with experience from big name tech and banking companies, prestigious schools, etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nguua9,True,,trackerFF,,139,True,all_ads,False,[],False,,/r/datascience/comments/nguua9/its_crazy_how_effective_its_to_include_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nguua9/its_crazy_how_effective_its_to_include_data/,515405,1621500081.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,,t2_1xx5out6,False,,0,False,"Data Scientists, what are few things that you wish you knew before starting your data science journey?",[],r/datascience,False,6,discussion,0,,,False,t3_nh1as2,False,dark,0.94,,public,109,0,{},,,False,[],,False,False,,{},Discussion,False,109,,False,False,self,False,,[],{},,True,,1621548763.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh1as2,True,,freakNinja39,,103,True,all_ads,False,[],False,,/r/datascience/comments/nh1as2/data_scientists_what_are_few_things_that_you_wish/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh1as2/data_scientists_what_are_few_things_that_you_wish/,515405,1621519963.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Would love some advice/recommendations from Causal Inference Data Scientists - We are trying to look for tools/frameworks/platforms that can help boost the productivity of Data Scientists in a causal inferencing team. Right now, the Data Scientists are just doing their experimentation on AI Platform Notebooks, but would like to try to standardize their methodology, automate processes whenever possible and  track their experimentation.

I believe the current workflow is:

1. DS writes SQL query to pull in treatments, outcome variables data from Snowflake.
2. DS uses ECONML or DoWhy libraries to get the causal estimate, statistical significance, etc.
3. DS tracks experiments and different variables used on Mlflow

I'm sure this current workflow can be greatly approved and was wondering if there are some established industry best practices  that we can learn from with regards to causal inferencing and experimentation. Also, we would love to leverage any open source tooling you would recommend that would help in this domain",t2_53pxmg7h,False,,0,False,What are some tools/best practices that Causal Inferencing teams use for experimentation?,[],r/datascience,False,6,tooling,0,,,False,t3_nhfdr3,False,dark,0.94,,public,15,0,{},,,False,[],,False,False,,{},Tooling,False,15,,False,False,self,1621556151.0,,[],{},,True,,1621584633.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Would love some advice/recommendations from Causal Inference Data Scientists - We are trying to look for tools/frameworks/platforms that can help boost the productivity of Data Scientists in a causal inferencing team. Right now, the Data Scientists are just doing their experimentation on AI Platform Notebooks, but would like to try to standardize their methodology, automate processes whenever possible and  track their experimentation.&lt;/p&gt;

&lt;p&gt;I believe the current workflow is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;DS writes SQL query to pull in treatments, outcome variables data from Snowflake.&lt;/li&gt;
&lt;li&gt;DS uses ECONML or DoWhy libraries to get the causal estimate, statistical significance, etc.&lt;/li&gt;
&lt;li&gt;DS tracks experiments and different variables used on Mlflow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m sure this current workflow can be greatly approved and was wondering if there are some established industry best practices  that we can learn from with regards to causal inferencing and experimentation. Also, we would love to leverage any open source tooling you would recommend that would help in this domain&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhfdr3,True,,rirhun,,3,True,all_ads,False,[],False,,/r/datascience/comments/nhfdr3/what_are_some_toolsbest_practices_that_causal/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhfdr3/what_are_some_toolsbest_practices_that_causal/,515405,1621555833.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"Hey guys, I have a scenario where a ML model needs to be deployed on client's server. There exists the possibility that the model might be reverse engineered and therefore there is a need to protect it.

Suggestions for achieving this?

Thanks!",t2_2mmql89p,False,,0,False,Securing Machine learning model,[],r/datascience,False,6,discussion,0,,,False,t3_nhp2ui,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621620960.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I have a scenario where a ML model needs to be deployed on client&amp;#39;s server. There exists the possibility that the model might be reverse engineered and therefore there is a need to protect it.&lt;/p&gt;

&lt;p&gt;Suggestions for achieving this?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhp2ui,True,,grid_world,,12,True,all_ads,False,[],False,,/r/datascience/comments/nhp2ui/securing_machine_learning_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhp2ui/securing_machine_learning_model/,515405,1621592160.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I would think it would be a bonus to have experience in various industries, but my encounters in applications and interviews makes me think otherwise",t2_5zbsxzuq,False,,0,False,"All else equal, would hiring managers rather have a candidate with all their experience in their industry or prefer someone with experience in multiple fields?",[],r/datascience,False,6,discussion,0,,,False,t3_nhbc7e,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1621573150.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would think it would be a bonus to have experience in various industries, but my encounters in applications and interviews makes me think otherwise&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhbc7e,True,,wsb146,,5,False,all_ads,False,[],False,,/r/datascience/comments/nhbc7e/all_else_equal_would_hiring_managers_rather_have/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhbc7e/all_else_equal_would_hiring_managers_rather_have/,515405,1621544350.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi all,

I recently started my first job working as an entry level Data Scientist. I’ve been working at this company for roughly 3.5 months now and was put on a project where I am to extract phrases and classification codes from PDF documents in different languages (there is more to it than that - I’m just keeping it brief without disclosing too much).

I had relatively finished most of the algorithm that is able to extract and compile these phrases/codes - however, the dataset that I am using has all been entered manually by multiple different people who work at the company (~100+ people). This requires a lot of data cleaning to process duplicate phrases that are mapped to different codes, categories of codes, etc. Additionally, it appears that many people have formatted their inputs drastically differently. I am currently only doing this for the English language and then will have to do it for French, Spanish, and German in the coming weeks. Each dataset is initially 250,000 records where I can automate roughly 90% of the cleaning - the rest are all either really obscure cases or the classification of the duplicate phrases are too close to call causing me to have to closely examine and google them online to determine which one shouldn’t be there. 

I know all of this is all super vague - I am trying my best to explain what I can share (some things I can’t)

Back to my question - I have weekly meetings with management where some of them seem surprised when I tell them that I am still working on data cleaning (been working on it for 2 weeks now and will likely need more time than this as I haven’t even finished the English dataset). I would estimate that up to this point 70%-75% of the code I’ve written is for the sole purpose of data cleaning, preprocessing, and determining what belongs where (using fuzzy logic and embeddings). My question is how do I explain to them that the data cleaning process is most of the work a data scientist needs to do? Am I looking into this too much? Had I been given a perfectly clean dataset, I would be able to complete this in no time. Also, this is my first job out of college (bachelors degree in Data Science) and I definitely acknowledge the skill gap between me and the other members on my team who are Sr. Data Scientists. They are much more efficient than I am when it comes to things such as Deep Learning, the cloud, etc. 

Any advice is greatly appreciated



TL;DR My first job out of college. Been working at the company for 3.5 months as a data scientist. Management seems to be surprised that data cleaning is taking me so long (2 weeks and counting) to complete which makes me feel like I am not working efficiently enough. Does management have it backwards where they think building the ML models is more intense than the Data Cleaning portion?


Edit: Thank you all for the input and advice! I have a meeting with management later this week and I will definitely be using the suggestions and advice provided here

Edit 2: Wow!! I really can thank everyone enough for all the advice and feedback I received. You all have gave me some great guidance as to how I can navigate this issue. Thank you!

Edit 3: Grammar + Formatting",t2_m016cr,False,,0,False,How to explain to Management that Data Cleaning is a really important part of my job,[],r/datascience,False,6,career,0,,,False,t3_ngls7t,False,dark,0.99,,public,345,1,{},,,False,[],,False,False,,{},Career,False,345,,False,False,self,1621501617.0,,[],{},,True,,1621499248.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I recently started my first job working as an entry level Data Scientist. I’ve been working at this company for roughly 3.5 months now and was put on a project where I am to extract phrases and classification codes from PDF documents in different languages (there is more to it than that - I’m just keeping it brief without disclosing too much).&lt;/p&gt;

&lt;p&gt;I had relatively finished most of the algorithm that is able to extract and compile these phrases/codes - however, the dataset that I am using has all been entered manually by multiple different people who work at the company (~100+ people). This requires a lot of data cleaning to process duplicate phrases that are mapped to different codes, categories of codes, etc. Additionally, it appears that many people have formatted their inputs drastically differently. I am currently only doing this for the English language and then will have to do it for French, Spanish, and German in the coming weeks. Each dataset is initially 250,000 records where I can automate roughly 90% of the cleaning - the rest are all either really obscure cases or the classification of the duplicate phrases are too close to call causing me to have to closely examine and google them online to determine which one shouldn’t be there. &lt;/p&gt;

&lt;p&gt;I know all of this is all super vague - I am trying my best to explain what I can share (some things I can’t)&lt;/p&gt;

&lt;p&gt;Back to my question - I have weekly meetings with management where some of them seem surprised when I tell them that I am still working on data cleaning (been working on it for 2 weeks now and will likely need more time than this as I haven’t even finished the English dataset). I would estimate that up to this point 70%-75% of the code I’ve written is for the sole purpose of data cleaning, preprocessing, and determining what belongs where (using fuzzy logic and embeddings). My question is how do I explain to them that the data cleaning process is most of the work a data scientist needs to do? Am I looking into this too much? Had I been given a perfectly clean dataset, I would be able to complete this in no time. Also, this is my first job out of college (bachelors degree in Data Science) and I definitely acknowledge the skill gap between me and the other members on my team who are Sr. Data Scientists. They are much more efficient than I am when it comes to things such as Deep Learning, the cloud, etc. &lt;/p&gt;

&lt;p&gt;Any advice is greatly appreciated&lt;/p&gt;

&lt;p&gt;TL;DR My first job out of college. Been working at the company for 3.5 months as a data scientist. Management seems to be surprised that data cleaning is taking me so long (2 weeks and counting) to complete which makes me feel like I am not working efficiently enough. Does management have it backwards where they think building the ML models is more intense than the Data Cleaning portion?&lt;/p&gt;

&lt;p&gt;Edit: Thank you all for the input and advice! I have a meeting with management later this week and I will definitely be using the suggestions and advice provided here&lt;/p&gt;

&lt;p&gt;Edit 2: Wow!! I really can thank everyone enough for all the advice and feedback I received. You all have gave me some great guidance as to how I can navigate this issue. Thank you!&lt;/p&gt;

&lt;p&gt;Edit 3: Grammar + Formatting&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngls7t,True,,MAXnRUSSEL,,73,True,all_ads,False,[],False,,/r/datascience/comments/ngls7t/how_to_explain_to_management_that_data_cleaning/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngls7t/how_to_explain_to_management_that_data_cleaning/,515405,1621470448.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"

[View Poll](https://www.reddit.com/poll/nhzi43)",t2_8azmn3,False,,0,False,What is ONE single essential tool/program/skill that a new person absolutely must master when transitioning into a data science/analyst role?,[],r/datascience,False,6,discussion,0,,,False,t3_nhzi43,False,dark,0.23,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621650817.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/nhzi43""&gt;View Poll&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhzi43,True,,TimboCA,,14,True,all_ads,False,[],False,,/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/,515405,1621622017.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,"{'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1621881217866, 'options': [{'text': 'R (or Python or similar code)', 'vote_count': 134, 'id': '8088652'}, {'text': 'Power BI (or Tableau)', 'vote_count': 5, 'id': '8088653'}, {'text': 'Excel', 'vote_count': 13, 'id': '8088654'}, {'text': 'SQL', 'vote_count': 59, 'id': '8088655'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 211, 'total_stake_amount': None}"
,datascience,"This drives me nuts. 

Unless you are looking for a relationship between time and your variable, graphing in time is useless. 

I so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. 

This isn't limited to the fresh faced grad who's just discovered R. But experienced technical experts in a field, typically engineering data in the fields I work (data logging etc.)

If you want to visually assess correlation between variables, plot them X vs Y, and then lets have a look.

* Where time is not a critical factor.",t2_aegwp2g1,False,,0,False,I'm fed up of seeing continuous data from tests in the time domain. It offers very little value.,[],r/datascience,False,6,discussion,0,,,False,t3_nhstoa,False,dark,0.36,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,1621609528.0,,[],{},,True,,1621633317.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This drives me nuts. &lt;/p&gt;

&lt;p&gt;Unless you are looking for a relationship between time and your variable, graphing in time is useless. &lt;/p&gt;

&lt;p&gt;I so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. &lt;/p&gt;

&lt;p&gt;This isn&amp;#39;t limited to the fresh faced grad who&amp;#39;s just discovered R. But experienced technical experts in a field, typically engineering data in the fields I work (data logging etc.)&lt;/p&gt;

&lt;p&gt;If you want to visually assess correlation between variables, plot them X vs Y, and then lets have a look.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Where time is not a critical factor.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nhstoa,True,,fortuitous_monkey,,51,True,all_ads,False,[],False,,/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/,515405,1621604517.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," I am weighing the pros and cons of taking a contract position, which I haven’t done before. What are things I should consider?

All I know about contract work is: -they don’t take any taxes out -no benefits, no health insurance -less networking -not a part of team/culture -may or may not turn into a job -less stable

This is for a data analyst role. They are offering 40/hr. But my current annual salary is 75k with great health coverage.

What factors should I consider? What do you wish you knew before taking your first contract role? Why do people generally look down on contract offers, I don’t get it...?

Any help would be greatly appreciated",t2_2avj5jvp,False,,0,False,Pro/Cons of Contract vs FTE?,[],r/datascience,False,6,,0,,,False,t3_nh9tku,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1621569309.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am weighing the pros and cons of taking a contract position, which I haven’t done before. What are things I should consider?&lt;/p&gt;

&lt;p&gt;All I know about contract work is: -they don’t take any taxes out -no benefits, no health insurance -less networking -not a part of team/culture -may or may not turn into a job -less stable&lt;/p&gt;

&lt;p&gt;This is for a data analyst role. They are offering 40/hr. But my current annual salary is 75k with great health coverage.&lt;/p&gt;

&lt;p&gt;What factors should I consider? What do you wish you knew before taking your first contract role? Why do people generally look down on contract offers, I don’t get it...?&lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nh9tku,True,,jerodme,,10,True,all_ads,False,[],False,,/r/datascience/comments/nh9tku/procons_of_contract_vs_fte/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh9tku/procons_of_contract_vs_fte/,515405,1621540509.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"I recently came across these topics - they look very interesting, but also very complicated. Has anyone ever dealt with them before? What kind of projects did you use them for?",t2_3tosvccj,False,,0,False,"Has anyone worked with ""structural equation modeling"" or ""statistical process control""?",[],r/datascience,False,6,discussion,0,,,False,t3_nh9g9y,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621568386.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently came across these topics - they look very interesting, but also very complicated. Has anyone ever dealt with them before? What kind of projects did you use them for?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh9g9y,True,,jj4646,,8,True,all_ads,False,[],False,,/r/datascience/comments/nh9g9y/has_anyone_worked_with_structural_equation/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh9g9y/has_anyone_worked_with_structural_equation/,515405,1621539586.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Background info: I was a stats major in undergrad and now I'm getting my masters in cs from OMSCS (Georgia tech). I've been working at a large fortune 500 company for a year now as a data analyst/data scientist. 

My job consists mostly of building tables and metrics in SQL, and I recently started doing some python work involving solving the vehicle routing problem, so I've been introduced to a lot of machine learning models and libraries. I'm also familiar with cloud computing and use cloud technology in my day to day. 

The problem is that this team has absolutely no guidance, no one checks on my work and no one cares about my work, it's really assigned to me as a learning opportunity. The team doesn't really need a data scientist at all. Also, I'm the only data scientist on the team, everyone else is a software developer. I've learned a ton but the lack of guidance really really frustrates me and I think it puts me at a big disadvantage when it comes to learning. 

My question is: given my experience will I even be able to find another data science job or should I just stay at my current job?",t2_uf9472q,False,,0,False,Should I get a new job?,[],r/datascience,False,6,career,0,,,False,t3_ngzhno,False,dark,0.57,,public,2,0,{},,,False,[],,False,False,,{},Career,False,2,,False,False,self,False,,[],{},,True,,1621544211.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Background info: I was a stats major in undergrad and now I&amp;#39;m getting my masters in cs from OMSCS (Georgia tech). I&amp;#39;ve been working at a large fortune 500 company for a year now as a data analyst/data scientist. &lt;/p&gt;

&lt;p&gt;My job consists mostly of building tables and metrics in SQL, and I recently started doing some python work involving solving the vehicle routing problem, so I&amp;#39;ve been introduced to a lot of machine learning models and libraries. I&amp;#39;m also familiar with cloud computing and use cloud technology in my day to day. &lt;/p&gt;

&lt;p&gt;The problem is that this team has absolutely no guidance, no one checks on my work and no one cares about my work, it&amp;#39;s really assigned to me as a learning opportunity. The team doesn&amp;#39;t really need a data scientist at all. Also, I&amp;#39;m the only data scientist on the team, everyone else is a software developer. I&amp;#39;ve learned a ton but the lack of guidance really really frustrates me and I think it puts me at a big disadvantage when it comes to learning. &lt;/p&gt;

&lt;p&gt;My question is: given my experience will I even be able to find another data science job or should I just stay at my current job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngzhno,True,,ithsefinque,,12,True,all_ads,False,[],False,,/r/datascience/comments/ngzhno/should_i_get_a_new_job/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngzhno/should_i_get_a_new_job/,515405,1621515411.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I'm beginning to learn data science and in most courses and programs I'm seeing so far, students are often asked to start with learning environments like Jupyter notebooks, Spyder or some sort of text editor like G Edit or something.   


I'm just curious as to how things happen in the real world. Do people still work in these environments? I use Jupyter notebooks and sometimes Spyder, but I understand that one can work in Terminal/Command Prompt as well. Although, I'm not sure I understand why - because editing code and stuff seems a lot easier otherwise. Just curious.",t2_9487g0zm,False,,0,False,Working environments in the real world.,[],r/datascience,False,6,discussion,0,,,False,t3_ng6gej,False,dark,0.98,,public,115,0,{},,,False,[],,False,False,,{},Discussion,False,115,,False,False,self,False,,[],{},,True,,1621461183.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m beginning to learn data science and in most courses and programs I&amp;#39;m seeing so far, students are often asked to start with learning environments like Jupyter notebooks, Spyder or some sort of text editor like G Edit or something.   &lt;/p&gt;

&lt;p&gt;I&amp;#39;m just curious as to how things happen in the real world. Do people still work in these environments? I use Jupyter notebooks and sometimes Spyder, but I understand that one can work in Terminal/Command Prompt as well. Although, I&amp;#39;m not sure I understand why - because editing code and stuff seems a lot easier otherwise. Just curious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng6gej,True,,Quaternion253,,58,True,all_ads,False,[],False,,/r/datascience/comments/ng6gej/working_environments_in_the_real_world/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng6gej/working_environments_in_the_real_world/,515405,1621432383.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I recently came across a newer technique called ""accumulated local effects"", that attempts to explain the effect of predictor variables on the response variable :  

https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html

Has anyone tried using this method on real data? Did you find it useful? Any stories/anecdotes/experiences/comments/reviews you would be willing to share regaeding this method?",t2_3f0i9m72,False,,0,False,Accumulated Local Effects,[],r/datascience,False,6,discussion,0,,,False,t3_nh19zq,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621548711.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently came across a newer technique called &amp;quot;accumulated local effects&amp;quot;, that attempts to explain the effect of predictor variables on the response variable :  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html""&gt;https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Has anyone tried using this method on real data? Did you find it useful? Any stories/anecdotes/experiences/comments/reviews you would be willing to share regaeding this method?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nh19zq,True,,SQL_beginner,,1,True,all_ads,False,[],False,,/r/datascience/comments/nh19zq/accumulated_local_effects/,all_ads,False,https://www.reddit.com/r/datascience/comments/nh19zq/accumulated_local_effects/,515405,1621519911.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?auto=webp&amp;s=d357a478ca9eb7a2f140df123f7e0f456b4e6d28', 'width': 568, 'height': 352}, 'resolutions': [{'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0fab85cd22f4dba076917acb91b18e80547951d', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a320afc3c4baa211fc0a34bbc6a0cbbf746a317', 'width': 216, 'height': 133}, {'url': 'https://external-preview.redd.it/x_EMPD7O4yCZsSyIFIoKBMAyUkR4kXFhbXZpX4s9blE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f8c471500038e92f0443135b411c99fd52cc70a', 'width': 320, 'height': 198}], 'variants': {}, 'id': 'CYFWfKV4HsIBu2g4-p4-alP-cSR5KE_NS9BlqhMKU5U'}], 'enabled': False}",,,,,
,datascience,"I'm in a situation where I need to ""productionise"" a large number of models written in various languages. We have a system set up for deploying Python models in Docker containers, accessible via API. 

Currently our approach is to attempt to wrap any non-Python models in Python code and deploy using our existing framework. For example, we can wrap an R model to resemble a Python one via a library such as RPy2. However, this isn't particularly elegant, and new wrappers have to be written for each new language (or even for particularly different models in the same language).

Another option I'd considered was having the models kept in their own language-specific scripts, which can be executed from inside Python app via calls to the command line. This seems a better approach since we're no longer dealing with wrappers, though I'm concerned that having to run import statements/load the model for each inference could cause high latency.

A third option I've been toying with (though haven't managed to figure out the details for) is to have the web app run in one container, and the model in a second, with some very minimal model serving code. I think this might allow us to sidestep the issue of loading dependencies, though it seems we'd have to write the model serving code in the model's language (the original problem).

I'm interested to hear your thoughts on this - has anyone else found an elegant solution to this?",t2_eetn0,False,,0,False,Language-agnostic deployment setup?,[],r/datascience,False,6,projects,0,,,False,t3_ngv3ts,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Projects,False,1,,False,False,self,False,,[],{},,True,,1621529877.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in a situation where I need to &amp;quot;productionise&amp;quot; a large number of models written in various languages. We have a system set up for deploying Python models in Docker containers, accessible via API. &lt;/p&gt;

&lt;p&gt;Currently our approach is to attempt to wrap any non-Python models in Python code and deploy using our existing framework. For example, we can wrap an R model to resemble a Python one via a library such as RPy2. However, this isn&amp;#39;t particularly elegant, and new wrappers have to be written for each new language (or even for particularly different models in the same language).&lt;/p&gt;

&lt;p&gt;Another option I&amp;#39;d considered was having the models kept in their own language-specific scripts, which can be executed from inside Python app via calls to the command line. This seems a better approach since we&amp;#39;re no longer dealing with wrappers, though I&amp;#39;m concerned that having to run import statements/load the model for each inference could cause high latency.&lt;/p&gt;

&lt;p&gt;A third option I&amp;#39;ve been toying with (though haven&amp;#39;t managed to figure out the details for) is to have the web app run in one container, and the model in a second, with some very minimal model serving code. I think this might allow us to sidestep the issue of loading dependencies, though it seems we&amp;#39;d have to write the model serving code in the model&amp;#39;s language (the original problem).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested to hear your thoughts on this - has anyone else found an elegant solution to this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngv3ts,True,,Coprosmo,,3,True,all_ads,False,[],False,,/r/datascience/comments/ngv3ts/languageagnostic_deployment_setup/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngv3ts/languageagnostic_deployment_setup/,515405,1621501077.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"When I studied DS for mastery, I noticed that people came from different backgrounds. Consequently, people were struggling in different classes according to what they studied before. For example, people that did statistics as bachelor mentioned that they had a lot of problems with the programming aspect of DS, but were much more comfortable with modeling. I had a lot of trouble with Bayesian statistics but was much more comfortable with programming (I was a Software engineer before).   


So out of curiosity, what was your experience like, what topic did you find difficult, and which ones did you find easy?",t2_j1u3fe,False,,0,False,What is the hardest topic that you encountered in Data Science when you studied it?,[],r/datascience,False,6,discussion,0,,,False,t3_ng951l,False,dark,0.83,,public,17,0,{},,,False,[],,False,False,,{},Discussion,False,17,,False,False,self,False,,[],{},,True,,1621467587.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I studied DS for mastery, I noticed that people came from different backgrounds. Consequently, people were struggling in different classes according to what they studied before. For example, people that did statistics as bachelor mentioned that they had a lot of problems with the programming aspect of DS, but were much more comfortable with modeling. I had a lot of trouble with Bayesian statistics but was much more comfortable with programming (I was a Software engineer before).   &lt;/p&gt;

&lt;p&gt;So out of curiosity, what was your experience like, what topic did you find difficult, and which ones did you find easy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng951l,True,,ngorovitch,,32,True,all_ads,False,[],False,,/r/datascience/comments/ng951l/what_is_the_hardest_topic_that_you_encountered_in/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng951l/what_is_the_hardest_topic_that_you_encountered_in/,515405,1621438787.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I've been contacted by an acquaintance to do some freelance work, and I'm unsure how to bill it: should I just set an hourly rate and charge according to time invested until completion? They are interested in the final product (not on the code), and since I would be using some personal libraries of mine, on the one hand I feel that being too fast is counterproductive with an hourly rate (= little money). However if I just set a flat price and there's unforeseen issues, I might have to work more, ending up with an effective hourly rate that is too low.  


How do you deal with this and what do you recommend?",t2_nqspn,False,,0,False,Question regarding freelancing,[],r/datascience,False,6,discussion,0,,,False,t3_ngg2su,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621484688.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been contacted by an acquaintance to do some freelance work, and I&amp;#39;m unsure how to bill it: should I just set an hourly rate and charge according to time invested until completion? They are interested in the final product (not on the code), and since I would be using some personal libraries of mine, on the one hand I feel that being too fast is counterproductive with an hourly rate (= little money). However if I just set a flat price and there&amp;#39;s unforeseen issues, I might have to work more, ending up with an effective hourly rate that is too low.  &lt;/p&gt;

&lt;p&gt;How do you deal with this and what do you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngg2su,True,,polidrupa,,2,True,all_ads,False,[],False,,/r/datascience/comments/ngg2su/question_regarding_freelancing/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngg2su/question_regarding_freelancing/,515405,1621455888.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have started to work for a small start-up company. We are only 2 data scientists. 

I am trying to understand consumer satisfaction by analyzing reviews. Sentiment Analysis and NER will be methods I will go for as initial step. 

My company doesn't have an NLP pipeline yet. 

I wonder which one is better: using Paid NLP Tools like Google Could NLP, IBM's Watson NLU or a self build NLP model?

I would be happy to hear what you think. Pros and cons.

Google's service seems a bit expensive. But, pricing is still confusing. For example, if I have 5 million reviews, how much am I expected to pay for Sentiment Analysis and NER services? (an estimate) 

Does Google charge me again whenever I run sentiment analysis? 

Is there any cheaper but still effective cloud computing NLP tool that you can suggest?

I would be happy to hear your insights!",t2_yio7w,False,,0,False,Paid NLP Tools vs Building Own Model,[],r/datascience,False,6,tooling,0,,,False,t3_ngcufb,False,dark,0.6,,public,1,1,{},,,False,[],,False,False,,{},Tooling,False,1,,False,False,self,1621456064.0,,[],{},,True,,1621476691.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have started to work for a small start-up company. We are only 2 data scientists. &lt;/p&gt;

&lt;p&gt;I am trying to understand consumer satisfaction by analyzing reviews. Sentiment Analysis and NER will be methods I will go for as initial step. &lt;/p&gt;

&lt;p&gt;My company doesn&amp;#39;t have an NLP pipeline yet. &lt;/p&gt;

&lt;p&gt;I wonder which one is better: using Paid NLP Tools like Google Could NLP, IBM&amp;#39;s Watson NLU or a self build NLP model?&lt;/p&gt;

&lt;p&gt;I would be happy to hear what you think. Pros and cons.&lt;/p&gt;

&lt;p&gt;Google&amp;#39;s service seems a bit expensive. But, pricing is still confusing. For example, if I have 5 million reviews, how much am I expected to pay for Sentiment Analysis and NER services? (an estimate) &lt;/p&gt;

&lt;p&gt;Does Google charge me again whenever I run sentiment analysis? &lt;/p&gt;

&lt;p&gt;Is there any cheaper but still effective cloud computing NLP tool that you can suggest?&lt;/p&gt;

&lt;p&gt;I would be happy to hear your insights!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ngcufb,True,,geldersekifuzuli,,7,True,all_ads,False,[],False,,/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/,all_ads,False,https://www.reddit.com/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/,515405,1621447891.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,"See title.

I'm interested in working with a large dataset of almost 100,000 faces for a research project, but I wish to remain within UK, EU and US  regulations for doing so.

What are, or where can I find, the regulations for such work?",t2_4mk1e,False,,0,False,Laws and regulations for working with large anonymised datasets of faces?,[],r/datascience,False,6,discussion,0,,,False,t3_ng287u,False,dark,0.67,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1621449225.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;See title.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in working with a large dataset of almost 100,000 faces for a research project, but I wish to remain within UK, EU and US  regulations for doing so.&lt;/p&gt;

&lt;p&gt;What are, or where can I find, the regulations for such work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng287u,True,,Jimbobmij,,6,True,all_ads,False,[],False,,/r/datascience/comments/ng287u/laws_and_regulations_for_working_with_large/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng287u/laws_and_regulations_for_working_with_large/,515405,1621420425.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Due to restrictions from the software development team, I need to restrict the number of features (variables) for my model to within a maximum number, say 10 or less. I am wondering if anyone here has run into such a constraint and how you went about handling it (assuming the restriction above is non-negotiable; and trust me, we have tried negotiating with the software folks :)). I have tried a number of selection methods to go about doing this and selected the ""top"" 10 features, for instance, as sorted by the following:

1. Feature importance (Xgboost)
2. Boruta
3. SHAP

Some drop in performance was obviously expected relative to model with all features available but the drop in performance has been been much sharper than I am comfortable with (using either of the three methods listed above). Therefore, I am hoping folks here have experience with better ways to go about doing this. 

I would very much appreciate any feedback.",t2_fw5vcdv,False,,0,False,Selecting a limited number of features,[],r/datascience,False,6,discussion,0,,,False,t3_ng6lns,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,False,self,False,,[],{},,True,,1621461537.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Due to restrictions from the software development team, I need to restrict the number of features (variables) for my model to within a maximum number, say 10 or less. I am wondering if anyone here has run into such a constraint and how you went about handling it (assuming the restriction above is non-negotiable; and trust me, we have tried negotiating with the software folks :)). I have tried a number of selection methods to go about doing this and selected the &amp;quot;top&amp;quot; 10 features, for instance, as sorted by the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feature importance (Xgboost)&lt;/li&gt;
&lt;li&gt;Boruta&lt;/li&gt;
&lt;li&gt;SHAP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some drop in performance was obviously expected relative to model with all features available but the drop in performance has been been much sharper than I am comfortable with (using either of the three methods listed above). Therefore, I am hoping folks here have experience with better ways to go about doing this. &lt;/p&gt;

&lt;p&gt;I would very much appreciate any feedback.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ng6lns,True,,unnamedn00b,,27,True,all_ads,False,[],False,,/r/datascience/comments/ng6lns/selecting_a_limited_number_of_features/,all_ads,False,https://www.reddit.com/r/datascience/comments/ng6lns/selecting_a_limited_number_of_features/,515404,1621432737.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"This is a unique situation...

Let me start out by saying I am a “IT Support analyst intern” at my job, part time. What I do however is not all that complex, I use pivot tables and excel as forms to show company spending at several locations(I don’t recommend anything I simply show the bills in the best way I can, currently it’s a pivot table from the previous employee) 

My career goal is Data Science and starting out as a Data Analyst to get there. Perhaps getting a masters while being a Data Analyst. Currently, my higher ups told me if I can learn Python and how to somehow implement it in my job I can use it for resume building purposes, so I’m reading “Automate the Boring Stuff” since it has parts about Python with excel and PDFs.

Allow me to also note I am a CS major specializing in Data Science. This does have a class for Python with data science but I’d rather learn it sooner for experience purposes. This has nice a machine learning class too I won’t be able to take for another year. Of course SQL is in the database class next semester . 

My question is, what else should I be doing now to help get an actual data science internship sooner? Or data analyst if not, since that’s not my current job title. Would using Python with excel to show bill amounts count as a “Data analytic” experience? I would think not because it really doesn’t cover the broad strokes of the full job position “Data Scientist/Analyst” unless there’s a way I can visualize excel data I’m missing, apart from python. Is there any key skills I have to learn ASAP, even with a class coming up? Like SQL? And during this, what actual Data Science skills should I be looking at right now to aid in actually getting a possible data science internship? 

Is there any key skills I’m missing? Are there any good resources to learn these skills like Python(if not my current book), SQL, Spark, etc?",t2_55fytx,False,,0,False,Starting out as a Data Analyst to move into Data Science?,[],r/datascience,False,6,career,0,,,False,t3_nf8hxs,False,dark,0.94,,public,175,0,{},,,False,[],,False,False,,{},Career,False,175,,False,False,self,False,,[],{},,True,,1621366913.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a unique situation...&lt;/p&gt;

&lt;p&gt;Let me start out by saying I am a “IT Support analyst intern” at my job, part time. What I do however is not all that complex, I use pivot tables and excel as forms to show company spending at several locations(I don’t recommend anything I simply show the bills in the best way I can, currently it’s a pivot table from the previous employee) &lt;/p&gt;

&lt;p&gt;My career goal is Data Science and starting out as a Data Analyst to get there. Perhaps getting a masters while being a Data Analyst. Currently, my higher ups told me if I can learn Python and how to somehow implement it in my job I can use it for resume building purposes, so I’m reading “Automate the Boring Stuff” since it has parts about Python with excel and PDFs.&lt;/p&gt;

&lt;p&gt;Allow me to also note I am a CS major specializing in Data Science. This does have a class for Python with data science but I’d rather learn it sooner for experience purposes. This has nice a machine learning class too I won’t be able to take for another year. Of course SQL is in the database class next semester . &lt;/p&gt;

&lt;p&gt;My question is, what else should I be doing now to help get an actual data science internship sooner? Or data analyst if not, since that’s not my current job title. Would using Python with excel to show bill amounts count as a “Data analytic” experience? I would think not because it really doesn’t cover the broad strokes of the full job position “Data Scientist/Analyst” unless there’s a way I can visualize excel data I’m missing, apart from python. Is there any key skills I have to learn ASAP, even with a class coming up? Like SQL? And during this, what actual Data Science skills should I be looking at right now to aid in actually getting a possible data science internship? &lt;/p&gt;

&lt;p&gt;Is there any key skills I’m missing? Are there any good resources to learn these skills like Python(if not my current book), SQL, Spark, etc?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf8hxs,True,,ToothPickLegs,,106,True,all_ads,False,[],False,,/r/datascience/comments/nf8hxs/starting_out_as_a_data_analyst_to_move_into_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf8hxs/starting_out_as_a_data_analyst_to_move_into_data/,515404,1621338113.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"&gt;TL;DR: Neural Search is a new approach to retrieving information using neural networks. Traditional techniques to search typically meant writing rules to “understand” the data being searched and return the best results. But with neural search, developers don’t need to wrack their brains for these rules; The system learns the rules by itself and gets better as it goes along. Even developers who don’t know machine learning can quickly build a search engine using open-source frameworks such as [Jina](https://github.com/jina-ai/jina).

Table of contents

* What is Neural Search
* Evolution of search methods
* Rule-based Search vs Neural Search
* Applications of Neural Search
* Get started with Neural Search

## What is Neural Search?

There is a massive amount of data on the web; how can we effectively search through it for relevant information? And it’s not just the web where we need it: Our computers store terabytes of company and personal data that we need to work with; we need effective search to get our day-to-day job done. And what do I mean by effective search

* Can we go beyond just matching keywords?
* Can we search using natural language, just like we would write or speak?
* Can we make the search smart enough to forgive our minor mistakes?
* Can we search for things that aren’t an exact match but are “close enough”?

We can answer all those questions with one word: Yes. To understand how, we need to enter the world of Natural Language Processing. NLP is a field of computer science that deals with analyzing natural language data, like the conversations people have every day. NLP is the foundation of intelligent search, and we have seen three different approaches in this field as follows.

## Evolution of search methods

1. **Rules (1950–1990s)**Complex handwritten rules that emulate Natural Language Understanding.**Drawbacks:** Handwritten rules can only be made more accurate by increasing their complexity, which is a much more difficult task that becomes unmanageable over time.
2. **Statistics (1990s — 2010s)**Probabilistic decisions based on weights, machine learning and feature engineering.Creating and managing rules was solved with machine learning, where the system automatically learns rules by analysing large real-world texts.**Drawbacks:** These statistical methods require elaborate feature engineering.
3. **Neural Networks (Present)**Advanced machine learning methods such as deep neural networks and representation learning.Since 2015, statistical methods have been largely abandoned, and there has been a shift to [neural networks](https://en.wikipedia.org/wiki/Neural_network) in machine learning. Popular techniques using this method make it a more accurate and a scalable alternative. It involves

* Use of \`word embeddings\` to capture semantic properties of words
* Focus on end-to-end learning of higher-level tasks (e.g., question answering)

&amp;#x200B;

&gt;When you use Neural Networks to make your search smarter, we call this a **Neural Search System**. And as you will see, it addresses some of the critical shortcomings of other methods.

Note that the applications of Neural Search are not just limited to text. It goes well beyond what NLP covers. With neural search, we get additional capabilities to search images, audio, video, etc. Let’s look at a comparison of the extreme ends of search methods — “Rules” vs “Neural Networks”:

## Rules (Symbolic Search) vs Neural Networks (Neural Search)

While the Neural Search method has become more widespread since 2015, and should be the primary focus area of any new search system. However, we shouldn’t completely rule out Symbolic (rule-based) Search methods. In fact, using a combination of Neural Search and Symbolic Search may result in optimized results. Let’s look at some of the powerful applications of Neural Search

## Applications Of Neural Search

**Semantic search**

🔍 addidsa trosers (misspelled brand and category, still returns relevant results similar to query “adidas trousers”)

**Search between data types**

With Neural Search, you can use one kind of data to search another kind of data, for example using text to search for images, or audio to search for video.

**Search with multiple data types**

With Neural Search, you can build queries with multiple query data types e.g. search images with text+image

## Get started with Neural Search

For rule-based searches, **Apache Solr, Elasticsearch, and Lucene** are the de-facto solutions. On the other hand, Neural Search is relatively new domain, there aren’t so many off-the-shelf packages. Also training the neural network for such a system requires a lot of data. These challenges can be solved using [Jina](http://github.com/jina-ai/jina/), an open-source neural search framework. To get started with building your own Neural Search system using [Jina](http://github.com/jina-ai/jina/). 

&amp;#x200B;

**References/Notes:** 

Neural Search term is less academic form of the term **Neural Information Retrieval** which first appeared during a [research workshop in 2016](https://www.microsoft.com/en-us/research/event/neuir2016/). I also found it useful to learn about [how google search  works](https://www.youtube.com/watch?v=0eKVizvYSUQ).",t2_auwgbh53,False,,0,False,Neural Search - I did research on the topic and this is what I learned,[],r/datascience,False,6,education,0,,,False,t3_nfulh9,False,dark,0.81,,public,6,1,{},,,False,[],,False,False,,{},Education,False,6,,False,True,self,False,,[],{},,True,,1621423258.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;blockquote&gt;
&lt;p&gt;TL;DR: Neural Search is a new approach to retrieving information using neural networks. Traditional techniques to search typically meant writing rules to “understand” the data being searched and return the best results. But with neural search, developers don’t need to wrack their brains for these rules; The system learns the rules by itself and gets better as it goes along. Even developers who don’t know machine learning can quickly build a search engine using open-source frameworks such as &lt;a href=""https://github.com/jina-ai/jina""&gt;Jina&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Table of contents&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is Neural Search&lt;/li&gt;
&lt;li&gt;Evolution of search methods&lt;/li&gt;
&lt;li&gt;Rule-based Search vs Neural Search&lt;/li&gt;
&lt;li&gt;Applications of Neural Search&lt;/li&gt;
&lt;li&gt;Get started with Neural Search&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;What is Neural Search?&lt;/h2&gt;

&lt;p&gt;There is a massive amount of data on the web; how can we effectively search through it for relevant information? And it’s not just the web where we need it: Our computers store terabytes of company and personal data that we need to work with; we need effective search to get our day-to-day job done. And what do I mean by effective search&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can we go beyond just matching keywords?&lt;/li&gt;
&lt;li&gt;Can we search using natural language, just like we would write or speak?&lt;/li&gt;
&lt;li&gt;Can we make the search smart enough to forgive our minor mistakes?&lt;/li&gt;
&lt;li&gt;Can we search for things that aren’t an exact match but are “close enough”?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can answer all those questions with one word: Yes. To understand how, we need to enter the world of Natural Language Processing. NLP is a field of computer science that deals with analyzing natural language data, like the conversations people have every day. NLP is the foundation of intelligent search, and we have seen three different approaches in this field as follows.&lt;/p&gt;

&lt;h2&gt;Evolution of search methods&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Rules (1950–1990s)&lt;/strong&gt;Complex handwritten rules that emulate Natural Language Understanding.&lt;strong&gt;Drawbacks:&lt;/strong&gt; Handwritten rules can only be made more accurate by increasing their complexity, which is a much more difficult task that becomes unmanageable over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics (1990s — 2010s)&lt;/strong&gt;Probabilistic decisions based on weights, machine learning and feature engineering.Creating and managing rules was solved with machine learning, where the system automatically learns rules by analysing large real-world texts.&lt;strong&gt;Drawbacks:&lt;/strong&gt; These statistical methods require elaborate feature engineering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural Networks (Present)&lt;/strong&gt;Advanced machine learning methods such as deep neural networks and representation learning.Since 2015, statistical methods have been largely abandoned, and there has been a shift to &lt;a href=""https://en.wikipedia.org/wiki/Neural_network""&gt;neural networks&lt;/a&gt; in machine learning. Popular techniques using this method make it a more accurate and a scalable alternative. It involves&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Use of `word embeddings` to capture semantic properties of words&lt;/li&gt;
&lt;li&gt;Focus on end-to-end learning of higher-level tasks (e.g., question answering)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When you use Neural Networks to make your search smarter, we call this a &lt;strong&gt;Neural Search System&lt;/strong&gt;. And as you will see, it addresses some of the critical shortcomings of other methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that the applications of Neural Search are not just limited to text. It goes well beyond what NLP covers. With neural search, we get additional capabilities to search images, audio, video, etc. Let’s look at a comparison of the extreme ends of search methods — “Rules” vs “Neural Networks”:&lt;/p&gt;

&lt;h2&gt;Rules (Symbolic Search) vs Neural Networks (Neural Search)&lt;/h2&gt;

&lt;p&gt;While the Neural Search method has become more widespread since 2015, and should be the primary focus area of any new search system. However, we shouldn’t completely rule out Symbolic (rule-based) Search methods. In fact, using a combination of Neural Search and Symbolic Search may result in optimized results. Let’s look at some of the powerful applications of Neural Search&lt;/p&gt;

&lt;h2&gt;Applications Of Neural Search&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Semantic search&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;🔍 addidsa trosers (misspelled brand and category, still returns relevant results similar to query “adidas trousers”)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search between data types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Neural Search, you can use one kind of data to search another kind of data, for example using text to search for images, or audio to search for video.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search with multiple data types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Neural Search, you can build queries with multiple query data types e.g. search images with text+image&lt;/p&gt;

&lt;h2&gt;Get started with Neural Search&lt;/h2&gt;

&lt;p&gt;For rule-based searches, &lt;strong&gt;Apache Solr, Elasticsearch, and Lucene&lt;/strong&gt; are the de-facto solutions. On the other hand, Neural Search is relatively new domain, there aren’t so many off-the-shelf packages. Also training the neural network for such a system requires a lot of data. These challenges can be solved using &lt;a href=""http://github.com/jina-ai/jina/""&gt;Jina&lt;/a&gt;, an open-source neural search framework. To get started with building your own Neural Search system using &lt;a href=""http://github.com/jina-ai/jina/""&gt;Jina&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References/Notes:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Neural Search term is less academic form of the term &lt;strong&gt;Neural Information Retrieval&lt;/strong&gt; which first appeared during a &lt;a href=""https://www.microsoft.com/en-us/research/event/neuir2016/""&gt;research workshop in 2016&lt;/a&gt;. I also found it useful to learn about &lt;a href=""https://www.youtube.com/watch?v=0eKVizvYSUQ""&gt;how google search  works&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 250, 'id': 'award_a67d649d-5aa5-407e-a98b-32fd9e3a9696', 'penny_donate': None, 'award_sub_type': 'APPRECIATION', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Today I Learned', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfulh9,True,,opensourcecolumbus,,0,True,all_ads,False,[],False,,/r/datascience/comments/nfulh9/neural_search_i_did_research_on_the_topic_and/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfulh9/neural_search_i_did_research_on_the_topic_and/,515404,1621394458.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?auto=webp&amp;s=cd2e63dadee37edb122e0516c362b8ce99643a25', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82d184a49ef058c205b9d7612d08f4cf48443e57', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c6ef80d771bc40508e93c6f9bbe077ba29b6e74', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=570759531bc30f67662966eecdb8d9cd1d532eca', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85757a8a8a035a522f11ae1d746981f402a7b9db', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5404653f12829d0dda0c3c38825e6d54a9e56623', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/-Gqkn0oynN47DnWAhjFOgcLQ_DNaBxZ1HvqMCnYl6Qs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7cf7fc15587f5a240b1ee84d9da7c702f9ad57df', 'width': 1080, 'height': 540}], 'variants': {}, 'id': '3qR6MNvriXkC-_X_6so4ExfyWsU16ABi90cpsuNLfUs'}], 'enabled': False}",,,,,
,datascience,"I love Jupyter Notebooks but never thought of them as a tool to put code into production.

So I was very surprised by this article [Beyond Interactive: Notebook Innovation at Netflix](https://netflixtechblog.com/notebook-innovation-591ee3221233) (found thanks to [u/yoursdata](https://www.reddit.com/user/yoursdata/)'s [recent post](https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/) introducing what it seems a very interesting [newsletter](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)).

This is a 2018 article, anyone can confirm whether this philosophy continues at Netflix? Any other companies out there doing this?",t2_c4rvshhm,False,,0,False,Does Netflix use Jupyter Notebooks in production?,[],r/datascience,False,6,tooling,0,,,False,t3_nf47se,False,dark,0.98,,public,137,0,{},,,False,[],,False,False,,{},Tooling,False,137,,False,False,self,1621323018.0,,[],{},,True,,1621351586.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I love Jupyter Notebooks but never thought of them as a tool to put code into production.&lt;/p&gt;

&lt;p&gt;So I was very surprised by this article &lt;a href=""https://netflixtechblog.com/notebook-innovation-591ee3221233""&gt;Beyond Interactive: Notebook Innovation at Netflix&lt;/a&gt; (found thanks to &lt;a href=""https://www.reddit.com/user/yoursdata/""&gt;u/yoursdata&lt;/a&gt;&amp;#39;s &lt;a href=""https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/""&gt;recent post&lt;/a&gt; introducing what it seems a very interesting &lt;a href=""https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1""&gt;newsletter&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This is a 2018 article, anyone can confirm whether this philosophy continues at Netflix? Any other companies out there doing this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf47se,True,,JB__Quix,,51,True,all_ads,False,[],False,,/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/,515404,1621322786.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?auto=webp&amp;s=241d8db42bc0ed254033e9a6c1b8f1c054344a01', 'width': 1200, 'height': 631}, 'resolutions': [{'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c73ed082ec17ae91d15d8fa4cc5f8319277e90bb', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de7bc04ff15c421e368256ee3b5b5d9f2ee82759', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=728c4f95a84ed33bfd5169f92a1e941b06155860', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2b163de365bda7d77a7b2e6d1eae01539a9f61b', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd7c6a2a309194329f4d43aa4ef409024ce85117', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/fI9Fff3nPmXCgCQWKFvCTi62dnlub698BS2jBynAywk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=996a7abdd41b8852c170229bf02b67349bd1669e', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'U9tbXWRkvDO4t9f4Xeaz26on67WaEhmbVQuU5JRY3To'}], 'enabled': False}",,,,,
,datascience,"I am a self-taught data scientist who is working for a mining company. One thing I have always struggled with is to upskill in this field. If you are like me - who is not a beginner but have some years of experience, I am sure even you must have struggled with this.

Most of the youtube videos and blogs are focused on beginners and toy projects, which is not really helpful. I started reading companies engineering blogs and think this is the way to upskill after a certain level. I have also started curating these articles in a newsletter and will be publishing three links each week.

Links for this weeks are:-

1. [**A Five-Step Guide for Conducting Exploratory Data Analysis**](https://shopify.engineering/conducting-exploratory-data-analysis)
2. [**Beyond Interactive: Notebook Innovation at Netflix**](https://netflixtechblog.com/notebook-innovation-591ee3221233)
3. [**How machine learning powers Facebook’s News Feed ranking algorithm**](https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/)

If you are preparing for any system design interview, the third link can be helpful.

Link for my newsletter - [https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)

Will love to discuss it and any suggestion is welcome.

P.S:- If it breaks any community guidelines, let me know and I will delete this post.",t2_qv8q4b1,False,,0,False,Data Science in Practice,[],r/datascience,False,6,education,0,,,False,t3_neylas,False,dark,0.98,,public,347,3,{},,,False,[],,False,False,,{},Education,False,347,,False,False,self,False,,[],{},,True,,1621333010.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a self-taught data scientist who is working for a mining company. One thing I have always struggled with is to upskill in this field. If you are like me - who is not a beginner but have some years of experience, I am sure even you must have struggled with this.&lt;/p&gt;

&lt;p&gt;Most of the youtube videos and blogs are focused on beginners and toy projects, which is not really helpful. I started reading companies engineering blogs and think this is the way to upskill after a certain level. I have also started curating these articles in a newsletter and will be publishing three links each week.&lt;/p&gt;

&lt;p&gt;Links for this weeks are:-&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=""https://shopify.engineering/conducting-exploratory-data-analysis""&gt;&lt;strong&gt;A Five-Step Guide for Conducting Exploratory Data Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://netflixtechblog.com/notebook-innovation-591ee3221233""&gt;&lt;strong&gt;Beyond Interactive: Notebook Innovation at Netflix&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/""&gt;&lt;strong&gt;How machine learning powers Facebook’s News Feed ranking algorithm&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are preparing for any system design interview, the third link can be helpful.&lt;/p&gt;

&lt;p&gt;Link for my newsletter - &lt;a href=""https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1""&gt;https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Will love to discuss it and any suggestion is welcome.&lt;/p&gt;

&lt;p&gt;P.S:- If it breaks any community guidelines, let me know and I will delete this post.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 400, 'id': 'award_84276b1e-cc8f-484f-a19c-be6c09adc1a5', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/SnooClapping_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'An amazing showing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Bravo!', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=16&amp;height=16&amp;auto=webp&amp;s=647cccf78702582f30d23908180da092b135cffe', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=32&amp;height=32&amp;auto=webp&amp;s=4644ac0618ecdef010ae2368e2e58669953fd9a3', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca4efb2faa26429279f44ced2822f5e81ff37537', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=64&amp;height=64&amp;auto=webp&amp;s=3a307ad71aad031accfd47f1af82a2b1e09195cc', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png?width=128&amp;height=128&amp;auto=webp&amp;s=fb9b2c432b1ddd85fd653ef3cc1a28e5edc40a1f', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/m5fdvo7cl4451_Bravo-Static.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neylas,True,,yoursdata,,48,True,all_ads,False,[],False,,/r/datascience/comments/neylas/data_science_in_practice/,all_ads,False,https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/,515404,1621304210.0,1,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?auto=webp&amp;s=913d3da85bb969ebe82241c03e6bb763b5e392a0', 'width': 1215, 'height': 510}, 'resolutions': [{'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=203500cfd14054ecebe791e8fe8412ae757dd1e4', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb91ca58f725bd794e098b08af5c42cfb8c0c8c7', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25bdc8313df5a3e39164acc34812f41b4669355c', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e51c17ec9349b0db56614ae0744a4783796b152b', 'width': 640, 'height': 268}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbbbba2887179ddb14bb428ec242fd33876bc064', 'width': 960, 'height': 402}, {'url': 'https://external-preview.redd.it/3zb0VWtCEHabqbiSt6WLZhNUKORew8oq6H6WzbQRsdg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=633cd545b10fa99da6c61fd59bdb598452288409', 'width': 1080, 'height': 453}], 'variants': {}, 'id': '_BoKND9MoFf7PgT2dWpr5tp1bcI9Htozzg34wYezLQI'}], 'enabled': False}",,,,,
,datascience,"Hi all,

I work for a fairly small to mid-sized firm, and we're facing some growing challenges as a data team. Our leadership/management team currently respects teams that add value to the company, so our marketing team tends to receive significant recognition.

Other teams tend to take credit for the reports, models, and any other analysis we do. This lends to our team *apparently* not adding business value to the company and makes it more difficult to do salary raises, promotions, hiring, etc. Another issue is that another non-technical team is currently asking me to teach them how to use SQL to do their own data pulls. I want to be a team player, but this adds no value to my team except diminishing the workload they put on us.

How do I, as well as my team navigate through these office politics?",t2_f9vap,False,,0,False,How to make sure my team receives appropriate recognition?,[],r/datascience,False,6,career,0,,,False,t3_nfalsq,False,dark,0.88,,public,38,0,{},,,False,[],,False,False,,{},Career,False,38,,False,False,self,False,,[],{},,True,,1621372952.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I work for a fairly small to mid-sized firm, and we&amp;#39;re facing some growing challenges as a data team. Our leadership/management team currently respects teams that add value to the company, so our marketing team tends to receive significant recognition.&lt;/p&gt;

&lt;p&gt;Other teams tend to take credit for the reports, models, and any other analysis we do. This lends to our team &lt;em&gt;apparently&lt;/em&gt; not adding business value to the company and makes it more difficult to do salary raises, promotions, hiring, etc. Another issue is that another non-technical team is currently asking me to teach them how to use SQL to do their own data pulls. I want to be a team player, but this adds no value to my team except diminishing the workload they put on us.&lt;/p&gt;

&lt;p&gt;How do I, as well as my team navigate through these office politics?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfalsq,True,,snsdreborn,,18,True,all_ads,False,[],False,,/r/datascience/comments/nfalsq/how_to_make_sure_my_team_receives_appropriate/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfalsq/how_to_make_sure_my_team_receives_appropriate/,515404,1621344152.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,So I have a silly newbie question.  I am trying to work out pros  as cons for using these tools. We have a team of 20 people and some ( like 3 people) use Alteryx and the rest use python. I am trying to work out what the pros and cons would be for working a team. Does Alteryx make it easier or harder to work together.  What are the other pros and cons for a large team trying to work together vs in their own silos?,t2_deri9h9,False,,0,False,Alteryx vs Python for collaborating in a team,[],r/datascience,False,6,tooling,0,,,False,t3_nfgxdm,False,dark,0.84,,public,9,0,{},,,False,[],,False,False,,{},Tooling,False,9,,False,False,self,False,,[],{},,True,,1621388285.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have a silly newbie question.  I am trying to work out pros  as cons for using these tools. We have a team of 20 people and some ( like 3 people) use Alteryx and the rest use python. I am trying to work out what the pros and cons would be for working a team. Does Alteryx make it easier or harder to work together.  What are the other pros and cons for a large team trying to work together vs in their own silos?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfgxdm,True,,starbucksnamemike,,6,True,all_ads,False,[],False,,/r/datascience/comments/nfgxdm/alteryx_vs_python_for_collaborating_in_a_team/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfgxdm/alteryx_vs_python_for_collaborating_in_a_team/,515404,1621359485.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,Can you do freelance works in this field?,t2_6lgvt614,False,,0,False,Freelancing?,[],r/datascience,False,6,career,0,,,False,t3_nfa2e4,False,dark,0.72,,public,14,0,{},,,False,[],,False,False,,{},Career,False,14,,False,False,self,False,,[],{},,True,,1621371514.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can you do freelance works in this field?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfa2e4,True,,joacloz,,16,True,all_ads,False,[],False,,/r/datascience/comments/nfa2e4/freelancing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfa2e4/freelancing/,515404,1621342714.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,Hey guys so I’m doing a personal project using Spotify data to recommend songs and also using kmeans to cluster songs for analysis/playlist. I use Pycharm and I have a pretty beefy cpu/gpu but sometimes it’ll take forever to run. I’m not sure if my IDE is just not optimized or is there anything else I can do to get faster runtimes when creating models etc. The dataset has 600k+ rows if that helps!,t2_4rukxh3o,False,,0,False,Personal project runtime,[],r/datascience,False,6,projects,0,,,False,t3_nfph2x,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,False,,[],{},,True,,1621408870.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys so I’m doing a personal project using Spotify data to recommend songs and also using kmeans to cluster songs for analysis/playlist. I use Pycharm and I have a pretty beefy cpu/gpu but sometimes it’ll take forever to run. I’m not sure if my IDE is just not optimized or is there anything else I can do to get faster runtimes when creating models etc. The dataset has 600k+ rows if that helps!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfph2x,True,,FreeDominosPizzaGuy,,10,True,all_ads,False,[],False,,/r/datascience/comments/nfph2x/personal_project_runtime/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfph2x/personal_project_runtime/,515404,1621380070.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi all!

I’m a Data Scientist working in Australia. I’m the sole DS in my company and all the other DS I know outside of there are at a similar level I am. I used to be in academia in a different field so I don’t have a more senior person to talk to.

All this is to ask if you folks know of any mentoring networks that I could join.",t2_zru6c3n,False,,0,False,Interested in mentoring networks,[],r/datascience,False,6,network,0,,,False,t3_nf4rnz,False,dark,0.75,,public,11,0,{},,,False,[],,False,False,,{},Networking,False,11,,False,False,self,False,,[],{},,True,,1621353728.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all!&lt;/p&gt;

&lt;p&gt;I’m a Data Scientist working in Australia. I’m the sole DS in my company and all the other DS I know outside of there are at a similar level I am. I used to be in academia in a different field so I don’t have a more senior person to talk to.&lt;/p&gt;

&lt;p&gt;All this is to ask if you folks know of any mentoring networks that I could join.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf4rnz,True,,ndurst,,12,True,all_ads,False,[],False,,/r/datascience/comments/nf4rnz/interested_in_mentoring_networks/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf4rnz/interested_in_mentoring_networks/,515404,1621324928.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"Most anomaly detection techniques/packages focus on anomaly detection within a single time series; ie, take some sort of steady-state average and alert if the data suddenly goes above or below some threshold. My problem is totally different, however.

I have a hardware device that performs the same operation repeatedly. Most of the time it succeeds, but sometimes it fails. I have sensors that measure position and angle (6DOF), and I have a large data set of each attempt, whether or not it was a success or failure, and the sensor data (and first and second derivatives) from a few seconds before the event to a few seconds after.

What I'm looking for is a technique or python package that can analyze all this time series data and, given label of Success or Failure, identify if there are any anomalies that typically lead to a failure. I've done quite a bit of Googling and Stack Overflowing, but keep coming up with ""typical"" anomaly detection packages. Maybe I'm using the wrong keywords or language here to describe what I'm looking for? Any suggestions or pointers in the right direction would be greatly appreciated!",t2_9bcqo,False,,0,False,Techniques for anomaly detection across different time series?,[],r/datascience,False,6,discussion,0,,,False,t3_nf2q8p,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},Discussion,False,10,,False,False,self,1621317721.0,,[],{},,True,,1621346308.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most anomaly detection techniques/packages focus on anomaly detection within a single time series; ie, take some sort of steady-state average and alert if the data suddenly goes above or below some threshold. My problem is totally different, however.&lt;/p&gt;

&lt;p&gt;I have a hardware device that performs the same operation repeatedly. Most of the time it succeeds, but sometimes it fails. I have sensors that measure position and angle (6DOF), and I have a large data set of each attempt, whether or not it was a success or failure, and the sensor data (and first and second derivatives) from a few seconds before the event to a few seconds after.&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m looking for is a technique or python package that can analyze all this time series data and, given label of Success or Failure, identify if there are any anomalies that typically lead to a failure. I&amp;#39;ve done quite a bit of Googling and Stack Overflowing, but keep coming up with &amp;quot;typical&amp;quot; anomaly detection packages. Maybe I&amp;#39;m using the wrong keywords or language here to describe what I&amp;#39;m looking for? Any suggestions or pointers in the right direction would be greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf2q8p,True,,Archa3opt3ryx,,10,True,all_ads,False,[],False,,/r/datascience/comments/nf2q8p/techniques_for_anomaly_detection_across_different/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf2q8p/techniques_for_anomaly_detection_across_different/,515404,1621317508.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"After reading through bunch of articles about the different roles I am still confused on specific jobs within the data science community. I really want to focus on the coding, model creation, model deployment and monitoring. Maybe where to ML models are already predefined but my job would be putting it into production and hooking everything up. 

What would this role be called? MLops? ML engineering? Dev ops? The sense that I get is no one seems to know what to call these guys or they are sort of interchangeable titles that might mean something totally different to each company you apply for. Also, would this include making data pipelines or is that a data engineer's job?",t2_sqqip,False,,0,False,Confused on different job titles/roles of data science,[],r/datascience,False,6,discussion,0,,,False,t3_nfezz6,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621383776.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After reading through bunch of articles about the different roles I am still confused on specific jobs within the data science community. I really want to focus on the coding, model creation, model deployment and monitoring. Maybe where to ML models are already predefined but my job would be putting it into production and hooking everything up. &lt;/p&gt;

&lt;p&gt;What would this role be called? MLops? ML engineering? Dev ops? The sense that I get is no one seems to know what to call these guys or they are sort of interchangeable titles that might mean something totally different to each company you apply for. Also, would this include making data pipelines or is that a data engineer&amp;#39;s job?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nfezz6,True,,AKing2713,,6,True,all_ads,False,[],False,,/r/datascience/comments/nfezz6/confused_on_different_job_titlesroles_of_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nfezz6/confused_on_different_job_titlesroles_of_data/,515404,1621354976.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I have been looking for a data science role in the financial sector for quite a while but its been tough with no experience. I have an interview for Mindshare which is a media agency, this is a more analytics position which gives me exposure to projects and clients etc so I thought it would be valuable. Would it being a media agency primarily limit me in any way or is the analytics experience good regardless?

&amp;#x200B;

[https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com](https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com)",t2_2gqjctj0,False,,0,False,Does the type of company you work for matter for career progression?,[],r/datascience,False,6,,0,,,False,t3_nf61wa,False,dark,0.56,,public,1,0,{},,,False,[],,False,False,,{},Job Search,False,1,,False,False,self,False,,[],{},,True,,1621358483.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been looking for a data science role in the financial sector for quite a while but its been tough with no experience. I have an interview for Mindshare which is a media agency, this is a more analytics position which gives me exposure to projects and clients etc so I thought it would be valuable. Would it being a media agency primarily limit me in any way or is the analytics experience good regardless?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com""&gt;https://www.mindshareworld.com/germany/careers/analyst-integrated-analytics-consulting?codes=Indeed.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nf61wa,True,,randomperson3333,,7,True,all_ads,False,[],False,,/r/datascience/comments/nf61wa/does_the_type_of_company_you_work_for_matter_for/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf61wa/does_the_type_of_company_you_work_for_matter_for/,515404,1621329683.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?auto=webp&amp;s=44e61adf8464f54226d9cc8623e78c97ee548269', 'width': 1440, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9923c9936a78b633f20ee068505fadb00b319aaa', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbed24ac19bb47c27c8dc3f1856152d8f9ed8c34', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f435d04a1480908721cd140f4ea8b2d1fe293b08', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ffa47ba10e0cc61185f9dd080b2796135b79064', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=69aa28c9198d1bf7a119b589e62a051293f5369a', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/zd3J4tiGc0YXo_3BHMJIOEAqKAyWJdSkA8K1DLYj80I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4890a4bf5f95ee28853d7c145f61fa16628bb703', 'width': 1080, 'height': 675}], 'variants': {}, 'id': 'ehVvgccxokingeDnwFLbmjzBgbLv6nT_0WwfwDuxpcY'}], 'enabled': False}",,,,,
,datascience,Hello! I was just curious what everyone's on the job training and employer training program experiences have been like. I've really worked at some smaller startups where a senior data scientist would supervise and give advice. I recently got a job offer at a much bigger company so I'm curious about whether or not is normal for larger companies to have more organized training opportunities. Thank you!,t2_84fxp9sq,False,,0,False,On the job training and education,[],r/datascience,False,6,discussion,0,,,False,t3_nf24f6,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,False,self,False,,[],{},,True,,1621344210.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I was just curious what everyone&amp;#39;s on the job training and employer training program experiences have been like. I&amp;#39;ve really worked at some smaller startups where a senior data scientist would supervise and give advice. I recently got a job offer at a much bigger company so I&amp;#39;m curious about whether or not is normal for larger companies to have more organized training opportunities. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf24f6,True,,trmntr308,,8,True,all_ads,False,[],False,,/r/datascience/comments/nf24f6/on_the_job_training_and_education/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf24f6/on_the_job_training_and_education/,515404,1621315410.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,I am a full time data scientist but thinking about pivoting some of my skills on the side to get some extra income. Anyone have experience having a sperate company or offering certain services?,t2_6zqdq,False,,0,False,Anyone got a side hustle?,[],r/datascience,False,6,career,0,,,False,t3_nersx8,False,dark,0.77,,public,16,0,{},,,False,[],,False,False,,{},Career,False,16,,False,False,self,False,,[],{},,True,,1621314166.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a full time data scientist but thinking about pivoting some of my skills on the side to get some extra income. Anyone have experience having a sperate company or offering certain services?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nersx8,True,,edisekeed,,16,True,all_ads,False,[],False,,/r/datascience/comments/nersx8/anyone_got_a_side_hustle/,all_ads,False,https://www.reddit.com/r/datascience/comments/nersx8/anyone_got_a_side_hustle/,515404,1621285366.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"For example I have a linear regression model ready. I dont get all the features at the start. for example, I get 6 out of 10 features. I need to see the target variable range I will get.

&amp;#x200B;

 Also  if I need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range.",t2_2kxvoy8x,False,,0,False,How to deploy a real time model which gets the variables(features) in phases?,[],r/datascience,False,6,discussion,0,,,False,t3_nf6hhq,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621360102.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example I have a linear regression model ready. I dont get all the features at the start. for example, I get 6 out of 10 features. I need to see the target variable range I will get.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Also  if I need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nf6hhq,True,,spiritbear1,,7,True,all_ads,False,[],False,,/r/datascience/comments/nf6hhq/how_to_deploy_a_real_time_model_which_gets_the/,all_ads,False,https://www.reddit.com/r/datascience/comments/nf6hhq/how_to_deploy_a_real_time_model_which_gets_the/,515404,1621331302.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi there! I'm an aspiring data scientist hoping to one day work at a music analytics/streaming company such as Spotify, Pandora, or Tidal. I would like to learn more about what data scientists do at such companies and how they analyse user data to eventually drive company decisions.

If there's anyone here working in a similar field (or even have similar interests), I would love to have a chat with you to learn more about what you do. 

If interested, please send me a message or comment on this post so that I can get in touch with you.

Thanks!",t2_3loxj2cz,False,,0,False,Anyone working in a music analytics/streaming company like Spotify?,[],r/datascience,False,6,network,0,,,False,t3_ne72ct,False,dark,0.99,,public,192,0,{},,,False,[],,False,False,,{},Networking,False,192,,False,False,self,False,,[],{},,True,,1621255927.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there! I&amp;#39;m an aspiring data scientist hoping to one day work at a music analytics/streaming company such as Spotify, Pandora, or Tidal. I would like to learn more about what data scientists do at such companies and how they analyse user data to eventually drive company decisions.&lt;/p&gt;

&lt;p&gt;If there&amp;#39;s anyone here working in a similar field (or even have similar interests), I would love to have a chat with you to learn more about what you do. &lt;/p&gt;

&lt;p&gt;If interested, please send me a message or comment on this post so that I can get in touch with you.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ne72ct,True,,hedgehogist,,42,True,all_ads,False,[],False,,/r/datascience/comments/ne72ct/anyone_working_in_a_music_analyticsstreaming/,all_ads,False,https://www.reddit.com/r/datascience/comments/ne72ct/anyone_working_in_a_music_analyticsstreaming/,515404,1621227127.0,0,,False,8addf236-d780-11e7-932d-0e90af9dfe6e,,,,,,,
,datascience,"I am in the process of preparing a course on neural networks - it's a broad strokes walk through both basics and various different topics. I have decided that one week will be dedicated to transfer learning and multi task learning (together, since there are some interesting transfer learning approaches which leverage MTL). As part of the course, I want the students to solve a 'small' exercise where they improve some machine learning task by combining it with another one. 

However, it is really hard to come up with a good task like that :) regular fine tuning was easy enough - I decided to lean on the pytorch tutorial for that one. So far, I have not been able to find a similar 'toy example' that can both be run in google colab (I can not assume that my students have access to a GPU of their own) and where the benefit is tangible. My best bet so far (which is not working out yet) is to combine age and gender identification on a subset of the cropped version of the wiki face dataset ([https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)). I'm hoping that with a sufficiently small subset, I'll have a task which doesn't quite work on its own, but where real improvement can be seen when each image is analyzed in two different tasks.

 But, perhaps someone knows of a better problem?",t2_3moso,False,,0,False,anyone know of a good 'textbook example' for multi task learning?,[],r/datascience,False,6,discussion,0,,,False,t3_neqi90,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1621311082.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am in the process of preparing a course on neural networks - it&amp;#39;s a broad strokes walk through both basics and various different topics. I have decided that one week will be dedicated to transfer learning and multi task learning (together, since there are some interesting transfer learning approaches which leverage MTL). As part of the course, I want the students to solve a &amp;#39;small&amp;#39; exercise where they improve some machine learning task by combining it with another one. &lt;/p&gt;

&lt;p&gt;However, it is really hard to come up with a good task like that :) regular fine tuning was easy enough - I decided to lean on the pytorch tutorial for that one. So far, I have not been able to find a similar &amp;#39;toy example&amp;#39; that can both be run in google colab (I can not assume that my students have access to a GPU of their own) and where the benefit is tangible. My best bet so far (which is not working out yet) is to combine age and gender identification on a subset of the cropped version of the wiki face dataset (&lt;a href=""https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/""&gt;https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/&lt;/a&gt;). I&amp;#39;m hoping that with a sufficiently small subset, I&amp;#39;ll have a task which doesn&amp;#39;t quite work on its own, but where real improvement can be seen when each image is analyzed in two different tasks.&lt;/p&gt;

&lt;p&gt;But, perhaps someone knows of a better problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neqi90,True,,miturian,,6,True,all_ads,False,[],False,,/r/datascience/comments/neqi90/anyone_know_of_a_good_textbook_example_for_multi/,all_ads,False,https://www.reddit.com/r/datascience/comments/neqi90/anyone_know_of_a_good_textbook_example_for_multi/,515404,1621282282.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?auto=webp&amp;s=a545e6cc58a39ec5b13aef5c521072f8d70a1ad4', 'width': 2400, 'height': 1800}, 'resolutions': [{'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af876d35ca7565cda28d1011c76a294b15d42fa5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7143cbff70d22d656d3e88bf6ec51051e495eff5', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec2236cd3de263eda0386841570f1a833468f641', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ee27475e8d6e100192807d8f64403fafcf8cabc', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=309cf9ad0fe864e26d92b791cc3facb6b846b8a9', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/fKU2WFNvKyjPd7CR1XjTboBcCTjExlmNab2pObzJNS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e8c0b6b254f170ef3a1c1215e8bbb41524bea7f', 'width': 1080, 'height': 810}], 'variants': {}, 'id': '9B_lz_-bwwQH4RzXVOgA3ziUA1HmzZs56DFeFdMjsHU'}], 'enabled': False}",,,,,
,datascience,"My current position is kind of a consultant style role where there isn't a specific product/project to work on and it's kind of whatever projects come through the door. My next position however is working with a specific domain Risk/Fraud. I'm curious if working on a specific area e.g. Fraud/Ads/Recommender Systems/etc ends up locking you into that area for the remainder of your career and future positions. Also, out of curiosity which domain has the greatest potential for future positions.",t2_1k6f27,False,,0,False,Thoughts on getting stuck with working on a specific domain?,[],r/datascience,False,6,career,0,,,False,t3_neqdpq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Career,False,1,,False,False,self,False,,[],{},,True,,1621310785.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My current position is kind of a consultant style role where there isn&amp;#39;t a specific product/project to work on and it&amp;#39;s kind of whatever projects come through the door. My next position however is working with a specific domain Risk/Fraud. I&amp;#39;m curious if working on a specific area e.g. Fraud/Ads/Recommender Systems/etc ends up locking you into that area for the remainder of your career and future positions. Also, out of curiosity which domain has the greatest potential for future positions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,neqdpq,True,,m908f,,3,True,all_ads,False,[],False,,/r/datascience/comments/neqdpq/thoughts_on_getting_stuck_with_working_on_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/neqdpq/thoughts_on_getting_stuck_with_working_on_a/,515404,1621281985.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I have so far mostly worked in AdTech as a data scientist. Soon in about a month, I will be switching to electric vehicle fleet management which I have never worked with. I think the aspect which I mostly improved upon while working with AdTech was handling huge amount of data but the underlying models weren't so complex. It was almost always logistic regression. I did deal with some deep bayesian models.

Can anyone who has worked in similar field or related fields give me some pointers regarding the problems and the methodologies being used there? What I could gather so far was that the general problem is about vehicle routing and fleet management. What are the underlying methods being used there? What algorithms/models should I brush up on?",t2_3bdkl2w1,False,,0,False,Getting a head start when switching to a completely new domain,[],r/datascience,False,6,discussion,0,,,False,t3_necyqc,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1621277299.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have so far mostly worked in AdTech as a data scientist. Soon in about a month, I will be switching to electric vehicle fleet management which I have never worked with. I think the aspect which I mostly improved upon while working with AdTech was handling huge amount of data but the underlying models weren&amp;#39;t so complex. It was almost always logistic regression. I did deal with some deep bayesian models.&lt;/p&gt;

&lt;p&gt;Can anyone who has worked in similar field or related fields give me some pointers regarding the problems and the methodologies being used there? What I could gather so far was that the general problem is about vehicle routing and fleet management. What are the underlying methods being used there? What algorithms/models should I brush up on?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,necyqc,True,,proof_required,,11,True,all_ads,False,[],False,,/r/datascience/comments/necyqc/getting_a_head_start_when_switching_to_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/necyqc/getting_a_head_start_when_switching_to_a/,515404,1621248499.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,What are the differences? Is one just in academia and one in industry or is it like a rectangles and squares kinda deal?,t2_44oxrfns,False,,0,False,Statistician vs data scientist?,[],r/datascience,False,6,meta,0,,,False,t3_ndpft6,False,dark,0.93,,public,162,1,{},,,False,[],,False,False,,{},Meta,False,162,,False,False,self,False,,[],{},,True,,1621203939.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are the differences? Is one just in academia and one in industry or is it like a rectangles and squares kinda deal?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndpft6,True,,medylan,,118,True,all_ads,False,[],False,,/r/datascience/comments/ndpft6/statistician_vs_data_scientist/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndpft6/statistician_vs_data_scientist/,515404,1621175139.0,0,,False,481ee318-d77d-11e7-a4a3-0e8624d7129a,,,,,,,
,datascience,"Based on my experience, role titles such as Data Engineer and Data Scientist mean very different things depending on the company.

I see three main cases:

1. **Normally Bigger (and Older) Companies**  
DEs do all MLOps: They build anything needed to gather and store data. They put DS models into production too.  
DSs train models, do exploratory analysis, create variables, validate hypothesis, but don't put any of that into production themselves.
2. **Normally Modern (Tech) Companies**  
DEs gather and store data.  
DSs don't just train but also deploy their models into production.
3. **Normally Smaller Companies**  
One profile does it all.

I work now for a startup whose product is a realtime end to end platform which eases the otherwise complicated MLOps stuff. I'm writing some docs that explain how realtime is done now VS my company proposal and I'm finding that defining how things are done now is not straightforward. So, do you agree with the three cases above? Would you add more? Where would you put Data Architects, ML Engineers, etc.?",t2_c4rvshhm,False,,0,False,Differences between DE and DS actual job along different companies,[],r/datascience,False,6,discussion,0,,,False,t3_necjee,False,dark,0.67,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,1621278054.0,,[],{},,True,,1621275820.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Based on my experience, role titles such as Data Engineer and Data Scientist mean very different things depending on the company.&lt;/p&gt;

&lt;p&gt;I see three main cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normally Bigger (and Older) Companies&lt;/strong&gt;&lt;br/&gt;
DEs do all MLOps: They build anything needed to gather and store data. They put DS models into production too.&lt;br/&gt;
DSs train models, do exploratory analysis, create variables, validate hypothesis, but don&amp;#39;t put any of that into production themselves.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normally Modern (Tech) Companies&lt;/strong&gt;&lt;br/&gt;
DEs gather and store data.&lt;br/&gt;
DSs don&amp;#39;t just train but also deploy their models into production.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normally Smaller Companies&lt;/strong&gt;&lt;br/&gt;
One profile does it all.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I work now for a startup whose product is a realtime end to end platform which eases the otherwise complicated MLOps stuff. I&amp;#39;m writing some docs that explain how realtime is done now VS my company proposal and I&amp;#39;m finding that defining how things are done now is not straightforward. So, do you agree with the three cases above? Would you add more? Where would you put Data Architects, ML Engineers, etc.?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,necjee,True,,JB__Quix,,5,True,all_ads,False,[],False,,/r/datascience/comments/necjee/differences_between_de_and_ds_actual_job_along/,all_ads,False,https://www.reddit.com/r/datascience/comments/necjee/differences_between_de_and_ds_actual_job_along/,515404,1621247020.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"I am a self taught data professional, well versed in most data analytics/data engineering program, tools, math/statistics and software. At my current work place, I am constantly coming up with new analysis, tools, automation,  processes, troubleshooting and QA. I have earned the respect of my peers and managers, and I have excellent technical, teamwork, and communication skills. I am at par if not better overall than most of my colleagues whom have masters and PhDs. 

I personally feel as if I have a chip on my shoulder because of me being self taught. The grittiness needed to learn the material and find things efficiently on my own is something I value in myself.

It’s just really unfortunate that at face value, no matter what I do, there will be tons of people with advanced degrees in data, who get put ahead of me, and paid more than me, in the job search. I also feel like in the workplace, I also have to go the extra mile with new seniors I meet a lot of the times for them to take me seriously. I understand why it happens, but it really is just saddening and upsetting. 

Please tell me there are people in here that have felt and experienced the same thing as I have with this. If so, how have you learned to engage with this? 

Yes, I know one of the answers is to get a advanced degree.",t2_8x16rrzg,False,,0,False,Feeling overlooked and undervalued as a Self-Taught data professional when compared to advanced degree holders.,[],r/datascience,False,6,career,0,,,False,t3_nesj5b,False,dark,0.44,,public,0,0,{},,,False,[],,False,False,,{},Career,False,0,,False,False,self,False,,[],{},,True,,1621315979.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a self taught data professional, well versed in most data analytics/data engineering program, tools, math/statistics and software. At my current work place, I am constantly coming up with new analysis, tools, automation,  processes, troubleshooting and QA. I have earned the respect of my peers and managers, and I have excellent technical, teamwork, and communication skills. I am at par if not better overall than most of my colleagues whom have masters and PhDs. &lt;/p&gt;

&lt;p&gt;I personally feel as if I have a chip on my shoulder because of me being self taught. The grittiness needed to learn the material and find things efficiently on my own is something I value in myself.&lt;/p&gt;

&lt;p&gt;It’s just really unfortunate that at face value, no matter what I do, there will be tons of people with advanced degrees in data, who get put ahead of me, and paid more than me, in the job search. I also feel like in the workplace, I also have to go the extra mile with new seniors I meet a lot of the times for them to take me seriously. I understand why it happens, but it really is just saddening and upsetting. &lt;/p&gt;

&lt;p&gt;Please tell me there are people in here that have felt and experienced the same thing as I have with this. If so, how have you learned to engage with this? &lt;/p&gt;

&lt;p&gt;Yes, I know one of the answers is to get a advanced degree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nesj5b,True,,Justanotherguy2022,,38,True,all_ads,False,[],False,,/r/datascience/comments/nesj5b/feeling_overlooked_and_undervalued_as_a/,all_ads,False,https://www.reddit.com/r/datascience/comments/nesj5b/feeling_overlooked_and_undervalued_as_a/,515404,1621287179.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"[Source Code on Github](https://github.com/jina-ai/examples/tree/master/multimodal-search-pdf)

In this project I am using [Jina](https://github.com/jina-ai/jina) to search a repository of PDF files. The project allows a user to query the data by providing text, or an image, or both simultaneously.

**How to use it?**

Clone the project and run following commands

    # Install requirements
    pip install -r requirements.txt
    
    # Start the server
    python app.py -t query_restful
    
    # Query via REST API
    curl --request POST -d '{""top_k"": 10, ""mode"": ""search"",  ""data"": [""jina hello multimodal""]}' -H 'Content-Type: application/json' 'http://0.0.0.0:45670/api/search'

What's included in this example:

* Search text, image, PDF all in one Flow or in separate Flows
* Speed up indexing time with parallel Peas
* Use customized executors to better fit your needs
* Provide detailed docstrings for YAML files to help you understand the example  


Let me know your feedback and what would you use this project for. I'd love to help",t2_auwgbh53,False,,0,False,PDF search - Another project I built using Jina(AI Search framework),[],r/datascience,False,6,projects,0,,,False,t3_ne4tco,False,dark,0.78,,public,13,3,{},,,False,[],,False,False,,{},Projects,False,13,,False,True,self,False,,[],{},,True,,1621248594.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/jina-ai/examples/tree/master/multimodal-search-pdf""&gt;Source Code on Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this project I am using &lt;a href=""https://github.com/jina-ai/jina""&gt;Jina&lt;/a&gt; to search a repository of PDF files. The project allows a user to query the data by providing text, or an image, or both simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to use it?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Clone the project and run following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Install requirements
pip install -r requirements.txt

# Start the server
python app.py -t query_restful

# Query via REST API
curl --request POST -d &amp;#39;{&amp;quot;top_k&amp;quot;: 10, &amp;quot;mode&amp;quot;: &amp;quot;search&amp;quot;,  &amp;quot;data&amp;quot;: [&amp;quot;jina hello multimodal&amp;quot;]}&amp;#39; -H &amp;#39;Content-Type: application/json&amp;#39; &amp;#39;http://0.0.0.0:45670/api/search&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What&amp;#39;s included in this example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search text, image, PDF all in one Flow or in separate Flows&lt;/li&gt;
&lt;li&gt;Speed up indexing time with parallel Peas&lt;/li&gt;
&lt;li&gt;Use customized executors to better fit your needs&lt;/li&gt;
&lt;li&gt;Provide detailed docstrings for YAML files to help you understand the example&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let me know your feedback and what would you use this project for. I&amp;#39;d love to help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'award_2ae56630-cfe0-424e-b810-4945b9145358', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful (Pro)', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=57278b329d19fd1d345888bfff68a51528777538', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=db7b3f20402a8a6820a4ffebf35160d2557986e2', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=0100d8da8f4dae0dc81d430733aa622d752c268c', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=1029c080a179f45b6d83a51ed79dfd57997ae266', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=50e7f8a870f79df7bc38bedb8a12e01137df5a77', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ne4tco,True,,opensourcecolumbus,,2,True,all_ads,False,[],False,,/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/,all_ads,False,https://www.reddit.com/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/,515404,1621219794.0,3,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?auto=webp&amp;s=ce98c31438ca2d2be9fbbc00504d993b0e76200f', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e627429c5dcad41c7d9f8b18915a614615b12345', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4af1f92f7671efc51315e041b71df4d43421e66', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5086c155aa654acbcca2a87a6dc89b656bcda223', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=250ac075eb528bdb06ada9f8286f14581d9870a4', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d499c181a1d373dd210f26236d53ae577b8a1ab', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/9NcA9mLFZooOTSjuYaOQ0V_y880B5LkQro9HNSQFnn8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7c363d5b2a43f9e38123f8267966fe0148c47031', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'AfzM_QLKF3z7opsY74zTP-LhPancxGqB5QjRhBwqUiE'}], 'enabled': False}",,,,,
,datascience,"Hello!

I am personally loving work from home because it saves me a lot of time. But eventually everyone has to go back to the offices so I thought maybe it's a good idea to talk about the problems we face when working from physical office and how can we make the experience better.

Thanks!",t2_bv171ji2,False,,0,False,What is something you're absolutely NOT looking forward to once work from home ends and how do you plan to fix it?,[],r/datascience,False,6,discussion,0,,,False,t3_ndt83g,False,dark,0.94,,public,32,1,{},,,False,[],,False,False,,{},Discussion,False,32,,False,False,self,False,,[],{},,True,,1621214824.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I am personally loving work from home because it saves me a lot of time. But eventually everyone has to go back to the offices so I thought maybe it&amp;#39;s a good idea to talk about the problems we face when working from physical office and how can we make the experience better.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndt83g,True,,quite--average,,49,True,all_ads,False,[],False,,/r/datascience/comments/ndt83g/what_is_something_youre_absolutely_not_looking/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndt83g/what_is_something_youre_absolutely_not_looking/,515404,1621186024.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Why bother mastering SQL when you can simply extract all of the data using a few basic SELECT commands and then do all of the data wrangling in pandas? 

Is there something important I’m missing by relying on pandas for data handling and manipulation?",t2_m7v8y,False,,0,False,SQL vs Pandas,[],r/datascience,False,6,discussion,0,,,False,t3_ndkwgm,False,dark,0.79,,public,92,1,{},,,False,[],,False,False,,{},Discussion,False,92,,False,False,self,False,,[],{},,True,,1621187390.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Why bother mastering SQL when you can simply extract all of the data using a few basic SELECT commands and then do all of the data wrangling in pandas? &lt;/p&gt;

&lt;p&gt;Is there something important I’m missing by relying on pandas for data handling and manipulation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndkwgm,True,,C_BearHill,,96,True,all_ads,False,[],False,,/r/datascience/comments/ndkwgm/sql_vs_pandas/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndkwgm/sql_vs_pandas/,515404,1621158590.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hi!

It's been a year since I finished grad school and it feels like I need a refresher on the concepts. I started reading ISLR and I felt like I need to take notes so that next time I need to refresh on the concepts I can just go through the notes instead of reading the entire book.
The note taking is primarily because of potential future interviews.

Do you guys just do it old school by taking notes in a notebook or do it differently now?
Also, is there anything else I should do in order to prepare notes for interview prep?

Any other advice is welcome.

Thanks!",t2_bv171ji2,False,,0,False,How do you take notes while reading a statistics book?,[],r/datascience,False,6,discussion,0,,,False,t3_ndrch1,False,dark,0.93,,public,24,0,{},,,False,[],,False,False,,{},Discussion,False,24,,False,False,self,False,,[],{},,True,,1621209510.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;It&amp;#39;s been a year since I finished grad school and it feels like I need a refresher on the concepts. I started reading ISLR and I felt like I need to take notes so that next time I need to refresh on the concepts I can just go through the notes instead of reading the entire book.
The note taking is primarily because of potential future interviews.&lt;/p&gt;

&lt;p&gt;Do you guys just do it old school by taking notes in a notebook or do it differently now?
Also, is there anything else I should do in order to prepare notes for interview prep?&lt;/p&gt;

&lt;p&gt;Any other advice is welcome.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndrch1,True,,quite--average,,21,True,all_ads,False,[],False,,/r/datascience/comments/ndrch1/how_do_you_take_notes_while_reading_a_statistics/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndrch1/how_do_you_take_notes_while_reading_a_statistics/,515404,1621180710.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"My team does not do much documentation. They tend to think it's not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it. 

We all tend to work on different models but often we need to understand how other models work and I think it's worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member.

I'm just wondering how other teams think about enforcing documentation. I don't think it's a waste of time and there will most likely be someone taking over someone's codebase eventually and it's painful to try to understand some functions when that person isn't around to ask questions to.",t2_zgml6,False,,0,False,Documenting code in data science...,[],r/datascience,False,6,discussion,0,,,False,t3_nd9io8,False,dark,0.99,,public,224,2,{},,,False,[],,False,False,,{},Discussion,False,224,,False,False,self,False,,[],{'gid_1': 1},,True,,1621144630.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My team does not do much documentation. They tend to think it&amp;#39;s not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it. &lt;/p&gt;

&lt;p&gt;We all tend to work on different models but often we need to understand how other models work and I think it&amp;#39;s worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m just wondering how other teams think about enforcing documentation. I don&amp;#39;t think it&amp;#39;s a waste of time and there will most likely be someone taking over someone&amp;#39;s codebase eventually and it&amp;#39;s painful to try to understand some functions when that person isn&amp;#39;t around to ask questions to.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd9io8,True,,svartgeit,,53,True,all_ads,False,[],False,,/r/datascience/comments/nd9io8/documenting_code_in_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd9io8/documenting_code_in_data_science/,515404,1621115830.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",t2_4l4cxw07,False,,0,False,Weekly Entering &amp; Transitioning Thread | 16 May 2021 - 23 May 2021,[],r/datascience,False,6,,0,,,False,t3_ndmuat,False,dark,0.76,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,False,self,False,,[],{},,True,,1621195230.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;
&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;
&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;
&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;
&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=""https://www.reddit.com/r/datascience/wiki/frequently-asked-questions""&gt;FAQ&lt;/a&gt; and [Resources](Resources) pages on our wiki. You can also search for answers in &lt;a href=""https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new""&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,new,,,False,False,False,False,False,[],[],False,False,False,False,v0.5.1,[],False,,,moderator,t5_2sptq,,,,ndmuat,True,,datascience-bot,,208,False,all_ads,False,[],False,dark,/r/datascience/comments/ndmuat/weekly_entering_transitioning_thread_16_may_2021/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndmuat/weekly_entering_transitioning_thread_16_may_2021/,515404,1621166430.0,0,,False,,,,,,,,
,datascience,,t2_gafbm9o,False,,0,False,What do you dislike most about the data science industry?,[],r/datascience,False,6,discussion,0,,,False,t3_ndhg4h,False,dark,0.89,,public,22,0,{},,,False,[],,False,False,,{},Discussion,False,22,,False,False,default,False,,[],{},,False,,1621172182.0,text,6,,,text,self.TheAnalystEconomy,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndhg4h,True,,Kobedoggg,,27,True,all_ads,False,[],False,,/r/datascience/comments/ndhg4h/what_do_you_dislike_most_about_the_data_science/,all_ads,False,/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/,515404,1621143382.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'TheAnalystEconomy', 'selftext': '', 'author_fullname': 't2_gafbm9o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you hate most about the legacy data analysis/insights industry?', 'link_flair_richtext': [{'e': 'text', 't': 'General discussion'}], 'subreddit_name_prefixed': 'r/TheAnalystEconomy', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n67zzg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'General discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1620338603.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.TheAnalystEconomy', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dcd9e614-ad4a-11eb-9428-0e37c2f586d7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_4bo841', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#dadada', 'id': 'n67zzg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Kobedoggg', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/', 'subreddit_subscribers': 1661, 'created_utc': 1620309803.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",/r/TheAnalystEconomy/comments/n67zzg/what_do_you_hate_most_about_the_legacy_data/,t3_n67zzg,
,datascience,"Hey everyone, I first posted this on r/analytics but realized it doesn't fit very well there. I need to scrape some elections data from a website. It has JavaScript and around a 1000 individual pages that all have the same format and variables, stored in a table. I'm new to scraping but have read a bit today and it seems like Python is my best bet. I was wondering if this is the type of thing I should use a full crawler on, like Scrapy. The URLs for the pages i need all have this format:

https://elections.amo.on.ca/web/en/municipal/XXXXX 

Where XXXXX seems to be an ID code made up of digits for each page.  I don't know which 5 digit codes actually correspond to the pages I need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.

Should I just get started with learning Scrapy in Python, or do you think there is a better tool for this task?",t2_16c56a,False,,0,False,need advice on the best web scraping tool/approach for this job,[],r/datascience,False,6,projects,0,,,False,t3_ndobxv,False,dark,0.6,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1621200464.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I first posted this on &lt;a href=""/r/analytics""&gt;r/analytics&lt;/a&gt; but realized it doesn&amp;#39;t fit very well there. I need to scrape some elections data from a website. It has JavaScript and around a 1000 individual pages that all have the same format and variables, stored in a table. I&amp;#39;m new to scraping but have read a bit today and it seems like Python is my best bet. I was wondering if this is the type of thing I should use a full crawler on, like Scrapy. The URLs for the pages i need all have this format:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://elections.amo.on.ca/web/en/municipal/XXXXX""&gt;https://elections.amo.on.ca/web/en/municipal/XXXXX&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Where XXXXX seems to be an ID code made up of digits for each page.  I don&amp;#39;t know which 5 digit codes actually correspond to the pages I need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.&lt;/p&gt;

&lt;p&gt;Should I just get started with learning Scrapy in Python, or do you think there is a better tool for this task?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndobxv,True,,ReedCube,,14,True,all_ads,False,[],False,,/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/,515404,1621171664.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Hi, i just started to learn about market basket analysis with FP-growth.  


what is the amount of data do i need to create an accurate association rule? is there a minimum? is it pure arbitrary?   


thanks for the help.",t2_4qf1547i,False,,0,False,Market Basket Analysis Help,[],r/datascience,False,6,discussion,0,,,False,t3_nddpj8,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,self,False,,[],{},,True,,1621158154.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, i just started to learn about market basket analysis with FP-growth.  &lt;/p&gt;

&lt;p&gt;what is the amount of data do i need to create an accurate association rule? is there a minimum? is it pure arbitrary?   &lt;/p&gt;

&lt;p&gt;thanks for the help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nddpj8,True,,MorBid23,,5,True,all_ads,False,[],False,,/r/datascience/comments/nddpj8/market_basket_analysis_help/,all_ads,False,https://www.reddit.com/r/datascience/comments/nddpj8/market_basket_analysis_help/,515404,1621129354.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"The idea of creating confidence intervals in regression models is quite straightforward. 

For example :
https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png

But do confidence intervals carry over to classification models?

1) For example, in this picture here (confidence interval for an ROC curve corresponding to a classification model ) https://i.stack.imgur.com/Y7KSNm.png - can the lower limit of the confidence interval from this ROC curve be used to gauge how well this model might generalize to new data?

2) For a classification model, can we make a confidence interval for the prediction of an individual observation? For instance, I wrote some R code (you can directly copy/paste the code) for a particular example where a random forest (i.e. classification model) is used to predict whether if an observation will be ""approved"" or ""rejected"" (see here for the code: https://shrib.com/#Madelyn_NMjYE8) .

Thus, for each observation, the classification model predicts the probability that this observation will be ""approved"" or ""rejected"". Whichever probability is higher, the model selects that outcome for the given observation.

Also, the higher one of these probabilities are, this means the classification model is more confident with its prediction (e.g. for two observations, the ratio of approved:rejected 0.9:0.1 vs 0.6:0.4 . The model is more confident about the first prediction)

Thus, is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions?

Thanks",t2_xtuyc,False,,0,False,Confidence Intervals for Classification Models,[],r/datascience,False,6,discussion,0,,,False,t3_nd81cs,False,dark,0.83,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,False,self,False,,[],{},,True,,1621140233.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The idea of creating confidence intervals in regression models is quite straightforward. &lt;/p&gt;

&lt;p&gt;For example :
&lt;a href=""https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png""&gt;https://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But do confidence intervals carry over to classification models?&lt;/p&gt;

&lt;p&gt;1) For example, in this picture here (confidence interval for an ROC curve corresponding to a classification model ) &lt;a href=""https://i.stack.imgur.com/Y7KSNm.png""&gt;https://i.stack.imgur.com/Y7KSNm.png&lt;/a&gt; - can the lower limit of the confidence interval from this ROC curve be used to gauge how well this model might generalize to new data?&lt;/p&gt;

&lt;p&gt;2) For a classification model, can we make a confidence interval for the prediction of an individual observation? For instance, I wrote some R code (you can directly copy/paste the code) for a particular example where a random forest (i.e. classification model) is used to predict whether if an observation will be &amp;quot;approved&amp;quot; or &amp;quot;rejected&amp;quot; (see here for the code: &lt;a href=""https://shrib.com/#Madelyn_NMjYE8""&gt;https://shrib.com/#Madelyn_NMjYE8&lt;/a&gt;) .&lt;/p&gt;

&lt;p&gt;Thus, for each observation, the classification model predicts the probability that this observation will be &amp;quot;approved&amp;quot; or &amp;quot;rejected&amp;quot;. Whichever probability is higher, the model selects that outcome for the given observation.&lt;/p&gt;

&lt;p&gt;Also, the higher one of these probabilities are, this means the classification model is more confident with its prediction (e.g. for two observations, the ratio of approved:rejected 0.9:0.1 vs 0.6:0.4 . The model is more confident about the first prediction)&lt;/p&gt;

&lt;p&gt;Thus, is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd81cs,True,,ottawalanguages,,1,True,all_ads,False,[],False,,/r/datascience/comments/nd81cs/confidence_intervals_for_classification_models/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd81cs/confidence_intervals_for_classification_models/,515404,1621111433.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?auto=webp&amp;s=8e860f1a1c8a6af4e65dcc625f963436be4e78f8', 'width': 850, 'height': 564}, 'resolutions': [{'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef9391d4cebbf98101bbb4097b78f458b80e813e', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f56792912fb59217da945f2fd0bf48bec989605', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=efefc3710f9d6e8c578ad72f2427a444746cd037', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/0VKq0RI7SiZ_1LRng29nwNLFgoNRQn0YFtmYs0DfU_E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76b95ca1ed0286bd1a4e74a6b5082f46016c54fe', 'width': 640, 'height': 424}], 'variants': {}, 'id': 'oGCJVM-H1RJsGRRY38Dnb6bSYoQ07ajwhpfO3pjwiWs'}], 'enabled': False}",,,,,
,datascience,"I recently moved from IT Governance Audit, in which we have codex like COBIT 5 to follow. I wonder if there is something like that for data pipelining?",t2_17d3z1,False,,0,False,What are some industry standard codex for data pipelining?,[],r/datascience,False,6,education,0,,,False,t3_ndcl23,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Education,False,2,,False,False,self,False,,[],{},,True,,1621154250.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently moved from IT Governance Audit, in which we have codex like COBIT 5 to follow. I wonder if there is something like that for data pipelining?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ndcl23,True,,votekonan,,0,True,all_ads,False,[],False,,/r/datascience/comments/ndcl23/what_are_some_industry_standard_codex_for_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/ndcl23/what_are_some_industry_standard_codex_for_data/,515404,1621125450.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,"Let me first say that I am not a data scientist, or even a college degree holder. I'm just someone who thought of something but has no idea how to do it - which I am going to ask how to do here.

Pretty much I run a Minecraft server, and want to show player activity over the course of the last 6 months that I have had it. Every 50 or so seconds the console log outputs the following line:

\[ {Day of Week}, {Numerical Day}. {Month} {Year} {Hour}:{Minute}:{Second} UTC INFO\] There are {# of players} of a max of {Max # of players} players online: {Player names seperated by ,}

&amp;#x200B;

For example:

\[Fri, 14. May 2021 05:15:34 UTC INFO\] There are 0 of a max of 15 players online:

\[Fri, 14. May 2021 14:51:30 UTC INFO\] There are 1 of a max of 15 players online: Player1

\[Fri, 14. May 2021 03:40:22 UTC INFO\] There are 3 of a max of 15 players online: Player1, Player2, Player3

&amp;#x200B;

Is there a program out there that I can use to look at those lines of text, and convert them into a graph that can show for how many minutes or something of the sort in a single day a player was on for each day over the last 6 months all the while ignore every other line? And, if possible, how I can make the program do that?

Any and all help is appreciated!",t2_4de1e9bv,False,,0,False,Silly Question,[],r/datascience,False,6,discussion,0,,,False,t3_ncfzss,False,dark,0.93,,public,195,1,{},,,False,[],,False,False,,{},Discussion,False,195,,False,False,self,False,,[],{},,True,,1621047110.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let me first say that I am not a data scientist, or even a college degree holder. I&amp;#39;m just someone who thought of something but has no idea how to do it - which I am going to ask how to do here.&lt;/p&gt;

&lt;p&gt;Pretty much I run a Minecraft server, and want to show player activity over the course of the last 6 months that I have had it. Every 50 or so seconds the console log outputs the following line:&lt;/p&gt;

&lt;p&gt;[ {Day of Week}, {Numerical Day}. {Month} {Year} {Hour}:{Minute}:{Second} UTC INFO] There are {# of players} of a max of {Max # of players} players online: {Player names seperated by ,}&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 05:15:34 UTC INFO] There are 0 of a max of 15 players online:&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 14:51:30 UTC INFO] There are 1 of a max of 15 players online: Player1&lt;/p&gt;

&lt;p&gt;[Fri, 14. May 2021 03:40:22 UTC INFO] There are 3 of a max of 15 players online: Player1, Player2, Player3&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there a program out there that I can use to look at those lines of text, and convert them into a graph that can show for how many minutes or something of the sort in a single day a player was on for each day over the last 6 months all the while ignore every other line? And, if possible, how I can make the program do that?&lt;/p&gt;

&lt;p&gt;Any and all help is appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncfzss,True,,_coop007,,68,True,all_ads,False,[],False,,/r/datascience/comments/ncfzss/silly_question/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncfzss/silly_question/,515404,1621018310.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"Hello,

I finally came to realize that I use almost every libraries as pure blackboxes and that it is a problem (I thought before it was not as soon as you can give the management a correct result) if I want to improve myself professionally speaking.

I have zero background in mathematics so learning through a university syllabus is very very hard and I don't think I'll be able to finish that. Would you have any resources that is a bit interactive and noob-friendly ? I enjoyed learning Python and R through small own made project (that's how I ended with my current data clerk position) but I don't see how I could do that for stats.

Thanks,",t2_k15r8mh,False,,0,False,Any good resources for learning statistics applied to datascience from scratch ?,[],r/datascience,False,6,education,0,,,False,t3_nd9bi3,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Education,False,1,,False,False,self,False,,[],{},,True,,1621144028.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I finally came to realize that I use almost every libraries as pure blackboxes and that it is a problem (I thought before it was not as soon as you can give the management a correct result) if I want to improve myself professionally speaking.&lt;/p&gt;

&lt;p&gt;I have zero background in mathematics so learning through a university syllabus is very very hard and I don&amp;#39;t think I&amp;#39;ll be able to finish that. Would you have any resources that is a bit interactive and noob-friendly ? I enjoyed learning Python and R through small own made project (that&amp;#39;s how I ended with my current data clerk position) but I don&amp;#39;t see how I could do that for stats.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd9bi3,True,,Dyolekythos,,9,True,all_ads,False,[],False,,/r/datascience/comments/nd9bi3/any_good_resources_for_learning_statistics/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd9bi3/any_good_resources_for_learning_statistics/,515404,1621115228.0,0,,False,99f9652a-d780-11e7-b558-0e52cdd59ace,,,,,,,
,datascience,Best answer becomes a meme :-),t2_8pzf0806,False,,0,False,Tell us you’re a data scientist without telling us you’re a data scientist.,[],r/datascience,False,6,fun,0,,,False,t3_ncqjfj,False,dark,0.68,,public,14,0,{},,,False,[],,False,False,,{},Fun/Trivia,False,14,,False,False,self,False,,[],{},,True,,1621080916.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Best answer becomes a meme :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncqjfj,True,,datasci-live,,71,True,all_ads,False,[],False,,/r/datascience/comments/ncqjfj/tell_us_youre_a_data_scientist_without_telling_us/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncqjfj/tell_us_youre_a_data_scientist_without_telling_us/,515404,1621052116.0,0,,False,b026063c-d780-11e7-aba9-0e030591a4b4,,,,,,,
,datascience,,t2_3xeoyrv0,False,,0,False,In your experience what were the questions the interviewer asked that made you realise you shouldn't work at this company (or under the interviewer)? (Or do you have any questions to ask that help you decide whether the company/interviewer is good),[],r/datascience,False,6,,0,,,False,t3_ncfk0c,False,dark,0.92,,public,69,0,{},,,False,[],,False,False,,{},Job Search,False,69,,False,False,self,False,,[],{},,True,,1621045946.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,ncfk0c,True,,m_o_n_t_e,,80,True,all_ads,False,[],False,,/r/datascience/comments/ncfk0c/in_your_experience_what_were_the_questions_the/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncfk0c/in_your_experience_what_were_the_questions_the/,515404,1621017146.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Hi All,

Are there any data scientists here working in manufacturing (food and beverage ideally)?

I'm part of a new startup data science teams and we're looking for good use cases

Thanks all from Dublin, Ireland",t2_1vl22t3k,False,,0,False,Data Science Projects in Manufacturing,[],r/datascience,False,6,projects,0,,,False,t3_nckuwn,False,dark,0.87,,public,28,0,{},,,False,[],,False,False,,{},Projects,False,28,,False,False,self,False,,[],{},,True,,1621060493.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;Are there any data scientists here working in manufacturing (food and beverage ideally)?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m part of a new startup data science teams and we&amp;#39;re looking for good use cases&lt;/p&gt;

&lt;p&gt;Thanks all from Dublin, Ireland&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nckuwn,True,,EoinJFleming,,10,True,all_ads,False,[],False,,/r/datascience/comments/nckuwn/data_science_projects_in_manufacturing/,all_ads,False,https://www.reddit.com/r/datascience/comments/nckuwn/data_science_projects_in_manufacturing/,515404,1621031693.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Let's say I've built a model using past bank data that predicts which customers will apply for an auto loan with us.  I now have a potential email list with our current customer data that I want to check which specific members who are most likely to apply for a loan and a predicted agg count of how many will apply for a loan, what would be the best way to do so? Can I have it read a CSV file or connect to a SQL table to do this?  I've read a little bit about FLASK, but I haven't seen anything like I'm wanting done in flask. Not looking for a step-by-step or for someone to do the work for me, more of a ""yes/no this can be done"" and maybe a link that might assist me. Thank you",t2_38c47u6w,False,,0,False,Best Way to Deploy a Model into Production,[],r/datascience,False,6,projects,0,,,False,t3_ncgqh7,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},Projects,False,3,,False,False,self,False,,[],{},,True,,1621049057.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I&amp;#39;ve built a model using past bank data that predicts which customers will apply for an auto loan with us.  I now have a potential email list with our current customer data that I want to check which specific members who are most likely to apply for a loan and a predicted agg count of how many will apply for a loan, what would be the best way to do so? Can I have it read a CSV file or connect to a SQL table to do this?  I&amp;#39;ve read a little bit about FLASK, but I haven&amp;#39;t seen anything like I&amp;#39;m wanting done in flask. Not looking for a step-by-step or for someone to do the work for me, more of a &amp;quot;yes/no this can be done&amp;quot; and maybe a link that might assist me. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncgqh7,True,,beauconstrictor,,5,True,all_ads,False,[],False,,/r/datascience/comments/ncgqh7/best_way_to_deploy_a_model_into_production/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncgqh7/best_way_to_deploy_a_model_into_production/,515404,1621020257.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,,t2_4bix301o,False,,0,False,What are some examples of data science application in Israel Iron Dome technology?,[],r/datascience,False,6,discussion,0,,,False,t3_nd3na2,False,dark,0.21,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,False,self,False,,[],{},,True,,1621127582.0,text,6,,,text,self.datascience,False,,,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nd3na2,True,,levenshteinn,,7,True,all_ads,False,[],False,,/r/datascience/comments/nd3na2/what_are_some_examples_of_data_science/,all_ads,False,https://www.reddit.com/r/datascience/comments/nd3na2/what_are_some_examples_of_data_science/,515404,1621098782.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience,"\-Also referring to - [https://www.reddit.com/r/datascience/comments/h8bz3b/anyone\_working\_in\_conservation\_wildlife/](https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/)

Anyone working in nature conservation (wildlife, deforestation, ..) AND also working on the field would like to elaborate how much is beneficial to have a background in data-science ?   


Do you use DS frequently, or manage team making use of DS ?  
How much is valuable for engaging in outdoor activities and negotiate opportunities to go on the field for part of the job ?  
What kind of activities do you do ?

\-   

Possibly, I'd like to hear about opportunities of fieldwork in the EU - and possibly, if only volunteering pops up, at least volunteering activities offering full cover of costs and relocation.  


My situation is that I'd like to gain a relevant background for PhD making use of tech (e.g. ML) and in perspective aim for jobs where I can live outdoor part of the time. In order to apply for a PhD, I need to get a MSc first (at least in the EU), and due to my background it may be easier to get a MSc in a tech domain rather than a knowledge-related domain (e.g. biology / natural science / ethology, .., that would require prior related academic background in these disciplines).  


I'd like to hear from other experiences to think about appropriate expectations and tips to consider - in perspective, I don't want to stay 130% of my time in front of a computer, while I find very motivating the possibility to match direct observations and outdoor activities with the part for processing.   


In perspectives, I think tech will open up for new opportunities also in this field, and again I wonder how a tech background would be considered and leveraged VS knowledge domain backgrounds or direct on-field experience.",t2_698l9,False,,0,False,On-the field jobs with DataScience - wildlife / nature conservation ?,[],r/datascience,False,6,career,0,,,False,t3_nc36cc,False,dark,0.85,,public,14,0,{},,,False,[],,False,False,,{},Career,False,14,,False,False,self,False,,[],{},,True,,1621007688.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;-Also referring to - &lt;a href=""https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/""&gt;https://www.reddit.com/r/datascience/comments/h8bz3b/anyone_working_in_conservation_wildlife/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Anyone working in nature conservation (wildlife, deforestation, ..) AND also working on the field would like to elaborate how much is beneficial to have a background in data-science ?   &lt;/p&gt;

&lt;p&gt;Do you use DS frequently, or manage team making use of DS ?&lt;br/&gt;
How much is valuable for engaging in outdoor activities and negotiate opportunities to go on the field for part of the job ?&lt;br/&gt;
What kind of activities do you do ?&lt;/p&gt;

&lt;p&gt;-   &lt;/p&gt;

&lt;p&gt;Possibly, I&amp;#39;d like to hear about opportunities of fieldwork in the EU - and possibly, if only volunteering pops up, at least volunteering activities offering full cover of costs and relocation.  &lt;/p&gt;

&lt;p&gt;My situation is that I&amp;#39;d like to gain a relevant background for PhD making use of tech (e.g. ML) and in perspective aim for jobs where I can live outdoor part of the time. In order to apply for a PhD, I need to get a MSc first (at least in the EU), and due to my background it may be easier to get a MSc in a tech domain rather than a knowledge-related domain (e.g. biology / natural science / ethology, .., that would require prior related academic background in these disciplines).  &lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to hear from other experiences to think about appropriate expectations and tips to consider - in perspective, I don&amp;#39;t want to stay 130% of my time in front of a computer, while I find very motivating the possibility to match direct observations and outdoor activities with the part for processing.   &lt;/p&gt;

&lt;p&gt;In perspectives, I think tech will open up for new opportunities also in this field, and again I wonder how a tech background would be considered and leveraged VS knowledge domain backgrounds or direct on-field experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nc36cc,True,,gg4u,,10,True,all_ads,False,[],False,,/r/datascience/comments/nc36cc/onthe_field_jobs_with_datascience_wildlife_nature/,all_ads,False,https://www.reddit.com/r/datascience/comments/nc36cc/onthe_field_jobs_with_datascience_wildlife_nature/,515404,1620978888.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"I know that we are all working remotely to an extent now. But, does anyone here have experience with working on a team which is fully remote?

I am in talk with a recruiter for an exciting position in a fully remote company (well funded startup and recruiter promises a good work-life balance). I have had colleagues in the same location in all the places I worked before- I very much enjoyed the social aspect of office. So being in a fully remote team is something new for me and I am being a bit cautious.",t2_awahof9r,False,,0,False,Anyone has experience on working with ‘Fully Remote Team’ ?,[],r/datascience,False,6,career,0,,,False,t3_nbo4ik,False,dark,0.96,,public,112,1,{},,,False,[],,False,False,,{},Career,False,112,,False,False,self,False,,[],{},,True,,1620960585.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that we are all working remotely to an extent now. But, does anyone here have experience with working on a team which is fully remote?&lt;/p&gt;

&lt;p&gt;I am in talk with a recruiter for an exciting position in a fully remote company (well funded startup and recruiter promises a good work-life balance). I have had colleagues in the same location in all the places I worked before- I very much enjoyed the social aspect of office. So being in a fully remote team is something new for me and I am being a bit cautious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbo4ik,True,,st_pallella,,69,True,all_ads,False,[],False,,/r/datascience/comments/nbo4ik/anyone_has_experience_on_working_with_fully/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbo4ik/anyone_has_experience_on_working_with_fully/,515404,1620931785.0,0,,False,a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e,,,,,,,
,datascience,"A client produces 6-12 spreadsheets each quarter.

They have an excel guru put together a big document that visualizes the data. The document contains things like bar charts and descriptive statistics.

I would like to prepare a dashboard application  that is hosted online or could be shared as a standalone application.

The application should be able to accept the data sets as input (ideally, with a drag-and-drop graphical interface), COMBINE the data sets behind the scenes, and produce the necessary data viz.

Goal is to automate the data viz process, and the mechanics should be straightforward since the data sets have the same structure from year to year.

CHALLENGES:

• The data sets are company sensitive. I cannot host a web application on any old web server. I need some contractual guarantee the data isn’t being spied on. My understanding is that most companies have freedom to intercept online info. Maybe then I don’t host the application online at all? An .html file could work.

• Client has restrictions on what kind of software they can install. I may be able to install PowerBI or Tableau on my system, but client may not be able.

I am aware that Tableau can easily visualize data from multiple sources. I.e. it would be simple for ME to organize the spreadsheets, visualize key metrics, and host it online. What I would like is a freestanding application where the CLIENT drags and drops the spreadsheets and an application spits out some visuals.

Thoughts?

Is this too advanced for PowerBI/Tableau? Do I need Shiny/Dash?",t2_8a6ts,False,,0,False,"Can dashboard software (Tableau, PowerBI) help with this business case?",[],r/datascience,False,6,tooling,0,,,False,t3_ncbvv2,False,dark,0.38,,public,0,0,{},,,False,[],,False,False,,{},Tooling,False,0,,False,False,self,False,,[],{},,True,,1621036417.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A client produces 6-12 spreadsheets each quarter.&lt;/p&gt;

&lt;p&gt;They have an excel guru put together a big document that visualizes the data. The document contains things like bar charts and descriptive statistics.&lt;/p&gt;

&lt;p&gt;I would like to prepare a dashboard application  that is hosted online or could be shared as a standalone application.&lt;/p&gt;

&lt;p&gt;The application should be able to accept the data sets as input (ideally, with a drag-and-drop graphical interface), COMBINE the data sets behind the scenes, and produce the necessary data viz.&lt;/p&gt;

&lt;p&gt;Goal is to automate the data viz process, and the mechanics should be straightforward since the data sets have the same structure from year to year.&lt;/p&gt;

&lt;p&gt;CHALLENGES:&lt;/p&gt;

&lt;p&gt;• The data sets are company sensitive. I cannot host a web application on any old web server. I need some contractual guarantee the data isn’t being spied on. My understanding is that most companies have freedom to intercept online info. Maybe then I don’t host the application online at all? An .html file could work.&lt;/p&gt;

&lt;p&gt;• Client has restrictions on what kind of software they can install. I may be able to install PowerBI or Tableau on my system, but client may not be able.&lt;/p&gt;

&lt;p&gt;I am aware that Tableau can easily visualize data from multiple sources. I.e. it would be simple for ME to organize the spreadsheets, visualize key metrics, and host it online. What I would like is a freestanding application where the CLIENT drags and drops the spreadsheets and an application spits out some visuals.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;

&lt;p&gt;Is this too advanced for PowerBI/Tableau? Do I need Shiny/Dash?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,ncbvv2,True,,BrisklyBrusque,,16,True,all_ads,False,[],False,,/r/datascience/comments/ncbvv2/can_dashboard_software_tableau_powerbi_help_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/ncbvv2/can_dashboard_software_tableau_powerbi_help_with/,515404,1621007617.0,0,,False,aaf5d8cc-d780-11e7-a4a5-0e68d01eab56,,,,,,,
,datascience,,t2_1umdosna,False,,0,False,How does differencing leads to stationarity in time series ?,[],r/datascience,False,6,discussion,0,,,False,t3_nc6jwu,False,dark,0.64,,public,3,0,{},,,False,[],,False,False,,{},Discussion,False,3,,False,False,default,False,,[],{},,False,,1621021190.0,text,6,,,text,self.statistics,False,,,confidence,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nc6jwu,True,,venkarafa,,6,False,all_ads,False,[],False,,/r/datascience/comments/nc6jwu/how_does_differencing_leads_to_stationarity_in/,all_ads,False,/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/,515404,1620992390.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,"[{'approved_at_utc': None, 'subreddit': 'statistics', 'selftext': 'When we talk about Stationarity of a Time Series, the properties we attribute  are constant mean, constant variance, constant autocorrelation structure and no seasonality. \n\nI came across several articles and books which says differencing makes a series stationary.\n\nThe illustration normally begins with taking an AR(1) model as example\n\n  Yt = ɸYt-1 + ɛt   \n\nAlso it is assumed that the above equation has a unit root i.e., |ɸ| = 1,  \n\nThen they difference the series and represent it as below\n\n   **Δ**Yt = Yt - Yt-1 = ɛt \n\nThey then claim that the error (ɛt) in AR process is White noise. \n\nWhite noise by definition has constant mean, finite and Constant variance and no correlation structure.\n\nThey then go on to take expected value of the error term and say  E(ɛt) = 0, \n\nThus implying that mean is constant.\n\nOk so far we have ticked one of the check boxes to prove it is stationary. The the other check box i.e. to prove constant variance remains unchecked.\n\nThe next part is what is very unclear and often left unproved\n\nThey say the variance of the error term is  Var(ɛt) = σ2 \n\nThen without proving that variance is constant they say differencing has lead to constant mean and variance.\n\nMy question is how can one prove that differencing leads to constant variance ? would be grateful for any explanation.', 'author_fullname': 't2_1umdosna', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Question] How differencing a time series leads to stationarity ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/statistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nc3co8', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1621008435.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.statistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When we talk about Stationarity of a Time Series, the properties we attribute  are constant mean, constant variance, constant autocorrelation structure and no seasonality. &lt;/p&gt;\n\n&lt;p&gt;I came across several articles and books which says differencing makes a series stationary.&lt;/p&gt;\n\n&lt;p&gt;The illustration normally begins with taking an AR(1) model as example&lt;/p&gt;\n\n&lt;p&gt;Yt = ɸYt-1 + ɛt   &lt;/p&gt;\n\n&lt;p&gt;Also it is assumed that the above equation has a unit root i.e., |ɸ| = 1,  &lt;/p&gt;\n\n&lt;p&gt;Then they difference the series and represent it as below&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Δ&lt;/strong&gt;Yt = Yt - Yt-1 = ɛt &lt;/p&gt;\n\n&lt;p&gt;They then claim that the error (ɛt) in AR process is White noise. &lt;/p&gt;\n\n&lt;p&gt;White noise by definition has constant mean, finite and Constant variance and no correlation structure.&lt;/p&gt;\n\n&lt;p&gt;They then go on to take expected value of the error term and say  E(ɛt) = 0, &lt;/p&gt;\n\n&lt;p&gt;Thus implying that mean is constant.&lt;/p&gt;\n\n&lt;p&gt;Ok so far we have ticked one of the check boxes to prove it is stationary. The the other check box i.e. to prove constant variance remains unchecked.&lt;/p&gt;\n\n&lt;p&gt;The next part is what is very unclear and often left unproved&lt;/p&gt;\n\n&lt;p&gt;They say the variance of the error term is  Var(ɛt) = σ2 &lt;/p&gt;\n\n&lt;p&gt;Then without proving that variance is constant they say differencing has lead to constant mean and variance.&lt;/p&gt;\n\n&lt;p&gt;My question is how can one prove that differencing leads to constant variance ? would be grateful for any explanation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nc3co8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'venkarafa', 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/', 'subreddit_subscribers': 310860, 'created_utc': 1620979635.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",/r/statistics/comments/nc3co8/question_how_differencing_a_time_series_leads_to/,t3_nc3co8,
,datascience,"I have an interview for a Data Analyst position coming up soon and have several questions.   


In the job responsibilities, it mentioned typical responsibilities for a Data Analyst except for 'Predictive Analytics'. Isn't this a Data Scientist's responsibility? Since it would require knowledge a typical Data Analyst would not know. Would it be fair to mention in the interview? The salary range was 60-70k. Assuming I can convince them it's a data scientist position and switch the title, would I be able to negotiate a salary above the range? 

&amp;#x200B;

Thanks in advance!",t2_5t13ti,False,,0,False,Data Scientist responsibilities in a Data Analyst job description?,[],r/datascience,False,6,,0,,,False,t3_nbneik,False,dark,0.86,,public,16,0,{},,,False,[],,False,False,,{},Job Search,False,16,,False,False,self,False,,[],{},,True,,1620958801.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an interview for a Data Analyst position coming up soon and have several questions.   &lt;/p&gt;

&lt;p&gt;In the job responsibilities, it mentioned typical responsibilities for a Data Analyst except for &amp;#39;Predictive Analytics&amp;#39;. Isn&amp;#39;t this a Data Scientist&amp;#39;s responsibility? Since it would require knowledge a typical Data Analyst would not know. Would it be fair to mention in the interview? The salary range was 60-70k. Assuming I can convince them it&amp;#39;s a data scientist position and switch the title, would I be able to negotiate a salary above the range? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,#edeff1,nbneik,True,,HongFu_Magic,,22,True,all_ads,False,[],False,,/r/datascience/comments/nbneik/data_scientist_responsibilities_in_a_data_analyst/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbneik/data_scientist_responsibilities_in_a_data_analyst/,515404,1620930001.0,0,,False,71803d7a-469d-11e9-890b-0e5d959976c8,,,,,,,
,datascience,"Many of my Data Science Candidates and Coaching Client's face Imposter syndrome, I compiled some resources on what is Imposter Syndrome, How to recognize and combat it. [Here is a link to the full article with YouTube videos.](https://www.rexrecruiting.com/staffing-recruitment-blogs/imposter-syndrome-what-is-imposter-syndrome-what-can-you-do-about-imposter-syndrome/)

# IMPOSTER SYNDROME

&gt;“It seems like whenever I have a problem and I go to StackExchange, I almost always get a response like  
&gt;  
&gt;“Well obviously you have to pass your indexed features into a Regix 3D optimizer before regressing every i-th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met.”  
&gt;  
&gt;It’s like **where are these guys learning this stuff?” -** [Link](https://www.reddit.com/r/datascience/comments/cnvc3e/does_anyone_else_get_intimidated_by_how_much_you/)

## CHARACTERISTICS OF IMPOSTER SYNDROME

Some of the common signs of imposter syndrome include ([reference](https://so06.tci-thaijo.org/index.php/IJBS/article/view/521/pdf)):

* Self-doubt
* An inability to realistically assess your competence and skills
* Attributing your success to external factors
* Berating your performance
* Fear that you won’t live up to expectations
* Overachieving
* Sabotaging your own success
* Setting incredibly challenging goals and feeling disappointed when you fall short

## WHAT IS IMPOSTER SYNDROME?

[YouTube Video - The Imposter Syndrome](https://youtu.be/eqhUHyVpAwE)

Imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud. It disproportionately affects high-achieving people, who find it difficult to accept their accomplishments. Many Data Scientists question whether they are deserving of accolades, their job, recognition, or the like.

* You do not have enough time to learn something you want to learn.
* You look around and see that there are other people that know that thing you don’t have time to learn.
* You feel incompetent.

Why do so many Data Scientists have it?

Data Science is an extremely broad field of study. There are core competencies required to have a successful career in data science, but there is also a lot of industry specific and technical knowledge that is ever changing.  
Data Science is a career which has many job options, all of which require a high level of expertise and knowledge. If the broad, seemingly confused data science job postings show us anything, it is that many companies do not really understand what a data scientist is, how they compare to a data engineer or software engineer, and how to train or support them within an organization. To add to this, the labor market for data scientists in predominantly new graduated or early career professionals.

When challenge is high, and expectations are unknown it encourages people to fall into high arousal, anxiety, and worry. You can see this from psychologist’s [Mihaly Csikszentmihalyi](https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi) flow model.

These feelings are compounded by a lack of support, feedback, and mentorship provided within a company. This is not generally intentional but a product of small data science departments, business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes.

## HOW CAN DATA SCIENTISTS DEAL WITH IMPOSTER SYNDROME?

[According to the American Psychology Association](https://www.apa.org/gradpsych/2013/11/fraud), If you recognize yourself in the description of the impostor phenomenon, take heart. There are ways to overcome the belief that you don’t measure up.

In a nutshell, there are three ideas that you need to get in your head in order to get over imposter syndrome:

* You are a generally competent person.
* There are always going to be people that know more about a certain area of data science than you and that’s ok and expected. Even more importantly: you’re not the smartest person in the planet, so if you look hard enough, you’re going to find people that are better than you at everything you do and that’s ok.
* You have a finite amount of time to learn things, and your goal shouldn’t be to learn the most, but to learn the things that maximize your specific goals – generally, this is going to be career advancement, but for some it may be something else.

When the Imposter Syndrome feeling comes up:

1. Remind yourself that you are a competent person – if you weren’t, you wouldn’t have gotten to the position you are in right now, whether that’s graduating from college or leading a data science team (yes, even DS team leaders catch the ‘drome from time to time).
2. Remind yourself that when you look for people who know more than you about a specific area, you are guaranteed to find them – that’s just how it works. People choose to specialize in certain areas, and if you only focus on that area of expertise, you are going to feel inadequate. But even more importantly, recognize that if you run into someone who is better than you at literally everything you do, that doesn’t diminish your value – it just means you have run into someone that is pretty special\*
3. Get back to prioritizing what to learn. Do you *need* to learn that or do you just *want* to learn it to feel better about yourself? If the latter, learn to let it go, and focus on the things you need to learn – and save the things you want to learn for when you have the time, which will come.

[u/dfphd – PhD | Head of Data Science &amp; Ecommerce](https://www.reddit.com/r/datascience/comments/m71ijk/imposter_syndrome_and_prioritizing_what_to_learn/)  


[Youtube - What is Imposter Syndrome and How can you  combat it?](https://youtu.be/ZQUxL4Jm1Lo)

### TALK TO YOUR MENTORS.

“The thing that made so much of a difference was supportive, encouraging supervision”.

Many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational. Though many will often struggle with these feelings, you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals.

### RECOGNIZE YOUR EXPERTISE.

Don’t just look to those who are more experienced, more popular, or more successful for help. Tutoring or working with younger students, for instance, can help you realize how far you’ve come and how much knowledge you have to impart. This can be a great way for a Data Scientist to give back to the industry as well as set a more realistic benchmark of your perceived value.

### REMEMBER WHAT YOU DO WELL.

Psychologists Suzanne Imes, PhD, and Pauline Rose Clance, PhD, in the 1970s, impostor phenomenon occurs among high achievers who are unable to internalize and accept their success.

Imes encourages her clients to make a realistic assessment of their abilities. “Most high achievers are pretty smart people, and many really smart people wish they were geniuses. But most of us aren’t,” she says. “We have areas where we’re quite smart and areas where we’re not so smart.” She suggests writing down the things you’re truly good at, and the areas that might need work. That can help you recognize where you’re doing well, and where there’s legitimate room for improvement.

## REALIZE NO ONE IS PERFECT.

Clance urges people with impostor feelings to stop focusing on perfection. “Do a task ‘well enough,'” she says. It’s also important to take time to appreciate the fruits of your hard work. “Develop and implement rewards for success — learn to celebrate,” she adds.

### CHANGE YOUR THINKING.

&gt;“let the challenge excite you rather than overwhelm you.”

People with impostor feelings must reframe the way they think about their achievements, says Imes. She helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle. That has best done incrementally, she says. For instance, rather than spending 10 hours on an assignment, you might cut yourself off at eight. Or you may let a friend read a draft that you haven’t yet perfectly polished. “Superstitions need to be changed very gradually because they are so strong,” she says.

Avoid all or nothing thinking. Just like a standard distribution, most Data Scientists fall within the center. If you find yourself comparing to outliers, then you are going to continue to feel like a fraud, which will in return stifle your career in data science.  


[YouTube - How you can use imposter syndrome to your benefit - Mike Cannon-Brookes](https://www.youtube.com/watch?v=ZkwqZfvbdFw&amp;ab_channel=TED)

### TALK TO SOMEONE WHO CAN HELP.

For many people with impostor feelings, individual therapy can be extremely helpful. A psychologist or other therapist can give you tools to help you break the cycle of impostor thinking.

The impostor phenomenon is still an experience that tends to fly under the radar. Often the people affected by impostor feelings don’t realize they could be living some other way. They don’t have any idea it’s possible not to feel so anxious and fearful all the time.",t2_16sq47,False,,0,False,In the spirit of Mental Health Month - Imposter Syndrome,[],r/datascience,False,6,discussion,0,,,False,t3_nauc4w,False,dark,0.98,,public,446,10,{},,,False,[],,False,False,,{},Discussion,False,446,,False,False,self,False,,[],{'gid_1': 3},,True,,1620869373.0,text,6,,,text,self.datascience,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of my Data Science Candidates and Coaching Client&amp;#39;s face Imposter syndrome, I compiled some resources on what is Imposter Syndrome, How to recognize and combat it. &lt;a href=""https://www.rexrecruiting.com/staffing-recruitment-blogs/imposter-syndrome-what-is-imposter-syndrome-what-can-you-do-about-imposter-syndrome/""&gt;Here is a link to the full article with YouTube videos.&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;IMPOSTER SYNDROME&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“It seems like whenever I have a problem and I go to StackExchange, I almost always get a response like  &lt;/p&gt;

&lt;p&gt;“Well obviously you have to pass your indexed features into a Regix 3D optimizer before regressing every i-th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met.”  &lt;/p&gt;

&lt;p&gt;It’s like &lt;strong&gt;where are these guys learning this stuff?” -&lt;/strong&gt; &lt;a href=""https://www.reddit.com/r/datascience/comments/cnvc3e/does_anyone_else_get_intimidated_by_how_much_you/""&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;CHARACTERISTICS OF IMPOSTER SYNDROME&lt;/h2&gt;

&lt;p&gt;Some of the common signs of imposter syndrome include (&lt;a href=""https://so06.tci-thaijo.org/index.php/IJBS/article/view/521/pdf""&gt;reference&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Self-doubt&lt;/li&gt;
&lt;li&gt;An inability to realistically assess your competence and skills&lt;/li&gt;
&lt;li&gt;Attributing your success to external factors&lt;/li&gt;
&lt;li&gt;Berating your performance&lt;/li&gt;
&lt;li&gt;Fear that you won’t live up to expectations&lt;/li&gt;
&lt;li&gt;Overachieving&lt;/li&gt;
&lt;li&gt;Sabotaging your own success&lt;/li&gt;
&lt;li&gt;Setting incredibly challenging goals and feeling disappointed when you fall short&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;WHAT IS IMPOSTER SYNDROME?&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/eqhUHyVpAwE""&gt;YouTube Video - The Imposter Syndrome&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud. It disproportionately affects high-achieving people, who find it difficult to accept their accomplishments. Many Data Scientists question whether they are deserving of accolades, their job, recognition, or the like.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You do not have enough time to learn something you want to learn.&lt;/li&gt;
&lt;li&gt;You look around and see that there are other people that know that thing you don’t have time to learn.&lt;/li&gt;
&lt;li&gt;You feel incompetent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why do so many Data Scientists have it?&lt;/p&gt;

&lt;p&gt;Data Science is an extremely broad field of study. There are core competencies required to have a successful career in data science, but there is also a lot of industry specific and technical knowledge that is ever changing.&lt;br/&gt;
Data Science is a career which has many job options, all of which require a high level of expertise and knowledge. If the broad, seemingly confused data science job postings show us anything, it is that many companies do not really understand what a data scientist is, how they compare to a data engineer or software engineer, and how to train or support them within an organization. To add to this, the labor market for data scientists in predominantly new graduated or early career professionals.&lt;/p&gt;

&lt;p&gt;When challenge is high, and expectations are unknown it encourages people to fall into high arousal, anxiety, and worry. You can see this from psychologist’s &lt;a href=""https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi""&gt;Mihaly Csikszentmihalyi&lt;/a&gt; flow model.&lt;/p&gt;

&lt;p&gt;These feelings are compounded by a lack of support, feedback, and mentorship provided within a company. This is not generally intentional but a product of small data science departments, business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes.&lt;/p&gt;

&lt;h2&gt;HOW CAN DATA SCIENTISTS DEAL WITH IMPOSTER SYNDROME?&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://www.apa.org/gradpsych/2013/11/fraud""&gt;According to the American Psychology Association&lt;/a&gt;, If you recognize yourself in the description of the impostor phenomenon, take heart. There are ways to overcome the belief that you don’t measure up.&lt;/p&gt;

&lt;p&gt;In a nutshell, there are three ideas that you need to get in your head in order to get over imposter syndrome:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are a generally competent person.&lt;/li&gt;
&lt;li&gt;There are always going to be people that know more about a certain area of data science than you and that’s ok and expected. Even more importantly: you’re not the smartest person in the planet, so if you look hard enough, you’re going to find people that are better than you at everything you do and that’s ok.&lt;/li&gt;
&lt;li&gt;You have a finite amount of time to learn things, and your goal shouldn’t be to learn the most, but to learn the things that maximize your specific goals – generally, this is going to be career advancement, but for some it may be something else.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the Imposter Syndrome feeling comes up:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remind yourself that you are a competent person – if you weren’t, you wouldn’t have gotten to the position you are in right now, whether that’s graduating from college or leading a data science team (yes, even DS team leaders catch the ‘drome from time to time).&lt;/li&gt;
&lt;li&gt;Remind yourself that when you look for people who know more than you about a specific area, you are guaranteed to find them – that’s just how it works. People choose to specialize in certain areas, and if you only focus on that area of expertise, you are going to feel inadequate. But even more importantly, recognize that if you run into someone who is better than you at literally everything you do, that doesn’t diminish your value – it just means you have run into someone that is pretty special*&lt;/li&gt;
&lt;li&gt;Get back to prioritizing what to learn. Do you &lt;em&gt;need&lt;/em&gt; to learn that or do you just &lt;em&gt;want&lt;/em&gt; to learn it to feel better about yourself? If the latter, learn to let it go, and focus on the things you need to learn – and save the things you want to learn for when you have the time, which will come.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/r/datascience/comments/m71ijk/imposter_syndrome_and_prioritizing_what_to_learn/""&gt;u/dfphd – PhD | Head of Data Science &amp;amp; Ecommerce&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/ZQUxL4Jm1Lo""&gt;Youtube - What is Imposter Syndrome and How can you  combat it?&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;TALK TO YOUR MENTORS.&lt;/h3&gt;

&lt;p&gt;“The thing that made so much of a difference was supportive, encouraging supervision”.&lt;/p&gt;

&lt;p&gt;Many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational. Though many will often struggle with these feelings, you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals.&lt;/p&gt;

&lt;h3&gt;RECOGNIZE YOUR EXPERTISE.&lt;/h3&gt;

&lt;p&gt;Don’t just look to those who are more experienced, more popular, or more successful for help. Tutoring or working with younger students, for instance, can help you realize how far you’ve come and how much knowledge you have to impart. This can be a great way for a Data Scientist to give back to the industry as well as set a more realistic benchmark of your perceived value.&lt;/p&gt;

&lt;h3&gt;REMEMBER WHAT YOU DO WELL.&lt;/h3&gt;

&lt;p&gt;Psychologists Suzanne Imes, PhD, and Pauline Rose Clance, PhD, in the 1970s, impostor phenomenon occurs among high achievers who are unable to internalize and accept their success.&lt;/p&gt;

&lt;p&gt;Imes encourages her clients to make a realistic assessment of their abilities. “Most high achievers are pretty smart people, and many really smart people wish they were geniuses. But most of us aren’t,” she says. “We have areas where we’re quite smart and areas where we’re not so smart.” She suggests writing down the things you’re truly good at, and the areas that might need work. That can help you recognize where you’re doing well, and where there’s legitimate room for improvement.&lt;/p&gt;

&lt;h2&gt;REALIZE NO ONE IS PERFECT.&lt;/h2&gt;

&lt;p&gt;Clance urges people with impostor feelings to stop focusing on perfection. “Do a task ‘well enough,&amp;#39;” she says. It’s also important to take time to appreciate the fruits of your hard work. “Develop and implement rewards for success — learn to celebrate,” she adds.&lt;/p&gt;

&lt;h3&gt;CHANGE YOUR THINKING.&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;“let the challenge excite you rather than overwhelm you.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;People with impostor feelings must reframe the way they think about their achievements, says Imes. She helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle. That has best done incrementally, she says. For instance, rather than spending 10 hours on an assignment, you might cut yourself off at eight. Or you may let a friend read a draft that you haven’t yet perfectly polished. “Superstitions need to be changed very gradually because they are so strong,” she says.&lt;/p&gt;

&lt;p&gt;Avoid all or nothing thinking. Just like a standard distribution, most Data Scientists fall within the center. If you find yourself comparing to outliers, then you are going to continue to feel like a fraud, which will in return stifle your career in data science.  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=ZkwqZfvbdFw&amp;amp;ab_channel=TED""&gt;YouTube - How you can use imposter syndrome to your benefit - Mike Cannon-Brookes&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;TALK TO SOMEONE WHO CAN HELP.&lt;/h3&gt;

&lt;p&gt;For many people with impostor feelings, individual therapy can be extremely helpful. A psychologist or other therapist can give you tools to help you break the cycle of impostor thinking.&lt;/p&gt;

&lt;p&gt;The impostor phenomenon is still an experience that tends to fly under the radar. Often the people affected by impostor feelings don’t realize they could be living some other way. They don’t have any idea it’s possible not to feel so anxious and fearful all the time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_beccaae0-d745-44f9-bc5c-3c9f8117699b', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/I_shy_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""No matter how hard I try, I'm too shy to confess my love!"", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'I Shy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=16&amp;height=16&amp;auto=webp&amp;s=ef2899c48784a11826c15c0d93e44adf63f49b39', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=32&amp;height=32&amp;auto=webp&amp;s=d5ac1c72de38a183f491506658c8939116242abc', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=48&amp;height=48&amp;auto=webp&amp;s=2b7a39097b30d524faffd98bcef5df43c9d22011', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=64&amp;height=64&amp;auto=webp&amp;s=02dc06d251a3d96f9ef4c1561c05d9008df2920b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png?width=128&amp;height=128&amp;auto=webp&amp;s=f4591ce312385592dde56dc6077c55cf2393f8fd', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 512, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/x67zkpobicg61_IShy.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}]",[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nauc4w,True,,RexRecruiting,,45,True,all_ads,False,[],False,,/r/datascience/comments/nauc4w/in_the_spirit_of_mental_health_month_imposter/,all_ads,False,https://www.reddit.com/r/datascience/comments/nauc4w/in_the_spirit_of_mental_health_month_imposter/,515404,1620840573.0,2,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,self,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?auto=webp&amp;s=b32e93c31050224cd1bb52e1c8e97daf191dd240', 'width': 720, 'height': 460}, 'resolutions': [{'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74951330ef876d7f8133da781c52e5b501269866', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c197ccc83b6567ba2f97961175055d1838f9776', 'width': 216, 'height': 138}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=221b1b517ab5da9a1910251a94835e82d41209e5', 'width': 320, 'height': 204}, {'url': 'https://external-preview.redd.it/7rlDDTnIHggPOUzhMN2E2we3fhVhJOQvMLta1h3Tnx0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=770d0dd84d645ad7b16c9ddb766feb3e3a975bff', 'width': 640, 'height': 408}], 'variants': {}, 'id': 'WZY2EB3c7gM2-edTy62h9KaszfzuHM0wlYfe11DJ_qs'}], 'enabled': False}",,,,,
,datascience,"I'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I'm going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.",t2_2dwso7l3,False,,0,False,Sentiment Analysis Recommendations on Review data,[],r/datascience,False,6,discussion,0,,,False,t3_nbj29s,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,False,self,False,,[],{},,True,,1620947960.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I&amp;#39;m going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbj29s,True,,Jaypal17,,10,True,all_ads,False,[],False,,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,515404,1620919160.0,1,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
,datascience," I've got a relatively big dataset (8 GB) and pandas crashes when trying to load it into a dataframe. I've tried modin and pyspark, all with no luck. Are there any Python packages that can work with big data? Currently the data is stored in SQL.

I'm running this on a company VM which has 16GB RAM I believe.",t2_8g8aebq1,False,,0,False,Are there any Python packages that can work with big data?,[],r/datascience,False,6,projects,0,,,False,t3_nbjbv6,False,dark,0.56,,public,2,0,{},,,False,[],,False,False,,{},Projects,False,2,,False,False,self,1620937256.0,,[],{},,True,,1620948644.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got a relatively big dataset (8 GB) and pandas crashes when trying to load it into a dataframe. I&amp;#39;ve tried modin and pyspark, all with no luck. Are there any Python packages that can work with big data? Currently the data is stored in SQL.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m running this on a company VM which has 16GB RAM I believe.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbjbv6,True,,karthur4,,27,True,all_ads,False,[],False,,/r/datascience/comments/nbjbv6/are_there_any_python_packages_that_can_work_with/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbjbv6/are_there_any_python_packages_that_can_work_with/,515404,1620919844.0,0,,False,937a6f50-d780-11e7-826d-0ed1beddcc82,,,,,,,
,datascience,"Do you guys have any idea? Sklearn doenst have a built in library to do that

edit: i mean inhomogeneous INTERNAL densities",t2_15fsjo,False,,0,False,How can i create a dataset featuring clusters with inhomogeneous densities with python?,[],r/datascience,False,6,discussion,0,,,False,t3_nbk99e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,False,self,False,,[],{},,True,,1620951046.0,text,6,,,text,self.datascience,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you guys have any idea? Sklearn doenst have a built in library to do that&lt;/p&gt;

&lt;p&gt;edit: i mean inhomogeneous INTERNAL densities&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,confidence,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2sptq,,,,nbk99e,True,,gng3quionbve4,,5,True,all_ads,False,[],False,,/r/datascience/comments/nbk99e/how_can_i_create_a_dataset_featuring_clusters/,all_ads,False,https://www.reddit.com/r/datascience/comments/nbk99e/how_can_i_create_a_dataset_featuring_clusters/,515404,1620922246.0,0,,False,4fad7108-d77d-11e7-b0c6-0ee69f155af2,,,,,,,
